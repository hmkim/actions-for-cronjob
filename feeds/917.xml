<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Junho Cho - Tech Maketh Man Sexy]]></title><description><![CDATA[Tech Maketh Man Sexy: We share our technical knowledge.]]></description><link>http://tmmse.xyz/</link><image><url>http://tmmse.xyz/favicon.png</url><title>Junho Cho - Tech Maketh Man Sexy</title><link>http://tmmse.xyz/</link></image><generator>Ghost 1.22</generator><lastBuildDate>Thu, 27 Sep 2018 05:09:54 GMT</lastBuildDate><atom:link href="http://tmmse.xyz/author/junho/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Normalization layer]]></title><description><![CDATA[<div class="kg-card-markdown"><p>What is Batch Normalization?<br>
Instance Normalization?<br>
Conditional Batch Normalization?<br>
Conditional Instance Normalization?</p>
<p><strong>Batch Normalization</strong> is first introduced by Sergey Ioffe, Christian Szegedy. <strong>It increased image classification performance significantly.</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1502.03167">arxiv</a><br>
<img src="https://dynalist.io/u/bDoKdkIJlMClpWgljrA2Vve_" alt="Pasted image"></li>
</ul>
<p>Interested in &quot;Conditional Batch Normalization (CBN)&quot;, here's wrap up of <code>normalization</code> layers.</p>
<p>refer to &quot;Arbitrary Style Transfer in</p></div>]]></description><link>http://tmmse.xyz/2018/04/12/normalization-layer/</link><guid isPermaLink="false">5acea89f95309e21a0de4127</guid><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Thu, 12 Apr 2018 01:58:46 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1521411515611-f4dd6e7ef4fb?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=f464883a0dacdf170e0fcac2fee9ea7d" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><img src="https://images.unsplash.com/photo-1521411515611-f4dd6e7ef4fb?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=f464883a0dacdf170e0fcac2fee9ea7d" alt="Normalization layer"><p>What is Batch Normalization?<br>
Instance Normalization?<br>
Conditional Batch Normalization?<br>
Conditional Instance Normalization?</p>
<p><strong>Batch Normalization</strong> is first introduced by Sergey Ioffe, Christian Szegedy. <strong>It increased image classification performance significantly.</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1502.03167">arxiv</a><br>
<img src="https://dynalist.io/u/bDoKdkIJlMClpWgljrA2Vve_" alt="Normalization layer"></li>
</ul>
<p>Interested in &quot;Conditional Batch Normalization (CBN)&quot;, here's wrap up of <code>normalization</code> layers.</p>
<p>refer to &quot;Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization&quot;<br>
Well compared in this paper.</p>
<ul>
<li><a href="https://github.com/xunhuang1995/AdaIN-style">Torch</a></li>
<li><a href="https://github.com/naoto0804/pytorch-AdaIN/blob/master/function.py">pytorch</a></li>
<li><a href="https://github.com/leehomyc/Style-Transfer-Adaptive-Instance-Normalization">pytorch</a></li>
<li><a href="https://arxiv.org/pdf/1703.06868.pdf">arxiv</a></li>
</ul>
<h2 id="batchnormalizationbn">Batch Normalization (BN)</h2>
<p>$$\text{BN}(x)=\gamma (\frac{x-\mu(x)}{\sigma(x)})+\beta$$</p>
<p>$\gamma, \beta \in \mathbb{R}^C$ are affine parameters learned from data;<br>
mean ad standard deviation, computed across <strong>batch size and spatial dimensions indenpedently for each feature channel</strong></p>
<p>$\mu_c(x)=\frac{1}{NHW}\sum\limits_{n=1}^N \sum\limits_{h=1}^H \sum\limits_{w=1}^W x_{nchw}$<br>
$\sigma_c(x)=\sqrt{\frac{1}{NHW}\sum\limits_{n=1}^N \sum\limits_{h=1}^H \sum\limits_{w=1}^W (x_{nchw}-\mu_c(x))^2+\epsilon}$</p>
<h2 id="instancenormalizationin">Instance Normalization (IN)</h2>
<p>$$\text{IN}(x)=\gamma (\frac{x-\mu(x)}{\sigma(x)})+\beta$$</p>
<p>$\mu_{nc}(x)=\frac{1}{HW} \sum\limits_{h=1}^H \sum\limits_{w=1}^W x_{nchw}$<br>
$\sigma_{nc}(x)=\sqrt{\frac{1}{HW} \sum\limits_{h=1}^H \sum\limits_{w=1}^W (x_{nchw}-\mu_{nc}(x))^2+\epsilon}$</p>
<p>While <code>BN</code> takes average among channels, <code>IN</code> takes average in each channels. Thus, each channels won't be affected. Image generation is very dependent on channels compared to image classification. <code>IN</code> takes very important parts in image generation. State-of-the-art such as CycleGAN, StarGAN ... uses <code>IN</code> instead of <code>BN</code>.</p>
<blockquote>
<p>In my opinion, <code>BN</code> is good for discrminative job and <code>IN</code> for generative job.</p>
</blockquote>
<p>Here's difference between <code>BN</code> and <code>IN</code><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-06-075151.jpg" alt="Normalization layer"></p>
<h2 id="conditionalbatchnormalizationcbn">Conditional Batch Normalization (CBN)</h2>
<p>First instoduced from <a href="https://arxiv.org/pdf/1707.00683.pdf">Modulating early visual processing by language</a>.<br>
in NIPS2017. <code>CBN</code> is introduced from Auron Courville's lab.</p>
<p><img src="https://dynalist.io/u/oqmDSTTlNweF4AvRkBCXD63a" alt="Normalization layer"></p>
<blockquote>
<p>Also predict delta value of $\mu$ and $\sigma$ on <code>BN</code>. Thus, $\mu$ and $\sigma$ of <code>BN</code> will be conditioned to some other Neural Net (questions, query, ...).</p>
</blockquote>
<h2 id="conditionalinstancenormalizationcin">Conditional Instance Normalization (CIN)</h2>
<p>$$\text{CIN}(x;s)=\gamma^s (\frac{x-\mu(x)}{\sigma(x)})+\beta^s, s \in {1,2,3,...,S}$$<br>
Surprisingly, the network can generate images in completely different styles by using the same convolutional parameters but different affine parameters in IN layers.</p>
<blockquote>
<p><code>CIN</code> is would be good for conditional image generation (sytle transfer for given style. Compute style and use the $\mu$ and $\sigma$ for image generation.)</p>
</blockquote>
<p>Ulyanov et al. [52] attribute the success of <code>IN</code> to its <strong>invariance to the contrast of the content image</strong>. However, <code>IN</code> takes place in the feature space, therefore it should have <strong>more profound impacts than a simple contrast normalization in the pixel space</strong>. Perhaps even more surprising is the fact that the <strong>affine parameters in <code>IN</code> can completely change the style of the output image</strong>.</p>
<h5 id="adaptiveinstancenormalizationadain">Adaptive Instance Normalization (AdaIN)</h5>
<p><code>AdaIN</code> has no learnable affine parameters. Instead, it adaptively computes the affine parameters from the style input:<br>
$$\text{AdaIN}(x;s)=\sigma(y) (\frac{x-\mu(x)}{\sigma(x)})+\mu(y)$$<br>
in which we simply scale the normalized content input with $\sigma(y)$, and shift it with $\mu(y)$.</p>
<blockquote>
<p>In my opinion, we already use the term <code>Conditional Normalization</code> instead of <code>Adative Normalization</code>.</p>
</blockquote>
<ul>
<li><a href="https://github.com/naoto0804/pytorch-AdaIN/blob/master/function.py">codeblock in github1</a></li>
<li><a href="https://github.com/leehomyc/Style-Transfer-Adaptive-Instance-Normalization/blob/master/AdaptiveInstanceNormalization.py">codeblock in github2</a></li>
</ul>
<h1 id="applications">Applications</h1>
<h2 id="filmvisualreasoningwithageneralconditioninglayer">FiLM: Visual Reasoning with a General Conditioning Layer</h2>
<p>by <a href="https://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1">Ethan Perez</a>, <a href="https://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1">Florian Strub</a>, <a href="https://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1">Harm de Vries</a>, <a href="https://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1">Vincent Dumoulin</a>, <a href="https://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a><br>
AAAI 2018. Code available at this http URL . Extends arXiv:1707.03017.</p>
<p>This work outperforms Deepmind's &quot;relation network&quot; VQA task.</p>
<p><a href="https://github.com/ethanjperez/film">github</a></p>
<h2 id="conditionalinstancenormalizationusedfromalearnedrepresentationforartisticstyle">Conditional Instance Normalization used from &quot;A LEARNED REPRESENTATION FOR ARTISTIC STYLE&quot;</h2>
<p>Vincent Dumoulin &amp; Jonathon Shlens &amp; Manjunath Kudlur, <strong>Google Brain</strong><br>
<img src="https://dynalist.io/u/qTfFm0m8pyxW_z4DA-mH6VU9" alt="Normalization layer"></p>
<p>Outputs multiple style trasfered output with a single network.</p>
<h2 id="augmentedcycleganlearningmanytomanymappingsfromunpaireddata">Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data</h2>
<p>by <strong>Amjad Almahairi</strong>, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, <strong>Aaron Courville</strong></p>
<p>Submitted to ICML2018, <a href="https://arxiv.org/abs/1802.10151">arXiv:1802.10151v1</a></p>
<p>Uses <code>CIN</code> for many-to-many mapping.</p>
</div>]]></content:encoded></item><item><title><![CDATA[Google HackFair 돌아보기 - MetaMong]]></title><description><![CDATA[<div class="kg-card-markdown"><p>2015년 석사 입학 후 연말쯤에 소소하게 했던 프로젝트. 예전 인터뷰했던게 생각나서 공유.<br>
<a href="https://developers-kr.googleblog.com/2016/01/google-hackfair-2-metamong.html">인터뷰 link</a></p>
<h3 id="0">0.</h3>
<p><a href="http://googledevkr.blogspot.kr/2015/10/hack.html">Google HackFair</a>는 구글 기술을 이용해서 만든 다양한 결과물들을 전시하고 공유하는 행사입니다. 지난 12월 5일, 프로젝트 참가자분들은 안드로이드, Cardboard, TensorFlow 등 다양한 구글 기술을 사용한 재미있는 프로젝트를 전시하고 풍성한 볼거리와 기술력 그리고 범상치 않은 업력(</p></div>]]></description><link>http://tmmse.xyz/2018/04/12/google-hackfair-interview/</link><guid isPermaLink="false">5ac1fc45f12fab7def40648d</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Thu, 12 Apr 2018 00:29:37 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/20151205_180738.jpg" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><img src="http://tmmse.xyz/content/images/2018/04/20151205_180738.jpg" alt="Google HackFair 돌아보기 - MetaMong"><p>2015년 석사 입학 후 연말쯤에 소소하게 했던 프로젝트. 예전 인터뷰했던게 생각나서 공유.<br>
<a href="https://developers-kr.googleblog.com/2016/01/google-hackfair-2-metamong.html">인터뷰 link</a></p>
<h3 id="0">0.</h3>
<p><a href="http://googledevkr.blogspot.kr/2015/10/hack.html">Google HackFair</a>는 구글 기술을 이용해서 만든 다양한 결과물들을 전시하고 공유하는 행사입니다. 지난 12월 5일, 프로젝트 참가자분들은 안드로이드, Cardboard, TensorFlow 등 다양한 구글 기술을 사용한 재미있는 프로젝트를 전시하고 풍성한 볼거리와 기술력 그리고 범상치 않은 업력(?)을 선보였습니다.<br>
행사에 참가하지 못한 분들도, 개발자들의 열정과 즐거움을 느껴볼 수 있도록, 구글 코리아 개발자 블로그를 통해 HackFair에 참가한 다양한 프로젝트 중 몇 가지를 골라, 프로젝트 참가자분들과의 인터뷰 내용을 소개해 드리려고 합니다. 두 번째로 소개해 드릴 프로젝트는 구글이 최근에 오픈 소스로 공개한 Machine Intelligence 라이브러리 TensorFlow를 활용한 MetaMong팀 입니다.</p>
<h3 id="1">1. 간단한 팀 소개를 부탁합니다.</h3>
<p>저희는 고등학교 친구들(순서대로 전영배, 조준호, 지완규)이 모여서 이번 구글 핵페어에 참가하게 된 메타몽입니다.</p>
<p><img src="https://lh6.googleusercontent.com/bY2WwJw0yddjYdpQiceOBP-1nMykpzkjtCIUm9l7a0omHOIxUwUf5-BSGhtg0i-QkXewKnOxbytGSgQWTij-18WbUepiHLVNpl1XGcM96qSILPWxEZE5NhNi0wG6MSVscI8LQ0S6" alt="Google HackFair 돌아보기 - MetaMong"></p>
<h3 id="2">2. 프로젝트 소개를 부탁합니다.</h3>
<p>사람들은 닮은 얼굴에 관심이 많습니다. 처음 보는 사람에게 연예인 누구를 닮았다고 덕담을 해주기도 하고, 아버지는 자신의 닮은 아들의 얼굴을 보고 행복해하죠. 이런 닮은 얼굴을 기술적으로 해석해서 “이 세상에 나랑 닮은 사람을 찾아주는 메타몽”이란 서비스를 기획하게 되었습니다.</p>
<iframe src="https://player.vimeo.com/video/146582399" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen></iframe>
<p>최근에 급부상하는 딥러닝이란 기계학습으로 인공지능 관점에서 얼굴을 인식하고 가장 닮은 얼굴을 찾아주며, 앱을 통해 사용하기 쉽게 구현하였습니다. 전영배 학생이 메타몽의 안드로이드 앱을 멋있는 머티리얼 디자인으로 구현하였고, 조준호 학생이 서비스 기획 및 얼굴 인식을 하고 닮은 사람을 찾아내 주는 알고리즘 부분을 구현, 지완규 학생이 서비스의 전체적인 부분, 서버와 얼굴 탐지, 한국인 얼굴 크롤링, 메타몽 웹서버 등을 구현했습니다.</p>
<p><img src="https://lh6.googleusercontent.com/jboEa7-OToEMTEdoVSjNlqXThjBWmFXs3JK6YNUaXfhbZ9bpoO6YPKnAuzU-FO_JUcSEHDDrETmIjAyqmfNTCceyJvtO1MEOaLqWY4C6ssRXWXhh_wDQWQQUtXd5GUxYXdr5cUp-" alt="Google HackFair 돌아보기 - MetaMong"><br>
<img src="https://lh4.googleusercontent.com/RffhNn769_Gts-xuCYsQ0nKcSdBG9i8XiteICNDGtJhskR9xUX5GIeG250gAT2Yby4tAik8oXh4HDp3AKiI61cKnToZtN7J1WWMCh9qHYxCqkwVNCBcXrXdfwbOQJZUTWV268Mgn" alt="Google HackFair 돌아보기 - MetaMong"></p>
<h3 id="3hackfair">3. 프로젝트를 시작하게 된 동기와 HackFair에 참가하시게 된 계기가 궁금합니다.</h3>
<p>저희는 각자의 독자적인 전문성을 가지고 힘을 합쳐서 재밌는 서비스를 개발해보고 싶었습니다. 마침 HackFair는 저희의 목적에 딱 맞는 프로젝트여서 재밌게 진행 할 수 있었습니다. 그리고 넥서스5x가 너무나 가지고 싶었습니다. ^^</p>
<h3 id="4">4. 프로젝트 진행 중 기술적으로 가장 어려웠던 부분은 어떤 것이었나요? 그리고 어떻게 해결하셨나요?</h3>
<p>아무래도 기계학습 같은 다소 장벽이 높은 알고리즘을 적용하느라 저희 수준에서 구현 가능한 범위가 무엇인지 아는 것이 가장 어려웠습니다. 조준호 학생이 관련 분야의 석사과정 중이라 얼굴인식 분야의 논문을 참고하여 팀에서 유용하게 사용할 수 있는 딥러닝 알고리즘 및 모델을 구현할 수 있었습니다. 또한, 닮은 얼굴을 찾아내야 하므로 얼굴 데이터셋이 굉장히 많이 필요한데요, 이 부분을 지완규 학생이 웹에서 한국인 얼굴만 크롤링하고 얼굴만 찾아내는 기능을 구현해서 양질의 데이터셋을 모을 수 있었습니다. 머티리얼 디자인의 “머” 자도 모르던 전영배 학생이 고군분투하며 깔끔한 인터페이스로 모든 디바이스에서 버그 없이 앱이 작동할 수 있도록 수고해주었습니다.</p>
<h3 id="5tensorflowtensorflow">5. 구글에서 최근에 발표한, TensorFlow 라이브러리를 활용하셨습니다. TensorFlow에 관해 간단히 설명해주실 수 있을까요?</h3>
<p>TensorFlow는 기계학습을 위한 라이브러리로 얼마전 구글이 오픈소스로 공개해서 많은 관심을 받고 있는 라이브러리입니다. 아직 공개된 지 얼마 안 되어 많은 개발자와 연구자들이 라이브러리 활용과 개선을 위해 열심히 기여하고 있습니다.</p>
<h3 id="6tensorflow">6. 공개된 지 시간이 별로 지나지 않아, 참고할 자료도 많지 않았을 텐데 TensorFlow 라이브러리를 적용하신 특별한 이유가 있나요?</h3>
<p>저희가 Google HackFair을 진행하면서 Google의 기술을 사용해보고 싶었는데 마침 핵페어를 진행할 때 갓 공개가 되어 뜨거운 관심을 받던 TensorFlow를 꼭 써보고 싶었습니다. TensorFlow는 얼굴에서 의미 있는 특징점(feature)을 뽑을 때 사용하였습니다. 즉 얼굴을 인식해주는 부분을 TensorFlow가 해주었습니다.</p>
<h3 id="7tensorflow">7. TensorFlow를 사용하려고 하는 다른 개발자분들과 공유하고 싶은 내용이 있으신가요?</h3>
<p>딥러닝 분야에는 이미 많은 라이브러리가 있는데 Google에서 많은 연구자들을 끌어모으기 위해 TensorFlow를 오픈소스로 공개했습니다. 공식 홈페이지의 Tutorial에서 매우 자세하게 설명되어 있으므로 참고하시면 큰 도움이 될 것 같습니다. 또한 많은 개발자/연구자들의 관심사이기 때문에 GitHub에서 활발하게 논의되고 있습니다. 설치도 다른 라이브러리에 비해 굉장히 쉽습니다. 특히 TensorBoard를 이용해 원하는 기능을 구현할 때, 데이터 학습을 쉽게 모니터링 할 수 있습니다. 단일 GPU 머신 등에서는 다른 라이브러리보다 조금 느리다고 하지만 알고리즘 구현이 다른 라이브러리와 비교해 쉽고 구글같은 분산 컴퓨팅 환경에서도 좋은 성능으로 쓸 수 있다는 장점이 있습니다. 또한 딥러닝에는 많은 변형 알고리즘들이 생기고 있는데 TensorFlow는 프레임워크가 굉장히 유연해서 널리 사용될 것으로 기대됩니다.</p>
<h3 id="8">8. 향후 프로젝트를 발전시킬 추가적인 계획이 있으신가요?</h3>
<p>사람들은 서로 닮은 사람을 찾고 SNS에서 공유를 많이 합니다. 어쩌면 저희 서비스가 닮은 사람들을 이어주는 SNS가 될 수 있다고 생각합니다.</p>
<h3 id="9hackfair">9. 마지막으로 HackFair 행사에 관한 소감 한마디 부탁합니다.</h3>
<p>메타몽은 졸업을 앞둔 대학생(지완규)과 석사 과정 중인 대학원생(조준호, 전영배)들이 각자의 개인 시간을 할애하며 학업과 병행한 프로젝트였습니다. 이번 프로젝트를 통해 고등학교 친구들이 다시 모여 재밌게 진행하고 전시를 통해 좋은 결실로 맺을 수 있어서 너무나 보람찬 추억이 되었습니다. 그리고 뼛속까지 공대생들의 속 터지는 디자인을 귀여운 감성으로 변신 시켜준 하영이에게 고마움을 표하고 싶습니다. 지하영 학생이 메타몽의 아기자기한 귀여운 로고, 명함을 디자인해주고 굉장히 높은 퀄리티의 영상을 밤을 새가며 제작해준 작업해준 덕분에 메타몽이 핵페어에서 더 빛났습니다. 그리고 또랑또랑한 목소리로 동영상의 나레이션을 맡아준 김예진양에게도 다시 한번 고맙습니다.</p>
<p>각 분야의 전문성을 살린 협업에서 의미가 많았던 것 같습니다. 아무리 간단한 서비스라도 혼자 다 할 수는 없습니다. 똑똑하고 성실한 친구들과 밤새가며 서로 머리를 맞대며, 서로 배우고 아는 것을 공유하며 한 단계 더 성장한 연구자/개발자가 된 것 같습니다.<br>
핵페어를 통해 우리가 가진 기술로 대중에게 재미를 주는 과정이 즐거웠습니다. 우리의 재능이 미술이나 음악 같은 예술은 아님에도 우리가 만든 기술 기반의 서비스를 자랑스럽게 보여줄 수 있던 핵페어 행사에 고맙습니다.</p>
<h1 id=""></h1>
<p>자신과 닮은 사람을 보고 즐거워하고 서로 놀리는 관람객 분들께서 메타몽을 참신하고 재밌게 생각해주셔서 감사합니다. 좋은 사람들과 좋은 인연을 가질 기회가 되었던 구글 핵페어, 다음 핵페어 행사가 있다면 당연히 다시 한번 더 재밌는 프로젝트로 참가하겠습니다!</p>
</div>]]></content:encoded></item><item><title><![CDATA[ComboGAN : Image Translation with GAN (7)]]></title><description><![CDATA[<div class="kg-card-markdown"><p><a href="https://arxiv.org/abs/1712.06909">arixv: ComboGAN: Unrestrained Scalability for Image Domain Translation</a><br>
<a href="https://openreview.net/forum?id=r1UXpPoLz">accpeted to ICLR2018 workshop</a><br>
<a href="https://www.reddit.com/r/MachineLearning/comments/7la3ux/r_combogan_unrestrained_scalability_for_image/">reddit</a><br>
<a href="https://github.com/AAnoosheh/ComboGAN">github</a></p>
<h1 id="scalibilityissueformultidomains">Scalibility issue for multi domains</h1>
<h1 id=""></h1>
<p>CycleGAN : Two-domain models <strong>taking days to train</strong> on current hardware, <strong>the number of domains quickly becomes limited by the time and resources</strong> required to process them.<br>
Propose a <strong>multi-component image translation</strong></p></div>]]></description><link>http://tmmse.xyz/2018/04/09/image-translation-with-gan-7/</link><guid isPermaLink="false">5acb07be95309e21a0de411f</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Mon, 09 Apr 2018 06:28:16 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><p><a href="https://arxiv.org/abs/1712.06909">arixv: ComboGAN: Unrestrained Scalability for Image Domain Translation</a><br>
<a href="https://openreview.net/forum?id=r1UXpPoLz">accpeted to ICLR2018 workshop</a><br>
<a href="https://www.reddit.com/r/MachineLearning/comments/7la3ux/r_combogan_unrestrained_scalability_for_image/">reddit</a><br>
<a href="https://github.com/AAnoosheh/ComboGAN">github</a></p>
<h1 id="scalibilityissueformultidomains">Scalibility issue for multi domains</h1>
<h1 id=""></h1>
<p>CycleGAN : Two-domain models <strong>taking days to train</strong> on current hardware, <strong>the number of domains quickly becomes limited by the time and resources</strong> required to process them.<br>
Propose a <strong>multi-component image translation model</strong> and training scheme which <strong>scales linearly - both in resource consumption and time</strong> required - with the number of domains.</p>
<h1 id="differencewithstargan">Difference with StarGAN</h1>
<p>So StarGAN uses a single generator and a single discriminator, which is adequate when the domains are actually slight variations of each other, such as human faces with different hair colors or smiles, as a shared network can potentially make use of common features. During the course of our own trial-and-error, we found using a StarGAN-like approach starts to break down when the domains are not so similar.</p>
<p>ComboGAN increases the number of model parameters as more domains are added, thus being able to scale up a large number of domains, at which StarGAN could be constrained by the one-model setup.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-165614.png" alt=""><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-165701.png" alt=""><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-165936.png" alt=""></p>
<p>Decouple Generator. $y=G_{YX}(x)=Decoder_Y(Encoder_X(x))$<br>
$Encoder_X(x)$ can be cached</p>
<p>One generator per domain, linearly increase. with $n$-domains, CycleGAN : $n(n-1)$ Generators needed, ComboGAN :  $n$ Generators.<br>
Training iterations, model capacity also reduces.<br>
the # of Discrimitors are same as CycleGAN. In the case of more than two domains, the encoder output has to be suitable for all other decoders, meaning encoders cannot specialize.<br>
Implies the encoders must put image into shared representation. Encoder sould make central content. (similar to my work PaletteNet)</p>
<h1 id="results">Results</h1>
<p><img src="https://dynalist.io/u/VnRi1fKGi011FcVLHBBPl5cL" alt="Pasted image"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-171905.jpg" alt=""></p>
</div>]]></content:encoded></item><item><title><![CDATA[StarGAN : Image Translation with GAN (6)]]></title><description><![CDATA[<div class="kg-card-markdown"><h2 id="starganacceptedascvpr2018oralpresentation">StarGAN : accepted as CVPR2018 oral presentation.</h2>
<p><a href="https://github.com/yunjey/StarGAN">github</a><br>
<a href="https://arxiv.org/abs/1711.09020">arxiv</a></p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144205.png" alt=""><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144217.png" alt=""></p>
<p>(a) Each domain shift needs generators. (b) Share one generator and use latent code of each domain</p>
<p>The previous limitation of pix2pix, DTN, CycleGAN &amp; DiscoGAN, BicycleGAN is that <strong>they only handle two domains: the source and the target. But, there are</strong></p></div>]]></description><link>http://tmmse.xyz/2018/04/05/image-translation-with-gan-6/</link><guid isPermaLink="false">5ac4e07b95309e21a0de4111</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Wed, 04 Apr 2018 15:53:41 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/2018-04-04-144205.png" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h2 id="starganacceptedascvpr2018oralpresentation">StarGAN : accepted as CVPR2018 oral presentation.</h2>
<img src="http://tmmse.xyz/content/images/2018/04/2018-04-04-144205.png" alt="StarGAN : Image Translation with GAN (6)"><p><a href="https://github.com/yunjey/StarGAN">github</a><br>
<a href="https://arxiv.org/abs/1711.09020">arxiv</a></p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144205.png" alt="StarGAN : Image Translation with GAN (6)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144217.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<p>(a) Each domain shift needs generators. (b) Share one generator and use latent code of each domain</p>
<p>The previous limitation of pix2pix, DTN, CycleGAN &amp; DiscoGAN, BicycleGAN is that <strong>they only handle two domains: the source and the target. But, there are multi-domains in general</strong>. Not only summer &amp; winter, there are spring and fall. <strong>With $n$ domains, It's not practical to learn all $n(n-1)$ generators.</strong></p>
<h1 id="contribution">Contribution</h1>
<ul>
<li>GAN that learns the mappings among <strong>multiple domains using only a single generator and a discriminator</strong>, training effectively from images of all domains.</li>
<li>Successfully <strong>learn multi-domain image translation between multiple datasets by utilizing a mask vector</strong> method that enables StarGAN to control all available domain labels.</li>
<li>Provide both <strong>qualitative and quantitative results on facial attribute transfer and facial expression synthesis</strong> tasks using StarGAN, showing its superiority over baseline models.</li>
</ul>
<h1 id="overview">Overview</h1>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144228.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<ul>
<li>Translate an input image $x$ into an output image $y$ conditioned on the target domain label $c$.
<ul>
<li>When training, translate $y$ back to $x$ with given orginal domain label $c'$.</li>
</ul>
</li>
<li>$D_{src}(x)$ as a probability distribution over sources given by $D$</li>
</ul>
<p>\begin{equation}<br>
D:x \rightarrow ({D_{src}(x), D_{cls}(x)})<br>
\end{equation}</p>
<p>Discriminator produces probability distributions over both sources and domain labels.</p>
<p>\begin{equation}<br>
G(x,c) \rightarrow y<br>
\end{equation}</p>
<h2 id="adversarialloss">Adversarial Loss</h2>
<p>\begin{equation}<br>
\mathcal{L}_{adv}=\mathbb{E}_x[\log D_{src}(x)] + \mathbb{E}_{x,c}[\log(1-D_{src}(G(x,c)))]<br>
\end{equation}<br>
Add auxiliary classifier on top of $D$, <a href="https://arxiv.org/abs/1610.09585">ACGAN</a></p>
<h3 id="domainclassificationloss">Domain Classification Loss</h3>
<h5 id="adomainclassificationlossofrealimagesusedtooptimized">A domain classification loss of real images used to optimize $D$</h5>
<p>\begin{equation}<br>
\mathcal{L}^r_{cls}=\mathbb{E}_{x,c'}[-\log D_{cls}(c'|x)]<br>
\end{equation}</p>
<ul>
<li>$D_{cls}(c'|x)$ : a probability distrbution over domain labels computed by $D$</li>
<li>input image and domain label pair $(x,c')$ given by training dataset</li>
<li>$D$ learns to classify a real image $x$ to its corresponding original domain $c'$</li>
</ul>
<h5 id="adomainclassificationlossoffakeimagesusedtooptimizeg">A domain classification loss of fake images used to optimize $G$</h5>
<p>\begin{equation}<br>
\mathcal{L}^f_{cls}=\mathbb{E}_{x,c'}[-\log D_{cls}(c|G(x,c))]<br>
\end{equation}</p>
<ul>
<li>identical to $\mathcal{L}^f_{cls}=\mathbb{E}_{x,c'}[-\log D_{cls}(c|y)]$ by $G(x,c) \rightarrow y$</li>
<li>$G$ minimizes $\mathcal{L}^f_{cls}$ to $y$ classified as target domain $c$</li>
</ul>
<h5 id="reconstructionloss">Reconstruction loss</h5>
<p>\begin{equation}<br>
\mathcal{L}_{rec} = \mathbb{E}_{x,c,c'}  [||x - G(G(x,c),c')||_1]<br>
\end{equation}</p>
<p>Note that a single generator used twice.</p>
<h5 id="fullobjective">Full Objective</h5>
<p>\begin{equation}<br>
\mathcal{L}_{D}=-\mathcal{L}_{adv}+ \lambda_{cls}\mathcal{L}^r_{cls}<br>
\end{equation}<br>
\begin{equation}<br>
\mathcal{L}_{G}=\mathcal{L}_{adv}+\lambda_{cls}\mathcal{L}^f_{cls}+\lambda_{rec}\mathcal{L}_{rec}<br>
\end{equation}</p>
<p>$\lambda_{cls}=1, \lambda_{rec}=10$</p>
<h1 id="trainingwithmultipledatasets">Training with Multiple Datasets</h1>
<p><strong>StarGAN</strong> simultaneously incorporates multiple datsets of different types of labels (gender/hair color and facial expression). An issue when learning from multiple datasets, however, is that the label information is only partially known to each dataset.</p>
<blockquote>
<p>CelebA : hair color and gender, RaFD : facial expressions</p>
</blockquote>
<p>Label vector $c'$ is required when reconstructing the input image $x$ from the translated image $G(x, c)$ (See $\mathcal{L}^f_{cls}$). Mask vector $m$ allows StarGAN ignore unspecified labels.</p>
<blockquote>
<p>$n$ : # of datasets, $m$ is $n$-dim one-hot vector</p>
</blockquote>
<p>Unified label $\tilde{c}=[c_1, ... , c_n, m]$, all concatenated, here with 2 datasets, $n=2$.</p>
<p>$G$ learns to <em>ignore</em> the unspecified labels... wich are zero vectors <em>focus</em> on the explicitly given label.</p>
<p>Extended auxiliary classifier of the $D$ over labels for all dataset. Only minimize to the know label.</p>
<h1 id="implementation">Implementation</h1>
<p>Replace Vanilla GAN loss $\mathcal{L}_{adv}$ with Wasserstein GAN with gradient penalty.<br>
\begin{equation}<br>
\mathcal{L}_{adv} =\mathbb{E}_x [D_{src}(x)] - \mathbb{E}_{x,c} [D_{src}(G(x,c))] - \lambda_{gp}\mathbb{E}_{\hat{x}} [(||\nabla_{\hat{x}} D_{src}(\hat{x})||_2 - 1)^2]<br>
\end{equation}</p>
<p>$\lambda_{gp} = 10$ and $\hat{x}$ is sampled uniformly along a straight line between a pair of a real and generated images.</p>
<p>Structure from <a href="http://tmmse.xyz/image-translation-with-gan-4">CycleGAN</a>, 2 Convs of 2-strided, 6 ResBlock, 2 Deconv of 2-strided.<br>
Instance Norm for $G$ no norm for $D$, leverage PatchGANs  (sec7.2).</p>
<h1 id="experiments">Experiments</h1>
<p>3 types of experiemtns.</p>
<ul>
<li>Compare recent methods on facial attribute transfer by user studies
<ul>
<li>DIAT, CycleGAN, IcGAN (cGAN). Trained multiple models for every pair of two different domains.</li>
</ul>
</li>
<li>Classification experiment</li>
<li>Empirical results that StarGAN can learn image-to-image translation from multiple dataset</li>
</ul>
<p><strong>CelebA</strong><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144313.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<blockquote>
<p>H+G : cross domain models like DIAT, CycleGAN transfer over again. H then G.</p>
</blockquote>
<p>One possible reason is the regularization effect of StarGAN through a multi-task learning framework. In other words, rather than training a model to perform a fixed translation (e.g., brown- to-blond hair), which is prone to <strong>overfitting</strong>. This allows model to learn <strong>reliable features universally applicable to multiple domains of images</strong> with different facial attribute values.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144324.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<p>Transfer in gender : DIAT, StarGAN are similar but G+A shows significant. StarGAN handles multi-attribute transfer tasks, trained to randomly generating image of a target domain label.</p>
<p><strong>RaFD</strong><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144358.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<p>Superiority of StarGAN in the image quality is due to its <strong>implicit data augmentation effect from a multi-task learning setting</strong>. (Good explanation but not verified.)</p>
<p>RaFD images contain a relatively small size of samples, e.g., 500 images per domain. When trained on two domains, DIAT and CycleGAN can only use 1,000 training images at a time, but StarGAN can use 4,000 images in total from all the available domains for its training. This allows StarGAN to properly learn how to maintain the quality and sharpness of the generated output.</p>
<p>Similar to the concept how &quot;Google Translate&quot; learns human language by training multiple languages together and map into some what central lanuage feature space. But still, not fully verified.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144533.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<p><strong>CelebA + RaFD</strong><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144415.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<ul>
<li>StarGAN-SNG (Single) only on RaFD, StarGAN-JNT (Joint) on CelebA and RaFD.</li>
<li>StarGAN-JNT can leverage both datasets. By utilizing both CelebA and RaFD, StarGAN-JNT can improve these low-level tasks, which is beneficial to learning facial expression synthesis.</li>
</ul>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144559.png" alt="StarGAN : Image Translation with GAN (6)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144605.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<ul>
<li>$m$ mask vector wrongly applied on second row.</li>
<li>'Young' attribute is masked thus became old.</li>
</ul>
<h1 id="appendix">Appendix</h1>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2018-04-04-144642.png" alt="StarGAN : Image Translation with GAN (6)"></p>
<h1 id="mythought">My thought</h1>
<p><strong>Actually, StarGAN not only deals with multi-domains but also multi-attributes.</strong> Thus it is more impressive and general setting, because domains can't be overlapped but attributes can be.</p>
<p>Masking vector is a little bit wierd but works good in practice.</p>
<p><strong>I still wonder how StarGAN, which I think it as a good assemble of cGAN + ACGAN, produces such high quality results.</strong> Maybe I underestimated the power of GAN, and using auxiliary classifier to augment dataset approach helped GAN training a lot. Generator might indeed learned the data distribution better than those in CycleGAN, or changing face might be easy problem for GAN.</p>
<p>To be fair, we also need to try CycleGAN with single generator concept as StarGAN and apply on other dataset (day2night, horse2zebra). Then, <strong>see if StarGAN concept really avoid overfitting and make reliable features universally applicable to multiple domains of images with implicit data augmentation</strong>.</p>
</div>]]></content:encoded></item><item><title><![CDATA[BicycleGAN : Image Translation with GAN (5)]]></title><description><![CDATA[<div class="kg-card-markdown"><h2 id="limitationsofpix2pixdtndiscogancyclegan">Limitations of pix2pix, DTN, DiscoGAN &amp; CycleGAN?</h2>
<ul>
<li>They produce single answer.</li>
<li>They are deterministic models.
<ul>
<li>Translates an image in one-to-one</li>
</ul>
</li>
</ul>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fpa9y2frdcj31400kajtc.jpg" alt="right fit"></p>
<ul>
<li>Paired set, One-to-One : pix2pix (CVPR2017)</li>
<li>Unpaired set, One-to-One : DTN (ICLR2017), CycleGAN (ICCV2017)</li>
<li>Paired set, One-to-Many : ???</li>
</ul>
<h1 id="bicyclegan">BicycleGAN:</h1>
<p>Toward Multimodal Image-to-Image Translation (NIPS2017)<br>
<a href="https://github.com/junyanz/BicycleGAN">BicycleGAN github</a></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpa9zhv6ivj30os07yabc.jpg" alt="inline"><br>
<img src="https://junyanz.github.io/BicycleGAN/index_files/day2night.gif" alt="fit"><br>
<img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpa9zpkf7aj31200p6ait.jpg" alt="inline"></p>
<h2 id="easyapproach">Easy approach:</h2>
<ul>
<li>Adopt stochastically sampled noise $N(</li></ul></div>]]></description><link>http://tmmse.xyz/2018/04/03/image-translation-with-gan-5/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf7706</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Tue, 03 Apr 2018 04:24:31 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/day2night.gif" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h2 id="limitationsofpix2pixdtndiscogancyclegan">Limitations of pix2pix, DTN, DiscoGAN &amp; CycleGAN?</h2>
<ul>
<li>They produce single answer.</li>
<li>They are deterministic models.
<ul>
<li>Translates an image in one-to-one</li>
</ul>
</li>
</ul>
<img src="http://tmmse.xyz/content/images/2018/04/day2night.gif" alt="BicycleGAN : Image Translation with GAN (5)"><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fpa9y2frdcj31400kajtc.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<ul>
<li>Paired set, One-to-One : pix2pix (CVPR2017)</li>
<li>Unpaired set, One-to-One : DTN (ICLR2017), CycleGAN (ICCV2017)</li>
<li>Paired set, One-to-Many : ???</li>
</ul>
<h1 id="bicyclegan">BicycleGAN:</h1>
<p>Toward Multimodal Image-to-Image Translation (NIPS2017)<br>
<a href="https://github.com/junyanz/BicycleGAN">BicycleGAN github</a></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpa9zhv6ivj30os07yabc.jpg" alt="BicycleGAN : Image Translation with GAN (5)"><br>
<img src="https://junyanz.github.io/BicycleGAN/index_files/day2night.gif" alt="BicycleGAN : Image Translation with GAN (5)"><br>
<img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpa9zpkf7aj31200p6ait.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<h2 id="easyapproach">Easy approach:</h2>
<ul>
<li>Adopt stochastically sampled noise $N(z)$ to the deterministic generator</li>
<li>Hope noise act as latent code to produce diverse results</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fpaa7m2wjkj30n20eyt9o.jpg" alt="BicycleGAN : Image Translation with GAN (5)"><br>
<img src="https://ws4.sinaimg.cn/large/006tNc79gy1fpaae06sovj314006cmyk.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<h3 id="howeveritcausesmodecollapse">However, it causes mode collapse</h3>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fpaaeq1d28j30y20eo78u.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<ul>
<li>Generates from multiple noises but mapped to similar outputs</li>
<li>Generator do not care noise</li>
<li>Generate learns to ignore random noise when conditioned on relevant context (input image)</li>
</ul>
<h2 id="encoder">Encoder</h2>
<ul>
<li>Encourage bijection between the output &lt;-&gt; latent space</li>
<li>Disturb two different latent codes to generate same output.
<ul>
<li>Avoid mode collapse</li>
</ul>
</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fpaaiosreqj30zs0f640q.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<h2 id="conditionalvariationalautoencodergan">Conditional Variational Autoencoder-GAN</h2>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fpaam8bn1sj30rc0f6dha.jpg" alt="BicycleGAN : Image Translation with GAN (5)"><br>
<img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpaalwfmobj3186086wjx.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<ul>
<li>Encoder predicts Gaussian</li>
<li>Encoding is trained with real data (B)</li>
<li>Generator takes latent code with rich info of $B$ and input $A$.</li>
<li>At test time, generated from random latent code may produce unrealistic image.</li>
<li>Generator never see random noise.</li>
<li>Discriminator never see samples from generated from random noise</li>
</ul>
<h2 id="conditionallatentregressorgan">Conditional Latent Regressor GAN</h2>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fpaat3h3x0j30o20e40tv.jpg" alt="BicycleGAN : Image Translation with GAN (5)"><br>
<img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpaatil0yaj310a04s3zb.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<ul>
<li>Encoder is latent code regressor.</li>
<li>Generated sample is encoded and mapped back to random noise.</li>
<li>Latent code is easily and randomly sampled, as test time.</li>
<li>Generator never sees ground truth $B$.</li>
<li>More vulnerable to mode collapse, probably small dimension of $z$ and $L_1$ loss on $z$ and $\hat{z}$ is not enough to prevent generator easily fool discriminator?</li>
</ul>
<h1 id="bicyclegan">BicycleGAN</h1>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fpaaukzcrmj314k0gin09.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<h3 id="trainbothmodeltogetherwithbenefitofcyclelossinzandb">Train both model together, with benefit of cycle-loss in $z$ and $B$.</h3>
<h2 id="result">Result</h2>
<ul>
<li>Pix2pix + noise : similar realistic outputs</li>
<li>cVAE-GAN : adds variation but artifacts caused from random sample at test</li>
<li>cLR-GAN : less variant in output and sometimes mode collapse</li>
<li>BicycleGAN : hybrid results both diverse and realistic</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fpaavasw1pj313q0g00xq.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<h2 id="quantativeexperiment">Quantative experiment</h2>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fpaavpgf1fj30ye0dg42l.jpg" alt="BicycleGAN : Image Translation with GAN (5)"></p>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>Propose solution to mode collapse in the conditional generative setting</li>
<li>Combine multiple objectives for encouraging a bijective mapping between the latent and output spaces</li>
<li>Produce both realistic and diverse</li>
<li>Latent code could be replaced with user controllable parameter in the future</li>
</ul>
<h2 id="trends">Trends</h2>
<ul>
<li>Paired set, One-to-One : <a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-2">pix2pix</a> (CVPR2017)</li>
<li>Unpaired set, One-to-One : <a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-3">DTN</a> (ICLR2017), <a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-4">DiscoGAN</a> (ICML2017), <a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-4">CycleGAN</a> (ICCV2017)</li>
<li>Paired set, One-to-Many : BicycleGAN (NIPS2017)</li>
<li>In the future:
<ul>
<li>Unpaired set, One-to-Many : <a href="https://arxiv.org/abs/1802.10151">Augmented CycleGAN</a> (probabily ICML2018 submitted), <a href="https://arxiv.org/abs/1711.05139">XGAN</a> (ICLR2018 <a href="https://openreview.net/forum?id=rkWN3g-AZ">rejected</a>)</li>
<li>Multi domains. Not only a source domain to a target domain: <a href="http://tmmse.xyz/2018/04/05/image-translation-with-gan-6">StarGAN</a> (CVPR2018 accepted)</li>
<li>User controllable noise vector in BicycleGAN</li>
</ul>
</li>
</ul>
</div>]]></content:encoded></item><item><title><![CDATA[Ghost : changing font, 폰트 바꾸기]]></title><description><![CDATA[<div class="kg-card-markdown"><h1 id="inshort">In short;</h1>
<p>Assuming you have Ghost v1.0.0 (or above) with default theme, Casper.<br>
Default font is Georgia. We will change font with <a href="https://fonts.google.com">Google Web fonts</a>.</p>
<pre><code>cd {your-ghost-dir}/content/themes/casper    # move to theme directory
npm install gulp-cli -g                      # install gulp globally
npm install                                  # install npm packages in theme</code></pre></div>]]></description><link>http://tmmse.xyz/2018/04/03/ghost-changing-font/</link><guid isPermaLink="false">5ac2434caafb670aa2e1f0c0</guid><category><![CDATA[ghost]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Mon, 02 Apr 2018 15:34:27 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1468404166635-56e2d75ee491?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=87f6659e311927a1102ebbb48f241db0" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h1 id="inshort">In short;</h1>
<img src="https://images.unsplash.com/photo-1468404166635-56e2d75ee491?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=87f6659e311927a1102ebbb48f241db0" alt="Ghost : changing font, 폰트 바꾸기"><p>Assuming you have Ghost v1.0.0 (or above) with default theme, Casper.<br>
Default font is Georgia. We will change font with <a href="https://fonts.google.com">Google Web fonts</a>.</p>
<pre><code>cd {your-ghost-dir}/content/themes/casper    # move to theme directory
npm install gulp-cli -g                      # install gulp globally
npm install                                  # install npm packages in theme dir
gulp                                  # execute gulp. monitoring changes in theme
</code></pre>
<p>And <strong>remove every  <code>font-family : Georgia, serif;</code> in <code>./assets/css/screen.css</code></strong><br>
For ex)</p>
<pre><code>.post-card-excerpt {
    font-family: Nanum Gothic, serif;
}
</code></pre>
<p>to</p>
<pre><code>.post-card-excerpt {
}
</code></pre>
<p>The changes in <code>./assets/css/screen.css</code> will be compiled to <code>./assets/built/screen.css</code> by <code>gulp</code>.<br>
<strong>And restart your blog at theme directory by <code>ghost restart</code></strong></p>
<blockquote>
<p>So, if you don't want to <code>npm install</code> anything, <strong>probabily</strong> removing all <code>font-family : Georgia, serif;</code> in <code>./assets/built/screen.css</code> also works.</p>
</blockquote>
<p>Then, find favorite font in <a href="https://fonts.google.com">Google Web fonts</a>. I picked <code>Nanum Gothic</code> font. <strong>Inject following code</strong> at the first row of <code>Blog Header</code> into in Labs,  <code>http://your-blog.com/ghost/#/settings/code-injection/</code></p>
<pre><code class="language-html">&lt;link href='https://fonts.googleapis.com/css?family=Nanum+Gothic' rel='stylesheet' type='text/css'&gt;
&lt;style&gt;
body,  
h1, h2, h3, h4, h5, h6,  
.main-nav a,
.subscribe-button,
.page-title,
.post-meta,
.read-next-story .post:before,
.pagination,
.site-footer,{
    font-family:&quot;Nanum Gothic&quot;, sans-serif;
}
[class^=&quot;icon-&quot;]:before, [class*=&quot; icon-&quot;]:before {
	font-family:&quot;Nanum Gothic&quot;, sans-serif;
&lt;/style&gt;
</code></pre>
<p>Done.</p>
<hr>
<h1 id="">우리말로 길게 말해서;</h1>
<p>어느 <a href="https://blog.lorentzca.me/change-ghost-blog-font/">일본 블로그</a>를 자동 한글 번역해서 봤는데, 참고해서 해봄. code injection을 통해서하는데, 목표는 구글 웹포트의 Nanum gothic을 사용하는 것. <a href="https://fonts.google.com">구글 웹폰트</a> 그리고 <a href="http://makebct.net/%EA%B5%AC%EA%B8%80%EC%9B%B9%ED%8F%B0%ED%8A%B8-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0/?cat=148/">무료 사용 가능 한글 구글 웹폰트</a>에서 원하는 폰트를 찾는다.</p>
<p>나눔고딕은</p>
<pre><code class="language-html">&lt;link href='https://fonts.googleapis.com/css?family=Nanum+Gothic' rel='stylesheet' type='text/css'&gt;
&lt;style&gt;
body,  
h1, h2, h3, h4, h5, h6,  
.main-nav a,
.subscribe-button,
.page-title,
.post-meta,
.read-next-story .post:before,
.pagination,
.site-footer,{
    font-family:&quot;Nanum Gothic&quot;, sans-serif;
}
[class^=&quot;icon-&quot;]:before, [class*=&quot; icon-&quot;]:before {
	font-family:&quot;Nanum Gothic&quot;, sans-serif;
&lt;/style&gt;
</code></pre>
<p>그리고 내가 더 좋아하는 <strong>나눔바른고딕</strong>은</p>
<pre><code class="language-html">&lt;link href='https://cdn.rawgit.com/openhiun/hangul/14c0f6faa2941116bb53001d6a7dcd5e82300c3f/nanumbarungothic.css' rel='stylesheet' type='text/css'&gt;
&lt;style&gt;
body,  
h1, h2, h3, h4, h5, h6,  
.main-nav a,
.subscribe-button,
.page-title,
.post-meta,
.read-next-story .post:before,
.pagination,
.site-footer,
[class^=&quot;icon-&quot;]:before, [class*=&quot; icon-&quot;]:before {
	font-family:&quot;Nanum Barun Gothic&quot;, sans-serif;
&lt;/style&gt;
</code></pre>
<p>를 code injection에서 <code>Blog Header</code>에서 맨 처음에 삽입해준다.</p>
<p>그러나 이상하게도 블로그 제목만 반영되는 현상 발견..<br>
블로그 front 페이지 abstract와 포스트의 메인 컨텐츠의 폰트가 바뀌지 않음 발견. 보니까 안바뀌는 폰트가 Georgia이었다. ghost의 theme폴더에서 <code>./asset/css/screen.css</code> 파일에서 <code>Georgia</code> 폰트를 발견했다.</p>
<p>특히 그부분에서 문제가 됬던 파트들, <code>.post-card-excerpt</code>, <code>.post-full-content</code> 등에서 <code>Georgia</code>로 고정되어 있음을 알수있었다. 따라서 그 부분  <code>font-family : Georgia, serif;</code>를 원하는 폰트로 바꾸거나 아니면 아예 삭제한다.</p>
<p>그리고 css파일이 바뀔때마다 테마 홈 디렉토리에서 <code>ghost restart</code> 해준다.</p>
<p>하지만 그래도 반영이 안되는데. 왜냐하면 <code>./default.hbs</code>에서 <code>&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;{{asset &quot;built/screen.css&quot;}}&quot; /&gt;</code> 로 built된 css만 반영되기 때문이다.</p>
<p>그래서 built가 무엇인가 하니 <code>./READEME.md</code>에서 자동으로 css의 폴더를 compile한 것이라 한다. 즉 <code>./assets/built/screen.css</code>는 <code>./assets/css/screen.css</code>를 압축시킨 동일한 code이다. 이는 <a href="https://brettdewoody.com/optimizing-ghost-pagespeed/">css의 inlining을 통해 optimize한다</a> 라는 글에서 힌트를 얻었다.</p>
<p>따라서 <code>./assets/built/screen.css</code> 문제되는 부분의 <code>Georgia</code>를 <code>Nanum Gothic</code>으로 바꿔주면 되는데 테마 개발 의도랑 다르니 그냥 테마 <code>READEME.md</code>가 하라는대로 하자.</p>
<p>그래서 우선 gulp가 필요한데, 지속적으로 테마의 file들을 보다가 수정사항이 있으면 자동으로 compile해주는 것이라 보면 된다. 전역 설치해줘야 하니 <code>npm install gulp-cli -g</code>. 테마 폴더에서 <code>npm install</code> 후 <code>gulp</code>실행 계속 켜져있는다.</p>
<p>그럼 아까 했던 바꿨던 <code>Georgia</code>를 <code>Nanum Gothic</code>으로 바꾼게 compile되는 것이 확인되면 블로그 폰트도 바뀌는 것을 확인할 수 있다.</p>
<p>나는 거기에 <code>code block</code>의 폰트까지 내가 좋아하는 Ubuntu로 바꾸었다. 만족감 에헿.</p>
</div>]]></content:encoded></item><item><title><![CDATA[Ghost 1.0.0 로 업그레이드 + 밀린 포스트들]]></title><description><![CDATA[<div class="kg-card-markdown"><h1 id="1">1</h1>
<p>귀찮아서 업그레이드를 미루고 있었는데, 거대 업데이트를 한참 전에 했길래 간만에 업그레이드했다. 간만에 블로그 만지작 거리는게 재밌었는데 이내 업그레이드는 커다란 귀찮음을 가져다주었다. 결론적으로 서버의 리눅스가 Ubuntu 16.04가 아니면 자잘한 에러가 많다. 이번 버전부터 <code>ghost-cli</code>를 통해 v1이상에서 쉽게 업그레이드가 가능토록 cli를 지원한다. <a href="http://tmmse.xyz/2018/04/01/command-line-interface/">역시 갓갓 cli다</a>.</p>
<p>그리고 힘들게 Nginx등을 설정해줬었는데(</p></div>]]></description><link>http://tmmse.xyz/2018/04/02/ghost-upgrade-more-posts/</link><guid isPermaLink="false">5ac19dd093c7ee06e8cf77a4</guid><category><![CDATA[ghost]]></category><category><![CDATA[Getting Started]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Mon, 02 Apr 2018 03:23:04 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1428790067070-0ebf4418d9d8?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=8a82be049adf657ff185b90e1954c62b" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h1 id="1">1</h1>
<img src="https://images.unsplash.com/photo-1428790067070-0ebf4418d9d8?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=8a82be049adf657ff185b90e1954c62b" alt="Ghost 1.0.0 로 업그레이드 + 밀린 포스트들"><p>귀찮아서 업그레이드를 미루고 있었는데, 거대 업데이트를 한참 전에 했길래 간만에 업그레이드했다. 간만에 블로그 만지작 거리는게 재밌었는데 이내 업그레이드는 커다란 귀찮음을 가져다주었다. 결론적으로 서버의 리눅스가 Ubuntu 16.04가 아니면 자잘한 에러가 많다. 이번 버전부터 <code>ghost-cli</code>를 통해 v1이상에서 쉽게 업그레이드가 가능토록 cli를 지원한다. <a href="http://tmmse.xyz/2018/04/01/command-line-interface/">역시 갓갓 cli다</a>.</p>
<p>그리고 힘들게 Nginx등을 설정해줬었는데(<a href="http://tmmse.xyz/2015/12/12/blogging-platform-ghost-start/">처음 블로그 글에서 소개</a>), 이제 <code>systemd</code>와 <code>systemctl</code>을 통해서 편하게 <code>ghost-cli</code>에 있는 <code>ghost start</code> 커맨드로 쉽게 블로그 서버를 운영할 수 있다. 그리고 사실 저런걸 몰라도, 설치과정에서 다 해주기 때문에 굉장히 편하다. 근데 그 설치가 ubuntu 16.04이어야 편하게 됬다. 재작년에 Digital Ocean에서 대여하기 시작한 서버는 14.04여서 업그레이드가 귀찮아서 어떻게든 해볼랬는데 결국은 업그레이드했다. 이게 벌써 재작년이라니 새삼스럽다.</p>
<p>Ghost 업그레이드는 굉장히 쉬운편이다. 전에 있던 <code>contents</code>폴더와 json파일을 백업해두고 <code>ghost-cli</code>설치후 그냥 json파일 import후 <code>contents</code>에 있던 이미지들을 도로 넣어주면 끝났다. 업그레이드하니 이쁜 블로그를 보니, 컨텐츠는 있었지만 여태 귀찮아서 안한 포스트들을 결국 다 뱉어내는 동기부여가 되었다.</p>
<h1 id="2">2</h1>
<p>그래서 이번에 슬라이드르 작업해서 발표했던 <a href="http://tmmse.xyz/2018/04/01/command-line-interface/">Command Line Interface</a>와 <a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-1/">Image Translation with GAN</a>을 재정리해서 다루었다.</p>
<p>사실 쓸 컨텐츠는 무지하게 많다.</p>
<ul>
<li>작년에 자취방을 이사하면서 생긴 공간에 엄청난 <strong><a href="http://tmmse.xyz/2017/07/23/home-automation-memo">Home Automation</a>을 한 작업내용과 결과</strong>와</li>
<li>작년 말에 아름답게 떡상했던 <s>(그리고 올해 초부터 지금까지 지옥가는)</s> <strong>Bitcoin</strong>과 관련되어 열심히 공부했던 블록체인 프로젝트들 및 암호학...</li>
<li>그리고 내가 연구하고 있는 Computer Vision 및 Deep Learning</li>
</ul>
<p>공부한 것을 풀어내어 공유하는 일은 별도의 힘이 들어가는 고된 작업이다.<br>
같이 운영하는 <a href="http://tmmse.xyz/author/park/">박종현<s>장박</s></a>은 회사다니면서 열심히 계속 글써주니 블로그가 안죽은 거 같아서 다행스럽게 생각하기도한다.</p>
<p>블로깅을 통해서 의외로 많은 결실들을 맺었기 때문에 다시금 내 노트에 정리했던걸 다듬어 공유해봐야겠다.</p>
</div>]]></content:encoded></item><item><title><![CDATA[CycleGAN : Image Translation with GAN (4)]]></title><description><![CDATA[<div class="kg-card-markdown"><h3 id="unpairedimagetoimagetranslationusingcycleconsistentadversarialnetworkscycleganfromucberkeleypix2pixupgrade">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) from UC Berkeley (pix2pix upgrade)</h3>
<h1 id="">&amp;</h1>
<h3 id="learningtodiscovercrossdomainrelationswithgenerativeadversarialnetworksdiscoganfromsktbrain">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks (DiscoGAN) from SK T-Brain</h3>
<h2 id="discogancyclegan">DiscoGAN &amp; CycleGAN</h2>
<ul>
<li>Almost Identical concept.</li>
<li>DiscoGAN came 15 days earlier. Low resolution ($64 \times 64$)</li>
<li>CycleGAN has better qualitative results ($256 \times</li></ul></div>]]></description><link>http://tmmse.xyz/2018/04/02/image-translation-with-gan-4/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf7705</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Mon, 02 Apr 2018 02:53:39 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/CycleGAN.jpg" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h3 id="unpairedimagetoimagetranslationusingcycleconsistentadversarialnetworkscycleganfromucberkeleypix2pixupgrade">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) from UC Berkeley (pix2pix upgrade)</h3>
<h1 id="">&amp;</h1>
<h3 id="learningtodiscovercrossdomainrelationswithgenerativeadversarialnetworksdiscoganfromsktbrain">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks (DiscoGAN) from SK T-Brain</h3>
<h2 id="discogancyclegan">DiscoGAN &amp; CycleGAN</h2>
<ul>
<li>Almost Identical concept.</li>
<li>DiscoGAN came 15 days earlier. Low resolution ($64 \times 64$)</li>
<li>CycleGAN has better qualitative results ($256 \times 256$) and quantative experiments.</li>
</ul>
<h2 id="differencefromdtn">Difference from DTN</h2>
<ul>
<li>No $f$-constancy. Do not need pre-trained context encoder</li>
<li>Only need dataset $S$ and $T$ by proposing <strong>cycle-consistency</strong></li>
</ul>
<h2 id="discogan">DiscoGAN</h2>
<img src="http://tmmse.xyz/content/images/2018/04/CycleGAN.jpg" alt="CycleGAN : Image Translation with GAN (4)"><p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-31-014136.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-31-013857.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<h3 id="withoutcrossdomainmatchingganhasmodecollapse">without cross domain matching, GAN has mode collapse</h3>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-31-014006.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<h3 id="learnprojectiontomodeindomainbwhiletwodomainshaveonetoonerelation">learn projection to mode in domain $B$, while two domains have one-to-one relation</h3>
<h2 id="typicalganissuemodecollapse">Typical GAN issue: Mode collapse</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-152828.jpg" alt="CycleGAN : Image Translation with GAN (4)"><img src="http://3.bp.blogspot.com/--_KSBB_yBJI/WKj7tRhju1I/AAAAAAAABUg/t7VHBDDPSrARfASSNRV1xF0EOP1kRevRACK4B/s1600/unrolled-GAN-mode-collapse-2.PNG" alt="CycleGAN : Image Translation with GAN (4)"></p>
<ul>
<li>top is ideal case, bottom is mode collapse failure case</li>
<li>Toy problem of 2-dim Gaussian mixture model</li>
<li>5 modes of domain A to 10 modes of domain B</li>
</ul>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-152956.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<ul>
<li>GAN, GAN + const show injective mapping &amp; mode collapse</li>
<li>DiscoGAN shows bijective mapping &amp; generate all 10 modes of B.</li>
</ul>
<h2 id="proposeddiscogan">proposed DiscoGAN</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-153025.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<h3 id="cycleganhassimilarcontributiononthispoint">CycleGAN has similar contribution on this point</h3>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-162701.jpg" alt="CycleGAN : Image Translation with GAN (4)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-11-074834.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<h2 id="results">Results</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-162138.jpg" alt="CycleGAN : Image Translation with GAN (4)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-162222.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<p>codes and more results in</p>
<ul>
<li><a href="https://github.com/SKTBrain/DiscoGAN">https://github.com/SKTBrain/DiscoGAN</a></li>
<li><a href="https://github.com/carpedm20/DiscoGAN-pytorch">https://github.com/carpedm20/DiscoGAN-pytorch</a></li>
</ul>
<h3 id="cyclegan">CycleGAN</h3>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-162623.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<p>Use more GAN techniques: LSGAN, use image buffer of previous generated samples</p>
<p><img src="https://junyanz.github.io/CycleGAN/images/painting2photo.jpg" alt="CycleGAN : Image Translation with GAN (4)"><br>
<img src="https://junyanz.github.io/CycleGAN/images/objects.jpg" alt="CycleGAN : Image Translation with GAN (4)"><br>
<img src="https://junyanz.github.io/CycleGAN/images/photo_enhancement.jpg" alt="CycleGAN : Image Translation with GAN (4)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-11-074213.jpg" alt="CycleGAN : Image Translation with GAN (4)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-11-074708.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<h3 id="failurecase">failure case</h3>
<p><img src="https://junyanz.github.io/CycleGAN/images/failure_putin.jpg" alt="CycleGAN : Image Translation with GAN (4)"></p>
<p>CycleGAN demonstrates more experiments!<br>
project page : <a href="https://junyanz.github.io/CycleGAN/">https://junyanz.github.io/CycleGAN/</a><br>
code available with Torch and PyTorch</p>
</div>]]></content:encoded></item><item><title><![CDATA[DTN : Image Translation with GAN (3)]]></title><description><![CDATA[<div class="kg-card-markdown"><h1 id="2unsupervisedcrossdomainimagegenerationdtn">2. Unsupervised Cross-Domain Image Generation (DTN)</h1>
<p>published to <strong>ICLR2017</strong> by Yaniv Taigman, Adam Polyak, Lior Wolf</p>
<p>Learn $ G: S \rightarrow T $ of two related domains, $ S $ and $ T $ without labels! (labels of images are usually expensive)</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-29-155516.jpg" alt="inline"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-100007.jpg" alt="inline"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-121602.jpg" alt="inline"></p>
<h2 id="baselinemodel">Baseline model</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-29-161854.jpg" alt="inline 80%"><br>
$ D $ : discriminator, $ G $ : generator,<br>
$ f $ : context encoder. outputs feature. (128-dim)</p>
<p>\begin{equation}</p></div>]]></description><link>http://tmmse.xyz/2018/04/02/image-translation-with-gan-3/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf7704</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Mon, 02 Apr 2018 02:50:24 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/C2ytKoaVQAAV_zC.jpg" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h1 id="2unsupervisedcrossdomainimagegenerationdtn">2. Unsupervised Cross-Domain Image Generation (DTN)</h1>
<img src="http://tmmse.xyz/content/images/2018/04/C2ytKoaVQAAV_zC.jpg" alt="DTN : Image Translation with GAN (3)"><p>published to <strong>ICLR2017</strong> by Yaniv Taigman, Adam Polyak, Lior Wolf</p>
<p>Learn $ G: S \rightarrow T $ of two related domains, $ S $ and $ T $ without labels! (labels of images are usually expensive)</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-29-155516.jpg" alt="DTN : Image Translation with GAN (3)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-100007.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-121602.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<h2 id="baselinemodel">Baseline model</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-29-161854.jpg" alt="DTN : Image Translation with GAN (3)"><br>
$ D $ : discriminator, $ G $ : generator,<br>
$ f $ : context encoder. outputs feature. (128-dim)</p>
<p>\begin{equation}<br>
R_{GAN} = \max_D \mathbb{E}_{x\sim\mathcal{D}_S} \log[1-D(G(x))] + \mathbb{E}_{x\sim\mathcal{D}_T} \log[D(x)]<br>
\end{equation}</p>
<p>\begin{equation}<br>
R_{CONST} = \mathbb{E}_{x\sim\mathcal{D}_S} d(f(x),f(G(x)))<br>
\end{equation}</p>
<p>$f$-constancy : Does $x, G(x)$ have similar context?<br>
$ d $ : distance metric. ex) MSE<br>
$ f $ : &quot;Pretrained&quot; context encoder. <em>Parameter fixed</em>.<br>
$f$ can be pretrained with classification task on $S$</p>
<p>Minimize two Risks : $ R_{GAN}$ and  $ R_{CONST} $</p>
<p>Experimentally, Baseline model didn't produce desirable results.<br>
Thus, similar but more elaborate architecture proposed</p>
<h2 id="proposeddomaintransfernetworkdtn">Proposed &quot;Domain Transfer Network (DTN)&quot;</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-29-142809.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<p>First, <strong>$ f $ : the context encoder</strong> now encode as $f(x)$ then $g$ will generate from it :  $ G = g(f(x)) $</p>
<p>$g$ focuses to generate from given context $f(x)$</p>
<p>Second, for $x \in \mathbf{t}$, $x$ is also encoded by $f$ and applied $g$</p>
<p>&quot;Pretrained $f$ on $S$&quot; would not be good as much as on $T$. But enough for context encoding purpose<br>
$ L_{TID}$ : $G(x)$ should be similar to $x$<br>
Also $D$ takes $G(x)$ and performs ternary (3-class) classification. (one real, two fakes)</p>
<h2 id="losses">Losses</h2>
<h3 id="discriminatorlossl_d">Discriminator loss : $L_D$</h3>
<p>\begin{equation}<br>
L_D = -\mathbb{E}_{x \in \mathbf{s}} \log D_1 (G(x)) - \mathbb{E}_{x \in \mathbf{t}} \log D_2 (G(x)) - \mathbb{E}_{x \in \mathbf{t}} \log D_3 (x)<br>
\end{equation}</p>
<p>$D_i(x): Probability$<br>
$D_1(x)$ : generated from $S$? /  $D_2(x)$ : generated from $T$? / $D_3(x)$ : sample from $T$?</p>
<h3 id="generatoradversariallossl_gang">Generator : Adversarial Loss $L_{GANG}$</h3>
<p>\begin{equation}<br>
L_{GANG} = - \mathbb{E}_{x \in \mathbf{s}} \log D_3 (G(x)) - \mathbb{E}_{x \in \mathbf{t}} \log D_3(G(x))<br>
\end{equation}</p>
<p>Fool $D$ to classify as sample from $T$</p>
<h3 id="generatorl_constandidentitypreservingl_tid">Generator : $L_{CONST}$ and Identity preserving $ L_{TID}$</h3>
<p>\begin{equation}<br>
L_{CONST} = \sum_{x \in \mathbf{s}} d(f(x),f(g(f(x)))<br>
\end{equation}<br>
, in feature level</p>
<p>\begin{equation}<br>
L_{TID} = \sum_{x \in \mathbf{t}} d_2(x,G(x))<br>
\end{equation}<br>
,  in pixel level</p>
<p>$d, d_2$ used as MSE in this work</p>
<p>\begin{equation}<br>
L_{G} = L_{GANG} + \alpha L_{CONST}+ \beta L_{TID} + \gamma L_{TV}<br>
\end{equation}</p>
<p>$L_{TV}$ is for output smoothing.</p>
<p>$L_G$ minimized over $g$<br>
$L_D$ minimized over $D$</p>
<h2 id="experiments">Experiments</h2>
<ol>
<li>Street View House Numbers (SVHN) $\rightarrow$ MNIST</li>
<li>Face $\rightarrow$ Emoji<br>
Both cases, $S$ and $T$ domains differ considerably<br>
SVHN $\rightarrow$ MNIST<br>
Pretrain $f$ on $SVHN_{f_TRAIN}$<br>
Learn $G: SVHN_{DTN_TRAIN} \rightarrow MNIST_{TEST}$<br>
Train a MNIST classifier on $MNIST_{TRAIN}$.  will be used as evaluation purpose later<br>
Domain transfer on $SVHN_{TEST}$ : $G(SVHN_{TEST})$</li>
</ol>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-114629.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<h3 id="f">$f$</h3>
<ul>
<li>4 convs (each filters 64,128,256,128) / max pooling / ReLU</li>
<li>input $32 \times 32$ RGB / output 128-dim vector.</li>
<li>$f$ do not need to be very powerful classifier.</li>
<li>achieves 4.95% error on SVHN test set</li>
<li>Weaker in $T$ : 23.92% error on MNIST.</li>
<li>Learn analogy of unlabeled examples</li>
</ul>
<h3 id="g">$g$</h3>
<ul>
<li>Inspired by DCGAN</li>
<li>SVHN-trained $f$'s 128D representation $\rightarrow 32\times32$</li>
<li>four blocks of deconv, BN, ReLU. TanH at final.</li>
<li>$$ L_{G} = L_{GANG} + \alpha L_{CONST}+ \beta L_{TID} + \gamma L_{TV}  $$</li>
</ul>
<p>$\alpha=\beta=15, \gamma=0$<br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-100007.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-100056.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<h3 id="evaluatedtn">Evaluate DTN</h3>
<p>Train classifier on $MNIST_{TRAIN}$.<br>
Architecture same as $f$<br>
MNIST performance 99.4% test set.<br>
Evaluate by testing MNIST classifier on $ G(\mathbf{s}_{TEST}) = { G(x)|x \in \mathbf{s}_{TEST} } $ using $Y$ : $\mathbf{s}_{TEST}$ label.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-114629.jpg" alt="DTN : Image Translation with GAN (3)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-100214.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<h2 id="experimentsunseendigits">Experiments: Unseen Digits</h2>
<p>Study the ability of DTN to overcome omission of a class in samples.<br>
For example, class '3' Ablation applied on</p>
<ol>
<li>training DTN, domain $S$</li>
<li>training DTN, domain $T$</li>
<li>training $f$.</li>
</ol>
<p>But '3' exists in testing DTN! Compare results.<br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-115440.jpg" alt="DTN : Image Translation with GAN (3)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-111009.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-111020.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-111048.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-111102.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-111120.jpg" alt="DTN : Image Translation with GAN (3)"><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-111137.jpg" alt="DTN : Image Translation with GAN (3)"><br>
(a) The input images. (b) Results of our DTN. (c)  3 was not in SVNH. (d) 3 was not in MNIST. (e) 3 was not shown in both SVHN and MNIST. (f) The digit 3 was not shown in SVHN, MNIST and during the training of f.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-115645.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<h2 id="domainadaptation">Domain Adaptation</h2>
<p>$S$ labeled, $T$ unlabeled, want to train classifier of $T$<br>
Train k-NN classifier<br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-31-052449.jpg" alt="DTN : Image Translation with GAN (3)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-104539.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<h3 id="facerightarrowemoji">Face $\rightarrow$Emoji</h3>
<ul>
<li>face from <a href="http://vintage.winklerbros.net/facescrub.html">Facescrub</a>/<a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a></li>
<li>emoji gained from <a href="https://www.bitmoji.com">bitmoji.com</a>, not publicized<br>
preprocess on emoji with heuristics. Align face.</li>
<li>$f$ from DeepFace pretrained network. (Taigman et al. 2014) the author's previous work</li>
<li>$f(x)$ is 256-dim</li>
<li>$g$ outputs $64 \times 64$</li>
<li>SR (Dong et al. 2015) to upscale final output.</li>
</ul>
<h3 id="results">Results</h3>
<p>choose $\alpha=100, \beta=1, \gamma=0.05$ via validation<br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-31-010439.jpg" alt="DTN : Image Translation with GAN (3)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-121602.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<p>Original style transfer can't solve it</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-122508.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<p>DTN also can style transfer.<br>
DTN is more general than Styler Transfer method.</p>
<h2 id="limitations">Limitations</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-123135.jpg" alt="DTN : Image Translation with GAN (3)"></p>
<ul>
<li>$f$ usually can be trained in one domain, thus asymmetric.</li>
<li>Handle two domains differently.
<ul>
<li>$T \rightarrow S$ is bad.</li>
</ul>
</li>
<li>Bounded by $f$. Needs pre-trained context encoder.</li>
<li>any better way to learn context without pretraining?</li>
<li>Any more $S \rightarrow T$ tasks?</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>Demonstrate Domain Transfer, as an unsupervised method.</li>
<li>Can be generalized to various $S \rightarrow T$ problems.</li>
<li>$f$-constancy to maintain context of domain $S$ &amp; $T$</li>
<li>Simple domain adaptation and good performance</li>
<li>inspiring work to future domain adaptation research</li>
</ul>
<p>More open reviews at <a href="https://openreview.net/forum?id=Sk2Im59ex&amp;noteId=Sk2Im59ex">OpenReview.net</a></p>
</div>]]></content:encoded></item><item><title><![CDATA[pix2pix : Image Translation with GAN (2)]]></title><description><![CDATA[<div class="kg-card-markdown"><h1 id="imagetoimagetranslationwithconditionaladversarialnetworkspix2pix">Image-to-Image Translation with Conditional Adversarial Networks (pix2pix)</h1>
<p>published to <strong>CVPR2017</strong> by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120245.jpg" alt="fit"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120608.jpg" alt="inline"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120352.jpg" alt="inline"></p>
<h2 id="learnpairwiseimagesofsandtlikebelow">Learn pair-wise images of $S$ and $T$ like below</h2>
<ul>
<li>BW &amp; Color image</li>
<li>Street Scene &amp; Label</li>
<li>Facade &amp; Label</li>
<li>Aerial &amp; Map</li>
<li>Day &amp; Night</li>
<li>Edges &amp; Photo</li>
</ul>
<p>source</p></div>]]></description><link>http://tmmse.xyz/2018/04/02/image-translation-with-gan-2/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf7703</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Mon, 02 Apr 2018 02:38:14 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/edges2cats.jpg" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h1 id="imagetoimagetranslationwithconditionaladversarialnetworkspix2pix">Image-to-Image Translation with Conditional Adversarial Networks (pix2pix)</h1>
<img src="http://tmmse.xyz/content/images/2018/04/edges2cats.jpg" alt="pix2pix : Image Translation with GAN (2)"><p>published to <strong>CVPR2017</strong> by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120245.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120608.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120352.jpg" alt="pix2pix : Image Translation with GAN (2)"></p>
<h2 id="learnpairwiseimagesofsandtlikebelow">Learn pair-wise images of $S$ and $T$ like below</h2>
<ul>
<li>BW &amp; Color image</li>
<li>Street Scene &amp; Label</li>
<li>Facade &amp; Label</li>
<li>Aerial &amp; Map</li>
<li>Day &amp; Night</li>
<li>Edges &amp; Photo</li>
</ul>
<p>source image $x \in S$, target image (label)  $y \in T$ is pair-wise.</p>
<h3 id="thusitissupervisedlearning">thus it is Supervised Learning</h3>
<h2 id="generatorofpix2pix">Generator of pix2pix</h2>
<p>$G(x,z)$ where $x$: image and $z$: noise<br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-132923.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
Use U-Net shaped network</p>
<ul>
<li>known to be powerful at segmentation task</li>
<li>use spatial information from features of bottom layer</li>
<li>use dropout as noise in decoder part</li>
</ul>
<h2 id="discriminatorofpix2pix">Discriminator of pix2pix</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-132829.jpg" alt="pix2pix : Image Translation with GAN (2)"></p>
<h2 id="lossfunction">Loss function</h2>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fpbew5684gj30pi0ckdgk.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
$x$: source image, $y$: target image, $z$: noise</p>
<h3 id="useadversariallossandl1loss">Use Adversarial loss and L1 loss</h3>
<p>\begin{equation}<br>
\mathcal{L}_{cGAN}(G,D) = \mathbb{E}_{x,y \sim p_{data}(x,y)}[\log D(x,y)] +  \mathbb{E}_{x \sim p_{data}(x), z \sim p_z(z)}[\log (1-D(x,G(x,z)))]<br>
\end{equation}</p>
<p>\begin{equation}<br>
\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y \sim p_{data}(x,y),z \sim p_z(z)}[||y-G(x,z)||_1]<br>
\end{equation}</p>
<h2 id="result">Result</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-143108.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-143305.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-143347.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-143449.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-143510.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-143530.jpg" alt="pix2pix : Image Translation with GAN (2)"><br>
Do <a href="https://affinelayer.com/pixsrv/">demo</a>!<br>
<code>https://affinelayer.com/pixsrv/</code></p>
</div>]]></content:encoded></item><item><title><![CDATA[Image Translation with GAN (1)]]></title><description><![CDATA[<div class="kg-card-markdown"><h1 id="problemstatementofimagetranslation">Problem statement of Image Translation</h1>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120608.jpg" alt="inline"></p>
<h3 id="learngsrightarrowt">Learn $G: (S \rightarrow T)$</h3>
<p>$G$ that convert an image of source domain $S$ to an image of target domain $T$<br>
Domain Adaptation/Transfer of an image.</p>
<h3 id="pairedandunpaireddataset">Paired and Unpaired dataset</h3>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpa9gc6l7bj30ds08ut9i.jpg" alt="inline"></p>
<h3 id="imagetranslationsandtarepairwiselabeledindataset">Image Translation: $S$ and $T$ are pair-wise labeled in dataset</h3>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120245.jpg" alt="inline"></p>
<h3 id="imagetranslationsandtarenotpairwisedindataset">Image Translation: $S$ and</h3></div>]]></description><link>http://tmmse.xyz/2018/04/01/image-translation-with-gan-1/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf76fd</guid><category><![CDATA[Computer Vision]]></category><category><![CDATA[Deep Learning]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Sun, 01 Apr 2018 08:05:56 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2018/04/horse2zebra.gif" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><h1 id="problemstatementofimagetranslation">Problem statement of Image Translation</h1>
<img src="http://tmmse.xyz/content/images/2018/04/horse2zebra.gif" alt="Image Translation with GAN (1)"><p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120608.jpg" alt="Image Translation with GAN (1)"></p>
<h3 id="learngsrightarrowt">Learn $G: (S \rightarrow T)$</h3>
<p>$G$ that convert an image of source domain $S$ to an image of target domain $T$<br>
Domain Adaptation/Transfer of an image.</p>
<h3 id="pairedandunpaireddataset">Paired and Unpaired dataset</h3>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fpa9gc6l7bj30ds08ut9i.jpg" alt="Image Translation with GAN (1)"></p>
<h3 id="imagetranslationsandtarepairwiselabeledindataset">Image Translation: $S$ and $T$ are pair-wise labeled in dataset</h3>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120245.jpg" alt="Image Translation with GAN (1)"></p>
<h3 id="imagetranslationsandtarenotpairwisedindataset">Image Translation: $S$ and $T$ are not pair-wised in dataset</h3>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-121602.jpg" alt="Image Translation with GAN (1)"></p>
<p><img src="https://github.com/junyanz/CycleGAN/raw/master/imgs/horse2zebra.gif" alt="Image Translation with GAN (1)"><br>
<img src="https://junyanz.github.io/CycleGAN/images/objects.jpg" alt="Image Translation with GAN (1)"></p>
<h1 id="beforestyletransferneuralartwasprominent">Before, Style Transfer (NeuralArt) was prominent</h1>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-120857.jpg" alt="Image Translation with GAN (1)"></p>
<p>But it largely depends on textual information of an target style</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-121911.jpg" alt="Image Translation with GAN (1)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-10-121932.jpg" alt="Image Translation with GAN (1)"></p>
<h1 id="howtolearnmoregeneralimagetranslation">How to learn more general Image Translation?</h1>
<h3 id="generativeadversarialnetwork">Generative Adversarial Network</h3>
<p><img src="https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/out.gif" alt="Image Translation with GAN (1)"><br>
<img src="https://cdn-images-1.medium.com/max/800/1*-gFsbymY9oJUQJ-A3GTfeg.png" alt="Image Translation with GAN (1)"><br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-03-30-070136.jpg" alt="Image Translation with GAN (1)"></p>
<p>\begin{equation}<br>
\min_G \max_D V(D,G) = \mathbf{E}_{x\sim p_{\text{data}}(x)} [\log D(x)] + \mathbf{E}_{z\sim p_{z}}(z)[\log (1 - D(G(z)))].<br>
\end{equation}</p>
<h3 id="alsohonorablementiondeepconvolutionalgandcgan">also honorable mention : Deep Convolutional GAN (DCGAN)</h3>
<h1 id="twomajorproblemsofimagetranslation">Two major problems of Image Translation</h1>
<ol>
<li>Convert to which domain?
<ul>
<li>learn which &quot;$G: (S \rightarrow T)$&quot;?</li>
</ul>
</li>
<li>How to learn the dataset?
<ul>
<li>how to properly form dataset?</li>
<li>pair-wise Supervised? or Unsupervised?</li>
</ul>
</li>
</ol>
<h3 id="todaypresentingsotaofimagetranslationpapersof">Today, presenting SOTA of Image Translation papers of</h3>
<ul>
<li><a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-2/">pix2pix</a>: Image-to-Image Translation with Conditional Adversarial Networks (CVPR2017)</li>
<li><a href="http://tmmse.xyz/2018/04/02/image-translation-with-gan-3/">Domain Transfer Network</a>: Unsupervised Cross-Domain Image Generation (ICLR2017)</li>
<li><a href="http://tmmse.xyz/2018/04/03/image-translation-with-gan-4/">CycleGAN &amp; DiscoGAN</a>: CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (ICCV2017) &amp; DiscoGAN: Learning to Discover Cross-Domain Relations with Generative Adversarial Networks (ICML2017)</li>
<li><a href="http://tmmse.xyz/2018/04/04/image-translation-with-gan-5/">BicycleGAN</a>: Toward Multimodal Image-to-Image Translation (NIPS2017)</li>
</ul>
</div>]]></content:encoded></item><item><title><![CDATA[Command Line Interface]]></title><description><![CDATA[<div class="kg-card-markdown"><p>dotfiles and materials available at <a href="https://github.com/junhocho/junhosetting">@junhocho</a><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<h1 id="guicui">GUI보다 좋은 CUI,</h1>
<p>CUI. Command Line Interface를 쓰시면 어디서든 쉽게, 인터넷만 있고, Terminal이나 Putty가 있으면 개발을 할 수 있다.</p>
<p><img src="https://www.teamviewer.com/resources/images/screenshots/win-mainwindow.png" alt="fit left"></p>
<p>이런 팀뷰어 없이도 말이다.</p>
<h1 id="cli">CLI. 그래서 어떻게?</h1>
<h3 id="tmuxvim">( Tmux + VIM ) 를 사용하자</h3>
<p><img src="https://lucaguidi.com/images/tmux-tdd.gif" alt="right bg"></p>
<p>CLI를 사용하는 이유.</p>
<ol>
<li>Portable (인터넷이 구리면 팀뷰어는 힘들지)</li>
<li>Simple</li>
<li>Automated (원하는 기능은 다</li></ol></div>]]></description><link>http://tmmse.xyz/2018/04/01/command-line-interface/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf76f4</guid><category><![CDATA[Linux]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Sun, 01 Apr 2018 05:15:57 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1515704089429-fd06e6668458?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=4e45131a3333adf14948887756627ace" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><img src="https://images.unsplash.com/photo-1515704089429-fd06e6668458?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=4e45131a3333adf14948887756627ace" alt="Command Line Interface"><p>dotfiles and materials available at <a href="https://github.com/junhocho/junhosetting">@junhocho</a><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<h1 id="guicui">GUI보다 좋은 CUI,</h1>
<p>CUI. Command Line Interface를 쓰시면 어디서든 쉽게, 인터넷만 있고, Terminal이나 Putty가 있으면 개발을 할 수 있다.</p>
<p><img src="https://www.teamviewer.com/resources/images/screenshots/win-mainwindow.png" alt="Command Line Interface"></p>
<p>이런 팀뷰어 없이도 말이다.</p>
<h1 id="cli">CLI. 그래서 어떻게?</h1>
<h3 id="tmuxvim">( Tmux + VIM ) 를 사용하자</h3>
<p><img src="https://lucaguidi.com/images/tmux-tdd.gif" alt="Command Line Interface"></p>
<p>CLI를 사용하는 이유.</p>
<ol>
<li>Portable (인터넷이 구리면 팀뷰어는 힘들지)</li>
<li>Simple</li>
<li>Automated (원하는 기능은 다 자동화 시킬 수 있다)</li>
<li>and <strong>Cool</strong> (남들이 보면 해커인줄 알꺼야)</li>
</ol>
<p>그리고 당신이</p>
<ul>
<li><strong>server</strong>가 있고 (노트북과 인터넷만 있으면 어딜가도 괜찮아)</li>
<li><strong>mac</strong>이 있고 (iTerm2는 편하고 이쁘니까)</li>
<li><strong>blogger</strong></li>
<li><strong>computer geek</strong></li>
</ul>
<p>답은 CLI.</p>
<p><img src="http://i.giphy.com/26AHM8fwpxlAWJdGE.gif" alt="Command Line Interface"><br>
드루와드루와</p>
<h3 id="thispresentationareinspiredbynicknisiandyoutubelink">This presentation are inspired by <a href="https://github.com/nicknisi/vim-workshop">@nicknisi</a> and <a href="https://www.youtube.com/watch?v=5r6yzFEXajQ">youtube link</a></h3>
<h1 id="coverage">Coverage</h1>
<ol>
<li>Ubuntu and ZSH (The OS 그리고 Shell)</li>
<li>Vim	(The editor)</li>
<li>Tmux (The workspace)</li>
</ol>
<p><a href="https://github.com/junhocho/junhosetting">공유할 만한 zsh, tmux, vim 설정 파일들, 클릭</a></p>
<h1 id="sshsecureshell"><strong>SSH</strong>: Secure Shell</h1>
<p><strong>Secure connection to server</strong></p>
<p>Host(server) IP : <strong><code>147.46.89.175</code></strong> (for ex)<br>
User ID	: <strong><code>bctjv-[yourname]</code></strong><br>
password : <strong><code>bctjv</code></strong></p>
<h1 id="login">Login !</h1>
<ul>
<li>Linux/macOS : <code>ssh bctjv-[yourname]@147.46.89.175</code></li>
<li>Windows : use <a href="http://www.putty.org">putty</a><br>
<img src="http://screenshots.en.sftcdn.net/en/scrn/20000/20678/putty-7.jpg" alt="Command Line Interface"></li>
</ul>
<h3 id="opensshssh">OpenSSH 가 서버에 설치 되어 있어야 ssh 접속 가능</h3>
<p><code>sudo apt-get install openssh-server</code></p>
<h3 id="ubuntulinux">Ubuntu (Linux)</h3>
<ul>
<li>One of Linux</li>
<li>Commonly used in Deep Learning libraries</li>
<li><a href="http://webdir.tistory.com/101">link about Directories  : tmp, root, var, usr, home, bin</a></li>
</ul>
<h3 id="simplecommands">Simple commands. 등등...</h3>
<p><code>mv</code> , <code>cp</code> , <code>rm</code> , <code>mkdir</code> , <code>rmdir</code><br>
<code>ls -lh</code>, <code>df -lh</code>, <code>du -sh ./*</code>, <code>whoami</code><br>
<code>cp *.py /path/to/dest/</code><br>
<code>rm -rf /다/지워/버리겠다/*.txt</code><br>
<code>sudo apt-get install tmux</code></p>
<p>이런 커맨드도 있음.</p>
<p><code>cowsay</code><br>
<code>cmatrix</code><br>
<a href="http://www.tecmint.com/20-funny-commands-of-linux-or-linux-is-fun-in-terminal/">funny commands</a></p>
<p><img src="https://www.dropbox.com/s/q3x64a5qa435hn3/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202016-11-08%2000.55.50.png?raw=1" alt="Command Line Interface"></p>
<h3 id="somesignals">Some Signals</h3>
<pre><code class="language-bash"># ==== Ex1) ====, $은 터미널에 쳐볼 것.
$ yes yell!
&lt;C-c&gt;   # Control - c. 돌아가고 있는 프로세스를 꺼버린다.
# ==== Ex2) ====
$ python -c '
c = 0
while True: print c; c= c+1'

&lt;C-z&gt;   # Control - z. 터미널상에서 잠깐 멈추고 백그라운드로!
$ fg    # 다시 아까 돌아가던 foreground로
# ==== Ex 3) ====
$ python
&lt;C-d&gt;   # Control - d. 아예 꺼버리기
</code></pre>
<h3 id="">파일 권한과 소유권</h3>
<ul>
<li>directory and file / <a href="http://brothernsister.tistory.com/27">permission, ownership</a></li>
</ul>
<p>다음 상황에서</p>
<p><img src="https://www.dropbox.com/s/kyqn1svnox8dtok/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202016-11-09%2020.46.06.png?raw=1" alt="Command Line Interface"></p>
<p>^<code>Workspace</code> : <code>drwxrwxr-x</code> : directory and has 775 permision<br>
<code>.zshrc</code> : <code>-rw-r--r--</code> : file and has 644 permision<br>
<code>hello.py</code> : <code>-rwxr-xr--</code> : file and has 754 permision</p>
<ul>
<li>change permssion : <code>$ chmod 755 hello.py</code> : 755는 이진법으로 111 101 101, 즉 rwxr-xr-x. 숫자 세개가 각 나, 그룹, 나머지를 뜻함. 그래서 나는 rwx (읽고 쓰고 실행 권한 있음), 그룸과 나머지는 r-x (읽고 실행 권한만 있음.)</li>
<li>change ownership : <code>$ chown userid hello.py</code></li>
</ul>
<p><code>sudo 어떤커맨드</code> : borrow sudo privilege and command</p>
<h3 id="executable">추가로) 쉘 스크립트를 <strong>executable</strong>하게 바꾸기.</h3>
<p>Write on top of shell script<br>
<code>#!bin/bash</code> if shell script,<br>
<code>#!bin/usr/python</code> if python script.<br>
<strong><code>chmod +x script</code></strong></p>
<p>then the script is executable as <strong><code>$ ./script</code></strong></p>
<h3 id="donotabusesudo">Do not abuse sudo 👎</h3>
<p>^permission이 없는 것은 sudo 명령어로 해결가능.<br>
하지만 함부로 사용 No<br>
linux를 root 계정을 사용하는 것은 안전하지 않음.<br>
보통 모든 유저에게 sudo 권한을 주진 않음.</p>
<h1 id="shell">Shell : 리눅스 터미널 소통 창구</h1>
<ul>
<li>Default shell is <code>bash shell</code> 그 유명한 배시쉘</li>
<li><code>bash shell</code> made in 1989 which is pretty <em>uncomfortable</em>. 불편해</li>
</ul>
<h1 id="zsh">그래서 <code>zsh</code>을 써보자</h1>
<ul>
<li>manage zsh with <a href="http://ohmyz.sh">http://ohmyz.sh</a></li>
</ul>
<p><img src="http://i.giphy.com/bmeGvNXq71kGs.gif" alt="Command Line Interface"></p>
<h1 id="whyzsh">Why ZSH?</h1>
<ol>
<li>smart autocomplete</li>
<li>highly customizable</li>
<li>Bundled functions, helpers, plugin, themes : ex) git</li>
</ol>
<h3 id="installzsh">Install zsh</h3>
<pre><code class="language-bash"># 지금은 안될 수도 있음! oh-my-zsh를 참고하자! http://ohmyz.sh
sudo apt-get install git # git 설치 ( 코드 개발 플랫폼 )
sudo apt-get install zsh # zsh 설치
cd # 홈폴도로 이동.
sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; # install
sudo chsh -s /usr/bin/zsh [your-id] # change your default shell into zsh
</code></pre>
<h3 id="gitclonejunhosettingfirst">커스터마이즈를 해보자! git clone <code>Junhosetting</code> first</h3>
<pre><code class="language-bash">cd # move to home folder
git clone https://github.com/junhocho/junhosetting #Star it
cd junhosetting
vi install.txt  # peek vim
</code></pre>
<h3 id="vi">엥? vi?</h3>
<p>특히 <strong>Vim</strong> : improved Vi. vi는 에디터. vi커맨드가 곧 vim임<br>
<em>Vim quit?</em> <s>[Alt F4]</s> 가 아닌 다음처럼</p>
<pre><code>:q 		# vim quit
:wq		# save and quit. identical to &lt;shift-ZZ&gt;
</code></pre>
<pre><code>1 # Zsh setup
2 zsh autojump - https://github.com/wting/autojump.git
3 zsh syntax highlighting - https://github.com/zsh-users/zsh-syntax-highlighting.git
4  
5
6 #alias-tips
7 $ cd ${ZSH_CUSTOM1:-$ZSH/custom}/plugins            # 커스텀 플러그인 폴더로 이동
8 $ git clone https://github.com/djui/alias-tips.git  # 저장소를 로컬로 복사
9 $ $EDITOR ~/.zshrc                                  # 에디터로 파일을 에디터로 불러들임
10 plugins=(git ... alias-tips) 추가
11 ...
</code></pre>
<h3 id="">더 해보자</h3>
<p><a href="https://github.com/zsh-users/zsh-syntax-highlighting.git">ZSH syntax highlight</a> : 맞는 커맨드면 초록색, 틀린 커맨드면 빨간색</p>
<pre><code class="language-bash">cd
# get source of zsh-syntax-highlighting
git clone https://github.com/zsh-users/zsh-syntax-highlighting.git
# put codes in .zshrc
echo &quot;source ${(q-)PWD}/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh&quot; &gt;&gt; ${ZDOTDIR:-$HOME}/.zshrc
# activate it now
source ./zsh-syntax-highlighting/zsh-syntax-highlighting.zsh

# this refreshes your .zshrc modofication to your current environment
source ~/.zshrc # dotfiles locate in home folder
</code></pre>
<p>It highlights if your command is valid</p>
<p>또 좋은 <a href="https://github.com/wting/autojump.git">autojump</a> : 내가 자주 가는 디렉토리로 j 커맨드로 빠르고 쉽게 이동.</p>
<h3 id="somemoreusefulcommands">some more useful commands</h3>
<pre><code class="language-bash">ls . | wc -l                        # how many files in this directory.
du -sh ./*                          # how big are folders and files in this directory
df -lh                              # list my Filesystem in human readable format
ln -s [target-folder] [make-link]   # make symbolic link
grep -nr --color &quot;keyword-to-find&quot; * # locate &quot;keyword-to-find&quot;

unzip some.zip                      # unzip
tar -xvf some.tar.gz                # untar
echo $CUDA_HOME                     # print some environment variable
wget https://goo.gl/ka7Yz5          # download file in web
wget http://www.ekn.kr/data/photos/20150938/art_1442387799.jpg # Download jpg image
python print-experiment-output.py 2&gt;&amp;1 | tee experiment.log # Store all print output in experiment.log
which python                        # useful to locate command

history                             # my command history
pstree                              # process tree
htop                                # system monitoring
nvidia-smi                          # gpu monitoring
who                                 # which users are online?
</code></pre>
<h2 id="alias">Alias</h2>
<ul>
<li>커맨드 단축해버리기!</li>
<li><code>alias wn=&quot;watch nvidia-smi&quot;</code> 을 .zshrc에 넣어놓으면. 앞으로 <code>wn</code>으로 <code>watch nvidia-smi</code>를 빠르게 실행 가능.</li>
</ul>
<h3 id="youdonthavetimetotypegrepnrcolorkeywordtofind">You don't have time to type : <code>grep -nr --color &quot;keyword-to-find&quot; *</code></h3>
<ul>
<li>put your alias at the end of <code>.zshrc</code></li>
</ul>
<pre><code class="language-bash">cd
cat junhosetting/alias &gt;&gt; ~/.zshrc # Copy paste alias to end of .zshrc
tail ~/.zshrc # print some lines of the end of .zshrc
source ~/.zshrc # refresh this environment

grepn &quot;key-word-to-find&quot; * # Check the alias working
</code></pre>
<h2 id="port">Port</h2>
<p>Well know ports : <code>22 : SSH</code> / <code>80 : HTTP</code> / <code>3389 : Window rdp</code> ...<br>
<a href="https://github.com/junhocho/junhosetting/blob/master/portforward">Portforward</a> from CLIENT to SERVER : <code>ssh -L localhost:$2:localhost:$2 $1</code></p>
<pre><code class="language-bash"># Ex) Do this at SERVER.
Jupyter notebook --no-browser --port 8888 # random ports 8888
# Do this at CLIENT
ssh -L localhost:8888:localhost:8888 junho@147.46.89.175
# Open web-browser in CLIENT and connect to `localhost:8888`
</code></pre>
<p><a href="http://tmmse.xyz/vnc-setup/">VNC (GUI)</a> :  Use GUI safely with SSH</p>
<h2 id="scpmovefilebetweenclientandserver"><strong>SCP:</strong> Move file between client and Server</h2>
<p>use it in CLIENT (unix) terminal.<br>
because Client : 유동 ip / Server : 고정 ip</p>
<pre><code class="language-bash">scp /client/A host:~/path/dest/B
# Client의 파일 A를 host의 경로 B로 옮긴다.
scp host:~/path/dest/A /client/B
# Host의 파일 A를 Client의 B로 옮긴다.
scp -r junho@147.46.89.175:~/Downloads/WordCount ~/Workspace/
# -r 은 재귀적으로. 서버의 WordCount 디렉토리와 안의
# 모든 파일을 내 client의 Workspace의 복사
</code></pre>
<p>Windows는 WinSCP</p>
<p><img src="https://winscp.net/pad/screenshot.png" alt="Command Line Interface"></p>
<h3 id="killingprocess">Killing Process</h3>
<pre><code class="language-bash">$ python -c '
c = 0
while True: print c; c= c+1'

&lt;C-z&gt; # go to sleep and background.
$ ps aux | grep python # print process that contains `python`
</code></pre>
<p><img src="https://www.dropbox.com/s/6iot6o239wp10z6/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202016-11-08%2001.46.20.png?raw=1" alt="Command Line Interface"></p>
<p><strong><code>kill -9 [process-id]</code></strong> : 356 in this case</p>
<p>process killed!</p>
<h1 id="2vim">2. Vim</h1>
<p><img src="http://www.unixstickers.com/image/cache/data/stickers/vim/vim.sh-600x600.png" alt="Command Line Interface"></p>
<p>유명한 IDE들,<br>
<img src="http://cdn03.androidauthority.net/wp-content/uploads/2015/08/ASFeaturedPic.png" alt="Command Line Interface"><br>
Visua Studio , Eclipse, Webstorm, Android studio, Pycharm, ZeroBaneStudio는 좋지만 client일때 좋음. server에서 사용하기는 힘들다.</p>
<h3 id="sshvim">SSH에서는 VIM만큼 편한게 없다!</h3>
<h2 id="vim">Vim</h2>
<ul>
<li>Vim : improved Vi.</li>
<li>probably already installed in your system</li>
<li>Highly customizable</li>
<li>setting file : <a href="https://github.com/junhocho/junhosetting/blob/master/vimrc#L19">.vimrc</a></li>
</ul>
<h3 id="modalediting">Modal editing</h3>
<p><img src="http://d.pr/i/15nA1+" alt="Command Line Interface"></p>
<ul>
<li>Normal - navigate the structure of the file <code>&lt;esc&gt;</code></li>
<li>Insert - editing the file <code>i, o, a, s, ...</code></li>
<li>Visual - highlight portions of the file to manipulate at once <code>v, shift-v, ctrl-v</code></li>
<li>Ex - command mode <code>:, /, ?</code></li>
</ul>
<h3 id="vim">vim에서 마우스는 사용할 필요가 없다.</h3>
<h3 id="hjkl"><code>h j k l</code></h3>
<h3 id="">⬅️ ⬇️ ⬆️ ➡️</h3>
<h2 id="12jmovedown12tiemsnumbersaffect"><code>12j</code> : move down 12 tiems, numbers affect</h2>
<ul>
<li><code>:29</code> : move to line 29</li>
<li><code>:set number</code> : show line numbers</li>
<li><code>:set nonumber</code> : <code>no</code> usually the opposite of command</li>
</ul>
<p>그외에</p>
<ul>
<li><code>^e</code> - scroll the window down</li>
<li><code>^y</code> - scroll the window up</li>
<li><code>^f</code> - scroll down one page</li>
<li><code>^b</code> - scroll up one page</li>
<li><code>H</code> - move cursor to the top of the window</li>
<li><code>M</code> - move cursor to the middle of the window</li>
<li><code>L</code> - move cursor to the bottom of the window</li>
<li><code>zz</code> : move window so that my cursor is at center</li>
<li><code>u</code> : undo</li>
<li><code>^r</code> : redo</li>
<li><code>:w</code> : save</li>
<li><code>:q</code> : quit</li>
<li><code>:wq</code> : save and quit</li>
<li><code>gg</code> - go to top of file</li>
<li><code>G</code> - go to bottom of file</li>
</ul>
<p><img src="https://i.stack.imgur.com/7Cu9Z.jpg" alt="Command Line Interface"><br>
vim의 learning curve.... 입문부터 고수까지 빡셈... 하지만 익숙하면 너무 편하다.</p>
<p>다음의 중요한 3 요소</p>
<h3 id="textobjects">text objects</h3>
<ul>
<li><code>w</code> - words</li>
<li><code>s</code> - sentences</li>
<li><code>p</code> - paragraphs</li>
</ul>
<h3 id="motions">Motions</h3>
<ul>
<li><code>a</code>- all</li>
<li><code>i</code> - in</li>
<li><code>t</code> - 'til</li>
<li><code>f</code> - find forward</li>
<li><code>F</code> - find backward</li>
</ul>
<h3 id="commands">Commands</h3>
<ul>
<li><code>d</code> - delete (also cut)</li>
<li><code>c</code> - change (delete, then place in insert mode)</li>
<li><code>y</code> - yank (copy)</li>
<li><code>v</code> - visually select</li>
</ul>
<p>이것들로</p>
<h2 id="commandtextobjectormotion"><code>{command}{text object or motion}</code> 조합으로 아주 편리하게 사용가능하다.</h2>
<h2 id="diwdeleteinword"><code>diw</code> : delete in word</h2>
<h2 id="cawchangeallword"><code>caw</code> : change all word</h2>
<h2 id="yiyankalltextinsideparentheses"><code>yi)</code> : yank all text inside parentheses</h2>
<h2 id="vavisuallyselectallinsidedoublequotesincludingdoublequotes"><code>va&quot;</code> : visually select all inside doublequotes including doublequotes</h2>
<p>방금 한 커맨드 또 하는 건 The DOT command <code>.</code>로 된다.</p>
<h2 id="additionalcommands">Additional commands</h2>
<ul>
<li><code>dd</code> / <code>yy</code> - delete/yank the current line</li>
<li><code>D</code> / <code>C</code> - delete/change until end of line</li>
<li><code>^</code> / <code>$</code> - move to the beginning/end of line</li>
<li><code>I</code> / <code>A</code> - move to the beginning/end of line and insert</li>
<li><code>o</code> / <code>O</code> - insert new line above/below current line and insert</li>
<li><code>J</code> - delete line break. pretty useful</li>
<li><code>p</code> - paste</li>
<li><code>*</code> on word - find all this words.</li>
<li><code>^a</code> on number - increment</li>
</ul>
<h2 id="vimmacro">근데 vim의 포텐에는 끝이 없다: Macro</h2>
<p>A sequence of commands recorded to a register</p>
<h3 id="recordamacro">Record a macro</h3>
<ul>
<li><code>q{register}</code></li>
<li>(do the things)</li>
<li><code>q</code></li>
</ul>
<h3 id="playamacro">Play a macro</h3>
<ul>
<li><code>@{register}</code></li>
</ul>
<h3 id="marking">텍스트에서 지금 커서의 위치를 marking해놓고 돌아오는게 가능</h3>
<ul>
<li><code>mk</code> : mark position at <code>k</code></li>
</ul>
<pre><code> `k : move cursor to position k
</code></pre>
<hr>
<h2 id="exmode">Ex mode에서 가능한 편리한 커맨드들</h2>
<ul>
<li><code>/someword</code> : find <code>someword</code></li>
<li><code>:spl</code> / <code>:vspl</code> : split (horizontal or vertical) vim pane</li>
<li><code>^w + w/arrow</code> : move pane in vim</li>
<li><code>:e.</code> : file tree</li>
<li><code>:%s/foo/bar/g</code> : replace all <code>foo</code> to <code>bar</code></li>
</ul>
<h3 id="vimrc">: 로 시작하는 것들은 vimrc에서 초기 명령어로도 사용 가능.</h3>
<ul>
<li><code>:syntax on</code> : syntax on</li>
<li><code>:set paste</code> : paste mode. Useful when using vim <code>indent</code> option</li>
<li><code>:set cursorline</code> : show cursor with underline</li>
</ul>
<p>vimrc는 vim의 세팅 파일.</p>
<pre><code class="language-bash">cp ~/junhosetting/vimrc ~/.vimrc
</code></pre>
<h2 id="pluginsinstallwhatyouneedvim">Plugins : install what you need. vim 편리의 끝판왕</h2>
<ul>
<li><a href="https://github.com/gmarik/vundle">vundle</a> - plugin manager</li>
<li><a href="https://github.com/scrooloose/nerdtree">nerdtree</a> - file drawer</li>
<li><a href="https://github.com/kien/ctrlp.vim">ctrlp</a> - fuzzy file finder</li>
<li><a href="https://github.com/tpope/vim-fugitive">fugitive</a> - git tool</li>
<li><a href="https://github.com/scrooloose/syntastic">syntastic</a> - syntax checker / linter</li>
</ul>
<h2 id="vimcustomizable">Vim은 엄청나게 <strong>customizable</strong>하다.</h2>
<p><img src="http://d.pr/i/R5c+" alt="Command Line Interface"></p>
<h3 id="notanideitsvim">Not an IDE. It's <strong>VIM</strong></h3>
<p>Install Vundle with plugins</p>
<pre><code class="language-bash">cd
git clone https://github.com/gmarik/Vundle.vim.git ~/.vim/bundle/Vundle.vim
# clone in ~/.vim/bundle/ and setup your vimrc with plugins
vi # then do :PluginInstall
# Test :NERDTree with &lt;F2&gt; in vim
# Test if autocomplete works
</code></pre>
<p><img src="https://www.dropbox.com/s/w61ntwzc452e0ia/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202016-11-10%2002.49.25.png?raw=1" alt="Command Line Interface"></p>
<h3 id="ctagssourcecodeinspection">ctags : Source code inspection</h3>
<pre><code class="language-bash">sudo apt-get install ctags
cd your-project
ctags -R . # will produce `tags` in the folder
vi code.cpp
&lt; C - t &gt; # to go inside declration
&lt; C - ] &gt; # to go back
</code></pre>
<h1 id="3tmuxterminalmultiplexer">3. Tmux : Terminal Multiplexer</h1>
<h3 id="vimtmux">vim + tmux</h3>
<p><img src="http://d.pr/i/R5c+" alt="Command Line Interface"></p>
<h3 id="whytmux">Why Tmux?</h3>
<ul>
<li>Don't want to stop your SSH session.
<ol>
<li>Experiment session</li>
<li>Downloading big file</li>
<li>web-server</li>
</ol>
</li>
<li>Extendable Workspace</li>
<li>And Maintain your session</li>
<li>Co-operation</li>
</ul>
<pre><code class="language-bash">tmux new -s [session-name]	  # create new session
tmux ls                       # list existing session
tmux attch -t [session-name]	# attach existing session
</code></pre>
<p><code>tmux new -s py-practice</code><br>
<strong><code>&lt;C-b&gt;</code></strong> : Tmux binding-key.</p>
<ul>
<li>will change it to <strong><code>&lt;C-a&gt;</code></strong> unless you have flexible finger bone</li>
</ul>
<p><code>&lt;C-b&gt; c</code> , <strong><code>bind c</code></strong> : New window<br>
<strong><code>bind d</code></strong> : dettach from working session</p>
<h3 id="letsconfiguretmux">Let's configure Tmux.</h3>
<ul>
<li><a href="http://www.haruair.com/blog/2124">ref1</a>, <a href="https://blog.outsider.ne.kr/699">ref2</a></li>
<li><a href="https://github.com/junhocho/junhosetting/blob/master/tmux.conf">.tmux.conf</a></li>
</ul>
<pre><code class="language-bash">cp ~/junhosetting/tmux.conf ~/.tmux.conf  #  dotfile로 이름바꿔서 복붙
tmux source .tmux.conf    # If you have working tmux session
# Now your Tmux looks better and &lt;C-a&gt; is the binding-key
</code></pre>
<p><strong><code>tmux new -s [session-name]</code></strong></p>
<p><code>bind c</code> : new window<br>
<code>bind n</code> : Next window<br>
<code>bind p</code> : Previous window<br>
<code>bind %</code> : vertical split pane<br>
<code>bind &quot;</code> : horizontal split pane<br>
<code>bind h(jkl, arrow)</code> : move my cursor pane to pane<br>
<code>bind [</code> : copy-mode / <code>q</code> : copy-mode exit<br>
<code>exit</code> / <code>&lt;C-d&gt;</code> : quit pane</p>
<h3 id="synchronizepanes">Synchronize-panes</h3>
<p><code>bind :setw synchronize-panes</code></p>
<p><img src="http://d.pr/i/vRVt+" alt="Command Line Interface"></p>
<h3 id="">화면 쪼개기!</h3>
<p><code>bind %</code> and <code>bind &quot;</code></p>
<p><img src="http://d.pr/i/12gE5+" alt="Command Line Interface"></p>
<h1 id="">결론</h1>
<h3 id="linuxgittmuxvim">답은 linux + GIT + tmux + vim</h3>
<p>참고할만한  dotfiles들</p>
<ul>
<li><a href="https://github.com/junhocho/junhosetting">junhocho/dotfiles</a></li>
<li><a href="https://github.com/nicknisi/dotfiles">nicknisi/dotfiles</a></li>
<li><a href="https://github.com/bryanforbes/dotfiles">bryanforbes/dotfiles</a></li>
<li><a href="https://github.com/jason0x43/dotfiles">jason0x43/dotfiles</a></li>
</ul>
<p>dotfiles and materials available at <a href="https://github.com/junhocho/junhosetting">@junhocho</a><sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Inspired from <a href="https://github.com/nicknisi/vim-workshop">@nicknisi</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div>]]></content:encoded></item><item><title><![CDATA[PaletteNet]]></title><description><![CDATA[<div class="kg-card-markdown"><p><strong>Junho Cho</strong>, Sangdoo Yun, Kyoungmu Lee and Jin Young Choi, <strong>PaletteNet: Image Recolorization with Given Color Palette</strong> <em>The IEEE Conference on Computer Vision and Pattern Recognition</em> <strong>(CVPR) Workshops</strong>, <em>NITRE2017</em>, July, 2017.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-07-26-NTIRE_junho.png" alt=""></p>
<p><a href="https://www.dropbox.com/s/66ynt8dcs7cw0e0/NTIRE_junho.pdf?dl=0">poster better resolution</a></p>
<p><a href="https://www.dropbox.com/s/7cw0xxc1yl81ug8/11.pdf?dl=0">paper</a></p>
</div>]]></description><link>http://tmmse.xyz/2017/07/27/palettenet/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf76fe</guid><category><![CDATA[Deep Learning]]></category><category><![CDATA[Computer Vision]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Wed, 26 Jul 2017 23:54:21 GMT</pubDate><media:content url="http://tmmse.xyz/content/images/2017/07/img-palette-pairs2.jpg" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><img src="http://tmmse.xyz/content/images/2017/07/img-palette-pairs2.jpg" alt="PaletteNet"><p><strong>Junho Cho</strong>, Sangdoo Yun, Kyoungmu Lee and Jin Young Choi, <strong>PaletteNet: Image Recolorization with Given Color Palette</strong> <em>The IEEE Conference on Computer Vision and Pattern Recognition</em> <strong>(CVPR) Workshops</strong>, <em>NITRE2017</em>, July, 2017.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-07-26-NTIRE_junho.png" alt="PaletteNet"></p>
<p><a href="https://www.dropbox.com/s/66ynt8dcs7cw0e0/NTIRE_junho.pdf?dl=0">poster better resolution</a></p>
<p><a href="https://www.dropbox.com/s/7cw0xxc1yl81ug8/11.pdf?dl=0">paper</a></p>
</div>]]></content:encoded></item><item><title><![CDATA[Home Automation memo]]></title><description><![CDATA[<div class="kg-card-markdown"><h1 id="">중요!</h1>
<p><a href="https://www.qoo10.com/item/XIAOMI-%EC%95%8C%EB%9C%B0%EC%83%81%ED%92%88-%EC%83%A4%EC%98%A4%EB%AF%B8-%EB%A9%80%ED%8B%B0%ED%83%AD-%EB%AA%A8%EC%9D%8C%EC%A0%84-%EC%83%A4%EC%98%A4%EB%AF%B8-5%EA%B5%AC-%EB%A9%80%ED%8B%B0%ED%83%AD-%EC%83%A4%EC%98%A4%EB%AF%B8-USB-%EB%A9%80%ED%8B%B0%ED%83%AD-%ED%99%94%EC%9D%B4%ED%8A%B8-%EB%B8%94%EB%9E%99-%EC%83%A4%EC%98%A4%EB%AF%B8-%EC%8A%A4%EB%A7%88%ED%8A%B8-%EC%86%8C%EC%BC%93-%EC%83%A4%EC%98%A4%EB%AF%B8/458021699">샤오미 멀티탭 특가</a></p>
<p><img src="https://www.dropbox.com/s/7etu4n28x9t6k75/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202017-01-28%2001.44.26.png?raw=1" alt=""></p>
<h1 id="">예산</h1>
<ul>
<li>조명
<ul>
<li>필립스 휴 2 + 브릿지 = 90</li>
<li><strong>샤오미 스탠드 = 28.1 ( 37.5 - 5.9 () - 3.5 (PAYCO) )</strong> (Qoo10. 그전에 G9가 39로 젤쌌음)</li>
<li>샤오미 전구 RGBW 26 중고나라 대리구매 (32.6  / 중고나라 20)</li>
<li><s>샤오미 전구 W 17 + 10(배송비)</s></li>
</ul>
</li>
<li>음성인식
<ul>
<li>에코닷2 = 70</li>
<li><strong>구글홈 = 160</strong></li></ul></li></ul></div>]]></description><link>http://tmmse.xyz/2017/07/23/home-automation-memo/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf76fc</guid><category><![CDATA[home automation]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Sat, 22 Jul 2017 20:36:41 GMT</pubDate><content:encoded><![CDATA[<div class="kg-card-markdown"><h1 id="">중요!</h1>
<p><a href="https://www.qoo10.com/item/XIAOMI-%EC%95%8C%EB%9C%B0%EC%83%81%ED%92%88-%EC%83%A4%EC%98%A4%EB%AF%B8-%EB%A9%80%ED%8B%B0%ED%83%AD-%EB%AA%A8%EC%9D%8C%EC%A0%84-%EC%83%A4%EC%98%A4%EB%AF%B8-5%EA%B5%AC-%EB%A9%80%ED%8B%B0%ED%83%AD-%EC%83%A4%EC%98%A4%EB%AF%B8-USB-%EB%A9%80%ED%8B%B0%ED%83%AD-%ED%99%94%EC%9D%B4%ED%8A%B8-%EB%B8%94%EB%9E%99-%EC%83%A4%EC%98%A4%EB%AF%B8-%EC%8A%A4%EB%A7%88%ED%8A%B8-%EC%86%8C%EC%BC%93-%EC%83%A4%EC%98%A4%EB%AF%B8/458021699">샤오미 멀티탭 특가</a></p>
<p><img src="https://www.dropbox.com/s/7etu4n28x9t6k75/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202017-01-28%2001.44.26.png?raw=1" alt=""></p>
<h1 id="">예산</h1>
<ul>
<li>조명
<ul>
<li>필립스 휴 2 + 브릿지 = 90</li>
<li><strong>샤오미 스탠드 = 28.1 ( 37.5 - 5.9 () - 3.5 (PAYCO) )</strong> (Qoo10. 그전에 G9가 39로 젤쌌음)</li>
<li>샤오미 전구 RGBW 26 중고나라 대리구매 (32.6  / 중고나라 20)</li>
<li><s>샤오미 전구 W 17 + 10(배송비)</s></li>
</ul>
</li>
<li>음성인식
<ul>
<li>에코닷2 = 70</li>
<li><strong>구글홈 = 160 —&gt; 155</strong></li>
<li><strong>라즈베리파이 = 40 (3)</strong>  (중고나라)</li>
<li><strong>USB 마이크 = 29.9</strong> (쿠팡이 최저가)</li>
<li><strong>sd 카드 = 8.3 x2</strong></li>
</ul>
</li>
<li>제품
<ul>
<li><strong>스위처 35.5</strong>  (<a href="https://www.switcher.kr">제품링크</a> , <a href="https://storyfunding.daum.net/project/11696">펀딩</a> 을 통한 영구구매로 가능. 본래 렌탈방식)</li>
<li>
<img src="https://ww2.sinaimg.cn/large/006tNbRwgy1fc5l54ml8wj30ru0vmq5j.jpg" style="width: 300px;">
</li>
<li>샤오미 Wifi 스마트 멀티탭 6구  18</li>
</ul>
</li>
<li>그외에 …
<ul>
<li>규조토 매트 17.5</li>
<li>건조대 28.9</li>
<li>화장실 선반 11.6</li>
<li>부직포 7.36</li>
<li>4포트 충전기 9.74</li>
<li>LED 벽시계 28.9</li>
<li>가구
<ul>
<li>게이트레그 <a href="http://www.11st.co.kr/product/SellerProductDetail.tmall?method=getSellerProductDetail&amp;prdNo=1598863362&amp;NaPm=ct=iy7io608%7Cci=934d3c0901dfbef225d073df41549008d7410a07%7Ctr=sls%7Csn=17703%7Chk=f18e556d7a1e0b0c1015528cd143bed13c0a7670&amp;utm_term=&amp;utm_campaign=-&amp;utm_source=%B3%D7%C0%CC%B9%F6_PCS&amp;utm_medium=%B0%A1%B0%DD%BA%F1%B1%B3">11번가</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="task">Task</h2>
<ol>
<li>조명</li>
<li>에어콘, 티비 컨트롤</li>
<li>Chromecast Netflix, google music</li>
</ol>
<h1 id="">배치</h1>
<h2 id="">전원</h2>
<p>4구 멀티탭 2개<br>
3구 긴거, 짧은거<br>
2구</p>
<h3 id="2">책상 2</h3>
<ul>
<li>4구 1
<ul>
<li>공유기</li>
<li>컴퓨터</li>
<li>모니터</li>
<li>스탠드</li>
</ul>
</li>
<li>2구
<ul>
<li>티비</li>
<li>3구
<ul>
<li>4 포트 USB 충전기</li>
<li>스피커</li>
<li>맥북</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="">천장</h3>
<ul>
<li>4구
<ul>
<li>헤어드라이기</li>
<li>공유기</li>
<li>천장 조명</li>
<li>충전기 / 청소기</li>
</ul>
</li>
</ul>
<h3 id="2">현관 2</h3>
<ul>
<li>냉장고</li>
<li>3구
<ul>
<li>무드등</li>
<li>시계</li>
<li>전자렌지 / 라면포트 /라즈베리파이</li>
</ul>
</li>
</ul>
<h1 id="">조사</h1>
<p><a href="http://blog.amaorche.com/37">http://blog.amaorche.com/37</a></p>
<p>스마트 전구 특집 <a href="https://brunch.co.kr/@yyja/248">https://brunch.co.kr/@yyja/248</a></p>
<p>형광등 갈아끼기<br>
소켓 <a href="https://brunch.co.kr/@yyja/242">https://brunch.co.kr/@yyja/242</a><br>
<a href="http://auto.danawa.com/news/?Work=detail&amp;no=3214220">http://auto.danawa.com/news/?Work=detail&amp;no=3214220</a><br>
<a href="http://jejuin.tistory.com/1812">http://jejuin.tistory.com/1812</a><br>
<a href="http://duga.tistory.com/1963">http://duga.tistory.com/1963</a><br>
<a href="http://btsweet.blogspot.kr/2016/05/install-led-light.html">http://btsweet.blogspot.kr/2016/05/install-led-light.html</a><br>
<a href="http://www.cnet.co.kr/view/91655">http://www.cnet.co.kr/view/91655</a></p>
<h2 id="23">스타터 킷 23만</h2>
<p>브릿지 7만 전구 8~9만 ?휴 램프 설치는 간단하다. 일반 백열전구(E26, 26mm) 규격에 소비전력이 10W 이상인 전구 소켓만 있으면 된다.<br>
<a href="http://www.cnet.co.kr/view/91655">http://www.cnet.co.kr/view/91655</a></p>
<p><a href="http://www.clien.net/cs2/bbs/board.php?bo_table=use&amp;wr_id=640706">http://www.clien.net/cs2/bbs/board.php?bo_table=use&amp;wr_id=640706</a></p>
<p>스탠드. 샤오미 미지아.</p>
<h2 id="">샤오미 에코 연동</h2>
<p><a href="https://www.youtube.com/watch?v=23DHbgFc89g">https://www.youtube.com/watch?v=23DHbgFc89g</a><br>
<a href="https://www.youtube.com/watch?v=18fD6zXEmno">https://www.youtube.com/watch?v=18fD6zXEmno</a><br>
<a href="https://www.youtube.com/watch?v=GlTucBuikCc">https://www.youtube.com/watch?v=GlTucBuikCc</a><br>
<a href="https://www.youtube.com/watch?v=AJ2vFY-vRsE">https://www.youtube.com/watch?v=AJ2vFY-vRsE</a><br>
a brief introduction to the Smart Home...</p>
<p>Using the Amazon Echo and the Energenie MiHome Gateway you are now able to control your home with the use of your voice.</p>
<p>Energenie - <a href="https://energenie4u.co.uk">https://energenie4u.co.uk</a></p>
<p>리모콘 삼만오천<br>
<a href="https://directjapan.qoo10.com/item/XIAOMI-XIAOMI-%EC%83%A4%EC%98%A4%EB%AF%B8-%EB%A7%8C%EB%8A%A5-%EB%A6%AC%EB%AA%A8%EC%BB%A8-%EC%BB%A8%ED%8A%B8%EB%A1%A4%EB%9F%AC-%EC%9B%90%EA%B2%A9%EC%A0%9C%EC%96%B4-%EB%AF%B8%ED%8B%B0%EB%B9%84-%EC%83%A4%EC%98%A4%EB%AF%B8-TV-%EC%8A%A4%EB%A7%88%ED%8A%B8-%ED%99%88/441078519">https://directjapan.qoo10.com/item/XIAOMI-XIAOMI-샤오미-만능-리모컨-컨트롤러-원격제어-미티비-샤오미-TV-스마트-홈/441078519</a><br>
리뷰 - <a href="http://blog.naver.com/twophase/220790203872">http://blog.naver.com/twophase/220790203872</a></p>
<p>멀티탭</p>
<p><a href="https://directjapan.qoo10.com/item/XIAOMI-%EC%95%8C%EB%9C%B0%EC%83%81%ED%92%88-%EC%83%A4%EC%98%A4%EB%AF%B8-%EB%A9%80%ED%8B%B0%ED%83%AD-%EB%AA%A8%EC%9D%8C%EC%A0%84-%EC%83%A4%EC%98%A4%EB%AF%B8-5%EA%B5%AC-%EB%A9%80%ED%8B%B0%ED%83%AD-%EC%83%A4%EC%98%A4%EB%AF%B8-USB-%EB%A9%80%ED%8B%B0%ED%83%AD-%ED%99%94%EC%9D%B4%ED%8A%B8-%EB%B8%94%EB%9E%99-%EC%83%A4%EC%98%A4%EB%AF%B8-%EC%8A%A4%EB%A7%88%ED%8A%B8-%EC%86%8C%EC%BC%93-%EC%83%A4%EC%98%A4%EB%AF%B8/458021699">https://directjapan.qoo10.com/item/XIAOMI-알뜰상품-샤오미-멀티탭-모음전-샤오미-5구-멀티탭-샤오미-USB-멀티탭-화이트-블랙-샤오미-스마트-소켓-샤오미/458021699</a></p>
<p>qoo10</p>
<p><a href="https://directjapan.qoo10.com/item/XIAOMI-%EC%83%A4%EC%98%A4%EB%AF%B8-%EC%8A%A4%EB%A7%88%ED%8A%B8-%ED%99%88-%EC%84%B8%ED%8A%B8-%EC%95%8C%EB%9C%B0%EC%83%81%ED%92%88-%EC%8A%A4%EB%A7%88%ED%8A%B8-%EB%B0%A9%EB%B2%94-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EC%8A%A4%EB%A7%88%ED%8A%B8%ED%99%88-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EC%83%A4%EC%98%A4%EB%AF%B8-%EC%8A%A4%EB%A7%88%ED%8A%B8-%ED%99%88-%ED%82%A4%ED%8A%B8-%EA%B2%8C%EC%9D%B4%ED%8A%B8%EC%9B%A8%EC%9D%B4-%EB%8F%84%EC%96%B4%EA%B0%90%EC%A7%80%EC%84%BC%EC%84%9C/454601613?stcode=77">https://directjapan.qoo10.com/item/XIAOMI-샤오미-스마트-홈-세트-알뜰상품-스마트-방범-시스템-스마트홈-시스템-샤오미-스마트-홈-키트-게이트웨이-도어감지센서/454601613?stcode=77</a></p>
<h2 id="tasker">Tasker</h2>
<h1 id="">교육</h1>
<p><img src="http://ideafactory.snu.ac.kr/uploads/ckeditor/pictures/285/content___16.jpg" alt=""></p>
<h1 id="raspberrypiblexiamiyeelight">Raspberry Pi - BLE Xiami Yeelight</h1>
<p><a href="https://www.youtube.com/watch?v=V0TJXxDzO_4">https://www.youtube.com/watch?v=V0TJXxDzO_4</a><br>
If you able to use Wireshark well, Yes.</p>
<p><a href="http://www.guillier.org/blog/2015/04/reverse-engineering-of-a-ble-bulb/">http://www.guillier.org/blog/2015/04/reverse-engineering-of-a-ble-bulb/</a></p>
<p><a href="https://www.youtube.com/watch?v=UhrUUW4zATY">https://www.youtube.com/watch?v=UhrUUW4zATY</a></p>
<p>리모트 컨트롤</p>
<p>로지텍하모니허브<br>
<a href="https://www.reddit.com/r/amazonecho/comments/4dltfw/opinion_what_is_the_best_ir_blaster_device_to_use/">https://www.reddit.com/r/amazonecho/comments/4dltfw/opinion_what_is_the_best_ir_blaster_device_to_use/</a></p>
<p>Alexa 그냥 <a href="http://lifehacker.com/how-to-build-your-own-amazon-echo-with-a-raspberry-pi-1787726931">http://lifehacker.com/how-to-build-your-own-amazon-echo-with-a-raspberry-pi-1787726931</a><br>
<a href="http://lifehacker.com/build-a-raspberry-pi-powered-diy-amazon-echo-1762678229">http://lifehacker.com/build-a-raspberry-pi-powered-diy-amazon-echo-1762678229</a></p>
<p><a href="https://www.raspberrypi.org/blog/amazon-echo-homebrew-version/">https://www.raspberrypi.org/blog/amazon-echo-homebrew-version/</a></p>
<h1 id="raspberrypi">Raspberry Pi</h1>
<p><a href="http://jhcompany.tistory.com/1">http://jhcompany.tistory.com/1</a></p>
<p><a href="https://github.com/alexa/alexa-avs-sample-app/issues/2">1B+ 모델에서 가능하게 하기</a></p>
<h3 id="raspbianos">RaspbianOS (비추)</h3>
<ol>
<li>
<p>SD-card format with SDformatter</p>
<ul>
<li><a href="https://www.sdcard.org/downloads/formatter_4/eula_mac/index.html">https://www.sdcard.org/downloads/formatter_4/eula_mac/index.html</a></li>
<li>그리고 Mac에 꽂아</li>
</ul>
</li>
<li>
<p>Rasbian 취향대로 다운</p>
<ul>
<li><a href="https://www.raspberrypi.org/downloads/raspbian/">download</a></li>
</ul>
</li>
<li>
<p><a href="https://www.raspberrypi.org/documentation/installation/installing-images/mac.md">Mac 설치 법</a> 따라서 CLI로 설치.</p>
<ul>
<li>꽤 걸리는데 Ctrl-T로 확인 가능.</li>
<li><img src="https://www.dropbox.com/s/y2dzvljvygk9jwz/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202017-01-13%2000.06.08.png?raw=1" alt=""></li>
</ul>
</li>
<li>
<p>Rasberry Pi에 꼽고 확인</p>
</li>
</ol>
<h3 id="noobs">NOOBS (추천)</h3>
<p>를 받아서 sd카드에 복붙 후 부팅.</p>
<ol>
<li><code>sudo raspi-config</code></li>
<li><code>sudo apt-get install tightvncserver</code></li>
<li>
<pre><code class="language-bash">sudo apt-get install xserver-xorg-core xserver-xorg-input-all \
xserver-xorg-video-fbdev libx11-6 x11-common \
x11-utils x11-xkb-utils x11-xserver-utils xterm lightdm openbox
</code></pre>
</li>
</ol>
<h1 id="googlehome">Google Home</h1>
<p>Nexus5X 에서 Google Assistant 가능.<br>
<a href="http://lifehacker.com/how-to-get-google-assistant-on-any-phone-running-androi-1787706402">http://lifehacker.com/how-to-get-google-assistant-on-any-phone-running-androi-1787706402</a></p>
<p>구글홈 - 넥서스5 - 여러앱 - 넷플릭스<br>
<a href="https://www.youtube.com/watch?v=whb2xS9dIjM">https://www.youtube.com/watch?v=whb2xS9dIjM</a></p>
<p><a href="http://www.androidpolice.com/2016/08/20/psa-can-control-youtube-playback-google-now/">Google Now - Chromecast</a></p>
<h2 id="commands"><a href="https://www.cnet.com/how-to/google-home-complete-list-of-commands/">Commands</a></h2>
<ul>
<li>
<p><strong>Basic commands</strong></p>
<ul>
<li>Ask for help: &quot;OK Google, help.&quot;</li>
<li>Control the volume: &quot;OK Google, turn it up&quot; or, &quot;Louder&quot; or, &quot;Turn it to 11.&quot; (Yes, the max is 11.)</li>
<li>Halt an action: &quot;OK Google, stop&quot; or, &quot;Pause&quot; or, &quot;Be quiet.&quot;</li>
</ul>
</li>
<li>
<p><strong>Tools</strong></p>
<ul>
<li>Roll a die: &quot;OK Google, roll a die&quot; or, &quot;OK Google, roll a 12-sided die.&quot;</li>
<li>Flip a coin: &quot;OK Google, flip a coin.&quot;</li>
<li>Math: &quot;OK Google, what's 354 times 11?&quot;</li>
<li>Measurements: &quot;OK Google, how many liters are in 4 gallons.&quot;</li>
<li>Time: &quot;OK Google, what time is it?&quot;</li>
<li>Location: &quot;OK Google, where am I?&quot;</li>
<li>Translations: &quot;OK Google, how do you say [word] in [language]?&quot;</li>
<li>International time: &quot;OK Google, what time is it in [city]?&quot;</li>
<li>Currency conversion: &quot;OK Google, how much is 100 Euros in dollars?&quot;</li>
<li>Alarm: &quot;OK Google, set an alarm for [time].&quot;</li>
<li>Snooze alarm: &quot;OK Google, snooze alarm.&quot;</li>
<li>Cancel alarm: &quot;OK Google, cancel my alarm for [time].&quot;</li>
<li>Timer: &quot;OK Google, set a timer for [time].&quot;</li>
<li>Check timer: &quot;OK Google, how much time is left on my timer?&quot;</li>
<li>Recipes: &quot;OK Google, how do I make [dish]&quot;</li>
<li>Add to shopping list: &quot;OK Google, add [item] to my shopping list.&quot;</li>
<li>Check shopping list: &quot;OK Google, what's on my shopping list?&quot;</li>
<li>Daily briefing: &quot;OK Google, good morning.&quot; (includes personalized greeting, info on weather, traffic, and curated news stories)</li>
<li><a href="https://www.cnet.com/tags/uber/"><strong>Uber</strong></a>: &quot;OK Google, order an Uber.&quot;</li>
<li>Tune an instrument: &quot;OK Google, tune my instrument&quot; or &quot;OK Google, play an F sharp.&quot; (If you don't specify &quot;flat&quot; or &quot;sharp,&quot; you must say &quot;note&quot; after stating which note you want Google Home to play, such as &quot;play an A note.&quot;)</li>
</ul>
</li>
<li>
<p><strong>Search</strong></p>
<ul>
<li>Stocks: &quot;OK Google, how are Alphabet's stocks doing?&quot;</li>
<li>Weather: &quot;OK Google, how's the weather today?&quot; or, &quot;Do I need an umbrella today?&quot;</li>
<li>Traffic: &quot;OK Google, what's the traffic like on the way to work?&quot;</li>
<li>Words: &quot;OK Google, what does [word] mean?&quot;</li>
<li>Spelling: &quot;OK Google, spell [word].&quot;</li>
<li>Special events: &quot;OK Google, when is [event]?&quot; (Easter, for example)</li>
<li>People: &quot;OK Google, who is [person]?&quot;</li>
<li>Facts: &quot;OK Google, how tall is [person]?&quot;</li>
<li>Things: &quot;OK Google, what is [thing]?&quot;</li>
<li>Places: &quot;OK Google, what country is [location] in?&quot;</li>
<li>Animal sounds: &quot;OK Google, what does [animal] sound like?&quot;</li>
<li>Distance: &quot;OK Google, how far is [business name] from here?&quot;</li>
<li>Restaurants: &quot;OK Google, what are the nearest restaurants to me?&quot;</li>
<li>Businesses: &quot;OK Google, are there any [business type] around here?&quot;</li>
<li>Business information: &quot;OK Google, how late is [business] open?&quot; or &quot;Is [business] open now?&quot;</li>
<li>Quotes: &quot;OK Google, give me a quote&quot; or, &quot;Give me a love quote.&quot;</li>
<li>Medical information: &quot;OK Google, what is a torn meniscus?&quot;</li>
<li>Calories: &quot;OK Google, how many calories are in [food item]?&quot;</li>
<li>Authors: &quot;OK Google, who wrote [book title]?&quot;</li>
<li>Inventors: &quot;OK Google, who invented [item]?&quot;</li>
</ul>
</li>
<li>
<p><strong>Media</strong></p>
<ul>
<li>Play music: &quot;OK Google, play some music&quot; or, &quot;Play some [genre] music.&quot;</li>
<li>Play an artist or song: &quot;OK Google, play [artist]&quot; or, &quot;Play [song].&quot;</li>
<li>Play a song by lyrics: &quot;OK Google, play the song that goes, 'Is this the real life?'&quot;</li>
<li>Play a Google Play playlist or album: &quot;OK Google, play some indie music&quot; or, &quot;OK Google, play [album].&quot;</li>
<li>Ask what's playing: &quot;OK Google, what song is this?&quot; or, &quot;OK Google, what album is this?&quot;</li>
<li>Get more information: &quot;OK Google, when did this album come out?&quot;</li>
<li>Fast forward and rewind: &quot;OK Google, skip forward 2 minutes&quot; or, &quot;Skip backward 30 seconds.&quot;</li>
<li>Play music through other <a href="https://www.cnet.com/topics/speakers/"><strong>speakers</strong></a> using <a href="https://www.cnet.com/products/chromecast-audio/"><strong>Chromecast</strong></a>: &quot;OK Google, cast [song] onto [speaker name].&quot;</li>
<li>Play music on Spotify: &quot;OK Google, play [artist] on Spotify.&quot;</li>
<li>Play music on <a href="https://www.cnet.com/tags/pandora/"><strong>Pandora</strong></a>: &quot;OK Google, play [artist] on Pandora.&quot;</li>
<li>Like or dislike a song on Pandora: &quot;OK Google, dislike this song.&quot;</li>
<li>Play music on <a href="https://www.cnet.com/products/youtube-music/preview/"><strong>YouTube Music</strong></a>: &quot;OK Google, play [artist] on YouTube.&quot;</li>
<li>Play stations on TuneIn: &quot;OK Google, play [station] on TuneIn.&quot;</li>
<li>Play videos on YouTube using Chromecast: &quot;OK Google, play [video] on the [TV name].&quot;</li>
<li>Pull up lists on YouTube: &quot;OK Google, let's look at what's trending on YouTube on [TV name].&quot;</li>
<li>Play a movie or TV show on Netflix using Chromecast: &quot;OK Google, play [show or movie title] on the [TV name].&quot;</li>
</ul>
</li>
<li>
<p><strong>Entertainment</strong></p>
<ul>
<li>Sports updates: &quot;OK Google, who is [team] playing next?&quot; or &quot;Did the [team] win last night?&quot;</li>
<li>Sports scores: &quot;OK Google, what was the score for the last [team] game?&quot;</li>
<li>Team information: &quot;OK Google, tell me about [team].&quot;</li>
<li>Movies: &quot;OK Google, what movies came out last Friday?&quot;</li>
<li>Casting for movies: &quot;OK Google, what actors are in [movie]?&quot;</li>
<li>Shows by network: &quot;Hey Google, what shows are on [network]?&quot;</li>
<li>News: &quot;OK Google, what's today's news?&quot;</li>
</ul>
</li>
<li>
<p><strong>Smart home</strong></p>
<p>Google works with only a few smart home devices/platforms at present: <a href="https://www.cnet.com/tags/philips/"><strong>Philips</strong></a> Hue, <a href="https://www.cnet.com/tags/nest/"><strong>Nest</strong></a>, <a href="https://www.cnet.com/tags/smartthings/"><strong>SmartThings</strong></a>, Chromecast and IFTTT. The roster of integrations will likely expand as time goes on. Even with these limited integrations, though, the flexibility of SmartThings and especially IFTTT allow the Google Home to control a wide variety of <a href="https://www.cnet.com/topics/gadgets/"><strong>gadgets</strong></a> using 3rd-party triggers. For now, here are the built-in Google Home commands for smart home gadgets.</p>
<ul>
<li>Turn Philips Hue lights on/off: &quot;OK Google, turn on/off my lights.&quot;</li>
<li>Dim Hue lights: &quot;OK Google, dim my lights to fifty percent.&quot;</li>
<li>Change Hue colors: &quot;OK Google, turn my lights [color].&quot;</li>
<li>Control Nest thermostat: &quot;OK Google, turn the temperature to [temp].&quot;</li>
<li>Make incremental changes: &quot;OK Google, raise the temperature 1 degree.&quot;</li>
<li>Customize trigger phrases for IFTTT. For example: &quot;OK Google, let's get this party started.&quot;</li>
</ul>
</li>
<li>
<p><strong>Third-party Actions</strong></p>
<p>Earlier this month, Google rolled out what it calls <a href="https://www.cnet.com/how-to/how-to-use-third-party-actions-on-google-home/"><strong>Actions for Google Assistant</strong></a>. These are third-party services and integrations that work much like Alexa skills, except you don't have to activate them one by one. Actions are enabled by default.</p>
<p>You can find the full list of Actions in the Google Home app by going to <strong>More settings &gt; Services</strong>. You will also find sample invocations there, which will tell you how to interact with the different services available.</p>
<ul>
<li>21 Blackjack: &quot;OK Google, let me talk to 21 Blackjack.&quot;</li>
<li>Best Dad Jokes: &quot;OK Google, talk to Best Dad Jokes.&quot;</li>
<li>Domino's: &quot;OK Google, talk to Domino's and get my Easy Order.&quot;</li>
<li>Product Hunt: &quot;OK Google, talk to Product Hunt.&quot;</li>
<li>Tender: &quot;OK Google, can I talk to Tender about drinks like an old fashioned?&quot;</li>
<li>Todoist: &quot;OK Google, tell me what my next task is with Todoist.&quot;</li>
</ul>
</li>
<li>
<p><strong>Easter eggs</strong></p>
<ul>
<li>&quot;Hey Google, always be closing.&quot;</li>
<li>&quot;Hey Google, what is your quest?&quot;</li>
<li>&quot;Hey Google, I am your father.&quot;</li>
<li>&quot;Hey Google, set phasers to kill.&quot;</li>
<li>&quot;Hey Google, are you SkyNet?&quot;</li>
<li>&quot;Hey Google, make me a sandwich.&quot;</li>
<li>&quot;Hey Google, up up down down left right left right B A Start.&quot;</li>
<li>&quot;Hey Google, do a barrel roll.&quot;</li>
<li>&quot;Hey Google, it's my birthday.&quot;</li>
<li>&quot;Hey Google, it's not my birthday.&quot;</li>
<li>&quot;Hey Google, did you fart?&quot;</li>
</ul>
</li>
</ul>
<h2 id="apiai"><a href="https://api.ai/blog/2016/12/08/build-conversation-actions-for-the-google-assistant/">API.AI</a></h2>
<h3 id="flaskassistantservingapiaisdk"><a href="https://github.com/treethought/flask-assistant">Flask-assistant. serving API.AI SDK</a></h3>
<p><a href="http://flask-assistant.readthedocs.io/en/latest/">tutorial</a><br>
<a href="http://flask-assistant.readthedocs.io/en/latest/quick_start.html">QuickStart</a></p>
<p><a href="https://developers.google.com/actions/develop/apiai/tutorials/getting-started">Actions on Google - tutorial</a></p>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLOU2XLYxmsIKgPTizdYWPPYEpU96FCJrQ">Youtube</a></li>
</ul>
<p><a href="https://api.ai/blog/2016/12/08/build-conversation-actions-for-the-google-assistant/">Build Conversational Actions for Google Assistant Users</a></p>
<h2 id="pixelphone">Pixel phone</h2>
<p><a href="https://forum.xda-developers.com/android/software/guide-how-to-enable-google-assistant-t3477879">넥서스 5x Google assistant</a></p>
<ul>
<li>부트로더 언락시 은행앱 안되서 보류</li>
</ul>
<p>갤럭시 S4 mini 를 누가로 올리고? --&gt; 적외선 리모콘<br>
<a href="http://ppomppu.co.kr/zboard/view.php?id=phone&amp;no=2637349">cm 올리기 - 누가는 아님</a><br>
<a href="http://www.cyanogenmods.org/forums/topic/galaxy-s4-mini-cm14-cyanogenmod-14-nougat-7-0-rom/">누가</a><br>
<a href="http://blog.naver.com/PostView.nhn?blogId=ekfsla1177&amp;logNo=220830443801">누가 한글</a><br>
<a href="http://forums.androidcentral.com/samsung-galaxy-s4/333746-ir-universal-remote-control-app-s4-s4-mini.html">IR Universal Remote Control-Watch On app</a><br>
<a href="https://www.youtube.com/watch?v=0D6lrsZUWc0">더 쉬워보이는 TWRP flashing</a></p>
<h1 id="alexa">Alexa</h1>
<p><a href="https://github.com/alexa/alexa-avs-sample-app/wiki/Create-Security-Profile">공통으로 앱제작</a><br>
<a href="http://blueidblues.tistory.com/33">캘린더 등록</a></p>
<p>IoTWonders</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=YbkFIlrF_XM">Amazon Echo Custom Skill</a></li>
<li><a href="https://www.youtube.com/watch?v=9237RZAOUqc&amp;list=PLD1gjwwK_VNaQB6xzYtuwrNnh20-KTYQV">Jarvis Skill</a></li>
</ul>
<h2 id="alexaskills">Alexa Skills</h2>
<h3 id="flaskalexa"><a href="https://alexatutorial.com/flask-ask/">Flask-alexa</a></h3>
<ol>
<li><a href="https://github.com/johnwheeler/flask-ask">github</a></li>
<li><a href="https://youtu.be/DFiCsMcipr4?list=PLQVvvaa0QuDe3E0TlV7q7bFyMqbnO5-TL">sentdex tutorial</a></li>
<li><a href="https://youtu.be/cXL8FDUag-s?list=PL6LVC9c1eflVHEbiNb0_dQF0LGLLrZtNL">JohnWheeler tutorial</a></li>
<li><a href="https://developer.amazon.com/blogs/post/Tx14R0IYYGH3SKT/Flask-Ask-A-New-Python-Framework-for-Rapid-Alexa-Skills-Kit-Development">Ngrok 까지 사용한 memory game 예제</a></li>
</ol>
<p>John Wheeler란 사람이 만든 것. Echo를 Python으로 프로그래밍 가능.</p>
<p>Today's guest post comes from <a href="https://twitter.com/johnwheeler_">John Wheeler</a>, the creator of <a href="https://alexatutorial.com/flask-ask/">Flask-Ask</a>. John has been programming for two decades and has written for O'Reilly and IBM developerWorks.</p>
<p>This post introduces <a href="https://github.com/johnwheeler/flask-ask">Flask-Ask</a>, a new Python micro-framework that significantly lowers the bar for developing Alexa skills. Flask-Ask is a <a href="http://flask.pocoo.org/extensions/">Flask extension</a> that makes building voice user interfaces with the <a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit">Alexa Skills Kit</a> easy and fun. We'll learn it by building a simple game that asks you to repeat three numbers backwards. Knowing Python and Flask are not required, but some experience programming will help.</p>
<p>If you prefer the video walkthrough of this post, check it out <a href="https://www.youtube.com/watch?v=cXL8FDUag-s">here</a>.</p>
<h2 id="macalexa">macAlexa</h2>
<p>Product ID : macAlexa</p>
<p>Client ID:amzn1.application-oa2-client.a971e925f29742a29bcad3efef93fdd5</p>
<p>Client Secret:470f972acb198dc6731b0ebd38cd20ba42e2fd54c965633bfa65148c9aa3a2a3</p>
<pre><code>➜  app git:(master) ✗ keytool -list -v -alias androiddebugkey -keystore keystore.jks
키 저장소 비밀번호 입력:
별칭 이름: androiddebugkey
생성 날짜: Jan 13, 2017
항목 유형: PrivateKeyEntry
인증서 체인 길이: 1
인증서[1]:
소유자: CN=Junho, OU=tmmsexy, O=Unknown, L=Unknown, ST=Unknown, C=US
발행자: CN=Junho, OU=tmmsexy, O=Unknown, L=Unknown, ST=Unknown, C=US
일련 번호: 51105628
적합한 시작 날짜: Fri Jan 13 16:07:34 KST 2017, 종료 날짜: Tue May 31 16:07:34 KST 2044
인증서 지문:
	 MD5: A4:C4:87:E1:F6:59:89:FF:3F:1F:D6:3E:A4:0C:B4:B9
	 SHA1: 29:FE:C9:42:D4:28:7B:7B:6E:B8:EA:F7:A7:C1:1A:45:6F:52:40:0E
	 SHA256: 4B:B3:A3:25:EB:C9:F2:59:E7:90:4B:BC:EC:A2:F8:08:94:03:D5:BB:54:F5:A5:10:C9:99:ED:7F:F9:51:98:82
	 서명 알고리즘 이름: SHA1withRSA
	 버전: 3

확장:

#1: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: C8 B5 18 A3 84 1C 64 CB   FA 5A 37 96 8F 5F 15 8D  ......d..Z7.._..
0010: 31 8C 0B 2D                                        1..-
]
]
</code></pre>
<p>는 하다가 안되서 버림.</p>
<h2 id="pialexa">piAlexa</h2>
<p>Security Profile Description<br>
Choose a description for your security profile for Amazon services to use in communicating with you.	piAlexa Description<br>
Security Profile ID</p>
<ul>
<li>amzn1.application.7b12d1fc8bc84d86a55862b294377da5</li>
</ul>
<p>Client ID</p>
<ul>
<li>amzn1.application-oa2-client.5adcc0ce71df427a9bdcb28eecdd39ae</li>
</ul>
<p>Client Secret</p>
<ul>
<li>7750995176a0b0c034ee29cb10ecbcf7c0619f3e25a2c7d116fa51eb3837ed81</li>
</ul>
<p>Client ID:amzn1.application-oa2-client.5adcc0ce71df427a9bdcb28eecdd39ae</p>
<p>Client Secret:7750995176a0b0c034ee29cb10ecbcf7c0619f3e25a2c7d116fa51eb3837ed81</p>
<h3 id="1bnode">1 B+ node 문제</h3>
<p>먼저 node 삭제 <a href="https://www.raspberrypi.org/forums/viewtopic.php?f=66&amp;t=130217">참고</a></p>
<pre><code class="language-basg">sudo apt-get remove nodered
sudo apt-get remove nodejs nodejs-legacy
sudo apt-get remove npm   # if you installed npm
</code></pre>
<h3 id="3b">3 B+ 모델</h3>
<p><a href="http://guzene.tistory.com/184">bluetooth 설정</a></p>
<p><a href="https://www.raspberrypi.org/magpi/bluetooth-audio-raspberry-pi-3/">디폴트 블투스피커</a></p>
<p><a href="http://maker1st.tistory.com/2">wifi 설정 국가 US로</a></p>
<p><a href="http://forums.rasplay.org/topic/196/%EA%B3%B5%EC%A7%80-raspberrypi-model-3b-wifi-issue">내 공유기 N104T는 지원을안함</a></p>
<h1 id="remotecontrol">Remote Control</h1>
<p>갤럭시 S4 mini : Tasker로 해보기<br>
그리고 안되면<br>
xiaomi controller --&gt; 보장없음</p>
<p><a href="https://www.youtube.com/watch?v=SuB_lRmvChs">ChromeCast 로 TV 켜기. Anynet HDMI-CEC</a></p>
<h1 id="tv">TV</h1>
<p><a href="http://11q.kr/g5s/bbs/board.php?bo_table=s21&amp;wr_id=3799">KODI (XBMC 신버전) iptv 체널및 한글 설정,홈화면 TV시청메뉴 나오게 설정하기 / 원도우10에서 TV 시청하기</a></p>
<p><a href="http://psychoria.tistory.com/482">자작NAS</a></p>
<p><a href="http://ibblog.tistory.com/22">KODI / XMBC</a></p>
<p><a href="https://www.youtube.com/watch?v=5LgtZEaRCMw&amp;feature=youtu.be">Rasberry Pi</a></p>
<p><a href="https://www.youtube.com/watch?v=9237RZAOUqc&amp;list=PLD1gjwwK_VNaQB6xzYtuwrNnh20-KTYQV">KODI - JARVIS</a></p>
<h1 id="beamprojector">Beam Projector</h1>
<p>고로 전 에이서 h6510bd 사용중. 3d도 잘나와서 좋네요. 아 그리고 사실거면 스크린도 무조건 사세요</p>
<h2 id="mpcl1a51">소니 MP-CL1A : 51만</h2>
<p><img src="https://tmmsexy.s3.amazonaws.com/imgs/2017-05-22-070439.jpg" alt=""></p>
<p>플레이스테이션4 슬림과 소니 MP-CL1A의 연결과 사용, 기존의 TV나 모니터에 연결해서 플레이스테이션을 즐기시던 분들에게는 새로운 경험이 되지 않을 까 싶습니다. 보통 프로젝터중에 FHD해상도를 가지는 프로젝터는 찾기 어렵기도 하고, 가격도 높은편입니다. 소니 MP-CL1A는 와이드한 화각의 FHD해상도 (1920 x 720)을 지원하기 때문에 플레이스테이션4 슬림으로 게임을 할 때 모자람이 없는 스펙이라고 생각합니다. 실제 게임을 해봤을 때도 모니터로 했을 때와 별다른 차이점이 없었고, 오히려 어두운 밤에 할 때는 더 큰 화면에서 즐기다보니 몰입감이 더 좋았습니다.</p>
<p>소니 MP-CL1A 모바일프로젝터와 소니 플레이스테이션4 슬림의 조합, 어떠셨나요? MP-CL1A가 모바일, 휴대용프로젝터로 출시되었지만, 집에서 플레이스테이션과 사용하는데도 문제 없었습니다. 보다 큰 화면에서 플레이스테이션을 즐기기를 원하시는 분들에게 좋은 조합이 될 것 같습니다.</p>
<h2 id="fhd">빔프로젝터 FHD</h2>
<p>벤큐 W2000 이나 뷰소닉 PRO7827 옵토마 HT210 중 고르시면될것같네요 ㅎㅎ</p>
<p><a href="http://m.blog.naver.com/mo_ko_zy/220794124525">W1110</a></p>
<p>벤큐 W1070+가 우리나라에서 보급형FHD 프로젝터 시장에 큰 돌풍을 일으 켰죠. 그당시에 저는 LG미니빔 600안시 프로젝터를 사용하고 있던터라 홈시어터용 프로젝터에는 관심이 없었습니다. 아파트로 이사 후 홈시어터 프로젝터에 관심이 생기고 W1070+의 후속작 W1110을 알게되고 구매하게 되었습니다.</p>
<p><a href="http://lagneid.tistory.com/87">옵토마 FHD HD141X</a></p>
<h1 id="telegram">Telegram</h1>
<p><a href="http://bakyeono.net/post/2015-08-24-using-telegram-bot-api.html">Telegram Bot. Official API - 박연오 telegram-cli</a></p>
</div>]]></content:encoded></item><item><title><![CDATA[TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)]]></title><description><![CDATA[<div class="kg-card-markdown"><p>참고 :<a href="https://groups.google.com/forum/#!topic/keras-users/_hXfBOjXow8">https://groups.google.com/forum/#!topic/keras-users/_hXfBOjXow8</a></p>
<p>선요약:</p>
<pre><code class="language-bash"># export PATH=~/anaconda/bin:$PATH # MAC
conda create -n tf python=3.5  
# 17/3/1 기준으로 윈도우에서 3.5 버전만 TensorFlow/Keras가 지원
activate tf # Windows
# source activate tf  : Linux/macOS

# 여기서부터 (tf) 환경. 설치 순서 중요
pip install</code></pre></div>]]></description><link>http://tmmse.xyz/2017/03/01/tensorflow-keras-installation-windows-linux-macos/</link><guid isPermaLink="false">5abff3c293c7ee06e8cf76f9</guid><category><![CDATA[Deep Learning]]></category><category><![CDATA[macOS]]></category><category><![CDATA[TensorFlow]]></category><category><![CDATA[Keras]]></category><category><![CDATA[Linux]]></category><category><![CDATA[Windows]]></category><dc:creator><![CDATA[Junho Cho]]></dc:creator><pubDate>Tue, 28 Feb 2017 17:16:13 GMT</pubDate><media:content url="https://tmmsexy.s3.amazonaws.com/2017-02-28-anders-jilden-108186.jpg" medium="image"/><content:encoded><![CDATA[<div class="kg-card-markdown"><img src="https://tmmsexy.s3.amazonaws.com/2017-02-28-anders-jilden-108186.jpg" alt="TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)"><p>참고 :<a href="https://groups.google.com/forum/#!topic/keras-users/_hXfBOjXow8">https://groups.google.com/forum/#!topic/keras-users/_hXfBOjXow8</a></p>
<p>선요약:</p>
<pre><code class="language-bash"># export PATH=~/anaconda/bin:$PATH # MAC
conda create -n tf python=3.5  
# 17/3/1 기준으로 윈도우에서 3.5 버전만 TensorFlow/Keras가 지원
activate tf # Windows
# source activate tf  : Linux/macOS

# 여기서부터 (tf) 환경. 설치 순서 중요
pip install tensorflow   # pip install tensorflow-gpu : GPU 버전
conda install pandas matplotlib scikit-learn 
pip install keras
conda install jupyter notebook  

jupyter notebook # Test 해보기
</code></pre>
<p>평소에 Source code로만 설치하는 것을 선호했지만 Windows에서 설치가 조금 난감하기 때문에 Anaconda를 통해 비교적 쉽게 설치를 할 수 있습니다. 그리고 Linux/macOS에서도 다 작동하는 것을 확인했습니다.</p>
<p>편의상 문어체를 사용합니다.<br>
<a href="https://www.continuum.io/downloads">Anaconda 다운로드</a>를 통해서 <code>Anaconda Python 3.X</code> 버전을 자신의 플랫폼에 맞게 설치한다.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-02-28-164351.jpg" alt="TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)"></p>
<p>나는 Python 3.6 버전이다.</p>
<p>Windows 경우 설치중에 Anaconda를 PATH 경로에 포함하는 체크란이 있음으로 반드시 체크 됨을 확인하자.</p>
<p>설치를 완료한 후에 linux/macOS 라면 terminal 그리고 Windows 라면 CMD 창에<br>
<code>conda --v</code> 명령어가 작동해야한다.</p>
<p>만약 linux/macOS에서 <code>conda</code> 명령어가 먹히지 않으면<br>
<code>export PATH=~/anaconda/bin:$PATH</code> 로 anaconda를 경로에 추가한다.  각자의 anaconda의 경로가 다를 수 있으므로 anaconda 혹은 anaconda3로 추가해준다. 편의상 자신의 <code>.bashrc</code> 등에 넣어주자.<br>
<code>echo 'export PATH=~/anaconda/bin:$PATH' &gt;&gt; ~/.bashrc</code></p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-02-28-164019.jpg" alt="TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)"></p>
<p>커맨드 작동을 확인한다.Anaconda의 현 버전은 상관없는 듯하다.</p>
<p>그리고 conda environment를 만든다.<br>
환경 설정후 환경을 활성화 시킨다.</p>
<pre><code class="language-bash">conda create -n tf python=3.5 # y 등으로 계속 진행
</code></pre>
<p>이 과정은 python의 virtualenv와 비슷하다. 안전하게 시스템의 python library가 꼬이지 않게 격리해서 만드는 과정이다. 예를들어 TensorFlow의 버전을 다르게 쓰고싶을때.</p>
<p>우선 현재(17/3/1)기준 윈도우에선 python 3.5.X 버전을 사용해야 현재 TensorFlow-v1.0.0와 Keras를 지원한다. 그래서 <code>python=3.5</code>를 해줘야하며 리눅스와 맥에서는 python 3.6도 지원한다고 한다. 나는 윈도우 기준으로 테스트해봤기때문에 3.5로만 해보았다. 따라서 Anaconda 기본 python3.6 버전을 사용하면 TensorFlow와 Keras 설치가 불가능했으므로 잘 선택하자.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-03-01-025948.jpg" alt="TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)"></p>
<blockquote>
<p>참고: <a href="https://pypi.python.org/pypi/tensorflow/1.0.0">https://pypi.python.org/pypi/tensorflow/1.0.0</a></p>
</blockquote>
<pre><code class="language-bash">activate tf # Windows
# source activate tf   # Linux/macOS
</code></pre>
<p>그리고 환경을 활성화시킬때 Windows와 Linux/macOS 명령어의 차이가 난다.</p>
<p><img src="https://tmmsexy.s3.amazonaws.com/2017-02-28-165746.jpg" alt="TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)"><br>
다음 처럼 (tf) 환경이라고 쉘/터미널의 모습이 바뀐다.</p>
<p>그리고 쭉쭉 이 순서대로 설치해준다.</p>
<pre><code class="language-bash">pip install tensorflow  # pip install tensorflow-gpu
conda install pandas matplotlib scikit-learn 
pip install keras
conda install jupyter notebook
</code></pre>
<p>TensorFlow를 GPU 버전으로 사용할 수 있다면<code>pip install tensorflow-gpu</code>로 설치한다.<br>
Keras 설치 중에 Theano를 설치하는 듯 하지만 기본 백엔드는 TensorFlow로 작동한다.</p>
<p>참고로 Keras 설치 전에 jupyter notebook 설치시 keras module을 import 하지 못하는 오류가 있다. 그래서 keras 설치후 jupyter notebook을 설치한다.</p>
<p>간단하게 Keras import가 되는지 확인한다.<br>
<img src="https://tmmsexy.s3.amazonaws.com/2017-02-28-171932.jpg" alt="TensorFlow-v1.0.0 + Keras 설치 (Windows/Linux/macOS)"></p>
<p>끝으로 ..</p>
<p>자세히 읽지는 않았지만 다음 블로그 글에서 Windows7 64bit에서 CUDA와 CuDNN을 포함한 Windows에서 Keras+TensorFlow를 설치하는 글도 참고하면 도움이 되겠습니다.<br>
<a href="http://skyer9.tistory.com/m/11">Python Keras+Tensorflow on Windows7 64bit 설치하기</a></p>
</div>]]></content:encoded></item></channel></rss>