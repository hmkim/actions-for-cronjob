<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><rss xmlns:creativeCommons="http://backend.userland.com/creativeCommonsRssModule" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">

<channel>
	<title>Channy's Blog</title>
	<link>http://channy.creation.net</link>
	<language>en</language>
	<description>Insights for Web 2.0, Open Source and Open Standards</description>

<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/channy" /><feedburner:info uri="channy" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><creativeCommons:license>http://creativecommons.org/licenses/by-nc-sa/3.0/</creativeCommons:license><image><link>http://creativecommons.org/licenses/by-nc-sa/3.0/</link><url>http://creativecommons.org/images/public/somerights20.gif</url><title>Some Rights Reserved</title></image><feedburner:emailServiceId>channy</feedburner:emailServiceId><feedburner:feedburnerHostname>https://feedburner.google.com</feedburner:feedburnerHostname><feedburner:feedFlare href="https://add.my.yahoo.com/rss?url=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="http://us.i1.yimg.com/us.yimg.com/i/us/my/addtomyyahoo4.gif">Subscribe with My Yahoo!</feedburner:feedFlare><feedburner:feedFlare href="http://www.netvibes.com/subscribe.php?url=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="//www.netvibes.com/img/add2netvibes.gif">Subscribe with Netvibes</feedburner:feedFlare><feedburner:feedFlare href="http://fusion.google.com/add?feedurl=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="http://buttons.googlesyndication.com/fusion/add.gif">Subscribe with Google</feedburner:feedFlare><feedburner:feedFlare href="http://www.feedly.com/home#subscription/feed/http://feeds.feedburner.com/channy" src="http://michaeltunnell.com/images/projects/feedlyflare.jpg">Subscribe with Feedly</feedburner:feedFlare><feedburner:feedFlare href="http://www.hanrss.com/add_sub.qst?url=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="http://static.hanrss.com/images/add_to_hanrss2.gif">Subscribe with HanRSS</feedburner:feedFlare><feedburner:browserFriendly>Thanks for your reading.</feedburner:browserFriendly><item>
	<title>故 박부웅 장로님을 추모하며</title>
	<link>http://blog.creation.net/parkbuwoong-memorial</link>
	<description>&lt;p&gt;&lt;em&gt;이 글은 지난 3월 19일 소천하신 저의 장인 어른이신 故 박부웅 원로 장로님의 추모의 마음을 담았습니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;1996년 7월. 지금의 와이프와 교제를 허락받기 위해 여자 친구의 부모님을 찾았던 무더운 여름날&amp;#8230; &amp;#8220;밥이나 많이 먹고 가라&amp;#8221;고 하시며 안타까운 마음으로 물끄러미 나를 쳐다 보셨다.&lt;/p&gt;
&lt;p&gt;故 박부웅 장로님은 고작 7살 나이에 어머니를 여의고 아버지의 보살핌도 잘 받지 못하신 채, 고교 시절에 하나님에 대한 신앙을 통해 매번 닥치는 인생의 난관들을 극복하셨다. 그렇기 때문에 10살때 어머니를 잃은 나에게 같은 연민을 느끼시고, 당시 직장도 없는 대학원생에 불과한 나와 귀하게 키운 딸과의 결혼을 허락하셨다.&lt;/p&gt;
&lt;p&gt;형제 자매도 많고 다들 집을 떠나 유학을 하면서 생활해서 가족간의 교제가 부재했던 우리집에 비해 2남 1녀의 처가는 늘 만남이 넘쳤다. 나로서는 어린 시절 쉽게 받지 못했던 생일 케이크도 매년 받고, 시골에 정착한 날을 기념일 삼아 가족들을 늘 불러모으시고 손주들을 축복해 주시는 그런 어른이셨다. 가족간의 사랑이 늘 흐르는 안식처를 만드는 분이셨다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2111" src="http://blog.creation.net/data/tisotry/2019/03/28031259/2019-parkbuwoong-memorial-2-1024x768.jpg" alt="" width="1024" height="768" /&gt;&lt;/p&gt;
&lt;p&gt;1969년에 우리 장모님과 결혼하신 후, 온천중앙교회에 다니기 시작하셨으니 돌아가시기까지 만 50년을 한 교회를 주일학교 교사로, 교육부장으로 믿음의 인재들을 길러내시고, 91년 부터 장로로 교회를 섬기셨다. 1995년에 시골로 정착하시고도 20년이 넘는 기간을 매주 주일 성수를 하시면서, 주차 봉사로도 수고하시며 교인들의 귀감이 되셨다. 장례를 치르는 동안 많은 교인들이 추모 예배와 함께 장지까지 동행 해주실 정도로 존경을 받는 어른이셨다.&lt;/p&gt;
&lt;p&gt;최근 1년간 몸이 점차 경직되는 힘든 와병중에 계시면서도 늘 유쾌함을 잃지 않으셨고, 가족들에게도 병을 이기는 용기를 보여주셨다. 연초에 지독한 폐렴을 앓으시고 난 후 잘 견디셨지만, 얼마전 다시 폐렴으로 입원하시고 일주일을 보내시다 새벽에 잠자듯이 편안하게 하나님 나라로 떠나셨다.&lt;/p&gt;
&lt;p&gt;가장 가까운 가족을 떠나보낸다는 것은 참 마음 아픈일이다. 오로지 남아 있는 사람이 감당하는 것도 쉬운 일은 아니다. 장례가 끝나고 돌아온 후, 며칠 동안 어제까지 있었던 분이 오늘 존재하지 않는&amp;#8230; 그래서 마치 꿈을 꾸고 깬 듯한 슬픔이 우리 장모님과 와이프에게도 찾아왔다.&lt;/p&gt;
&lt;p&gt;하지만, 천국에서 다시 만날거라는 소망을 가진 기독교인들에게 남은 이의 삶은 여전히 선물이고, 슬픔을 이겨내고 열심히 살 수 있도록 하는 원동력이다. 장로님이 생전에 남겨주신 신앙의 유지(&lt;span class="st"&gt;有志&lt;/span&gt;)를 이어가면서&amp;#8230;&lt;/p&gt;
&lt;p&gt;아버님, 그동안 열심히 사셨습니다. 천국에서 다시 만납시다!&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=zCnjhW7RMPo:lNLqHknpXho:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 27 Mar 2019 17:21:49 +0000</pubDate>
	<comments>http://blog.creation.net/parkbuwoong-memorial#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>WWW 30주년 – 최초의 웹 사이트들을 만나자!</title>
	<link>http://channy.creation.net/blog/1220</link>
	<description>&lt;p&gt;오늘이 바로 WWW 30주년이 되는 날입니다.&lt;/p&gt;
&lt;p&gt;유럽 입자가속기 연구소인 CERN에서 각기 다른 운영 체제와 시스템을 가진 컴퓨터들 사이에 저장된 정보를 공유하기 위해 팀 버너스 리 (Tim Berners-Lee)는 서로 다른 컴퓨터에서 정보를 연결하기위한 통합 구조를 구상했으며 1989년 3월 12일 &amp;#8220;&lt;a href="https://cds.cern.ch/record/369245/files/dd-89-001.pdf"&gt;Information Management: A Proposal&lt;/a&gt;&amp;#8220;이라는 제안서를 작성했습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channy.creation.net/wp/data/channy/cern/DSCF0301.jpg" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;출처: &lt;a href="http://channy.creation.net/blog/483"&gt;CERN 방문 후기(1) &amp;#8211; Microsom&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;이것이 바로 오늘날 월드와이드웹의 시초입니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;어디에서나 컴퓨터에 저장된 모든 정보가 링크 될 수 있다고 상상해보세요. 모든 것을 다른 모든 것에 연결할 수있는 공간을 만들기 위해 컴퓨터로 프로그래밍 할 수 있다고 상상해 보세요.&lt;/p&gt;
&lt;p&gt;&amp;#8211; Tim Berners-Lee, World Wide Web 발명가&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;저도 학부 3학년인 1994년 처음으로 웹을 사용해 보면서, 국내에 처음 도입될 당시 웹 기술 커뮤니티인 &lt;a href="http://channy.creation.net/blog/969"&gt;WWW-KR&lt;/a&gt;에 참여하면서 새로운 기술과 그 사상에 매료되었습니다. 그래서, 96년에는 진로를 웹 개발자로 바꾸면서 인터넷 업계에 뛰어들었습니다. 웹 10주년이 되는 1999년에는 한참 바쁘게 일하느라 아무 생각이 없었는데, 웹 표준 및 Mozilla 커뮤니티에 활동을 하면서 &lt;span class="search-result-panel-list-title"&gt;&lt;a href="http://channy.creation.net/blog/680"&gt;WWW 20주년&lt;/a&gt;, &lt;/span&gt;&lt;span class="search-result-panel-list-title"&gt;&lt;a href="http://channy.creation.net/blog/959"&gt;25주년&lt;/a&gt;&lt;/span&gt; 그리고 &lt;a href="http://channy.creation.net/blog/1004"&gt;한국웹20주년&lt;/a&gt; 등 기념일을 조금씩 챙기기 시작했네요.&lt;/p&gt;
&lt;p&gt;웹 30주년을 맞이하여 CERN과 웹 재단에서 &lt;a href="https://web30.web.cern.ch/"&gt;기념 행사&lt;/a&gt;도 마련했네요.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channyblog.s3-ap-northeast-2.amazonaws.com/data/channy/2019/03/13084520/web@30_News-580x322.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;오늘은 몇 가지 기념 스크린샷을 추가해 보고자 합니다. 아래는  1993년 우리나라 최초의 웹 서버로 불리는 &lt;a href="https://web.archive.org/web/20010402024809/http://cair.kaist.ac.kr/"&gt;CAIR.KAIST.AC.KR&lt;/a&gt;의 웹 페이지입니다. 텍스트로만 되어 있어서 엄청 심플하지만, 한국 정보, KAIST 정보, 그리고 각종 링크 정보나 당시 자바, 윈도95 등 다양한 정보도 포함하고 있었네요.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1222" src="http://channy.creation.net/data/channy/2015/1993-kaist-cair-webpage.png" alt="" width="885" height="304" /&gt;아래 페이지는 1995년에 제가 처음 만든 홈페이지이면서, 학교 비공식 홈페이지인 &amp;#8216;&lt;a href="http://web.archive.org/web/19961230093308/http://hyowon.cc.pusan.ac.kr:8080/"&gt;효원인의 방&lt;/a&gt;&amp;#8216;였습니다. 당시 교내 인터넷 사용자 모임에 참여하면서  저의 유닉스 계정에 만든 서버입니다. 학교 공식 홈페이지가 만들어질때까지 몇 년간 학교 정보, 교내 시스템 활용 방법, 교내 홈페이지 목록 서비스 등 다양한 정보를 제공했습니다.&lt;/p&gt;
&lt;p&gt;나름 사진 디지털화 작업하고, 디자인 작업도 하고 해서 꽤 그래픽하게 만들었죠?&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1223" src="http://channy.creation.net/data/channy/2015/1995-pnu-hyowon-webpage.png" alt="" width="876" height="1093" /&gt;&lt;/p&gt;
&lt;p&gt;이쯤되면 최초의 홈페이지도 궁금하시죠. CERN에서 아예 옛날 기억을 떠올릴 수 있도록 &lt;a href="http://info.cern.ch/hypertext/WWW/TheProject.html"&gt;최초의 웹 사이트&lt;/a&gt;를 박물관 처럼 저장해두었습니다. 저도 웹코리아 커뮤니티의 웹마스터로서, &lt;a href="http://www-kr.org/"&gt;1999년 당시 모습&lt;/a&gt;을 그대로 박제해 두었습니다.&lt;/p&gt;
&lt;p&gt;그런데, 1991년 웹 사이트가 처음 나왔더라도 우리가 흔히 생각하는 그런 웹 페이지는 아니었습니다.&lt;/p&gt;
&lt;p&gt;사실 본격적인 그래픽 브라우저인 Mosaic과 Netsscape가 대중화 되기 전에는 주로 터미널에서 키보드로 웹 서핑을 했었답니다. (&lt;a href="https://lynx.browser.org/"&gt;Lynx&lt;/a&gt;라는 터미널 브라우저가 있었습니다.)  이를 체험해볼 수 있도록 &lt;a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html"&gt;터미널에서 웹 사이트 보기 시뮬레이터&lt;/a&gt;를 제공하고 있습니다. 여러분도 한번 해보시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1224" src="http://channy.creation.net/data/channy/2015/1989-cern-webpage-linemode.png" alt="" width="1740" height="1204" /&gt;&lt;/p&gt;
&lt;p&gt;여러분이 만든 첫 웹 사이트를 기억하시나요? 사실 과거 자료를 보존한다는 건 어려운 일이라 옛날 기억을 떠올릴 수 있게 만든 &lt;a href="https://archive.org/"&gt;archive.org&lt;/a&gt;라는 웹 사이트가 참 고맙기까지 합니다. 여러분도 추억속의 웹 페이지를 한번 떠 올려 보세요.&lt;/p&gt;
&lt;p&gt;그리고  &lt;a href="https://twitter.com/hashtag/www30?src=hash"&gt;#www30&lt;/a&gt;, &lt;a href="https://twitter.com/hashtag/MyWeb30?src=hash"&gt;#MyWeb30&lt;/a&gt; 등으로 한번 공유해 보시기 바랍니다!&lt;/p&gt;
&lt;p&gt;p.s 웹 30주년 기념 짤방~!&lt;/p&gt;
&lt;p&gt;&lt;img src="http://farm4.static.flickr.com/3467/3388056138_19aa749b6e.jpg?v=0" /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;10년 전 웹 20주년에 &lt;a href="http://channy.creation.net/blog/686"&gt;WebSci09 컨퍼런스&lt;/a&gt; 중 Tim Burners-Lee와 함께 내 포스터 발표 앞에서&lt;/small&gt;&lt;/center&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WQOpwxCRGmw:pBndbZlkBQY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 13 Mar 2019 00:40:27 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1220#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>넥슨 매각에 따른 국내 IT 산업의 영향 가상 예측</title>
	<link>http://channy.creation.net/blog/1213</link>
	<description>&lt;p&gt;넥슨의 김정주 회장이 지주회사인 &lt;a href="https://m.news.naver.com/read.nhn?mode=LSD&amp;mid=sec&amp;sid1=101&amp;oid=015&amp;aid=0004072071"&gt;NXC를 매각&lt;/a&gt;한다고 하네요. 예전에도 인수 합병 루머가 많았고, 아직 공식적인 발표가 없는 이상 섣부른 판단은 금물이겠습니다만… 최근의 넥슨의 상황을 보면 당연한 수순이 아닌가 싶네요. &lt;/p&gt;



&lt;p&gt;2018년 넥슨 연매출은 총 3조원 안팎으로 예상되는데, 이중 70%가 해외 매출입니다. 그 중에서도 중국  의존도가 매우 큽니다. 영업 이익이 1조원이 넘는데, 90% 가량이 텐센트가 중국에서 서비스하고 있는 네오플의 던전앤파이터에서 나옵니다. &lt;em&gt;(2017년 기준 넥슨코리아의 영업 이익은 700억 안팎인데, 네오플은 1조원 가량 됨).&lt;/em&gt; 텐센트가 중국 매출의 30% 정도를 로열티로 지급하고 있는 것을 고려하면, 던전의 중국 매출은 3조원이 넘는 거고, 텐센트 입장에서는 안정적인 수익원을 확보해야 할거에요. &lt;/p&gt;



&lt;div class="wp-block-image"&gt;&lt;img src="http://channy.creation.net/data/channy/public_html/data/channy/2015/5384_5777_3948.png" alt="" class="wp-image-1214" /&gt;던전앤파이터 (c) 네오플 홈페이지&lt;/div&gt;



&lt;p&gt;근데 이럴 수 밖에 없는게… 최근 몇 년 사이에 던전 매출이 너무 급격하게 늘어났구요. 그러다 보니, 넥슨 전체 매출 및 이익에서 중국 의존도가 엄청나게 심화되었습니다. 따라서, 텐센트가 넥슨에 인수의사를 제안했고, 그나마 네오플이 제일 가격이 좋을 때 김정주 회장이 결단한 것이 아닐까 예상됩니다. 텐센트는 이미 슈퍼셀, 라이엇게임즈를 계열사로 가지고 있고, 에픽게임즈의 대주주기도 합니다. 국내 PUBG의 지분도 10% 가량 가지고 있구요. 투자 여력은 아직도 많이 있는 것으로 압니다. &lt;/p&gt;



&lt;p&gt;아직 공식적인 발표가 나오기 까지 루머에 불과하고 해프닝이 될 가능성도 있겠습니다만… 이번 매각이 진짜 이루어진다면, 국내 IT 산업의 최대 인수 합병 사례가 되는 만큼 그 영향도 클 것으로 보여집니다. 몇 가지 섣부른 예측을 해봅니다.&lt;/p&gt;



&lt;ol&gt;&lt;li&gt;&lt;strong&gt;넥슨이 1위를 지키고 있는 국내 게임 업계 판도는 크게 변화가 없을 것입니다. &lt;/strong&gt;주주가 교체됐다고 넥슨의 사업 방향이 크게 바뀌지는 않을 것이니까요. 넥슨 저팬은 도쿄에, 코리아는 서울에, 네오플이 제주에 본사가 있는데&amp;#8230; 갑작스럽게 변화를 꽤하기도 힘든 구조라 직원들의 신상 변화는 크지 않을 것으로 예상됩니다. 다만, 넥슨 저팬 및 코리아의 사업이 장기적으로 좀 더 중국 지향화 된다면 실적이 크게 향샹될 가능성도 있지 않을까 싶네요. &lt;/li&gt;&lt;li&gt;&lt;strong&gt;해외 시장에서 국내 게임 업계의 위상도 여전할 것으로 예상됩니다.&lt;/strong&gt; 넷마블과 PUBG, 펄비어스 등이 미국, 일본, 유럽 시장에서 큰 성과를 거두고 있으니까요. 국내에서는 김택진 대표가 책임 경영을 하고 있기 때문에, 엔씨소프트가 좀 더 업계 큰 형님으로서 역할을 할 것으로 기대 됩니다. 이번 매각으로 게임 산업을 바라보는 시각과 규제 등에 긍정적인 영향을 미친다면 장기적으로는 국내 게임 업계에 도움이 될 것으로 보이네요.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;게임을 비롯한 신규 IT 산업에 큰 도움이 될 것입니다.&lt;/strong&gt; 김정주 회장 입장에서 넥슨을 10조원에 매각한다고 했을 때 사실상 넥슨의 10년치 이익을 땡겨 받는 건데, 오히려 이 거액의 자금이 국내 미래 IT 산업에 재투자 된다면 그게 더 유익하다고 생각합니다. 모 대기업처럼 10조원을 건물 짓는데 쓰는 거 보다는 장기적으로 많은 도움이 되겠죠. 이미 김정주 회장은 비트스탬프나 코빗 등 가상 화폐 거래소 등을 천억 이상 들여서 인수하고, 그외 다양한 산업의 유력 기업에도 개인적인 투자를 해왔습니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;국내 스타트업 투자 규모가 확대 될 수 있습니다. &lt;/strong&gt;국내에 이미 1세대 벤처 기업 창업자들이 투자자로서 큰 역할을 하고 있는데, 여기에 김정주 의장까지 가세한다면 다양한 큰 규모의 투자가 이뤄질 수 있을 것이라고 생각합니다. 대개 스타트업을 유니콘 기업을 키우는데 있어 그 다음 단계로 나아갈 때, 국내 VC업계의 투자 규모는 글로벌 기업에 못 미칩니다. 예를 들어, 쿠팡의 경우도 조 단위 투자는 해외에서 받았으니까요. 김정주 회장이 손정의에 버금가는 투자자가 된다면 국내 IT 산업의 미래는 더 밝지 않을까 생각되네요. &lt;/li&gt;&lt;/ol&gt;



&lt;p&gt;아무튼 새해 부터 다이나믹하네요~&lt;br /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=XKAXm_iqkbo:TJTIdQ0OGdw:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 03 Jan 2019 02:43:19 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1213#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>왜 기술 커뮤니티가 중요할까?</title>
	<link>http://channy.creation.net/blog/1204</link>
	<description>&lt;p&gt;벌써&amp;nbsp;연말이네요.&amp;nbsp;다들&amp;nbsp;한해를&amp;nbsp;마무리하고,&amp;nbsp;내년을&amp;nbsp;준비할&amp;nbsp;때죠.&amp;nbsp;저도&amp;nbsp;매년&amp;nbsp;이맘때면&amp;nbsp;올해를&amp;nbsp;회고하고,이력서도&amp;nbsp;정리합니다.&amp;nbsp;평가나&amp;nbsp;이직을&amp;nbsp;위해&amp;nbsp;하는&amp;nbsp;게&amp;nbsp;아니라,&amp;nbsp;올해&amp;nbsp;내가&amp;nbsp;이력서에&amp;nbsp;적을&amp;nbsp;수&amp;nbsp;있을&amp;nbsp;만큼&amp;nbsp;가치있고&amp;nbsp;의미&amp;nbsp;있는&amp;nbsp;게&amp;nbsp;무엇인지&amp;nbsp;정리해&amp;nbsp;둘&amp;nbsp;목적입니다.&amp;nbsp;&lt;em&gt;(&lt;/em&gt;&lt;em&gt;예전에&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;제가&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;매니저일&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;때&lt;/em&gt;&lt;em&gt;,&amp;nbsp;&lt;/em&gt;&lt;em&gt;연말이면&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;팀원들&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;이력서&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;리뷰도&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;해주고&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;그랬어요&lt;/em&gt;&lt;em&gt;.&amp;nbsp;&lt;/em&gt;&lt;em&gt;여러분도&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;이력서&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;정리&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;한번&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;해보시길&lt;/em&gt;&lt;em&gt;~&amp;nbsp;&lt;/em&gt;&lt;em&gt;참고&lt;/em&gt;&lt;em&gt;.&amp;nbsp;&lt;a href="http://channy.creation.net/blog/1130"&gt;Channy의&amp;nbsp;개발자&amp;nbsp;경력&amp;nbsp;개발&amp;nbsp;관리조언&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;이번 주에도 연말 회고를&amp;nbsp;하면서&amp;nbsp;오늘&amp;nbsp;여러분께&amp;nbsp;이야기&amp;nbsp;하고&amp;nbsp;싶은&amp;nbsp;주제는&amp;nbsp;&lt;strong&gt;&amp;#8216;기술&amp;nbsp;커뮤니티(Tech Community)&amp;#8217;&lt;/strong&gt;에대한&amp;nbsp;것입니다.&amp;nbsp;일반적으로&amp;nbsp;개발자&amp;nbsp;커뮤니티에&amp;nbsp;대해&amp;nbsp;편견들이&amp;nbsp;많습니다.&amp;nbsp;외부에&amp;nbsp;유명해질려고,&amp;nbsp;혹은&amp;nbsp;실력이&amp;nbsp;없는&amp;nbsp;사람들이&amp;nbsp;그냥&amp;nbsp;주워듣는&amp;nbsp;용도로&amp;nbsp;행사에나&amp;nbsp;참여한다는&amp;nbsp;것이죠.&amp;nbsp;실제로&amp;nbsp;커뮤니티에서&amp;nbsp;유명한사람을&amp;nbsp;회사에&amp;nbsp;데려다&amp;nbsp;놨더니&amp;nbsp;일을&amp;nbsp;잘&amp;nbsp;못하더라는&amp;nbsp;소문도&amp;nbsp;꽤&amp;nbsp;많은&amp;nbsp;게&amp;nbsp;사실이구요.&amp;nbsp;무엇이든&amp;nbsp;케바케지만완전히&amp;nbsp;틀린&amp;nbsp;사실도&amp;nbsp;아닙니다.&lt;/p&gt;



&lt;img src="http://channy.creation.net/data/channy/2015/IMG_0392-940x459.jpg" alt="" class="wp-image-1205" /&gt;AWS한국사용자모임의 송년회 (2018.12.20)



&lt;p&gt;저도&amp;nbsp;대학생일&amp;nbsp;때&amp;nbsp;우연히&amp;nbsp;커뮤니티를&amp;nbsp;통해&amp;nbsp;IT에&amp;nbsp;입문해서, 20여년이&amp;nbsp;넘는&amp;nbsp;IT&amp;nbsp;경력&amp;nbsp;중에&amp;nbsp; 5년&amp;nbsp;마다&amp;nbsp;활동&amp;nbsp;커뮤니티를&amp;nbsp;바꿔&amp;nbsp;가면서&amp;nbsp;참여하고&amp;nbsp;있습니다.&amp;nbsp;그간&amp;nbsp;경험에&amp;nbsp;따르면,&amp;nbsp;모든&amp;nbsp;신흥&amp;nbsp;IT&amp;nbsp;기술은&amp;nbsp;항상&amp;nbsp;커뮤니티가&amp;nbsp;선행한다는&amp;nbsp;것입니다.&amp;nbsp;따라서,&amp;nbsp;초기에&amp;nbsp;커뮤니티에&amp;nbsp;진입하는&amp;nbsp;사람은&amp;nbsp;신&amp;nbsp;기술에&amp;nbsp;업무&amp;nbsp;경험이&amp;nbsp;전혀&amp;nbsp;없는&amp;nbsp;문외한인데다,&amp;nbsp;대개&amp;nbsp;기존&amp;nbsp;회사에서&amp;nbsp;일이&amp;nbsp;별로&amp;nbsp;없어&amp;nbsp;새&amp;nbsp;기술을&amp;nbsp;배우는데&amp;nbsp;여유가&amp;nbsp;있거나&amp;nbsp;혹인&amp;nbsp;진짜&amp;nbsp;백수인&amp;nbsp;경우가&amp;nbsp;태반입니다.&amp;nbsp;다만,&amp;nbsp;이런&amp;nbsp;사람들은&amp;nbsp;신&amp;nbsp;기술을&amp;nbsp;배우는데&amp;nbsp;주저하지&amp;nbsp;않고,&amp;nbsp;적극적으로&amp;nbsp;사람들을&amp;nbsp;만나서&amp;nbsp;배우는&amp;nbsp;데&amp;nbsp;꺼리낌이&amp;nbsp;없다는&amp;nbsp;점이&amp;nbsp;조금&amp;nbsp;다릅니다.&lt;/p&gt;



&lt;p&gt;제가&amp;nbsp;Daum에&amp;nbsp;근무할&amp;nbsp;때인&amp;nbsp;2012년에&amp;nbsp;&lt;a href="https://awskrug.github.io/index.html"&gt;AWS한국&amp;nbsp;사용자모임(AWSKRUG)&lt;/a&gt;이&amp;nbsp;처음&amp;nbsp;만들어졌습니다.&amp;nbsp;그&amp;nbsp;당시AWS를&amp;nbsp;쓰던&amp;nbsp;몇몇&amp;nbsp;스타트업&amp;nbsp;개발자들과&amp;nbsp;재미로&amp;nbsp;사용해보던&amp;nbsp;학생들이&amp;nbsp;일년에&amp;nbsp;한&amp;nbsp;두번&amp;nbsp;모여&amp;nbsp;세션을&amp;nbsp;진행하던&amp;nbsp;커뮤니티인데,&amp;nbsp;당시&amp;nbsp;저도&amp;nbsp;참여를&amp;nbsp;하면서,&amp;nbsp;모임&amp;nbsp;장소가&amp;nbsp;없어서&amp;nbsp;다음&amp;nbsp;오피스&amp;nbsp;회의실을&amp;nbsp;빌려주기도&amp;nbsp;했습니다.&amp;nbsp;제가&amp;nbsp;참여한&amp;nbsp;이유는&amp;nbsp;2009년&amp;nbsp;부터&amp;nbsp;2년간&amp;nbsp;박사과정&amp;nbsp;코스웍을&amp;nbsp;할&amp;nbsp;때,&amp;nbsp;처음으로&amp;nbsp;AWS를&amp;nbsp;기반으로&amp;nbsp;하둡클러스터를&amp;nbsp;만들었는데요.&amp;nbsp;다음에&amp;nbsp;근무할때는&amp;nbsp;풍성한(?)&amp;nbsp;물리&amp;nbsp;장비를&amp;nbsp;쓰다가,&amp;nbsp;가난한&amp;nbsp;랩에서&amp;nbsp;어쩔&amp;nbsp;수&amp;nbsp;없이&amp;nbsp;AWS를&amp;nbsp;쓸&amp;nbsp;수&amp;nbsp;밖에&amp;nbsp;없더라구요.&lt;/p&gt;



&lt;p&gt;2011년에&amp;nbsp;다음&amp;nbsp;복귀하면서&amp;nbsp;처음&amp;nbsp;했던&amp;nbsp;일이&amp;nbsp;바로&amp;nbsp;내부에 프라이빗&amp;nbsp;클라우드팜을&amp;nbsp;만들고,글로벌&amp;nbsp;서비스&amp;nbsp;만들던&amp;nbsp;개발팀에&amp;nbsp;&lt;a href="https://www.youtube.com/watch?v=FAsDEsVFGDU"&gt;AWS 활용을 지원&lt;/a&gt;하기도 했습니다. &amp;nbsp;덕분에 지금의 회사로 오게 된 인연이 되기도 했죠. 사실 좋은&amp;nbsp;회사에&amp;nbsp;다니고&amp;nbsp;있다&amp;nbsp;보면,&amp;nbsp;밖에서&amp;nbsp;어떤&amp;nbsp;일이&amp;nbsp;벌어지는지&amp;nbsp;잘&amp;nbsp;알&amp;nbsp;수가&amp;nbsp;없습니다.&amp;nbsp;대부분의&amp;nbsp;혁신은&amp;nbsp;외부에서&amp;nbsp;일어나고,&amp;nbsp;밖에&amp;nbsp;있는&amp;nbsp;사람을&amp;nbsp;안으로&amp;nbsp;데리고&amp;nbsp;오던지&amp;nbsp;아니면&amp;nbsp;나가서&amp;nbsp;배워야&amp;nbsp;알&amp;nbsp;수가&amp;nbsp;있습니다.&lt;/p&gt;



&lt;p&gt;6년이&amp;nbsp;지난&amp;nbsp;AWSKRUG는&amp;nbsp;페이스북,&amp;nbsp;슬랙,&amp;nbsp;밋업닷컴을 합해 총&amp;nbsp;2만명이&amp;nbsp;모여있는&amp;nbsp;거대&amp;nbsp;커뮤니티로&amp;nbsp;성장했습니다. &amp;nbsp;커뮤니티 활동면에서도&amp;nbsp;올해만&amp;nbsp;20개가&amp;nbsp;넘는&amp;nbsp;소모임에서&amp;nbsp;97번의&amp;nbsp;밋업이&amp;nbsp;열렸고,&amp;nbsp;연인원&amp;nbsp;3,600여명이&amp;nbsp;참여했습니다.&amp;nbsp;밋업마다&amp;nbsp;평균&amp;nbsp;2개의&amp;nbsp;발표가&amp;nbsp;있으니&amp;nbsp;진행한&amp;nbsp;세션만&amp;nbsp;&lt;a href="https://github.com/awskrug/meetups"&gt;200개&lt;/a&gt;가&amp;nbsp;넘고,&amp;nbsp;공식적인 AWS Summit이나&amp;nbsp;DevDay, Community Day등에서&amp;nbsp;진행한&amp;nbsp;품질&amp;nbsp;높은 세션&amp;nbsp;숫자도&amp;nbsp;40개에&amp;nbsp;달합니다.&amp;nbsp;지난&amp;nbsp;리인벤트에&amp;nbsp;참여했던&amp;nbsp;15명의&amp;nbsp;개발자들이&amp;nbsp;내년&amp;nbsp;초에&amp;nbsp;가장&amp;nbsp;관심이&amp;nbsp;높은&amp;nbsp;신규&amp;nbsp;서비스만&amp;nbsp;뽑아서 &lt;a href="https://pages.awscloud.com/aws-community-day-seoul-2019.html"&gt;AWS Community Day&lt;/a&gt;도&amp;nbsp;진행합니다.&amp;nbsp;각&amp;nbsp;소모임에는&amp;nbsp;수십&amp;nbsp;명의&amp;nbsp;개발자들이&amp;nbsp;운영진으로&amp;nbsp;참여하고&amp;nbsp;있고,&amp;nbsp;여기에&amp;nbsp;참여하는&amp;nbsp;사람들간의정보&amp;nbsp;교류는&amp;nbsp;우리&amp;nbsp;상상을&amp;nbsp;초월합니다.&lt;/p&gt;



&lt;p&gt;제가&amp;nbsp;개발자&amp;nbsp;커뮤니티를&amp;nbsp;중요시하는&amp;nbsp;것은&amp;nbsp;두&amp;nbsp;가지&amp;nbsp;이유에서입니다.&amp;nbsp;하나는&amp;nbsp;개발자의&amp;nbsp;후생&amp;nbsp;때문입니다.신기술은&amp;nbsp;개발자들의&amp;nbsp;새로운&amp;nbsp;일자리를&amp;nbsp;만드는데&amp;nbsp;굉장히&amp;nbsp;큰&amp;nbsp;영향을&amp;nbsp;끼치고,&amp;nbsp;커뮤니티에서&amp;nbsp;신기술을&amp;nbsp;장착한&amp;nbsp;사람들이&amp;nbsp;회사를&amp;nbsp;옮겨&amp;nbsp;다니면서&amp;nbsp;기술&amp;nbsp;전파를&amp;nbsp;하고&amp;nbsp;다른&amp;nbsp;사람에게&amp;nbsp;영향을&amp;nbsp;줍니다.&amp;nbsp;주기적으로&amp;nbsp;신기술이&amp;nbsp;IT&amp;nbsp;시장을&amp;nbsp;변혁하기&amp;nbsp;때문에,&amp;nbsp;그&amp;nbsp;씨앗(seed)은&amp;nbsp;회사가&amp;nbsp;아니라&amp;nbsp;커뮤니티일&amp;nbsp;수&amp;nbsp;밖에&amp;nbsp;없습니다.&amp;nbsp;지난&amp;nbsp;목요일에&amp;nbsp;AWSKRUG&amp;nbsp;송년회가&amp;nbsp;열렸는데,&amp;nbsp;끝나고&amp;nbsp;라이트닝&amp;nbsp;토크를&amp;nbsp;시작했더니&amp;nbsp;20명이&amp;nbsp;넘는&amp;nbsp;사람들이&amp;nbsp;AWS가&amp;nbsp;자기를&amp;nbsp;어떻게&amp;nbsp;바꿨는지&amp;nbsp;간증(?)을&amp;nbsp;하더군요.&amp;nbsp;대부분&amp;nbsp;자기&amp;nbsp;회사에서&amp;nbsp;AWS를&amp;nbsp;잘하는&amp;nbsp;사람&amp;nbsp;구인한다는&amp;nbsp;이야기를&amp;nbsp;포함해서요.&lt;/p&gt;



&lt;img src="http://channy.creation.net/data/channy/2015/IMG_0363-940x705.jpg" alt="" class="wp-image-1206" /&gt;AWSKRUG 송년회 라이트닝 토크 모습 (2018.12.20)



&lt;p&gt;두번째는&amp;nbsp;커뮤니티가&amp;nbsp;개발자를&amp;nbsp;돋보이게&amp;nbsp;해&amp;nbsp;줍니다.&amp;nbsp;일부의&amp;nbsp;부작용이&amp;nbsp;분명히&amp;nbsp;존재하지만,&amp;nbsp;자기&amp;nbsp;일의&amp;nbsp;열정과&amp;nbsp;가치를&amp;nbsp;만들어&amp;nbsp;주는&amp;nbsp;곳입니다.&amp;nbsp;대개&amp;nbsp;커뮤니티에서는&amp;nbsp;발표를&amp;nbsp;장려하고,&amp;nbsp;작은&amp;nbsp;것도&amp;nbsp;칭찬해주고&amp;nbsp;서로의&amp;nbsp;고민을&amp;nbsp;들어주고&amp;nbsp;격려해주거든요.&amp;nbsp;회사에서는&amp;nbsp;생존을&amp;nbsp;위해&amp;nbsp;기술을&amp;nbsp;장착해야&amp;nbsp;하고&amp;nbsp;평가를&amp;nbsp;받아야&amp;nbsp;하지만&amp;nbsp;커뮤니티는&amp;nbsp;그런&amp;nbsp;이해&amp;nbsp;관계가&amp;nbsp;전혀&amp;nbsp;없기&amp;nbsp;때문이죠.&amp;nbsp;일부는&amp;nbsp;유명한&amp;nbsp;사람이&amp;nbsp;되고,&amp;nbsp;다른&amp;nbsp;개발자에게&amp;nbsp;주는&amp;nbsp;영향력이&amp;nbsp;크게&amp;nbsp;올라갑니다.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;그만큼 개발자에게 커뮤니티는 필수 불가결한 것이구요. 혹시 여러분이 마음에 있는 커뮤니티가 있다면, 어떤 주제라도 상관없으니 내년에는 적극적으로 참여해 보셨으면 합니다. 다만, 커뮤니티는 기술의 변화에 따라 흥망성쇠를 거듭하는 생물과 같습니다. 지금 가장 인기있는 커뮤니티를 고른다면, 가장 열정적인 사람들을 만날 수 있겠죠.&lt;/p&gt;



&lt;p&gt;아직까지&amp;nbsp;&lt;a href="https://awskrug.github.io/"&gt;AWSKRUG&lt;/a&gt;에&amp;nbsp;가입하지&amp;nbsp;않으신&amp;nbsp;분들이라면&amp;nbsp;&lt;a href="https://www.facebook.com/groups/awskrug"&gt;페북에&amp;nbsp;가입&lt;/a&gt;을&amp;nbsp;해주시고,&amp;nbsp;어떤&amp;nbsp;일이&amp;nbsp;벌어지고&amp;nbsp;있는지&amp;nbsp;눈팅하셔도&amp;nbsp;좋을&amp;nbsp;것&amp;nbsp;같습니다. ^^&lt;/p&gt;



&lt;p&gt;즐거운&amp;nbsp;연말&amp;nbsp;연시&amp;nbsp;보내시구요.&amp;nbsp;새해에&amp;nbsp;새&amp;nbsp;마음으로&amp;nbsp;만나요~!&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=hVbAtZxXrok:WijZ6jmwK1E:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 26 Dec 2018 19:10:37 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1204#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>훌륭한 개발 문화의 이면(5) – 소통 비용의 절약: 서로 API로 말하자</title>
	<link>http://channy.creation.net/blog/1199</link>
	<description>&lt;p&gt;효율적인 개발과 운영을 위해서 개발팀 내에서 다양한 의사 소통 수단은 필수적입니다. 이메일, 메신저, 이슈트래커와 코드 리뷰 등 다양한 방법이 동원되죠. 우리가 만드는 소프트웨어 혹은 서비스간 소통도 매우 중요합니다. 대개 팀 내 혹은 팀간 서비스간 인터페이스(Interface)는 각양각색입니다. CSV나 엑셀 파일을 필요할 때 메일로 보내 준다거나, 주기적으로 XML 파일을 대량 다운로드 받게 하기도 합니다. 최근들어서는 JSON 방식의 REST API를 제공하는 경우가 늘고 있습니다.&lt;/p&gt;
&lt;p&gt;사실 API라고 하면 우리가 프로그램을 짤 때, (자바 API나 닷넷 API 처럼) 특정 라이브러리나 플랫폼의 원하는 기능을 호출하는 인터페이스를 말하는 것인데, 최근에는 데이터를 교환하거나 애플리케이션 내 구현 기능을 호출할 때 사용하고 있습니다. API를 통한 메시지 교환 방식은 이른바 서비스 지향 아키텍터(SOA)가 나온 시기로 거슬러 올라가지만, 본격적으로 활용 되기 시작한 때는 바로 웹 2.0이 태동 되던 2000년대 중반이라고 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 웹2.0, 오픈 API의 태동&lt;/strong&gt;&lt;br /&gt;
2005년 초반에 저는 Daum의 CTO와 함께 전사 기술 전략과 개발자 채용, 경력 관리 등을 새로 셋팅하는 작업을 하고 있었습니다. 당시 300여명이 넘는 개발자가 20여개가 넘는 개발팀에서 다양한 방식으로 협업을 하는데, 서비스간 소통 방식을 각양각색이었죠. 웹 서비스에서 어떻게 하면 확장 가능하고 효율적인 소통을 만들고, 이를 사내 뿐만 아니라 회사 밖 제3자 개발자에게 전달할 수 있을까 하는 게 주요 관심사였습니다.&lt;/p&gt;
&lt;p&gt;2004년 열린 &lt;a href="http://channy.creation.net/blog/133"&gt;오라일리 웹2.0 컨퍼런스&lt;/a&gt;에서는 닷컴버블 이후에 살아남은 인터넷 서비스 기업들이 어떻게 플랫폼 기업으로 살아 남았는지 알려주는 큰 계기가 되었습니다. 즉, 구글, 아마존, 이베이 같은 기업들은 자사의 서비스 인터페이스를 공개함으로서 플랫폼 영향력을 확대했다는 것입니다. 원래 소프트웨어 플랫폼 API는 폐쇄적인데 반해, 웹 서비스 기반 API는 공개되어 있다고 해서 이를 오픈 API라고 불렀습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://blog.creation.net/wp-content/uploads/1/dk14.jpg" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;당시 다음은 웹 검색 엔진을 구글의 유료 API 서비스를 차용하고 있었습니다. 즉, REST API로 검색어 질의를 보내면, 반환된 XML 기반 메시지를  파싱 후 제공하는 간단한 것이었지만, 연간 API 서비스 비용이 수억원에 달하고 있었습니다. 마찬가지로 아마존은 야후 같은 포털에 상품 및 리뷰 API를 제공해 주고, 사용자가 상품 구매 시 커미션을 주는 비지니스를 하고 있었고, 이베이는 경매 상품을 올리는 API를 제공하고 있었는데 무려 50%에 달하는 상품이 리스팅 API를 통해 이루어지고 있었습니다. 대량 판매자들에게 이베이에 상품을 올려주는 프로그램이 엄청나게 많이 만들어지고 있었죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 플랫폼 vs. 서비스&lt;/strong&gt;&lt;br /&gt;
제가 주목한 건 바로 오픈 API가 플랫폼 사업의 가능성을 보여준다는 사실이었습니다. Daum 사용자는 웹 브라우저와 모바일로 메일, 카페, 검색, 지도를 이용하지만, 이를 오픈 API로 제공하면 이를 활용하는 기업의 또 다른 사용자층까지 확보할 수 있습니다. 예를 들어, 다음 메일을 기업용 인트라넷을 만드는 회사에 API로 제공하면, 이들은 사내용 메일을 구축하려는 기업에게 판매를 할 수 있겠죠. 그러면, 개인용으로만 다음 메일을 사용하는 사람들이 회사에서도 사용하니까, 보이지 않는 사용자의 사용 시간도 대거 확보할 수 있겠죠.&lt;/p&gt;
&lt;p&gt;그러나, 이러한 전략은 회사에서 크게 힘을 발휘하지 못했습니다. 왜냐하면, 각 팀의 역량을 모드 일반 사용자에게 쏟고 있는데 외부 기업이나 사용자까지 지원할 여력이 부족하고, 제공하더라도 3자 서비스는 사용하는 다음 밖에 사용자가 다음 사용자냐 아니냐 하는 문제도 있으니까요. 요즘은 일반화한 API 유료 판매 모델이나 광고 등 수익화 방식 제휴 등도 명확하지 않았던 시기였습니다. &lt;i&gt;(왜 플랫폼을 위한 사내 문화가 중요한지 &lt;a href="http://blog.creation.net/515"&gt;플랫폼은 문화다!&lt;/a&gt;라는 글을 참고하세요.)&lt;/i&gt; &lt;/p&gt;
&lt;p&gt;2006년 3월 저는 웹 기술 커뮤니티 멤버들과 함께 &lt;a href="http://channy.creation.net/blog/293"&gt;NGWEB 2.0&lt;/a&gt; 행사 프로그램에 참여하고 있었는데, 그 행사에는 당시 아마존웹서비스(AWS)의 에반젤리스트였던 Jeff Barr가 방한해 &amp;#8220;&lt;a href="https://news.naver.com/main/read.nhn?mode=LSD&amp;mid=shm&amp;sid1=105&amp;oid=022&amp;aid=0000151446"&gt;AWS는 웹 2.0 핵심&lt;/a&gt;&amp;#8220;라는 주제로 기조 연설을 하였습니다. 당시만 해도 AWS는 오늘날 같은 클라우드 서비스가 아니라, &lt;a href="http://channy.creation.net/blog/465"&gt;상품 및 검색 API 등을 제공&lt;/a&gt;하고 있었습니다. 제프는 행사 전날 도착해서 기조 연설을 마치고 부랴부랴 귀국길에 올랐는데, 그 날이 AWS 클라우드의 사실상 최초 서비스인 Amazon S3가 출시한 날이었습니다. (왜 그렇게 빨리 갔는지 그 뒤에 알게 되었죠.)&lt;/p&gt;
&lt;p&gt;그 해, 저는 &lt;a href="http://channy.creation.net/blog/336"&gt;이베이 개발자 컨퍼런스&lt;/a&gt; 및 &lt;a href="http://channy.creation.net/blog/332"&gt;Where 2.0 컨퍼런스&lt;/a&gt; 그리고 Google Maps Developer Day에서 세상의 변화를 현장에서 목도 할 수 있었습니다. 여기서 보고 느낀 점이 새로운 일을 만드는데 큰 동기 부여가 되었습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ Daum DNA, 오픈 API 플랫폼을 만들다&lt;/strong&gt;&lt;br /&gt;
안타깝게도 제가 하려고 하는 일에 회사는 관심이 없었지만, 다행히 못하게 말리지는 않더군요. CTO를 설득해서 저의 사이드잡으로 하기로 하고, 우선 검색팀에 가서 막무가내로 몇 가지 API를 만들어달라고 했습니다. 당시는 다음소프트에서 운영하던 검색 서비스를 직접 개발 운영하는 내재화를 시작하고 있었을 뿐 아니라, 때마침 네이버 검색 API가 오픈했던 시기라서 잘 설득할 수 있었습니다. 그리고, 당시 오픈 ID에 관심이 있던 회원정보팀 팀장에게 부탁해서 API를 위한 XML기반 인증 수단(그 이후에 OAuth로 변경)도 만들었습니다. 이는 메일, 블로그, 카페처럼 다음 아이디 기반 서비스 API를 만들 수 있게 하기 위함이었습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" title="" src="http://channy.creation.net/wp/data/channy/2006/2006-daum-dna-screenshot.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;이래저래 십시일반 도움을 받아 2006년 10월&lt;a href="http://channy.creation.net/blog/359"&gt; 다음개발자네트워크(Daum DNA)를 오픈&lt;/a&gt;할 수 있게 되고, 1명의 팀원과 인턴을 받아서 작은팀을 꾸렸습니다. 그 후 여러분이 아시는 대로, API 매쉬업과 해커톤 같은 다양한 개발자 전도 활동과 기업간 API 제휴 사업을 확대할 수 있었습니다.&lt;/p&gt;
&lt;p&gt;7년이 지나 2013년 연말에는 30여개의 API가 공개되어 있었고, 매일 7천개의 API키에서 &lt;a href="http://biz.chosun.com/site/data/html_dir/2013/12/03/2013120301304.html"&gt;월간 3억건의 호출&lt;/a&gt;이 일어나고 있었습니다. 검색 및 지도를 비롯해서 다음 오픈 API는 국내에서 가장 앞선 API 서비스였다고 자부할 수 있었습니다.&lt;/p&gt;
&lt;p&gt;그 와중에서 오픈 API 플랫폼을 구축하고 운영하는 일은 매우 어려운 일이었습니다. 가장 애로점은 API 공개를 설득하는 일이었습니다. 앞에서 말씀 드린대로 없던 API를 만들고 이를 공개하게 하는 건 서비스 기획자들이 이해하기 힘들 일이죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 내부 API 활성화라는 부수 효과&lt;/strong&gt;&lt;br /&gt;
그런데, 생각지도 못한 일이 벌어졌습니다. 맨 처음 API를 만든 검색팀의 경우, 오픈 API로 만든 인터페이스를 내부 개발팀이 사용하고 싶다고 요청이 늘어난 것이죠. 블로그나 카페 검색 결과 같은 기능을 내부 여러 서비스에서 추가하려다 보니 필연적으로 편한 API를 활용하겠다는 것입니다. 그러다 보니, 검색팀에서 오픈 API외에도 다른 종류의 내부 API를 추가로 만들고 심지어 API 제공을 위한 서버팜을 구성하기에 이르렀습니다.&lt;/p&gt;
&lt;p&gt;실제 검색 API의 경우 오픈 API로 나가는 트래픽은 얼마 되지 않았는데, 내부 API 호출이 기하급수적으로 늘어나는 것이었습니다. 게다가 API를 만들어 공유하는데 소극적인 개발팀들이 좀 더 쉽게 협업을 하기 위해 API를 만들어 직접 내부 공개하고, 심지어 요청하지도 않았는데 Daum DNA에 공개해달라고 부탁하기도 했습니다. 외부 개발자를 위한 플랫폼을 만들겠다고 시작한 일인데, 오히려 사내의 활용도가 더 커지고 있었던 것이죠. 이를 통해 크게 배운 게 있습니다. 결국 내부에서 (개밥먹기 방식으로) 효율성이 인정되면, 결국 옳은 방향으로 가게 된다는 점입니다.&lt;/p&gt;
&lt;p&gt;이렇게 API간 직접 호출이 늘어나니 보이지 않은 부작용도 생겼습니다. 예를 들어, 여행 섹션 같이 새로 오픈 하는 섹션은 지도 API나 블로그 검색 API 등을 활용한 정보 페이지가 주요 기능이 되다 보니, 타 팀의 API의 직접 호출이 그 팀 API 서비스 트래픽 및 부하에 영향을 주는 것입니다. 그러다 보니, 타팀에서 오는 운영 부하에 드는 부담을 도대체 어디까지 질건가 하는게 큰 이슈가 되었죠. 공개된 API를 누가 쓰는지 얼마나 쓰는지 알아야 하는 문제도 발생하였습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1200" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/internal-api.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;당시 오픈 API 플랫폼은 외부 개발자 인증/키발급과 캐싱을 통해 어느 정도 부하를 처리하고 있었는데, 오로지 다음 내부 개발자를 위한 API 서비스 플랫폼이 필요하게 되었고 결국 저희팀에서 TF를 만들어 내부 API Hub를 만들게 되었습니다. 그 후 사내에 있었는지도 몰랐던 API들이 하나둘씩 등록하게 되고, 서로 서로 손쉽게 찾고 키 발급 받아 활용할 수 있게 되었습니다. 2013년 내부 API 호출 수는 일간 5억건에 달했습니다. 사실 99% 트래픽이 사내에서 소비되고, 1%만이 외부에서 사용하는 꼴인데, 이건 당시 에버노트나 넷플릭스의 API 사례에서도 마찬가지였습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ API 소통을 통한 마이크로서비스로의 전환&lt;/strong&gt;&lt;br /&gt;
이제는 API을 통한 소통이 일상화 되었습니다. 작은 스타트업도 자신들의 서비스를 API로 공개해 놓고 있으며, 많은 국내 IT 기업들도 API를 외부에 오픈해 놓고 있습니다. 누구나 이러한 흐름에 동의할 것입니다. 최근에 &lt;a href="http://channy.creation.net/articles/microservices-by-james_lewes-martin_fowler"&gt;마이크로서비스(Microservices) 아키텍처&lt;/a&gt;가 각광을 받고 있습니다. 즉, 독립적인 작은 서비스 단위로 기능을 쪼개고, API로 소통함으로서 개발 및 배포 속도를 높이는 기법입니다. 하지만, 아직까지 내부 팀간 협업을 REST API로 하고 있지 않다면, 좀 더 민첩한 개발 방식을 도입하는데 큰 장애가 될 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2015/microservice-netflix.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;당장 모든 기능을 API로 만들 필요는 없습니다. 우선 외부 팀에서 많이 요청하는 몇 가지 기능 부터 별도로 쪼갠 후, API로 만들어 제공하는 것이 좋습니다. 일단 누군가 API를 쓰기 시작하면, 이들 트래픽이 주 사용자에 영향을 미치지 않아야 합니다. 또한, 주기적인 피드백으로 기능 개선을 따로 할 수 있기 때문이죠. 마이크로서비스라는게 어려운게 아니고, 바로 &lt;a href="http://channy.creation.net/blog/1051"&gt;이렇게 시작&lt;/a&gt;하는 것입니다. 아직 여러분 회사는 API로 소통하지 않으세요? 지금 시작하세요!&lt;/p&gt;
&lt;p&gt;참고. 아래는 제가 발표한 Daum API 사례 및 마이크로서비스 아키텍처 구현 사례에 대한 발표 자료 및 영상입니다.&lt;br /&gt;
 &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;혹시 API 모범 사례에 대한 다양한 정보를 수집하시려면 아래 해외 컨퍼런스를 참고하시기 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://events.linuxfoundation.org/events/apistrat-2018/"&gt;API Strategy &amp;amp; Practice Conference 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://apiworld.co/"&gt;API World 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.apidays.co/"&gt;API Days 2018 Series &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gluecon.com/"&gt;GlueCon 2019 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;연재 목차&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1104"&gt;훌륭한 개발 문화의 이면(1) – 코딩 테스트인터뷰 제대로 하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1107"&gt;훌륭한 개발 문화의 이면(2) – 자율적 개발 환경을 선택하라!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1110"&gt;훌륭한 개발 문화의 이면(3) – 다른 팀 소스 코드를 볼 수 있는가?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1183"&gt;훌륭한 개발 문화의 이면(4) – 사내 라이브러리를 잘 관리하려면?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1199"&gt;훌륭한 개발 문화의 이면(5) – 소통 비용의 절약 &amp;#8211; 서로 API로 말하자&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;훌륭한 개발 문화의 이면(6) – 오픈 소스를 통한 외부 개발자 전략 구사하기&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=GUYTDkZENSQ:uCarCIxNuzs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 29 Jul 2018 23:30:46 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1199#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>트럼프는 어떻게 김정은을 만났나?</title>
	<link>http://channy.creation.net/blog/1194</link>
	<description>&lt;p&gt;70년 적대 관계 후 북미 정상의 역사적 첫 만남! ‬1년 전 도널드 트럼프 미국 대통령과 김정은 북한 국무위원장의 서로의 행보를 볼때, 일어나지 않을 정말 기적과 같은 일이 벌어졌습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1195" src="http://channy.creation.net/data/channy/2018/06/13150217/dprk-us-summit.jpg" alt="" width="960" height="540" /&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Donald Trump &amp;#8220;위대한 회담에서 놀랄만한 성공으로 의심 없이 좋은 관계 맺을 것이다.&amp;#8221; 김정은, &amp;#8220;우리 발목을 잡는 과거들이 때론 눈과 귀를 가렸으나 모든 것을 이겨내고 이 자리까지 왔다.&amp;#8221;‬&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;지난 미국 대선에서 한국에서도 정말 누구도 대통령이 될 거라 생각도 못했고 깜도 안된다고 생각했지만, 반전은 일어났고요. 북핵 고도화 상황에서 온탕과 냉탕을 오갔지만, 결국은 변화와 결과를 만들어 내고 있습니다. 무엇 보다 주목할 점은 회담 이후, 트럼프 대통령 한 시간 동안 전 세계 기자들과 혼자서 기자 회견을 했다는 점입니다. 김정은 위원장은 호텔에 가서 봤겠죠.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;span class="mce_SELRES_start"&gt;﻿&lt;/span&gt;&lt;/center&gt;쏟아지는 질문에 대해 막힘없이 다 막아 내면서 드러난 트럼프의 인식은 그동안 봐왔던 정치인들과 확연히 다른 것이었습니다. 이 날의 기록을 잠깐 남겨볼려고 하는데, 본 기자 회담을 살펴 보면서 어떻게 이런 일이 벌어질 수 있었는지 나름의 생각을 정리해 봤습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Bottom-up이 아닌 Top-down &lt;/strong&gt;&lt;br /&gt;
그동안 북미 협상은 사실상 (민주당 정권에서) 우선 순위에 밀려있거나, (공화당 정권에서) 북한에 대한 불신이 강한 상태에서, 북미 실무진 혹은 다자 회담에서만 서로 이야기하고 통큰 결단이 일어나지 못했죠. 역사적으로 닉슨-모택동(1972), 레이건-고르바초프(1986), 오바마-카스트로(2016) 등 공산 정권과의 관계 정상화는 탑-다운인데 진행 됐던데 반해, 그동안 북미 관계 접근 방식이 맞지 않았다고 생각합니다. 오히려 트럼프 대통령의 리더쉽 자체가 탑-다운을 지향하고, 자기를 돋보이게 하는 쇼맨쉽이 강하다는 측면에서 큰 도움이 되었네요. 기존 미국 대통령들이 명분과 주변 관계에 너무 신경썼었던 반면, 기존 정치권과 완전히 다른 트럼프의 성향이 드러난 결과입니다. 김정은 위원장에게도 국제 사회에 데뷰할 수 있게 하는 원동력이 됐을 겁니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 대북 관계도 돈의 문제로&lt;/strong&gt;&lt;br /&gt;
기자 회견 내내 트럼프는 대북 관계를 돈으로 계산했습니다. &amp;#8220;북한이 평창 올림픽에 참여함으로서 (티켓 판매에 도움을 줘) 오히려 성공을 거두었다&amp;#8230; 북한 핵사찰을 할 때 검증단이 들어가도 돈이 많이 든다&amp;#8230; 한미 군사 훈련시, 괌에서 비싼 장비를 6시간 동안 날아가 폭탄 투하 훈련을 하는 것도 돈이 많이 든다.&amp;#8221; 이는 한반도 문제만 아니라, 유럽에서 NATO 운영 비용에 대한 교역 불균형 문제, 중국과 캐나다의 무역 적자 이슈 등 트럼프의 국제 외교 이슈도 모든 문제를 경제적인 관점에서 바라보고 있다. 기존 미국 대통령들이 정치적인 명분과 관계에 집중하는 것과는 확연이 다른 점입니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. 인권 문제도 자국 이익 우선 &lt;/strong&gt;&lt;br /&gt;
북한 인권에 대해 이야기했느냐, 어떻게 해결할거냐는 질문에 대해 트럼프 대통령은 오히려 &amp;#8220;미군 유해 송환 합의&amp;#8221;에 방점을 두었습니다. 사실 북한 주민의 인권 문제 보다도 자국의 이익을 우선한 것이죠. 한번에 하나씩 해결하는 방법이기도 하지만, 미국 자국민의 인권을 먼저 생각한것입니다. 실질적인 북미 관계가 좋았던 1990년 &lt;a href="http://shindonga.donga.com/Print?cid=100656"&gt;미군 유해 발굴 사업&lt;/a&gt;을 시작하여 2007년까지 443구의 유해가 미국으로 송환됐고, 2007년 빌 리처드슨 미국 뉴멕시코 주지사 방문으로 송환된 6구를 마지막으로 유해 발굴은 중단됐습니다. &lt;/p&gt;
&lt;p&gt;과거에 미군 유해 송환시, 유해 발굴 비용(북한 주민 임금, 대지 활용 비용)등을 북한측에 제공하기도 했구요. 실제로, 즉각적인 진행이 되면 미군 유해 발굴단이 북한에 직접 들어가게 되므로 이러한 과정 중에도 평화 유지가 될 수 있는 좋은 합의라고 생각됩니다.&lt;/p&gt;
&lt;p&gt;‪트럼프 대통령은 지극히 이기적인 사람입니다. 북미 관계 개선을 자신의 정치적 성과로 만들 목적이 뚜렸합니다. 기자 회견 중에도 기존 정권과 달리 본인에게 우선 순위가 높다고 했으니까요. 하지만, 이런 상황이 오히려 한반도 평화에 도움을 주고 있는 역설적인 상황입니다. &lt;/p&gt;
&lt;p&gt;미국 보수적 공화당 정권의 이상한(?) 대통령 그리고 한국의 민주 정권, 그리고 이제 막 밖으로 나오려는 북한의 젊은 지도자&amp;#8230; 이러한 조합은 정말 하늘이 준 기회라고 생각이 됩니다. &lt;/p&gt;
&lt;p&gt;역사는 늘 바뀝니다. 어제의 적이 오늘의 친구가 되고, 모로가도 서울만 가면 되는거죠. (북한 로동신문에 게재된) 대표적인 매파인 존 볼턴과 김정은의 악수가 그런것을 보여줍니다. &lt;/p&gt;
&lt;p&gt;&lt;img src="http://channy.creation.net/data/channy/2018/06/13154213/bolton-kim-hands.png" alt="" width="640" height="381" class="alignnone size-full wp-image-1197" /&gt;&lt;/p&gt;
&lt;p&gt;이제 시작입니다! 이를 기점으로 진정한 평화가 오길 바랍니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Donald Trump, “누구나 전쟁을 일으킬 수 있지만, 오직 용기 있는 자만이 평화를 만들 수 있다. (Anyone can make war, but only the most courageous can make peace.)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;h3 class="collapseomatic " id="id5cc878460688c" tabindex="0" title="참고- 북미 공동 성명 한국어 전문 자세히보기"&gt;참고- 북미 공동 성명 한국어 전문 자세히보기&lt;/h3&gt;&lt;div id="target-id5cc878460688c" class="collapseomatic_content "&gt;&lt;br /&gt;
&lt;small&gt;&lt;br /&gt;
도널드 트럼프 미국 대통령과 김정은 북한 국무위원장은 역사적인 첫 정상회담을 2018년 6월 12일 개최했다.&lt;/small&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;트럼프 대통령과 김정은 위원장은 새로운 미국과 북한의 관계와 한반도의 평화를 위한 포괄적이고, 심도있고, 진심이 담긴 의견을 교환했다.&lt;/p&gt;
&lt;p&gt;트럼프 대통령은 북한에 체제 안정을 제공하기로 약속했고, 김정은 위원장은 한반도의 완전한 비핵화를 위한 확실한 약속을 재확인했다.&lt;/p&gt;
&lt;p&gt;새로운 미·북 관계가 한반도와 세계의 평화와 번영을 가져오는 것을 확신하며, 이러한 양측의 자신감은 한반도의 비핵화를 이룰 수 있기 때문에 트럼프 대통령과 김정은 위원장은 다음 내용에 합의한다.&lt;/p&gt;
&lt;p&gt;1. 미국과 조선인민민주주의공화국은 평화와 번영을 위한 양국 국민의 열망에 따라 새로운 미-조 관계를 수립할 것을 약속한다.&lt;/p&gt;
&lt;p&gt;2. 미국과 조선인민민주주의공화국은 한반도에 항구적이고 안정적인 평화 체제를 구축하기 위한 노력에 동참할 것이다.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;3. 조선인민민주주의공화국은 2018년 4월 27일 ‘판문점 선언’을 재확인하고 한반도의 완전한 비핵화를 위해 노력할 것을 약속한다.&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;4. 미국과 조선인민민주주의공화국은 이미 확인된 전쟁 포로 유골의 즉각적인 송환을 포함해 전쟁포로와 실종자의 유해 복구를 약속한다.&lt;/p&gt;
&lt;p&gt;역사상 첫 미·북 정상회담은 두 나라의 수십년간 지속된 적대적인 관계를 청산하고 새로운 미래를 여는 역사적인 행사였다. 트럼프 대통령과 김정은 위원장은 공동 협약의 조항을 완전하고 신속하게 이행하기로 약속한다. 이후 미국과 조선인민민주주의공화국은 마이크 폼페이오 미 국무장관이 진행하는 고위급 실무 회담을 최대한 빨리 추진해 미·북 정상회담의 결과를 실행에 옮길 것이다.&lt;/p&gt;
&lt;p&gt;도널드 트럼프 대통령과 김정은 북한 국무위원장은 새로운 미-조 관계 형성과 더불어, 한반도뿐 아니라 전 세계의 평화·번영·안보를 위해 협력하기로 약속했다.&lt;/p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;img src="http://channy.creation.net/data/channy/2018/06/13154048/trump-kim-sign.jpg" alt="" width="1024" height="443" class="alignnone size-full wp-image-1196" /&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=isYFhCbhIiI:YP-CWcVe4XU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 13 Jun 2018 06:05:09 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1194#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>[후기] 밥잘사주는 예쁜 누나, 지금 만나러 갑니다!</title>
	<link>http://blog.creation.net/sonyejin-pretty-meet-you</link>
	<description>&lt;p&gt;1983년 9월 3일 엄마가 돌아가셨다.&lt;/p&gt;
&lt;p&gt;이미 35년이 흘렀지만 그 날은 잊혀지지 않는다. 내 나이 고작 열살&amp;#8230; 딴에 그 극한의 슬픔을 알았는지, 집 마루에 누워 울다가 지쳐 어렴풋이 쳐다본 하늘은 더럽게도 푸르렀다. 장례가 끝난 후, 몇 달 동안 꿈에 엄마가 집에 돌아 올 정도로 그리웠다. 그렇게 잊혀졌던 나의 아픔과 기억을 소환하고 치유한 영화를 만났다.&lt;/p&gt;
&lt;p&gt;얼마전 개봉했던 &lt;a href="https://search.naver.com/search.naver?query=%EC%A7%80%EA%B8%88+%EB%A7%8C%EB%82%98%EB%9F%AC+%EA%B0%91%EB%8B%88%EB%8B%A4"&gt;&amp;#8216;&lt;strong&gt;지금 만나러 갑니다&amp;#8217;&lt;/strong&gt;&lt;/a&gt;. 배우 소지섭(정우진 역)과 손예진(임수아 역) 뿐만 아니라 엄마를 기다리는 아역 김지환 군(정지호 역)이 출연한 영화다. 이 영화는 장마가 시작되면 다시 돌아오겠다는 약속을 남기고 죽은 엄마가 기억을 잃은 채 부자에게 모습을 나타내면서 시작된다. 그리고, 장마가 끝나면 이들은 다시 이별을 고하게 된다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;small&gt;영화 &amp;#8216;지금 만나러 갑니다&amp;#8217; 엄마 펭귄의 여행 오프닝&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;영화속 대부분의 이야기는 엄마 아빠의 첫사랑과 결혼, 출산 그리고 때 이른 이별에 이르는 필연적 과정을 담고 있지만&amp;#8230; 내가 정말 공감했던 부분은 바로 극중 수아가 아들 지호에게 다시 떠나기 전에 이별 연습을 하던 장면이다. 아들이 엄마 없이도 꿋꿋이 살아갈 수 있도록 계란 후라이를 하는 방법, 빨래 말리는 방법, 청소하는 방법, 머리 감는 방법 등을 가르쳐 준다. 그리고, 매년 아들의 성년이 될때까지 아들 생일을 위해 미리 축하 카드를 써서 전달하는 아빠 친구에게 부탁도 한다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2104" src="http://blog.creation.net/data/tisotry/2018/05/25050711/soyejin-pretty-meet-you-1-1024x513.png" alt="" width="1024" height="513" /&gt;&lt;br /&gt;
&lt;small&gt;수아와 아들 지호와 이별하는 장면&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;과거 엄마와 갑작스런 이별을 했던 나에게 이 장면들 속에 있는 엄마와 아들의 교감과 과정에서 크게 힐링이 되었다. 엄마와 지호가 다시 이별하는 장면에서 &amp;#8216;지호가 없는 세상에서는 백년을 살아도 행복하지 않았을 거야. 엄마는 구름 나라에 가서 지호를 계속 보고 있을께. 멋진 어른이 되야돼&amp;#8217;라는 대사에서 너무 가슴뭉클했다. 천국에 계신 엄마가 보시기에 나도 멋진 어른이 되어 있을까? (이 장면은 두고 두고 다시 볼 수 있게 휴대폰에 클립을 저장해 두었다. 돌아가신 엄마 생전 육성 녹음과 함께&amp;#8230;)&lt;/p&gt;
&lt;p&gt;수많은 영화나 드라마를 보면서 자신의 인생과 경험에 감정을 이입하게 하는 절절한 연기를 만난다는 건 쉽지 않다. 그래서, 이 작품을 통해 이러한 경험을 해주게 한 수아역의 손예진이라는 배우를 다시 보게 되었다. 멜로퀸이라는 수식어가 가능하게 했던 20대의 풋풋한 사랑 이야기, 그리고 그 이후의 다양한 연기 변신을 거듭했지만 공감 연기가 빛을 발하는 그녀의 최근 작품에 매료되었다.&lt;/p&gt;
&lt;p&gt;얼마전 종영한 JTBC 드라마 &amp;#8216;&lt;strong&gt;&lt;a href="https://search.naver.com/search.naver?query=밥+잘+사주는+예쁜+누나"&gt;밥 잘 사주는 예쁜 누나&lt;/a&gt;&lt;/strong&gt;&amp;#8216;도 마찬가지다. 드라마에서 윤진아역을 맡은 손예진은 연하의 친구 동생인 서준희(정해인 분)과 달달한 사랑에 빠지지만 현실 연애의 장벽과 가족의 반대, 사회적인 압박 등을 이겨나가는 30대 여성의 삶을 연기했다.&lt;/p&gt;
&lt;p&gt;우연히 이 드라마를 보게 되었는데 실제로 우리 부부도 연상연하 커플이다 보니 내가 실제 겪은 연애 과정에서의 고민과 아픔이 고스란히 드러나 보였다. 이전에도 연상연하 커플일 다루는 드라마는 많았지만, 대부분 호기심과 케미에 집중했던 반면 이 드라마는 정말 일어나는 일이여서 공감할 수 있었다.&lt;/p&gt;
&lt;p&gt;연상연하 커플의 경우 3살 차이라고 해도 사회 경력이 짧게는 3년 길게는 6년 정도 차이가 나기 때문에, 여자는 (집안의 반대, 연애 후 결혼까지의 과정, 이후 경제적 측면의) 위험 감수를 해야 하고, 남자는 그러한 여자를 안심시켜 주어야 하는 끈질기고 순애보적인 사랑이 있어야만 어느 정도 이뤄진다. 그래서, 내가 결혼했던 20년전에도 우리 나라 전체 결혼 커플의 4.5%만이 연상연하였다. (물론 최근에는 그 비율이 15%까지 늘었다고 한다.)&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2102" src="http://blog.creation.net/data/tisotry/2018/05/25050708/soyejin-pretty-meet-you-2-1024x506.png" alt="" width="1024" height="506" /&gt;&lt;br /&gt;
&lt;small&gt;드라마 &amp;#8216;밥 잘 사주는 예쁜 누나&amp;#8217; 사랑 고백 장면&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;극 중 둘의 사랑이 무르익어가던 6회와 7회에서 준희가 전화로 &amp;#8216;사랑해&amp;#8217;라는 고백을 해도, 아무 말을 할 수 없었던 진아. 그리고 그런 진아에게 &amp;#8216;사랑한다는 데 왜 말이 없어?&amp;#8217;라고 애타하는 마음이 충분히 이해가 갔다. 그리고, 준희의 아낌없이 쏟아내는 사랑에 결국 음성 녹음이지만 &amp;#8216;고마워 나를 사랑해줘서&amp;#8230; 많이 배우고 있어. 준희야 사랑해 아주 많이, 아주 오래 오래 사랑할께&amp;#8217;라는 장면에서 다시 한번 공감이 갔다. (우리 와이프도 나에게 사랑한다는 말을 그렇게 아꼈었다. 하하&amp;#8230;)&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;그 이후에 극 전개가 좀 답답하고 캐릭터간의 감정들이 대치되는 면이 있었으나, 현실 연애에서 늘 있을 법한 배려와 오해가 교차하는 부분이라 충분히 이해할 수 있었다. 다행히 우리 부부는 3년의 연애 끝에 부모님들이 허락을 해주어서 행복한 결혼에 골인할 수 있었다. 딸이 나이가 차고, 아들이 그렇게 죽고 못사는데 자식이기는 장사는 없다. 극 중 진아와 준희도 서로를 못잊고 마지막에 제주도에서 다시 만나 해피엔딩으로 끝났는데, 아무리 속물 캐릭터 엄마도 그쯤되면 허락할 수 밖에 없을거다&amp;#8230; 하하!&lt;/p&gt;
&lt;p&gt;안판석 감독님 드라마 특성상 유독 롱테이크가 많고, 대사 하나하나가 실제로 나의 경험에 또 다시 감정이입을 할 수 있는 멜로 드라마다 보니,  인생 드라마가 되었다. (&amp;#8216;지금 만나러 갑니다&amp;#8217; 처럼 드라마속 몇 가지 공감 장면은 클립을 만들어 OST와 함께 폰에 저장해뒀다.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;내 인생에서 없어서는 안될 두 여자. 나의 어머니 그리고 와이프&amp;#8230;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;그녀들과의 아픈 기억, 잊지 못할 경험을 다시 꺼내고 힐링해 주었던 좋은 영화와 드라마였다. 아마 손예진 배우님을 만난다면 꼭 아래 사진에 싸인을 한번 받아두고 싶다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2103" src="http://blog.creation.net/data/tisotry/2018/05/25050709/soyejin-pretty-meet-you-3-1024x547.png" alt="" width="1024" height="547" /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=NCvmVyqSktc:zUbNdH_Wm40:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 24 May 2018 20:12:34 +0000</pubDate>
	<comments>http://blog.creation.net/sonyejin-pretty-meet-you#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>개발자 비급(祕笈) – 1. 연봉은 실력의 결과가 아니다</title>
	<link>http://channy.creation.net/blog/1186</link>
	<description>&lt;p&gt;&lt;em&gt;회사에서 개발자가 프로그래밍 실력만으로 대우 받으면 좋겠지만, 세상이 그렇게 원하는대로 돌아가지는 않습니다. 개발자들의 연봉(몸값), 경력 관리, 그리고 협업 등에 대한 경험담을 시리즈로 풀어보려고 합니다. 요즘은 이렇게 쓰겠다고 맘을 먹어야 그나마 시작할 수 있네요.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;연말이면 평가 시즌이 돌아오고 CTO와 함께 수 많은 개발자들의 연봉을 결정했던 적이 있습니다. 대개 팀장이 1차 평가를 하고, 상위 임원이 2차 평가를 하게 됩니다. 진리의 케바케(?)라는 말이 있듯이, 대부분의 회사가 같은 연차의 개발자라도 연봉은 천차만별입니다.&lt;/p&gt;
&lt;p&gt;그러다 보니, 능력에 걸맞는 업무 성과가 나왔는지를 판단해 개인별 연봉 인상률을 결정하는 건 여간 까다로운게 아닙니다. 특별히 예외적 성과나 정말 고려해야 하는 경우를 제외하고, 전체 개발자의 80%는 늘 같은 결과를 얻습니다.&lt;/p&gt;
&lt;p&gt;그런데도, 많은 개발자들이 연봉 협상에 들어가면 내가 올해 올린 성과로 협상의 여지가 있을거라고 믿는 착각을 합니다. 사실상 연봉 협상이 아니라 &amp;#8216;연봉 결정&amp;#8217;이 되어 있는 상황인데도 말입니다. 역량 평가나 업무 성과라는 것은 나의 실력과 업무 성과가 내 몸값을 결정하는 게 아니라면 도대체 무엇일까요?&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter wp-image-1187 size-full" src="http://channy.creation.net/data/channy/2018/05/02065739/dev-story-1-1.jpg" alt="" width="600" height="330" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 연봉은 내가 다닌 회사의 궤적일 뿐&lt;/strong&gt;&lt;br /&gt;
여러분이 받는 연봉이라는 건 그냥 다녔던 회사의 종류와 특성에 따른 궤적에 불과합니다. 대기업에 입사해서 초봉을 높은 수준으로 받아, 스타트업에 한번 가서 대박을 치고, 컨설팅 업계로 왔다면 5년만에 억대 연봉도 가능합니다. 다만, 병특으로 작은 기업에서 2-3년 보내면 동년배보다 경력이 높은데도, 졸업 후 들어온 공채보다 연봉이 낮을 수도 있습니다.&lt;/p&gt;
&lt;p&gt;실제 Daum에 다녔던 어떤 개발자는 대학교 2학년때 병특으로 입사해서 3년을 일한 후, 휴직하고 복학 및 졸업을 해서 2년후 회사에 다시 온 경우가 있었습니다. 회사가 성장하다 보니 신입 사원들의 연봉 수준은 매년 올라가고, 이 친구는 휴직 후 복직이다 보니 연봉은 휴직 전 연봉으로 책정되는 불상사가 생긴겁니다. 그렇다고 HR 규정상 엄청나게 올려줄 수는 없으니 연차 대비 연봉 수준은 시작부터 낮게 되는 것이죠.&lt;/p&gt;
&lt;p&gt;이런 경력 패턴으로 몇 군데 회사를 옮기다 보면, 같은 회사에서 비슷한 경력과 실력을 가졌는데도 연봉 차이가 많이 나는 현상이 발생합니다. 그래서, 저 사람은 실력도 없는데 왜 저렇게 연봉을 많이 받는 건가라는 의문과 뒷담화가 난무하기도 합니다. 회사에서는 사람을 뽑을 때 연봉이란 게 일단 뽑고 나면, 최대한 맞춰주기 때문이죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 돈이냐? 행복이냐?&lt;/strong&gt;&lt;br /&gt;
자! 그러면 몸값을 올리는 방법은 무엇일까요? 처음 부터 대기업에 가서 하기 싫은 일 하면서 꾸역꾸역 일하다가 연봉 높게 올리는 경력 개발을 하는게 맞느냐? 아니면 연봉을 낮추어 보람찬 일을 하더라도 만족하면서 살거냐? 정말 돈 안되는 직업을 때려치우고 &lt;a href="http://moneyman.kr/archives/5379"&gt;돈되는 새로운 직업으로 갈거냐&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;한 가지 중요한 사실은 김창준님이 쓰신 &amp;#8220;&lt;a href="http://agile.egloos.com/5783372"&gt;몸 값 안 올리기&lt;/a&gt;&amp;#8220;라는 글에서 보듯, &amp;#8220;다수의 경력 개발 연구에서 연봉이 많이 오르면 행복해 지는 경향은 있으나, 그렇게 크지 않으며 그 효과도 오래가지 않는다는 점&amp;#8221;입니다. &amp;#8220;오직 연봉 인상을 위해 이직을 하다보면, 그 외의 요소 (가령, 기업 문화)를 무시하는 경향이 있어, 그 후 만족도가 떨어질 수 있기도&amp;#8221; 합니다. 오히려, 업무에서 행복하기 위한 주관적인 요소가 많고 그렇게 일하는 방법을 찾으라는 조언입니다.&lt;/p&gt;
&lt;p&gt;저도 창준님 이야기에 동의합니다. 20년 넘게 회사를 다니면서, 내가 하는 일에서 행복을 얻고, 가급적 그럴 수 있는 회사로 이직을 결정했으니까요. 하지만, 인생이 나 혼자 사는 것도 아니고, 가족이 생기면 돈이 절대적인 건 아니지만 무시할 수 없는 요소가 됩니다.&lt;/p&gt;
&lt;p&gt;그러다 보니, 시간이 흘러 제가 했던 결정에 아쉬운 것도 있더라고요. 이를 기초로 연봉과 몸값에 대한 몇 가지 제가 얻은 몇 가지 경험적인 팁을 공유합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 스타트업도 고액 연봉을&amp;#8230; 대신 회사에 투자 기회를&lt;/strong&gt;&lt;br /&gt;
제 첫 직장은 스타트업이었습니다. 3명으로 시작한 회사에 직원으로 입사했는데, 열심히 하다 보니 기회를 얻어 회사의 지분을 얻게 되었고 주도적인 역할을 하게되었습니다. 직원들 연봉을 결정하는 자리에 올랐지만, 제 연봉 만큼은 동결을 하거나 낮게 유지했습니다. 그게 회사의 비용 구조에도 도움이 되고 궁극적으로 회사에 도움이 된다고 생각했거든요.&lt;/p&gt;
&lt;p&gt;하지만, 이직을 해야 하는 상황이 생기자 제 기본급이 너무 낮았던 것을 알게되었습니다. 언제나 그렇듯 초기 상수는 불변입니다. 요즘은 스타트업들도 신입 연봉 많이 준다고 합니다. 개발자를 뽑기가 어려우니까요. 그러나, 안정적인 연봉을 주는 대기업에 있는 개발자가 스타트업으로 옮기려면 연봉을 낮춰야 합니다.&lt;/p&gt;
&lt;p&gt;가끔 후배 스타트업 CTO를 만나면 비슷한 고민을 듣게 됩니다. 제가 주는 조언은 일단 CTO 자신 부터 시작해서 개발자들 연봉을 업계 수준 이상으로 책정을 하라고 합니다. 다만, 그렇게 받은 연봉을 다른 방법으로 회사에 재투자하도록 하는 것입니다. 예를 들어, 자기 주식을 매년 매입을 하는 등 합법적으로 회사로 다시 돌아오게 하는 방법은 많습니다.&lt;/p&gt;
&lt;p&gt;대개 스타트업으로 입사할 때, (대박을 친다는 가정하에) 주식을 얼마나 주냐에 관심이 많지 내가 얼마를 투자할 수 있느냐를 물어보는 사람은 극히 드뭅니다. 자기 돈을 직접 투자를 못하는 회사라면, 대박칠 가능성도 낮겠죠. 여기에 두 가지 이점이 있는데요. 우선 개발자들이 스스로 회사에 대한 오너쉽이 생기게 됩니다. 스타트업은 사람이 전부라 성공 가능성이 훨씬 높겠죠. 이 회사가 나의 현재 뿐만 아니라 미래도 보살피고 있구나라는 생각도 듭니다. 만약의 경우, 퇴사를 하는 경우라도 기본급에 대한 보장이 되고 있으니까요.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 대기업에서는 신규 서비스를 만드는 팀을 찾아라!&lt;/strong&gt;&lt;br /&gt;
어느 정도 규모가 있는 대기업에 이직할 때는 전통적인 개발팀 보다는 신규 사업을 시작하는 팀을 찾아 지원해야 합니다. 누구나 신규 사업에 대한 사내 이직은 꺼리고, 외부에서 뽑으려면 어쩔 수 없이 연봉을 높여야 하는 경우가 태반입니다. 따라서, 이직 시 원하는 대로 연봉을 부를 수 있는 자격(?)이 주어질 수 있습니다.&lt;/p&gt;
&lt;p&gt;많은 개발자들이 오해하고 있는 것 중 하나가 대기업에서 이미 알려진 주요 서비스를 핵심 부서라고 생각하는 것입니다. 예를 들어, 네이버는 검색팀, Daum은 한메일 및 카페팀 등등. 그러나, 이들 팀들은 너무 오래되어서 여러 사람이 거쳐간 레거시 코드가 너무 많습니다. 결국 유지 보수 업무에 투입이 되거나, 뭔가 새로운 걸 펼치기에 너무 많은 기술적 부채들이 존재할 가능성이 큽니다.&lt;/p&gt;
&lt;p&gt;물론 주니어 개발자라면, 이런 팀에서 배울 수 있는 점도 많을 것입니다. 그러나, 결국 대형 서비스를 하는 업체는 필연적으로 대용량 트래픽을 다루게 되는데, 그런 노하우는 어떻게든 습득하거나 사내 플랫폼으로 해결 가능합니다. 새로 생긴 팀에 입사하면, 개발 자유도도 훨씬 높고 의욕적인 사람도 많아 업무 만족도도 높습니다.&lt;/p&gt;
&lt;p&gt;저도 두번째 직장이었던 Daum의 첫 팀은 전사 빌링개발팀이었습니다. 다음에 한번 이야기할 때가 있겠지만, 시행 착오가 있었고 꽤 성공적이지 못했습니다. 그 후로, 회사에 없던 새로운 일을 만들고 직접 새 팀을 꾸렸습니다. 새로운 팀은 위험도많지만,기회도 많구요. 큰 기업에서는 안정적인 실험도 가능합니다.&lt;/p&gt;
&lt;p&gt;따라서, 대기업에서 어떤 신규 서비스가 만들어지고 있는지, 소셜 미디어나 인맥을 통해 어떤 부서에서 사람이 많이 필요로 하고 있는지 추적하고 있는 건 매우 중요하구요. 자신의 몸값을 한 단계 업그레이드하기 위한 선택에 도움이 될 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 국내 뿐만 아니라 해외에도 눈을 돌려야&amp;#8230;&lt;/strong&gt;&lt;br /&gt;
개발자들은 기본급에만 집착하는 경우가 많은데, 인센티브나 주식 보상 등에도 꼭 신경을 써야 합니다. 지금 다니는 글로벌 기업의 연봉 체계는 국내 기업과 완전히 다릅니다. 입사 시, 정해지는 레벨에 따라 구간이 설정된 기본급과 별도로 최초 1-2년에는 &lt;a href="https://ko.wikipedia.org/wiki/%EC%82%AC%EC%9D%B4%EB%8B%9D_%EB%B3%B4%EB%84%88%EC%8A%A4"&gt;사인온 보너스&lt;/a&gt;라는 정기 인센티브를 지급함으로서 이직에 따른 연봉 상승 효과를 극대화해줍니다.&lt;/p&gt;
&lt;p&gt;3-4년차에는 &lt;a href="http://www.workingus.com/forums/topic/restricted-stock-unit-rsu%EA%B0%80-%EB%AD%94%EA%B0%80%EC%9A%94/"&gt;RSU(Restricted Stock Unit)&lt;/a&gt;라는 상장된 주식을 사인온 보너스 만큼 줍니다. 즉, 그때 바로 팔수 있는 주식입니다. 만약 주식이 그대로라면 3년차가 되면 연봉이 떨어집니다. 주식이 떨어지면 그 격차는 너무 커서 자동적으로 이직 사유가 되겠죠. 다만, 그 반대의 경우라면 연봉은 지속적으로 인상되는 효과를 얻습니다. 이런 체계에서는 회사가 잘 안될 경우, 자연적인 정리 해고가 가능해지는 효율이 높은 방식입니다.대신 국내 기업은 그냥 매년 연봉 인상률 몇 %거나 팀이나 업무 성과에 따른 인센티브를 받는 정도입니다.&lt;/p&gt;
&lt;p&gt;따라서, 국내 기업에 있는 개발자들이 유망한 글로벌 IT 대기업으로 이직하는 경우가 많고, 저 또한 권장하는 바입니다. 최근 미국 증시에 상위 기업 대부분이 IT 기업이고 이들의 주식 가격 상승률은 최근 몇년간 지속적으로 상승했습니다. IT 기술이 모든 산업에서 중심이 되고 있어, 아마 그 추세는 향후에도 변하지 않을 겁니다.&lt;/p&gt;
&lt;p&gt;상장된 국내 IT 기업에서도 스톡 옵션을 통해 구성원들의 업무 의지를 높히는 바람직한 회사들이 있습니다. RSU와는 다르지만, 스톡옵션을 통한 이익도 중요하기 때문에 스톡옵션을 주는 회사를 선택해도 좋겠습니다. 기본급 그리고 인센티브, 주식 가치를 통한 금융 자산은 모두 보상에 해당되고요. 근로소득 원천징수 영수증을 끊으면 다 포함되어 합산됩니다. 따라서, 이직할 회사에 협상 시 증빙 자료로 유리한 고지를 점하게 되는 거죠.&lt;/p&gt;
&lt;p&gt;마지막으로, 저는 22년 경력에서 이제 세번째 회사에 다니고 있습니다. 한 회사에서 오래 다니면서 업무 성과를 올리고, 이를 몸값에 반영하는 노력도 중요합니다. 일을 통해서 만족감과 행복을 얻는 것은 물론 중요하구요. 때에 따라 주기적으로 나름의 위기가 찾아오고, 그걸 여러 가지 방법으로 극복해야만 합니다.&lt;/p&gt;
&lt;p&gt;너무 회사를 자주 옮기는 것도 좋지는 않지만, 한 회사에서 너무 오래 있는 것보다는 3-5년 주기적으로 새로운 일과 회사를 찾아 가는 것도 필요합니다. 이 때, 몸값을 올림으로서 얻는 (일시적인) 행복감을 조금이나마 더 얻을 수 있도록 제 경험담이 도움이 되었으면 좋겠네요.&lt;/p&gt;
&lt;p&gt;물론 이 글의 내용은 개인적이고, 주관적 의견이라 여러분의 생각이 다르시다면, 댓글을 남겨주세요.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 목차&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1186"&gt;개발자 비급(祕笈) &amp;#8211; 1. 연봉은 실력의 결과가 아니다&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;개발자 비급(祕笈) &amp;#8211; 2. 나의 실패한 프로젝트 답사기&lt;/li&gt;
&lt;li&gt;개발자 비급(祕笈) &amp;#8211; 3. 코드 구루들의 특징&lt;/li&gt;
&lt;li&gt;개발자 비급(祕笈) &amp;#8211; 4. 업계 연예인이 되는 방법&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=AON0sgk2eGQ:8-n7PRC5AAQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 01 May 2018 21:58:26 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1186#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>훌륭한 개발 문화의 이면(4) – 사내 라이브러리를 잘 관리하려면?</title>
	<link>http://channy.creation.net/blog/1183</link>
	<description>&lt;p&gt;&lt;em&gt;(지난번 연재 3편을 끝내고 너무 오래 쉬었네요. 최근에 예전 회사 OB 모임을 갔다가 이런저런 이야기를 해보면서, 필요한 분이 있으실 것 같아 다시 연재를 시작합니다. 연초에 시간을 많이 투자한 AWS Summit도 끝나서 잠깐의 여유를 찾았네요.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;현업을 하다보면 개발자에게 숙명같은 일들이 있습니다. 반복되는 일을 자동화해야 하거나, 다른 팀과 소통을 하면서 필요한 인터페이스를 구성하거나, 아니면 팀 내에 재사용 가능한 뭔가를 만들어야 하는 때이죠.&lt;/p&gt;
&lt;p&gt;이 세 가지는 어떤 식으로 만나게 되고, 이를 위해 개발자 혹은 팀은 다양한 비급(祕笈)을 담은 라이브러리나 프레임워크를 만듭니다. 대부분 오픈 소스로 가져다 쓰는 경우도 많지만, 딱 맞는 기능만 있으면 좋겠는데 너무 무겁거나 사내 기술 표준이나 (인증, 지불, 통계, 모니터링 같은) 내부 시스템과 연동을 위해 인-하우스 솔루션으로 만드는 경우도 많죠.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1184" src="http://channy.creation.net/data/channy/2018/04/29151411/library-framework.png" alt="" width="600" height="300" /&gt;&lt;/p&gt;
&lt;p&gt;물론 &lt;a href="https://blog.outsider.ne.kr/924"&gt;사내 프레임워크 만들지 말자.&lt;/a&gt; &lt;small&gt;(Outsider&amp;#8217;s Dev Story)&lt;/small&gt;라던가 &lt;a href="https://sangheon.com/2012/09/26/2521"&gt;자신만의 개인 라이브러리 또는 프레임웤이 필요할까&lt;/a&gt; &lt;small&gt;(Sangheon&amp;#8217;s Archive)&lt;/small&gt;라는 의견도 있습니다만 어느 정도 규모가 되는 회사에 좋은 팀이 꾸려지면, 사실 사내 라이브러리는 필연적으로 만들어집니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 라이브러리는 생산성에 큰 영향을 끼친다&lt;/strong&gt;&lt;br /&gt;
자신의 노하우를 가진 라이브러리 몇 개쯤은 가지고 있어야 신규 개발할 때 편하듯, 회사에서도 팀 생산성을 위해 필요해집니다. 먼저 이런일을 누가 할거냐 하느게 개발팀의 숙제죠. 많은 개발자가 게을러 터져서 자동화나 최적화를 잘 했으면 하는데 꼭 그렇지만도 않더라구요. 어떤 사람은 성실하게(?) 코딩하는 분도 계시고, 새로운 것만 좋은 개발자도 있게 마련이죠.&lt;/p&gt;
&lt;p&gt;그래서 대개 한 두 사람에게 이런 일이 가게 마련입니다. 아니, 그런 사람들이 이런 일을 벌리죠. &amp;#8220;얼마 전 캐싱 이슈가 있었는데, 어제 시간이 남아서 간단하게 만들어 봤어요.&amp;#8221; 하면서 팀 회의에 짜잔 하면서 내놓는 천재 개발자입니다. 사실 써보니 또 좋아요&amp;#8230; 그러면 계속 그 사람들에게 동기 부여가 되고, 전체적인 팀 생산성은 높아집니다. 따라서, 꼭 팀에 이런 분들이 한두분 있어야 하구요. 자신도 그렇게 되도록 노력하는게 꼭 필요합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 팀을 넘어서 회사로&lt;/strong&gt;&lt;br /&gt;
그런데, 이게 회사 차원으로 옮겨가면 이야기가 좀 달라집니다. 많은 개발팀들이 개별적으로 만든 라이브러리나 프레임워크 혹은 전사로 쓸 수 있는 서비스를 팀내에서만 아니라 다른 팀도 쓰면 좋겠다고 홍보를 하는 것이죠. 사내 세미나를 열기도 하고, 개발 본부 내에서 공유도 합니다.&lt;/p&gt;
&lt;p&gt;예를 들어, 대용량 트래픽을 감당해야 하는 팀은 DB Pool의 한계를 극복하기 위해 특정 SQL을 캐싱해주는 시스템을 만들기도 하고, 프론트엔드 개발자들은 우리 회사 가이드에 맞는 UI 라이브러리를 그리고 쇼핑 개발팀의 경우 이벤트를 많이 하니까 이벤트 개설 부터 추첨 및 팔로우업까지 하는 시스템을 만들어 배포하기도 하구요. 이러니 사내에서 입소문이 나서 알음알음 다른 팀이 가져다 쓰게 됩니다.&lt;/p&gt;
&lt;p&gt;제가 Daum 입사 초기에 몇 년간 기술 스탭을 하면서 &amp;#8220;우리 팀이 이런거 개발했다. 근데 이팀 저팀이 쓴다. 전사적으로 쓰면 좋겠다&amp;#8221; 이런식의 요청을 많이 받았습니다. 즉, 전사 공통 라이브러리가 됐으면 하는 바램이 있는 거죠. 하지만, 역할과 의무를 논하면 이야기가 달라집니다. 우리 팀은 다른 팀 요구 사항을 받고 처리하는 자원은 부족하니, 사내 공통 플랫폼팀에서 맡아줬으면 하는 겁니다.&lt;/p&gt;
&lt;p&gt;하지만, 사내 공통 플랫폼을 맡아서 하는 팀도 하는 일이 없는게 아니잖아요. 이미 그런식으로 올라와서 관리하는 라이브러리와 도구가 한 두개가 아닌데다 어떤 건 레거시처럼 거의 신경도 못쓰고 굴러가는 것도 있을 정도로 리소스가 부족합니다. 게다가, 사내 전체적인 수요에 의해 결정된 것도 아니고 궁극적으로 남이 만든 코드를 유지 보수를 하라는 건데 누가 좋아하겠습니까? 그래서 대개 그런 건 거절을 하게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 콘트롤 타워 부재의 문제점&lt;/strong&gt;&lt;br /&gt;
이렇게 되면 크게 두 가지 현상이 발생합니다. 여전히 공개에 적극적인 팀은 그냥 스스로 쓰겠다는 팀에 맘대로 배포를 합니다. 버전을 올려 배포를 하긴 하지만, 소스채로 배포를 해주고 팀마다 커스트마이징을 하니 버전이 몇 십개가 생깁니다. 다른 팀 피드백을 받아 고치면 자기들에게도 도움이 되니 적극적이지만, 팀 마다 여러 벌이 생기면 이마지도 쉽지 않게됩니다. 시간이 흘러 사내에서 너무 많이 쓰고 있어서 이제 전사적으로 관리를 해보겠다고 치면 세상에&amp;#8230; 팀마다 다 다른 버전을 들고 있습니다.&lt;/p&gt;
&lt;p&gt;더 큰 문제는 무작위 배포한 버그로 인한 장애가 전사에 영향을 주는 경우입니다. 어떤 팀에서 난 장애가 아직 다른 팀에서는 잠복하고 있는 경우도 있구요. 결국 전사 기술 스탭이 나서야 하는 상황이 생깁니다. 개별 팀에서 생긴 문제를 인지하고, 해결법에 대해 전사 공유, 그리고 각 팀이 해결했는지 코드 검증 등등 배보다 배꼽이 더 큰 경우가 생기게 돼죠.&lt;/p&gt;
&lt;p&gt;그나마 공개에 적극적이지 않은 팀은 그냥 자기네들만 쓰면 상관이 없습니다. 그런데, 공개를 요구하면서도 자기들은 직접 하기 싫은 팀은 오히려 뒷말을 하는 경우도 있습니다. 도대체 기술 스탭은 뭐하는 거냐? 공통 플랫폼팀은 그런거 하라고 있는거 아니냐? 결국 기술 스탭이 또 나서야 하는 상황이 생깁니다. 설득을 하든 강하게 거절을 하든&amp;#8230; 피곤합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 사내 공통 플랫폼 레포지터리 구축&lt;/strong&gt;&lt;br /&gt;
결국 제가 발견한 해법은 이런 겁니다. 사내에 오픈 소스 레포지터리를 만드는 것이죠. 개별 팀이 공개를 목적으로 만든 사내 라이브러리, 프레임워크 모두 올리게 합니다. 그리고, 그냥 오픈 소스 작동 원리에 맡깁니다. 잘되는 라이브러리는 잘될거고 안되는 건 안되겠죠.&lt;/p&gt;
&lt;p&gt;당시 오픈 소스 개발 원리를 회사에 주입(?) 시키는 건 쉬운 일이 아니었습니다. 제가 일하던 시절이 12년 전이라 그 흔한 Pull Request 개념도 없었습니다. 이미 Github이 대세이고, 대부분 개발자들이 오픈 소스 원리로 개발을 하는게 뭔지 많이 알고 있기 때문에 요즘에는 훨씬 쉬울겁니다.&lt;/p&gt;
&lt;p&gt;결과적으로 몇 가지 이점이 있습니다. 막상 코드를 공개하게 되면, 초기 개발팀에서 개별 배포하는 것 보다 좀 더 친절하게 매뉴얼이나 주석에 좀 더 신경을 쓰게 됩니다. 오픈 소스 레포지터리가 가진 기본 기능들, 즉 코드 리비전 관리, 버전 관리, 설치 방법, API Docs 문서 기능 등의 부담이 들지만, 누구나 쉽게 코드 개선 제안을 하고 버그가 발견되면 쉽게 고쳐지는 이점이 있습니다. 기술 스탭의 입장에서 선호도와 사용량을 보고 전사 관리해야 하는 것과 아닌 것을 판별할 수 있습니다. 설득에 도움도 됩니다.&lt;/p&gt;
&lt;p&gt;사내 공통 플랫폼팀이 관리하던 라이브러리나 시스템이 우선 공개 대상이 됐구요. 개별 팀이 만들어 공개하고 싶은 것도 자유롭게 공개하도록 했습니다. 그러니, 정리될 건 정리되고 시간이 흘러 더 이상 유지 보수가 안되는 쉽게 알 수 있게 되었습니다. 무엇보다 새로 들어온 개발자들이 사내 공통 플랫폼에 맞추어 선택해 사용할 수 있는게 많구나! 좋은 회사 왔구나 하는 심리적인 안정감도 큰 도움이 되었습니다. 그 전에는 궁금한 점이 있으면 누구에게 물어봐라 위키 찾아봐라 정도가 대부분이었거든요.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 사내 개발자 경력 개발의 이점&lt;/strong&gt;&lt;br /&gt;
12년 전에 이게 잘됐냐 물으신다면 저는 만족하지는 못했습니다. 오히려 잘 안됐다고 보는게 맞겠죠. 그렇지만, 이런 시도는 진짜 중요하구요. 이를 계기로 공통 플랫폼 개발하는 팀이 하는 일이 투명화 되고, 유지 리소스 산정에 도움이 되었습니다. 대개 서비스팀에 소속된 개발자들이 공통 플랫폼팀으로 팀 이직을 하는 경우도 많아졌습니다. 여기 오면 전사 관점에서 눈을 키울 수 있거든요.&lt;/p&gt;
&lt;p&gt;제가 꼭 하고 싶었는데 못했던 것이 서비스팀에서 몇년을 일하면 무조건 공통 플랫폼팀에서 1년을 일하도록 강제하는 개발자 HR제도입니다.&lt;/p&gt;
&lt;p&gt;대개 서비스 개발팀장들은 잘 하는 사람을 놓기 싫어하고, 그러다 보니 경력 개발 상 하나의 도메인에 집중하게 됩니다. 공통 개발팀에 오면, 전사의 서비스팀 관계자와 두루 소통을 하게 되어 시야도 넓어질 뿐 아니라 다른 팀으로 옮겨 가는 기회도 더 포착할 수 있거든요. 실제로 그런 경우도 많이 봤구요. (개발팀장들의 반대로 못했던게 끝내 아쉽습니다.)&lt;/p&gt;
&lt;p&gt;이렇게 누가 뭘하는지 서로 잘 알게되면 사내 개발 문화를 바꾸는데도 도움이 됩니다. 문제를 해결하고 전사 생산성을 높이는 사람이 좀 더 주목을 받을 수 있구요. 자연적으로 이런 정보를 전달하거나 받고자 하는 요구가 늘어나고 사내 개발자 콘퍼런스나 세미나가 활성화 됩니다. 좋은 개발자를 뽑는 거 다른 거 없습니다. 안에서 잘하면 자동적으로 입소문이 나고, 이를 외부에 잘 공유하면 누구나 잘 알게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 오픈 소스 기반 개발 환경은 필수&lt;/strong&gt;&lt;br /&gt;
이를 위해서는 회사 내에 오픈 소스 소프트웨어 사용 빈도를 높이는 것이 매우 중요합니다. Daum의 서비스는 리눅스, 자바 기반의 오픈 소스 플랫폼(톰캣, 스프링 등)을 주로 활용하고 개발 환경도 이클립스와 메이븐 등이고 개발 지원 서버들도 서브버전, Git, JIRA, 컨플루언스 위키, Sonar, Jenkins 등을 활용하고 있습니다. 개발자들이 오픈 소스를 모르면 개발하기 힘든 상황이었던 겁니다.&lt;/p&gt;
&lt;p&gt;그러다 보니, 오픈 소스를 바로 다운로드 받아 사용할 수 있는 미러(Mirror) 서버가 필요했고, 2002년부터 사내 FTP 미러 서버가 제공되었다고 한다. 제가 이를 2007년에 외부에 &lt;a href="http://ftp.daum.net"&gt;ftp.daum.net&lt;/a&gt;이라는 이름으로 공개하고 아파치, 이클립스, 우분투, 센트OS 등의 공식 미러로 지정되었습니다. 당시 17TB의 스토리지에 일간 피크타임 기준 600MB의 트래픽을 제공해서 당시 국내 웬만한 개발자들은 한번쯤 이용했습니다.&lt;/p&gt;
&lt;p&gt;물론 사용하는 거랑 소스 공헌하는 거는 완전히 다른 이야기입니다. 톰캣 가져다 커스트마이징해서 쓰다가 업스트림에 반영을 못해서 2.0으로 버전이 올라가도 1.0을 그대로 써야 하는 문제도 있었구요. 이런 문제를 해결하려면, 사내에서 부터 오픈 소스 개발 프로세스를 체득하고 실습해 볼 수 있는 내부 플랫폼은 필수입니다. &lt;em&gt;더 자세한 이야기는 &lt;a href="http://channy.creation.net/blog/569"&gt;기업 내 오픈 소스 개발 방식 도입記&lt;/a&gt;를 참고하시기 바랍니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;이제 회사가 성장함에 따라 몇 개의 백엔드/프론트엔드 라이브러리 정도가 아나라 수십개의 모바일 SDK, 수백개의 API 등 그 숫자는 갯수는 기하급수적으로 늘어났습니다. 이를 정리할 또 다른 방법이 필요했습니다. 다음 편에서는 어떻게 사내 API 플랫폼을 성장 시켰고, 개발 문화를 바꾸었는지 살펴보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 목차&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1104"&gt;훌륭한 개발 문화의 이면(1) – 코딩 테스트인터뷰 제대로 하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1107"&gt;훌륭한 개발 문화의 이면(2) – 자율적 개발 환경을 선택하라!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1110"&gt;훌륭한 개발 문화의 이면(3) – 다른 팀 소스 코드를 볼 수 있는가?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1183"&gt;훌륭한 개발 문화의 이면(4) – 사내 라이브러리를 잘 관리하려면?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;훌륭한 개발 문화의 이면(5) – 소통 비용의 절약 &amp;#8211; 서로 API로 말하자&lt;/li&gt;
&lt;li&gt;훌륭한 개발 문화의 이면(6) – 오픈 소스를 통한 외부 개발자 전략 구사하기&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=SW7ZgB9Pz0k:v7OM8zntQxY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 29 Apr 2018 06:19:39 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1183#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>블록체인 (비트코인) 열풍 속 현실적 조언</title>
	<link>http://channy.creation.net/blog/1175</link>
	<description>&lt;p&gt;&lt;em&gt;&lt;a href="http://channy.creation.net/blog/1145"&gt;인공 지능(AI) 시대의 현실적 조언&lt;/a&gt;에서 처럼 저는 블록체인(Blockchain)이나 암호화폐(Cryptocurrency) 전문가가 아닙니다. 다만, 이 글은 IT 기술로 인해 세상이 바뀌기 시작할 때 그 주변에서 일어나는 패턴을 몇번 경험해 본 결과, 각 분야에 있는 분들이 현실적으로 택할 수 있는 이야기를 간단하게 풀어 보려고 합니다. 이 내용을 진지하게 받아들일지 말지는 전적으로 여러분의 선택에 달려 있습니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;일반 투자자&lt;/strong&gt;&lt;br /&gt;
올해 블록체인 기반의 주요 암호화폐들이 폭등을 거듭하였습니다. 많은 분들이 미리 투자할 걸 하는 생각하셨을 겁니다. 하지만, (저를 포함하여) 여러분이 암호화폐에 아직 투자를 안하셨다면, 정말 잘 하신 겁니다. 소위 도박장에 한번 발을 들이면 쉽게 빠져 나오기도 어렵거니와 생업과 일상 생활에도 엄청난 지장을 주니까요. &lt;em&gt;(심지어 &lt;a href="http://www.hankookilbo.com/v/4057fcf820e44cedafc28db2d5488b13"&gt;목숨&lt;/a&gt;까지&amp;#8230;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;개인적으로 몇 년전 부터 블록체인 학습차 코인 채굴 과정이나 소량 코인 매매에 참여한적이 있습니다. 큰돈 벌었냐구요? 대부분 조금 오르면 팔고해서 큰 수익이 나지 않았습니다. 실제로 암호화폐에 관심있어 투자하셨던 분들도 사고 파는 과정에서 큰 수익을 내지 못했고, 변동성이 너무 커서 관심 끄고 그냥 투자차원에서 묻어두는 것도 힘들다고 하시더라구요. (최근 급등장 바로 앞에 진입하신 분이나 진득하니 오랜 기간 보유하고 있었던 분들은 대박이겠지만, 세상은 원래 불공평하고 일부는 항상 운이 엄청 좋죠.)&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1178" src="http://channy.creation.net/data/channy/2018/01/02060110/bitcoin-growth-2017.png" alt="" width="1340" height="668" /&gt;&lt;/p&gt;
&lt;p&gt;블록체인이 엄청 대단한 거처럼 포장되지만 IT 기술 거품은 항상 존재했고, 언젠가는 꺼집니다. 20년전 낙관론을 펴던 인터넷 닷컴 버블도 크게 붕괴했고, 지금은 소수 거대 사업자에 의해 시장이 움직이고 있습니다. 과거 블록체인과 유사한 MP3 공유 사이트, P2P 파일 공유 같은 탈중앙화 시도 역시 음지에서 양화로 바뀌는데 대부분 실패했습니다. 모든 기술 현상에는 역작용이 있으며, 블록체인도 그런 과도기를 거칠 것으로 예상됩니다. 누구에게는 선구자로서 부를 함께 얻을 수 있겠지만, 대다수에게는 큰 피해로 남을 가능성이 큽니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. Steemit #kr-coin &lt;a href="https://steemit.com/trending/kr-coin"&gt;대세글&lt;/a&gt;이나 &lt;a href="https://steemit.com/hot/kr-coin"&gt;인기글 목록&lt;/a&gt; (투자 정보)&lt;br /&gt;
2. 머니맨님의 &lt;a href="http://moneyman.kr/archives/tag/%ec%bd%94%ec%9d%b8"&gt;코인 투자 칼럼&lt;/a&gt; (긍정적)&lt;br /&gt;
3. 반달가면님의 &lt;a href="http://bahndal.egloos.com/tag/%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8"&gt;비트코인 칼럼&lt;/a&gt; (부정적)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;일반 개발자&lt;/strong&gt;&lt;br /&gt;
블록체인 기술은 개발자에게 친숙한 기술입니다. 분산 환경에서 각 노드에 분산된 정보를 공유해서 데이터 정합성을 유지하는 기본 개념을 가지지만, 이것을 인터넷 스케일로 가져가면 여러가지 네트워크나 스토리지 등 물리적 한계에 직면할 수 밖에 없습니다.&lt;/p&gt;
&lt;p&gt;거래량이 늘어나면서 체결에 많은 시간이 걸리고, 원장 크기도 기하급수적으로 늘어나서 탈 중앙화 개념 자체가 무력화 될 수 있습니다. 2017년 12월말 현재 비트코인의 총 원장 크기는 약 &lt;a href="https://blockchain.info/ko/charts/blocks-size?timespan=all"&gt;150GB&lt;/a&gt;입니다. 이제 개인 PC에 블록 데이터를 /bitcoin/blocks 폴더에 다운받는다는 건 이제 거의 불가능한 상태입니다. ㅠㅠ&lt;/p&gt;
&lt;p&gt;그럼에도 불구하고 새로운 기술에 대한 감을 익히는 것은 매우 중요합니다. 개발자라면 신규 기술의 개념을 살펴보고, 간단한 테스트를 하는 건 장려합니다. 하지만, 인공지능 기술 만큼은 아닙니다. 만약 새해 학습 계획에 우선 순위를 둔다면, AI/ML 그 다음에 블록체인을 공부 하세요.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. 박재현님의 이더리움프로그래밍 &lt;a href="http://wisefree.tistory.com/477?category=697903"&gt;http://wisefree.tistory.com&lt;/a&gt;&lt;br /&gt;
2. 이더리움 한국어 백서 &lt;a href="https://github.com/ethereum/wiki/wiki/%5BKorean%5D-White-Paper"&gt;https://github.com/ethereum/wiki/White-Paper&lt;/a&gt;&lt;br /&gt;
3. Awesome Blockchain &lt;a href="https://github.com/openblockchains/awesome-blockchains"&gt;https://github.com/openblockchains/awesome-blockchains&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;스타트업&lt;/strong&gt;&lt;br /&gt;
새해에도 블록체인 스타트업 투자는 계속 될것입니다. 사실 스타트업이 꿈에다 투자를 하는 거고, 이미 되든 안되는 바이럴에는 성공했으니까요. &lt;em&gt;(비트코인 덕에 이제 옆집 개도 블록체인이 뭔지 알게 됐어요.)&lt;/em&gt; 현재는 코인 거래소나 가능성 높은 &lt;a href="https://steemit.com/ico/@shiningmoon1969/ico"&gt;ICO(Initial Coin Offering)&lt;/a&gt;에 투자가 몰리는 것 같지만, 상위 코인거래소와 주요 암호화폐의 성패는 어느정도 드러났고, 투자 거품까지 끼고 있으니 언젠가는 (조만간) 정리될 것 같네요.&lt;/p&gt;
&lt;p&gt;여전히 스타트업에게 블록체인 기술만이 해결 가능한 문제가 무엇인가?라는 질문에 답을 찾는게 급선무입니다. 많은 분들이 분산화된 거래 원장, 사용자 인증, 크라우드 펀딩 다양한 유즈케이스를 이야기하지만 블록체인이 아니라도 해결 가능합니다. 과거 Web 2.0시대에도 분산화된 인증 기술(예: OpenID 등) 관련 스타트업이 많이 나와서 기존 인증을 대체하려고 했지만 일부 M&amp;amp;A한 경우를 제외하고는 모두 망했습니다. 그러나, 새로운 시도는 언제나 옳고 성공하지 못하더라도 뛰어드는게 스타트업이죠. 도전하실 분은 뛰어드세요~&lt;/p&gt;
&lt;p&gt;제가 가끔 5년 &lt;a href="http://blog.creation.net/452"&gt;웹 분산-집중 주기설&lt;/a&gt;을 주창해곤 했는데, 2015년 은 &amp;#8216;블록체인&amp;#8217;이네요. 그동안 주요 IT 기업에 의한 집중화된 플랫폼 비즈니스의 효용성이 분산 기술 보다도 훨씬 IT 산업 성장에 영향을 주었습니다만, 분산 기술이 뜰 때가 다음 성장을 예고하는 것처럼 보입니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1176 aligncenter" src="http://channy.creation.net/data/channy/2018/01/02052620/blockchain-2015.png" alt="" width="500" height="250" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. 표철민님의 &lt;a href="https://charlespyo.com/2017/11/16/%EB%B8%94%EB%A1%9D%EC%B2%B4%EC%9D%B8%EC%9D%80-%ED%98%84%EC%9E%AC-%EC%96%B4%EB%94%94%EC%AF%A4-%EC%99%80%EC%9E%88%EB%82%98/"&gt;블록체인은 현재 어디쯤 와있나?&lt;/a&gt; (스크롤 압박 및 말빨 현혹(?) 주의)&lt;br /&gt;
2. 박재현님 ZDNet 칼럼 &lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20170904160310"&gt;블록체인 기반 플랫폼 비즈니스를 이해하자&lt;/a&gt; 및 &lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20171201145305"&gt;블록체인 도입시 고려 사항&lt;/a&gt; (냉정하고 객관적인 조언)&lt;br /&gt;
3. Tony Kim님 &lt;a href="https://brunch.co.kr/@blockchainstory/2"&gt;블록체인에서 가능한 재미난 비즈니스 모델&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;그 밖에 분들&amp;#8230;&lt;/strong&gt;&lt;br /&gt;
그 밖에 분들은 이번 블록체인 흐름에는 그냥 비껴가는 것도 좋습니다. 세상의 모든 변화에 동조하고 관심가지고, 휩쓸릴 필요는 없습니다. 괜히 관심을 가지다가, 맨 앞의 일반 투자자로 들어서고 정신과 금전 모두 피폐해지는 것을 경험하느니 내가 하고 있는 소중한 일에 더 힘을 쏟고, 사랑하는 주변 사람들에게 관심을 가지는 게 더 효과적인 일이니까요.&lt;/p&gt;
&lt;p&gt;다만 국내 코인 거래소들의 경우, 통신 사업자 달랑 끊고 금융 거래에 준하는 &lt;em&gt;(불법도 탈법도 아닌)&lt;/em&gt; 사업을 하고 있는데, 정부와의 한판 대결이 올해 주요 관전 포인트입니다. 2000년대 초반 온라인 음악 스트리밍이랑 소규모 지불 결제(PG) 사업을 할 때, 음반 사업자와 저작권(법) 이슈로 합법화로 가는데 지리한 싸움에 직접 참여 한 적이 있습니다. PG 사업의 경우, 카드깡에 시달리면서 전자금융법 개정까지 불법 업체처럼 여겨졌었죠.&lt;/p&gt;
&lt;p&gt;당시 음반 사업자나 카드사처럼 민간 금융 기관 같은 기득권자가 들고 일어나야 하는데, 지금은 정부가 대리전을 하고 있는 양상입니다. 그럴수록 공신력은 더 높아가고, 정부가 버블을 자초하는게 우려됩니다. 음악 스트리밍과 지불 결제 사업은 그래도 뭔가 움직이는 재화가 있는데, 암호화폐란건 실체 없이 다단계 방식에 도박 같은 묻지마 투자를 하는 느낌입니다.&lt;/p&gt;
&lt;p&gt;암호화폐를 가장한 블록체인 붐도 언젠가 잠잠해 질 겁니다. 하지만, 그때는 블록체인이 필요 없는 것이 아니라 대안이 되는 서비스가 성장하는 시대가 되겠죠.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;p.s. 거품이 꺼지는 징조는 사람들이 특정 자산에 미치면서, 정산 나간 일을 시작할 때라고 합니다. 대표적인 경우가, &lt;a href="https://jesuscoin.network/"&gt;Jejus Coin&lt;/a&gt;인데, 예수님의 재림을 위해 만든 암호화폐라고&amp;#8230; 죄사함도 받을 수 있고, 기존 대형 교회가 가진 재산을 넘는 시가 총액을 목표로 한다고 하네요. 정말 말세(末世)네요.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;아무튼 &lt;a href="http://www.businessinsider.com/infographic-shows-how-close-we-could-be-to-an-economic-bubble-collapse-2017-9"&gt;세상의 모든 게 버블&lt;/a&gt;인 시대에 살고 있어요 ㅠㅠ&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1179" src="http://channy.creation.net/data/channy/2018/01/02061509/everything-bubble.png" alt="" width="812" height="2560" /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=9S1wi7wC4_I:R3FbOO_ksZg:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 01 Jan 2018 21:18:46 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1175#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>아이폰 10주년 아이폰X(10) 공개</title>
	<link>http://channy.creation.net/blog/1169</link>
	<description>&lt;p&gt;아이폰 10주년을 맞아 애플이 &lt;a href="https://www.apple.com/kr/iphone-x/"&gt;아이폰X(10)&lt;/a&gt;을 공개했습니다. 그동안 아이폰 출시에 대한 많은 비밀주의가 있었는데, 이번 만큼은 외부에 너무 많이 공개 되었죠. &lt;em&gt;(아는 사람은 다 아는 그래서 전혀 새롭진 않았습니다.) &lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;홈버튼은 역사속으로&lt;/strong&gt; &amp;#8211; 요즘 안드로이드 최신 버전에는 홈버튼이 없죠. 아이폰의 트레이드 마크였지만 이제 역사속으로 없애고, 아래에서 위로 쓸면 됩니다. 밀어서 잠금 해제가 없어지고, 눌러서 잠금 해제도 이제 사라졌습니다. 따라서, 지문 인식을 통한 TouchID도 사라짐&amp;#8230;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;인공지능 Face ID 향상&lt;/strong&gt; &amp;#8211; 얼굴을 통한 인식 기능에 신경을 많이 쓴 것 같습니다. 헤어스타일이나 화장을 해도 자기 얼굴을 인식할 수 있는데, 이를 위해 앞에 카메라 뿐만 아니라 각종 감지 센서, 적외선, 조명 등이 동원됐고, A11칩에 신경망 엔진(딥러닝)까지 집어 넣었군요. 이모지 캐릭터로 얼굴 감정 변화를 넣을 수 있다는 재미 요소도 넣었지만&amp;#8230; 너무 하이 스펙인 것 같아요.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;고성능 OLED 디스플레이&lt;/strong&gt; &amp;#8211; 화면은 5.8인치, 해상도는 2436 x 1125, 458ppi 기반 OLED 디스플레이를 탑재했습니다. 아이폰 7에 비해 거의 2배입니다. 자! 모바일 앱 개발자들과 디자이너들은 또 한번 디자인 리소스 업데이트를 해야 하고, 사용자들은 더 많은 저장공간과 데이터 사용량이 필요하겠네요. 점점 더 무거워지고 더 비싸지고&amp;#8230;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;가격은 999달러 부터&lt;/strong&gt; &amp;#8211; 예약 판매는 10월 27일, 출시는 11월 3일 입니다. (한국은 아무래도 연말이나 내년 초?) 64GB와 256GB 용량으로 999달러 부터 판매를 합니다. 1차 출시 예약을 빨리 해도 추수감사절 시즌에나 받을 수 있을 것 같네요. 요걸 노린듯&amp;#8230;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;놀라운 건 이게 아니라, 아이폰X 출시와 함께 &lt;a href="https://www.apple.com/kr/iphone-8/"&gt;아이폰 8&lt;/a&gt;이라는 어중간한 제품을 같이 출시했다는 겁니다. 아이폰 8에 대해 열심히 설명을 했지만, &lt;em&gt;(죄송 기억이 안납니다)&lt;/em&gt; 아이폰X에 다 묻혀버렸습니다. 이번달에 출시하는데, 아이폰 7을 대체할 만큼 좋은지는 잘 모르겠습니다. &lt;em&gt;(애플 워치3, 애플 TV 4K도 공개했습니다만&amp;#8230; 죄송 역시 기억이 안납니다.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;결론&amp;#8230; 저는 올해 1월 1일에 산 아이폰 7 그대로 쓸 겁니다 ㅎㅎ&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1170 size-full" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/2017-iphone10-1.png" alt="" width="1875" height="930" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;제품 사이트&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;아이폰 X : &lt;a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.apple.com%2Fkr%2Fiphone-x%2F&amp;h=ATMtdHE8D1k9mZSeWoB9-AW-3yycTzjzVzjAa801MEdDd6f71F-fdEl8hwu42uttNzSUBusvSyDRZRXtjZSvCB1V77HNHYTxuuGyKvMcl67weAHvMeXWBf1hXMKgpM3gEUsvi8-AgsDE7skkyGGZEmHw2htEb0WQIPKBe0V-uiD7zRIQHXmAdzofEyOyeyMpoW4WnJMI5K7e0VNce2GS8xuipsPZficxTFcRalOMeSOBSeAjPUiuhuoTrP8pj4V7YS2gSDeqmnnxzmCpUShZV3nqiFJ2v_aPmmpjuD59jG549w" target="_blank" rel="noopener"&gt;https://www.apple.com/kr/iphone-x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 8 : &lt;a href="https://www.apple.com/kr/iphone-8/" target="_blank" rel="noopener"&gt;https://www.apple.com/kr/iphone-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;동영상&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이벤트 전체 영상 &amp;#8211; &lt;a href="https://www.apple.com/apple-events/september-2017"&gt;https://www.apple.com/apple-events/september-2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 X &amp;#8211; &lt;a href="https://www.youtube.com/watch?v=mW6hFttt_KE"&gt;https://www.youtube.com/watch?v=mW6hFttt_KE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 X (조니 아이브) &amp;#8211; &lt;a href="https://www.youtube.com/watch?v=mW6hFttt_KE"&gt;https://www.youtube.com/watch?v=mW6hFttt_KE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 8과 8 플러스 &amp;#8211; &lt;a href="https://www.youtube.com/watch?v=UL3K5QJKOLg"&gt;https://www.youtube.com/watch?v=UL3K5QJKOLg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;p.s. &lt;a href="http://channy.creation.net/blog/381"&gt;2007년&lt;/a&gt; 아이폰이 출시 되었으니 벌써 10년이 지났군요. 10년이면 강산이 변한다는데, 정말 많은 게 바뀌었습니다. 한국 출시가 계속 늦어져서, 우리 나라 사람들은 &lt;a href="http://blog.creation.net/430"&gt;2010년&lt;/a&gt;이 되어서야 쓸 수 있게 되었고, 안드로이드가 나오고, 전 세계적인 거대한 모바일 환경 변화가 있었습니다. (Thanks, Jobs!)&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=YKm7a7PnPE4:xJuVloHFdow:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 12 Sep 2017 21:35:02 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1169#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>어른들의 소통 방식</title>
	<link>http://channy.creation.net/blog/1167</link>
	<description>&lt;p&gt;김상조 공정위원장이 &amp;#8216;스티브 잡스&amp;#8217;를 비유로 들어 이해진 의장의 경영 능력에 대해 비판한 것은 적절하지 못했습니다. 국내 IT 산업에 대한 무지를 그대로 드러낸 것이고, &lt;a href="http://[인터뷰] 김상조 “이해진에 잡스 얘기 해주고 싶었다”"&gt;&amp;#8220;당시 이 얘기를 할까 말까 하다가 안 했다&amp;#8221;&lt;/a&gt;라는 이야기를 언론에 한 것은 더욱 그렇지요. 당사자에게 개인적으로 하면 되지, 언론에 그 이야기를 왜 하시나요? &lt;em&gt;(9월 5일자 국민일보 인터뷰 기사)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;하지만, 비유가 적절하지 못했을 뿐, 김 위원장의 생각에 대한 맥락에 대해서는 다른 기사를 통해서는 이해할만 합니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;-지난달 14일 공정위를 직접 방문한 이 전 의장과 10분 가량 환담을 나눴는데.&lt;/p&gt;
&lt;p&gt;&amp;#8220;10분 가량 이야기를 나누면서 이 전 의장에 대해 우리나라 신(新) 산업을 일으킨 ‘개척자’라는 존경심을 갖게 됐다. 다만 개척자로서 이 전 의장이 우리 사회에서 보다 ‘영속성’을 지니기 위해서는 조금 더 고민이 깊어져야 한다는 느낌을 받았다. 친인척을 경영에 개입시키지 않고 지분을 최소한도로 줄이는 등 그 동안 사회적으로 지탄을 받아 온 재벌 총수와 다른 모습을 보여준 것만으로 미래까지 담보할 순 없다. 지금까지 이 전 의장이 보여준 모습은 우리가 개혁해야 할 ‘과거의 구태’로부터 벗어난 것일 뿐이다. 우리사회가 지향해야 할 새로운 기업의 모습을 아직 보여주진 못하고 있다.&amp;#8221;&lt;/p&gt;
&lt;p&gt;-네이버가 국내 검색 시장을 사실상 독점하는 것에 대한 우려의 목소리가 적지 않다.&lt;/p&gt;
&lt;p&gt;&amp;#8220;이 같은 우려에 대해 이 전 의장이 스스로 사회에 메시지를 전달할 필요가 있다. 이것을 하고 있지 않기 때문에 경쟁당국이 법으로 집행해야 한다는 필요성을 느끼게 된 단계까지 온 것이다. 이 전 의장이 지금까지 자신이 달성한 부분에 대해 우리 사회가 어떻게 평가하고 있는지를 폭넓게 청취하고, 사회에 메시지를 던져야 한다. 너무 늦어지면 법적 ‘태클’을 받을 수밖에 없다. 규모가 큰 기업의 리더가 해야 하는 중요한 역할은 시장 개척이나 신기술 개발이 아니다. 이는 전문경영인에게 위임할 수 있다. 진짜 리더의 역할은 사회 구성원들이 어떤 요구를 하고 있는가를 파악하고, 사회와 맞춰가는 것이다. 이재용 삼성전자 부회장이 실패한 게 바로 이 지점이다.&amp;#8221; &lt;a href="http://www.hankookilbo.com/v/9f0557149db146978e5da34e1184c7a2"&gt;[한국일보] 김상조 “4대 그룹 개혁, 12월이 데드라인” &lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;본 인터뷰에서는 네이버 뿐만 아니라, 삼성과 현대차의 지배 구조와 총수들에 대해서도 비판을 마다하지 않았습니다. 공정위원장 자리에서 주요 대기업의 이슈에 대해 직접적인 이야기하는 것이 부적절할 수 있습니다.&lt;/p&gt;
&lt;p&gt;하지만, 대기업들이 일반적인 국민 정서를 잘 이해하지 못하고, 총수의 지배 구조를 방어하기에만 급급하고, 기업 총수는 사회적 책임을 방기하고 있다면, 공정위원장으로서 시정하도록 조언을 해주는 것도 필요합니다. 삼성전자, 네이버를 포함 우리 나라 대다수 대기업의 주요 주주가 바로 국민 연금입니다. 일반적인 국민들이 느끼는 감수성에 맞출 필요가 있지요.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1168 size-full" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/naver-kimsangjo-jwlee.jpg" alt="" width="600" height="615" /&gt;&lt;/p&gt;
&lt;p&gt;다음 창업자인 이재웅님이 김상조 위원장의 발언에 &amp;#8216;부적절하다&amp;#8217;고 표현 하신 것은 수긍합니다. 기업가를 대하는 정부 관료의 자세는 아니니까요. 만약 이런 충고를 할 수는 있지만, 하더라도 개인적으로 하는 게 맞습니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;제 글이 언론에 인용될 줄 몰랐습니다. 오만이라는 표현은 부적절했습니다. 김상조위원장의 표현도 부적절했습니다만 제 표현도 부적절했습니다. 수정합니다.&lt;br /&gt;
&amp;#8212;&amp;#8212;&lt;br /&gt;
할 말이 많습니다만 딱 한 마디만 하겠습니다,&lt;br /&gt;
김상조 위원장이 지금까지 얼마나 대단한 일을 했고, 앞으로 얼마나 대단한 일을 할지는 모르겠지만, 아무것도 없이 맨몸으로 정부 도움 하나도 없이 한국과 일본 최고의 인터넷 기업을 일으킨 기업가를 이렇게 평가하는 것은 부적절합니다.&lt;br /&gt;
동료기업가로서 화가 납니다. (출처: &lt;a href="https://www.facebook.com/soventure/posts/10155749113883833"&gt;이재웅 페이스북&lt;/a&gt;)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;하지만, 이를 설명하는데 있어 맥락 없이 (나중에 수정하셨지만) &amp;#8216;오만하다&amp;#8217;고 표현하신 것은 적절하지 못했습니다. 뉴스로 쓰라고 미끼를 던진 것 뿐이고, &lt;a href="http://news.chosun.com/site/data/html_dir/2017/09/10/2017091002084.html"&gt;언론이 받아 쓸 수 밖에 없는 종류&lt;/a&gt;의 발언이었죠.&lt;/p&gt;
&lt;p&gt;그 뒤로 페북에 쓴 사견이 기사화 될 것을 몰랐다고 해명하시면서 &lt;a href="https://www.facebook.com/soventure/posts/10155754495243833"&gt;맥락을 다시 설명하셨는데&lt;/a&gt;, 제가 알기로 재웅님의 페북 발언 인용 기사가 나온 건 한두번도 아닙니다. 나는 이재웅님이 혁신 기업가로서 하신 일을 존경하고, 제 삶에서도 그 분의 경영자로서 하신 판단에 영향을 많이 받아 왔습니다. 하물며 내 개인에게도 그럴진데 우리 나라 2대 포털 사이트를 만든 창업자의 발언의 크기가 어느 정도인지 스스로 모르셔서 하는 이야기입니다.&lt;em&gt; (그때 마다 내 이야기가 기사화 될지 몰랐다고 이야기하실 건가요?)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;김상조 위원장 말처럼 이해진 의장이야 아예 입을 닫고 산다고 치고, 이재웅님처럼 그동안 사회적 발언을 해 오셨다면 그게 사견을 이야기하는 공간이라도 신중하게 이야기를 하고, 어떤 이야기를 할 때는 맥락을 충분히 설명해야 합니다.&lt;/p&gt;
&lt;p&gt;저도 페북에 한마디 한마디 할 때는 1만명 가까이 되는 팔로워 여러분들의 감수성을 고려하고 단어 하나 하나 고르고 바꾸고 퇴고합니다. (비공개로 이야기한 것도 공익이든 아니든 기사화하는 게 기자들입니다. 기자들을 믿어서는 안됩니다.)&lt;/p&gt;
&lt;p&gt;페이스북이나 트위터를 우리나라에서는 소셜 네트워크 서비스(SNS)라고 하지만, 해외에서는 소셜 미디어라고 합니다. 그만큼 공적 공간이죠. &lt;em&gt;(친구들끼리 잡담은 1:1로 메신저 하거나, 아예 사람을 지정해서 비공개로 하면 됩니다.)&lt;/em&gt; 아직까지 많은 어른들이 이러한 파급력을 모르거나, 알더라도 순진하게 생각하는 것 같습니다. 이제 언론만 미디어가 아닙니다. 나의 말이 어떻게 퍼질지 모르는 세상입니다. 어제도 어떤 시인 한분께서 예술적 감성으로 한 진담반 농담반의 특급 호텔의 숙박 요구에 대해 한바탕 홍역을 치르신것으로 압니다.&lt;/p&gt;
&lt;p&gt;어른들 뿐만 아니라 아이들도 소셜 미디어의 파급의 표적이 됩니다. 득이 될 수도 실이 되기도 하는 양날의 검이죠. 사람들과의 원활한 소통을 가능하게 도와주지만 그 반대의 영향을 가져올 수 있기 때문에 지혜롭게 사용할 줄 알아야 합니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=HVImOlxC3YM:tR7Q88MX_uo:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 10 Sep 2017 23:00:49 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1167#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>네이버 창업자의 대기업 동일인 지정에 대한 시각</title>
	<link>http://blog.creation.net/naver-kftc-issue</link>
	<description>&lt;p&gt;&amp;#8220;좋은 법은 선량한 사람에게는 해가 없다.&amp;#8221;&lt;/p&gt;
&lt;p&gt;일정 규모 이상의 기업 집단에 대한 순환 출자와 총수에 의한 사익 편취를 막기 위해 만든 대기업 규제가 네이버와 이해진 전 의장에게 얼마나 큰 피해를 주는 지 모르겠다. 오히려 기업의 사회적 책임을 더 한다는 측면에서, 공정위의 판단에 동의한다. (카카오나 넥슨의 경우, 대기업 지정에 따른 창업자에 대한 동일인 지정에 대해 이슈를 제기하지 않았다.)&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-full wp-image-2100" src="http://blog.creation.net/data/tisotry/2017/09/11132853/naver-kftc-issue.jpg" alt="" width="600" height="400" /&gt;&lt;/p&gt;
&lt;p&gt;또한, 전체적인 맥락에서 이 전 의장이 실효적인 의사 결정과 지배를 하고 있다는 공정위의 판단 역시 합리적이다. 단순히 현재 이 전 의장의 지분율이 낮고, 경영에 직접 관여하지 않고, 글로벌 시장에서 안 좋은 평판을 준다는 이유는 너무 아마추어적인 접근 같다.&lt;/p&gt;
&lt;div class="text_exposed_show"&gt;
&lt;p&gt;누가봐도 그가 지금까지 네이버을 실질적으로 지배해왔으며, 본인의 이사회 의장 사퇴와 경영진 변화가 대기업 집단 지정을 앞두고 전격적으로 이뤄졌다. 여전히 네이버에서 사내 이사로서, 글로벌 투자 담당으로서 영향력이 상당하고, 이번 동일인 선정 이슈와 관련해 (행정 소송 고려 등) 전사적으로 여론전을 펴는 것만 봐도 알 수 있다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;공정위는 &amp;#8220;이 전 의장이 최근 미래에셋대우와 맺은 자사주 교환도 총수 판단의 근거가 됐다. 공정위는 이 전 의장은 당시 교환으로 1.71%의 우호 지분을 확보했으며, 향후에도 10.9%에 달하는 잔여 자사주의 추가 활용 가능성을 배제할 수 없다고 판단했다.&amp;#8221;  &lt;a href="http://biz.khan.co.kr/khan_art_view.html?artid=201709031200001&amp;code=920100"&gt;[경향신문] 공정위 “네이버 총수는 이해진” 결론···‘재벌’로 규제 시작&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;일반인들은 잘 모르겠지만, 네이버는 상장 후 지금까지 거의 매년 2천억 이상의 자사주를 계속 사 들여왔다. 네이버 순이익이 연간 4천-5천억 정도임을 감안하면 거의 반이다. 그로 인해 네이버 전체 지분율에서 자사주 비율은 2008년 3%에 불과했으나, 2016년 말에는 12.6%가 되었다. 이해진 의장을 제외하면 가장 높은 비율이다. 물론 자사주 취득이 주주 가치를 높인다는 이유로 이뤄졌고 의결권을 가지고 있지는 않지만, 결과적으로 주가를 높여 적대적 M&amp;amp;A을 막고, 우호 지분 확보로 경영권을 방어할 수 있는 효과적인 수단이 되었다.&lt;/p&gt;
&lt;p&gt;국내 대표 IT기업이면서도, 글로벌과의 규모의 경쟁에서 힘들다고 늘 앓는 소리를 해 온 네이버가 순이익의 반을 직접 투자가 아닌 자사주 취득에 썼다는 점은 눈 여겨 볼 대목이다. 2007년 부터 10년간 자사주 매입에 쓴 돈만 1조 7천억이 넘는다.&lt;/p&gt;
&lt;p&gt;그럼에도 불구하고 나는 그동안 창업자 본인의 사업 철학과 비전을 흔들리지 않고 실현하고, 안정적인 경영권을 유지 하기 위해 네이버가 올바른 결정을 해왔다고 생각한다. 하지만, 그 결과로 인해 (공정위가) 책임을 질 위치에 있다고 판단했다면, 책임을 지는 것이 정도가 아닐까?&lt;/p&gt;
&lt;p&gt;하고 싶은 데로 다 하고, 막상 책임질 상황이 되면 발을 빼는 게 재벌 총수들이 하는 행태 아니던가. 오히려 모범을 보여 이런 규제가 네이버에게 정말 불필요 했구나라고 인식시키며, 변화를 이끌어내는 것이 업계 리더로서 올바른 자세일 것이다. 네이버에게 낡은 규제였음을 스스로 증명하면 된다.&lt;/p&gt;
&lt;/div&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=axpT2gF9STk:x1f137OM2II:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 04 Sep 2017 23:00:52 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>클라우드에 딱 맞는 MXNet의 5가지 딥러닝 학습 기능</title>
	<link>http://blog.creation.net/mxnet-deep-learning-features-aws-cloud</link>
	<description>&lt;p&gt;&lt;a href="https://aws.amazon.com/mxnet/" target="_blank" rel="noopener noreferrer"&gt;Apache MXNet&lt;/a&gt;  (인큐베이팅 프로젝트)는 최첨단 딥러닝(Deep Learning) 학습 모델 제작을 지원하는 확장 성이 뛰어난 오픈 소스 프레임 워크입니다. 이를 통해 CNN (Convolutional Neural Network), LSTM (Long Term Memory Network) 등을 만들 수 있고, Python, Scala, R 및 Julia를 포함한 다양한 언어를 지원합니다.&lt;/p&gt;
&lt;p&gt;이 글에서는 MXNet이 AWS 클라우드 개발자 친화적인 프레임워크로서 자리 매김하는 몇 가지 독특한 기능을 소개합니다.  Python에서 MXNet을 사용하여 신경망 코딩을 하는 분을 위한 한 장짜리 기능 요약집도 하단에 있으니, 많이 참고해 보시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#1  코드 몇 줄로 다중 GPU 학습 지원&lt;/strong&gt;&lt;br /&gt;
다중 GPU 기반 학습 실행 기능은 MXNet 아키텍처의 핵심 부분입니다. 모델을 학습시키려는 장치 목록을 전달하면 됩니다. 기본적으로 MXNet은 데이터 병렬 처리를 사용하여 여러 GPU에서 작업 부하를 분할합니다. 예를 들어, GPU가 3 개인 경우 각 모델은 전체 모델 사본을 받고 각 데이터 배치(Batch)의 1/3로 나눠 학습을 진행합니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;import mxnet as mx 
# Single GPU
module = mx.module.Module(context=mx.gpu(0))
# Train on multiple GPUs
module = mx.module.Module(context=[mx.gpu(i) for i in range(N)], ...)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img class="aligncenter" src="https://cdn-images-1.medium.com/max/1600/1*rfow_hmfd9AVCYiFyoXimA.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;MXNet은 다중 GPU 혹은 다중 서버 기반 학습에서 가장 뛰어난 효율을 보이고 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#2 다중 서버 기반 학습 가능&lt;/strong&gt;&lt;br /&gt;
MXNet은  다중 서버에서 여러 GPU에 대한 학습 또한 간소화하도록 설계한 분산형 딥러닝 학습 프레임 워크입니다. 서버 클러스터 전체에서 학습을 하려면, 모든 컴퓨터에 MXNet을 설치하고 SSH를 통해 서로 통신 할 수 있는지 확인한 다음 서버 IP가 포함 된 파일을 만들어야 합니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;$ cat hosts 
192.30.0.172 
192.30.0.171
$ python ../../tools/launch.py -n 2 --launcher ssh -H hosts python train_mnist.py --network lenet --kv-store dist_sync&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MXNet은 키-밸류 스토어를 사용하여 서버 간의 그라디언트와 파라미터를 동기화할 수 있습다. 이를 통해 분산 학습을 수행 할 수 있으며, 본 기능은 &lt;tt&gt;USE_DIST_KVSTORE = 1을&lt;/tt&gt; 사용하여 MXNet을 새로 컴파일하면 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#3 데이터는 Amazon S3!&lt;/strong&gt;&lt;br /&gt;
MXNet에서 데이터 반복자(iterators)는 Python iterator 객체와 비슷합니다. 단, 해당 레이블과 함께 &amp;#8220;n&amp;#8221;개의 학습 예제가 포함 된 &lt;a href="https://github.com/dmlc/mxnet/blob/master/python/mxnet/io.py" target="_blank" rel="noopener noreferrer"&gt;DataBatch&lt;/a&gt; 객체로 데이터 배치(batch)를 반환한다는 점이 다릅니다. MXNet에는 NDArray 및 CSV와 같은 공통 데이터 유형에 대해 미리 작성된 효율적인 데이터 반복자를 가지고 있습니다. 또한, HDFS와 같은 분산 파일 시스템에서 효율적인 I/O를 위해 바이너리 형식을 사용하기도 합니다. mx.io.DataIter 클래스를 확장하여 사용자 정의 데이터 반복기를 만들 수 있습니다. 이 기능을 구현하는 방법에 대한 자세한 내용은 &lt;a href="http://mxnet.io/tutorials/basic/data.html#custom-iterator" target="_blank" rel="noopener noreferrer"&gt;기본 튜토리얼&lt;/a&gt;을 참조하시면 됩니다 .&lt;/p&gt;
&lt;p&gt;특히, Amazon S3 (Amazon Simple Storage Service)는 대량의 데이터를 매우 저렴한 비용으로 저장해야 하는 고객에게 유용합니다. MXNet에서는 데이터를 디스크에 직접 다운로드 할 필요 없이 RecordIO, ImageRecordIO, CSV 또는 NDArray 형식의 Amazon S3에 저장된 데이터를 참조하는 반복자를 만들 수 있습니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;data_iter = mx.io.ImageRecordIter(     
     path_imgrec="s3://bucket-name/training-data/caltech_train.rec",
     data_shape=(3, 227, 227),
     batch_size=4,
     resize=256)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;# 4 신경망 시각화 기능 &lt;/strong&gt;&lt;br /&gt;
MXNet에서는 신경망 아키텍처를 시각화 할 수 있도록 &lt;a href="http://www.graphviz.org/" target="_blank" rel="noopener noreferrer"&gt;Graphviz&lt;/a&gt;와 통합되어 있습니다. 네트워크 시각화를 생성하려면, &lt;tt&gt;node_atters&lt;/tt&gt; 속성으로 정의한 대로 네트워크의 모양과 함께 네트워크의 마지막 레이어를 참조하는 심볼을 사용 합니다. 아래 예제는 &lt;a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener noreferrer"&gt;LeNet&lt;/a&gt; 표준 CNN 을 시각화하는 방법을 보여줍니다 .&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;mx.viz.plot_network(symbol=lenet, shape=shape)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_1.gif" target="_blank" rel="noopener noreferrer"&gt;&lt;img class="alignnone wp-image-1281 size-full" src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_1_thumb.gif" alt="" width="140" height="836" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;자세한 코드 및 구현 지침은이 &lt;a href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" target="_blank" rel="noopener noreferrer"&gt;자습서를&lt;/a&gt; 참조하십시오 .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#5 프로파일 러 지원&lt;/strong&gt;&lt;br /&gt;
MXNet에는 &lt;tt&gt;USE_PROFILER = 1&lt;/tt&gt; 플래그를 통해 사용 가능한 내장 프로파일러가 있습니다 . 이를 통해, 네트워크(심볼 수준) 실행 시간을 계층 별로 분류할 수 있습니다. 이 기능은 일반적인 프로파일링 도구인  &lt;em&gt;nvprof&lt;/em&gt;  및  &lt;em&gt;gprof을 &lt;/em&gt; 보완하며, 함수, 커널, 또는 학습 수준에서, 운영자 수준에서 처리할 수 있게 합니다. 환경 변수를 사용하여 전체 Python 프로그램에 대해 &lt;a href="http://mxnet.io/how_to/env_var.html#control-the-profiler" target="_blank" rel="noopener noreferrer"&gt;활성화&lt;/a&gt; 할 수 있습니다 . 또는 아래와 같이 프로그램의 하위 집합에 코드를 통합하여 활성화 할 수 있습니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;mx.profiler.profiler_set_config(mode='all', filename='output.json')     
mx.profiler.profiler_set_state('run')      
# Code to be profiled goes here...      
mx.profiler.profiler_set_state('stop')&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;프로파일링 출력을 Chrome과 같은 웹 브라우저에 로드하고 다음과 같이 브라우저의 추적 ( Chrome 브라우저에서 &lt;tt&gt;chrome://tracing)&lt;/tt&gt;으로 이동하여 프로필을 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1266 size-full" src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_2.gif" alt="" width="800" height="436" /&gt;&lt;/p&gt;
&lt;p&gt;위 스크린 샷은 프로파일링 도구를 사용하여 MXNet에 &lt;a href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" target="_blank" rel="noopener noreferrer"&gt;구현&lt;/a&gt; 된 원래의 LeNet 아키텍처로 MNIST 데이터 세트를 학습하는 프로파일을 보여줍니다 .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One more thing: MXNet CheetSheet&lt;/strong&gt;&lt;br /&gt;
이제 MXNet의 고유한 기능을 기반으로 신경망 학습을 시작하는데 아래 &lt;a href="https://s3.amazonaws.com/aws-bigdata-blog/artifacts/apache_mxnet/apache-mxnet-cheat.pdf" target="_blank" rel="noopener noreferrer"&gt;치트 시트&lt;/a&gt;가 도움이 될 것입니다. 여기에는 CNN, RNN/LSTM, 선형 회귀 및 로지스틱 회귀에 대한 몇 가지 일반적인 아키텍처가 포함되어 있습니다. 이를 사용하여, 데이터 반복자 및 Amazon S3 반복기를 작성하고 체크 포인트를 구현하며 모델 파일을 저장하는 방법에 대한 간단한 코드가 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://s3.amazonaws.com/aws-bigdata-blog/artifacts/apache_mxnet/apache-mxnet-cheat.pdf" target="_blank" rel="noopener noreferrer"&gt;&lt;img class="wp-image-1283 size-full" src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_1_thumb-1.gif" alt="Apache MXNet 치트 시트" width="1000" height="829" border="1" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p class="wp-caption-text"&gt;&lt;em&gt;확대하려면 클릭하십시오.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;MXNet 커뮤니티는 &lt;a href="http://gluon.mxnet.io/" target="_blank" rel="noopener noreferrer"&gt;Gluon&lt;/a&gt; 이라는 동적인 사용하기 쉬운 명령형 인터페이스를 지원하기 시작했고, MXNet으로 딥러닝을 시작하려면 &lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api" target="_blank" rel="noopener noreferrer"&gt;튜토리얼을&lt;/a&gt; 참조하십시오 .&lt;/p&gt;
&lt;p&gt;&lt;em&gt;이 글은 Sunil Mallya이 쓴 &lt;a href="https://aws.amazon.com/ko/blogs/ai/exploiting-the-unique-features-of-the-apache-mxnet-deep-learning-framework-with-a-cheat-sheet/#more-1264"&gt;Exploiting the Unique Features of the Apache MXNet Deep Learning Framework with a Cheat Sheet&lt;/a&gt;의 한국어 번역입니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;필수 참고 자료 &lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/all-about-mxnet"&gt;Apache MXNet의 모든 것 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://facebook.com/groups/mxnetkr/"&gt;MXNet 한국 사용자 모임&lt;/a&gt; (페이스북 그룹) 가입!&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=Fsivd57p9FY:mvCLHb0Au4g:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 10 Aug 2017 18:56:55 +0000</pubDate>
	<comments>http://blog.creation.net/mxnet-deep-learning-features-aws-cloud#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>클라우드 네이티브 컴퓨팅 – Adrian Cockcroft</title>
	<link>http://channy.creation.net/blog/1165</link>
	<description>&lt;p&gt;&lt;strong&gt;Written by Adrian Cockcroft &lt;a href="http://channy.creation.net/feed#Disclaimer"&gt;* Disclaimer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1600/1*nar6VC_mKyF-INRdV2ZAwQ.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;@adrianco의 사진 &amp;#8211; 무지개에 의한 굴절의 클라우드 네이티브 컴퓨팅.&lt;/small&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p id="dd05" class="graf graf--p graf-after--h3"&gt;Amazon Web Services는 &lt;a class="markup--anchor markup--p-anchor" href="https://www.cncf.io/announcement/2017/08/09/amazon-web-services-joins-cloud-native-computing-foundation-platinum-member/" target="_blank" rel="nofollow noopener"&gt;Cloud Native Computing Foundation에 가입했습니다.&lt;/a&gt;  저는 앞으로 AWS를 대표하여 CNCF 이사회 멤버로 참여하고, AWS 오픈 소스 팀의 &lt;a class="markup--anchor markup--p-anchor" href="https://twitter.com/arungupta" target="_blank" rel="nofollow noopener"&gt;Arun Gupta&lt;/a&gt; 와 함께 CNCF 프로젝트 및 워킹 그룹과 기술 협력을 진행할 예정입니다. 이러한 활동이 무엇을 의미하는지 잘 설명하기 위해, 저는 &amp;#8220;클라우드 네이티브&amp;#8221;가 의미하는 바를 설명해 드리고 싶습니다.&lt;/p&gt;
&lt;p id="331b" class="graf graf--p graf-after--p"&gt;저는 2009 년에 Netflix에서 일하고 있었습니다. 당시 엔지니어링 팀은 AWS로 마이그레이션  해야하는 새로운 애플리케이션 아키텍처 패턴을 파악하고 있었습니다.  팀 내에서는 eBay, Yahoo 및 Google에서 근무한 다양한 사람들이 클라우드에서 대규모 배포를 자동화하는 방법을 적용 중이었고, &lt;a class="markup--anchor markup--p-anchor" href="http://queue.acm.org/detail.cfm?id=1142065" target="_blank" rel="nofollow noopener"&gt;Werner Vogels&lt;/a&gt; 와 AWS로부터 새로운 아이디어를 배웠습니다 . 결과적으로 신규 아키텍처에는 클라우드 네이티브(Cloud Native)라는 기본 가정이 생겼습니다. 2010 년에 우리는 클라우드 마이그레이션에 관해 공개적으로 이야기하기 시작했고, 2012 년에는 &lt;a class="markup--anchor markup--p-anchor" href="http://netflix.github.io/" target="_blank" rel="nofollow noopener"&gt;NetflixOSS&lt;/a&gt; 로 &lt;a class="markup--anchor markup--p-anchor" href="http://netflix.github.io/" target="_blank" rel="nofollow noopener"&gt;통칭&lt;/a&gt; 되는 일련의 오픈 소스 프로젝트를 통해 전혀 새로운 플랫폼을 구축할 수 있었습니다.&lt;/p&gt;
&lt;p id="8865" class="graf graf--p graf-after--p"&gt;우리가 이러한 &amp;#8216;클라우드 네이티브&amp;#8217; 패턴을 모두 만들지는 않았지만, 함께 모아서 하나의 아키텍처로 모으고, 대규모로 구현하고, &lt;a class="markup--anchor markup--p-anchor" href="https://www.slideshare.net/adrianco/yowworkshop-131203193626phpapp01-1" target="_blank" rel="nofollow noopener"&gt;공개적으로 이야기&lt;/a&gt; 하고, 코드를 공유하는 공유하였습니다.&lt;/p&gt;
&lt;p id="0288" class="graf graf--p graf-after--p"&gt;클라우드 네이티브 아키텍처는 주문형(On-deman) 제공, 글로벌 배포, 탄력성 높은 클라우드 서비스를 최대한 활용합니다. 이를 통해 개발자 생산성, 비즈니스 민첩성, 확장성,  가용성, 활용도 및 비용 절감 효과를 크게 높일 수 있습니다.&lt;/p&gt;
&lt;p id="2409" class="graf graf--p graf-after--p"&gt;온-디멘드 방식은 사람들이 클라우드로 이동하는 가장 중요한 이유인 경우가 많지만, 기존 응용 프로그램 배포 시간을 단축 시키는 것은 아닙니다.  하지만, 클라우드 네이티브에서는 자원에 대한 임시(ephemeral ) 및 불변(immutable) 배포을 할 수 있습니다 . 자원 확보에 수주가 걸리는 구형 배포 모델에서는 불필요한 추가 용량을 미리 주문하고, 사용 후에도 돌려 주기를 꺼려하는 문제를 만들어냅니다. 대신,  클라우드 네이티브 패턴은 임시로 인스턴스를 받아오고, 콘테이너를 빌드하고, 필요한 만큼 많은 동일한 복제본을 배포하고, 사용 후에는 그냥 반납하고, 코드가 변경 될 때마다 새로운 이미지를 만들어 다시 반복할 수 있습니다. NetflixOSS는 맞춤형 Amazon Machine Images (AMI)를 만들어 이러한 개념을 적용하였습니다.&lt;/p&gt;
&lt;p id="3b1f" class="graf graf--p graf-after--p"&gt;기존에 물리적인 다수 데이터 센터에  애플리케이션을 배포하는 것은 상대적으로 드물고 구현하기 복잡합니다. 하지만, 클라우드 네이티브 아키텍처는 다중 가용 영역(Zone) 및 다중 리전(Region) 배포가 기본입니다. 이러한 배포 모델을 효과적으로 작업하려면, 개발자는 분산 시스템 개념을 잘 이해하고 있어야합니다.  그래서, 넷플릭스에서 개발자를 뽑을 때 &amp;#8220;&lt;a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="nofollow noopener"&gt;CAP 정리&lt;/a&gt;&amp;#8220;는 주요 인터뷰 질문 중에 하나였습니다.  기술의 엄청난 향상에도 불구하고, 빛의 속도는 일정하므로 네트워크 대기 시간, 특히 글로벌 다중 리전 대기 시간은 항상 제약이 될 것입니다.&lt;/p&gt;
&lt;p id="0e38" class="graf graf--p graf-after--p"&gt;클라우드 네이티브 아키텍처는 높은 확장성을 가집니다. 2010년 &lt;a class="markup--anchor markup--p-anchor" href="https://www.slideshare.net/adrianco/netflix-on-cloud-combined-slides-for-dev-and-ops" target="_blank" rel="nofollow noopener"&gt;Netflix의 AWS 사용에 대해 처음 발표&lt;/a&gt; 했을 때, 당시 수 천 개의 AWS 인스턴스에서 프론트 엔드 애플리케이션을 실행하면서 미국 내 약 1,600만 고객을 지원했습니다.  최근 Netflix는 AWS로 완벽하게 마이그레이션하였고, 현재 1 억 명이 넘는 글로벌 고객을 보유하고 있으며, &lt;a class="markup--anchor markup--p-anchor" href="https://aws.amazon.com/solutions/case-studies/netflix/" target="_blank" rel="nofollow noopener"&gt;100,000 개가 넘는 인스턴스를 실행&lt;/a&gt;하고 있습니다. 구현된 세부 사항은 몇 년에 걸쳐 변경되었지만, 근본적인 아키텍처 패턴은 동일합니다.&lt;/p&gt;
&lt;p id="10a9" class="graf graf--p graf-after--p"&gt;시간이 지남에 따라 클라우드 네이티브 아키텍처의 구성 요소는 실험적이고, 경쟁력 있는 구현을 통해 정의 된 외부 서비스로 이동하고 있습니다. 데이터베이스, 데이터 사이언스 파이프 라인, 콘테이너 스케줄러 및 모니터링 도구가 진화 하는 것을 목격했습니다.  이 부분에서 바로 CNCF가 이를 모아서 협력하는 역할을 합니다. CNCF &lt;a class="markup--anchor markup--p-anchor" href="https://github.com/cncf/toc" target="_blank" rel="nofollow noopener"&gt;기술 감독 위원회&lt;/a&gt;(&lt;a class="markup--anchor markup--p-anchor" href="https://github.com/cncf/toc" target="_blank" rel="nofollow noopener"&gt;Technical Oversight Committee)&lt;/a&gt;는 어느 클라우드 기반 프로젝트가 실험 단계에서 경쟁력 있는 단계로 넘어갈 때, 이를 검토하고, 인큐베이션 여부를 결정합니다. 너무 빠르게 움직여서 조금 혼란스러운 클라우드 기술 변화를 추적하려는 고객에게 CNCF가 승인하는 프로젝트 브랜드는 이를 판단하는데 조금이나마 도움을 줄 것입니다.  이들은 하나의 통합된 클라우드 네이티브 아키텍처가 아니라 느슨하게 연결된 모음으로서, &lt;span id="result_box" class="short_text" lang="ko" tabindex="-1"&gt;CNCF 회원이나 프로젝트 사용자에게 특정 프로젝트에 대한 편향적인 지지를 하지 않습니다.&lt;/span&gt;&lt;/p&gt;
&lt;p id="4625" class="graf graf--p graf-after--p"&gt;CNCF는 현재 10 개의 프로젝트를 진행하고 있으며 콘테이너 오케스트레이션을위한 Kubernetes, 모니터링을 위한 Prometheus, 애플리케이션 흐름 모니터링을 위한 Open Tracing, 로깅을 위한 Fluentd, 서비스 혼합를 위한 Linkerd, 원격 프로 시저 호출을 위한 gRPC, 서비스 디스커버리를 위한 CoreDNS, Containerd 콘테이너 런타임에 대한 Rkt, 콘테이너 네이티브 네트워킹에 대해서는 CNI 등이 있습니다.&lt;/p&gt;
&lt;p id="1b37" class="graf graf--p graf-after--p"&gt;AWS의 관점에서 여러 CNCF 프로젝트와 워킹 그룹에 관심을 가지고 있습니다. 특히, AWS는 &lt;a class="markup--anchor markup--p-anchor" href="http://containerd.io/" target="_blank" rel="nofollow noopener"&gt;Containerd&lt;/a&gt; 프로젝트 &lt;a class="markup--anchor markup--p-anchor" href="https://www.docker.com/docker-news-and-press/docker-extracts-and-donates-containerd-its-core-container-runtime-accelerate" target="_blank" rel="nofollow noopener"&gt;시작 멤버&lt;/a&gt; 였습니다. Containerd 커뮤니티를 통해 고객이 더 나은 경험을 할 수 있도록 어떻게 도와 줄 수 있는지 많은 아이디어를 가지고 있습니다. 또한, 곧 출시 될 Amazon ECS &lt;a class="markup--anchor markup--p-anchor" href="https://github.com/aaithal/amazon-ecs-agent/blob/cec1ece6d619aaafe4b485cb4b74e1aafa786428/proposals/eni.md" target="_blank" rel="nofollow noopener"&gt;Task Networking 기능&lt;/a&gt;은 &lt;a class="markup--anchor markup--p-anchor" href="https://github.com/containernetworking" target="_blank" rel="nofollow noopener"&gt;CNI&lt;/a&gt; 플러그인으로 작성되었으며, CNI가 AWS의 모든 콘테이너 기반 네트워킹의 기반이 될 것으로 기대합니다. 또한, 최근 &lt;a class="markup--anchor markup--p-anchor" href="https://www.cncf.io/blog/2017/06/28/survey-shows-kubernetes-leading-orchestration-platform/" target="_blank" rel="nofollow noopener"&gt;CNCF 조사&lt;/a&gt;에 따르면 응답자의 63 %가 Amazon EC2에서 Kubernetes를 호스팅 한다고 합니다. Arun Gupta는 AWS에서 Kubernetes 활용을 위한  &lt;a class="markup--anchor markup--p-anchor" href="http://kubernetes-aws.io/" target="_blank" rel="nofollow noopener"&gt;다양한 옵션&lt;/a&gt;과 &lt;a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/"&gt;Kops 기반 활용 방법&lt;/a&gt;에 대한 블로그를 쓰고 있습니다.&lt;/p&gt;
&lt;p id="61c5" class="graf graf--p graf-after--p"&gt;제가 맡고 있는 AWS의 오픈 소스팀의 사명은 오픈 소스 프로젝트, 커뮤니티 및 재단과 협력하고 AWS 엔지니어링에서 더 많은 오픈 소스 공헌을 유도하고 장려하는 데 도움을 주는 것입니다. AWS는 이미 CNCF를 운영하고 있는 The Linux Foundation의 멤버이며, 현대적인 분산 시스템에 최적화된 &lt;a class="markup--anchor markup--p-anchor" href="https://www.cncf.io/about/charter/" target="_blank" rel="nofollow noopener"&gt;새로운 컴퓨팅 패러다임&lt;/a&gt;을 발명하고 추진하기 위한 업계의 동료들과 함께 일하기를 기대 합니다.&lt;/p&gt;
&lt;p id="2757" class="graf graf--p graf-after--p graf--trailing"&gt;AWS에서 오픈 소스 활동에 대한 최신 정보를 얻으시려면, &lt;a class="markup--anchor markup--p-anchor" href="http://twitter.com/awsopen" target="_blank" rel="nofollow noopener"&gt;@AWSOpen&lt;/a&gt;을 팔로우 해 주시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;br /&gt;
&lt;em id="Disclaimer"&gt;본 글은 AWS의 클라우드 전략 담당 부사장인 Adrian Cockcroft의 허락하에 &lt;a href="https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf"&gt;Cloud Native Computing&lt;/a&gt;의 한국어 편집본입니다. 본 글은 Adrian Cockcroft의 개인적인 의견이며, 본 블로그의 의견이 아닙니다. 한국어 번역 기사인 본 글에 대한 인용 및 사용 허가는 오직 Adrian Cockcroft의 영문 원글에 효력이 있습니다. 본 글을 인용하실 때에는 이러한 주의 사항을 반드시 추가하시기 바랍니다. (Disclamar: This article is a Korean translation of &lt;a href="https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf"&gt;Cloud Native Computing&lt;/a&gt; written by Adrian Cockcroft. It is not opinion of this blog and this translation may contain some errors or incorrect expressions. Please only refer to the original article that was published in English for accuracy.)&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class="instagram-media"&gt;
&lt;div&gt;
&lt;div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://www.instagram.com/p/BJpl3SYjxSi/" target="_blank" rel="noopener"&gt;Rainbow over the cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Channy Yun(@channyun)님의 공유 게시물님, 2016 8월 28 오전 3:58 PDT&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=U_gS0ZtzK3M:RfNiGDJc05s:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 09 Aug 2017 16:34:35 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1165#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>영화 “택시운전사”에 나오지 않은 네가지 이야기</title>
	<link>http://channy.creation.net/blog/1159</link>
	<description>&lt;p&gt;주말에 영화 &amp;#8220;택시운전사&amp;#8221;를 보신 분들 많으시죠?&lt;/p&gt;
&lt;p&gt;우리 나라의 근대사의 가장 아픈 역사 중 하나인 5.18 민주화 운동에 대해 직접 잠입 취재를 한 외국인 기자  위르겐 힌츠페터와 그와 함께 운전을 해서 광주를 다녀온 기사이자 통역인 김사복씨에 대한 실화를 기반으로 만들어졌습니다.&lt;/p&gt;
&lt;p&gt;영화는 실화를 각색한 것으로 광주 밖의 일반 시민의 입장에서 참상을 겪으면서, 직접 현장에 뛰어드는 택시 운전사에 초점을 맞추고 있지만, 더욱 주목할 만한 사실은 외국인으로 비밀 잠입해서 취재를 한 힌츠페터에 있습니다. 이 글에서는 영화에서 나오지 않는 몇 가지 역사적 사실을 살펴 볼께요. (스포일러는 없음)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 광주로 향한 사람은 힌츠페터 뿐이 아니었다.&lt;/strong&gt;&lt;br /&gt;
독일 공영방송 ARD의 기자인 힌츠페터는 광주를 취재하기 위해 19일 나리타에서 서울로 들어왔다. 소개 받은 기사 김사복 외에도 녹음 기사 헤닝 루모어도 함께 있었다. 20일 그들은 광주까지 텅빈 고속도로를 달려 와서 검문소에서 군인들에게 길이 엇갈린 동료를 찾으러 광주로 들어가야 한다고 했다. 시위대에 희생당할 수 있으므로 구출해야 한다고 이야기를 만들었다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1162 aligncenter" src="http://channy.creation.net/data/channy/2017/08/06204907/tax-driver-3.jpg" alt="" width="80%" /&gt;&lt;br /&gt;
는 당시 광주에 있었던 외국인은 국제사면위원회 소속의 두 외국인 젊은이와 인터뷰를 하고, 그들이 목격한 끔찍한 일에 대해 알게 되었다. 군인들이 폭력을 쓰고 학대하면서 시위대를 진압했다는 것으로 상상을 초월하는 규모의 잔혹함이 처음 며칠 동안 일어났다고 전했다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 광주에서 나올 때 검문은 삼엄하진 않았다.&lt;/strong&gt;&lt;br /&gt;
21일 힌츠페터는 취재 자료를 외부에 공개하기 위해 광주에서 나오면서 두 번 검문 받았는데, 촬영 필름을 차 안에 숨겨두었고 대개 군인은 그 안에 총기가 있는지 살폈지, 촬영 자료를 발견하지 못했다. 서울에 도착한, 힌츠페터 일행은 필름 10개를 두 개로 나눠 5개는 허리춤에 넣고, 5개는 쿠키 상자에 넣는 방법으로 나누어 압수 위험에 대비했다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1161 aligncenter" src="http://channy.creation.net/data/channy/2017/08/06204906/tax-driver-4.jpg" alt="" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;그는 일본에 도착 한 후, 바로 필름을 독일로 보냈다. 독일 제1공영방송은 참상을 담은 그의 컬러 필름을 21일 TV 뉴스로 내보냈고, 많은 외신들이 이 기사를 통해 광주의 참상을 알게되었다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. 힌츠페터는 출국 후 곧장 한국으로 돌아왔다!&lt;/strong&gt;&lt;br /&gt;
힌츠페터는 일본에 3시간만 머물고 다시 한국으로 귀국했다. 귀국 후 그는 김영삼 자택앞에서 취재를 했으나 거부당하고, 23일 광주로 다시 들어갔다. 이미 그때는 계엄군과 시민군 사이의 총격전으로 많은 사람이 죽고 다친 상태였다.&lt;/p&gt;
&lt;p&gt;그의 두번째 취재 영상은 생각보다 평온했던 시민들의 일상을 담았고, 당시 계엄군 측이 언론에 흘린 &amp;#8216;폭도가 점령해 아비규환이 된 시내 상황&amp;#8217;같은 주장을 정면으로 반박하는 확실한 증거가 되었다.  그의 취재와 영상 자료가 없었다면 위 주장대로 사실이 날조되었을 가능성이 컸다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. 그의 필름은 23년 만에 한국에 공식 공개됐다!&lt;/strong&gt;&lt;br /&gt;
힌츠페터가 광주에서 찍은 영상은 1980년 9월 독일에서 《기로에 선 한국》이라는 다큐멘터리로 제작되었고, 독일에서 유학중이던 한국인 신부들이 들여와 국내 각처에서 비밀 상영되었다. 23년만인 2003년 5월 18일 KBS 1TV 「일요스페셜」&amp;#8217;80년 5월, 푸른 눈의 목격자&amp;#8217;편에서 처음 공개됐다.&lt;/p&gt;
&lt;p&gt;아래는 전편 영상!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/center&gt;1986년 11월에는 광화문 네거리에서 시위를 취재하던 도중 사복 경찰에게 집단 구타를 당해 목뼈와 척추가 부러지는 중상을 입고 귀국한 그는 한때 생명이 위태로울 때 국립 5.18 묘역에 묻히고 싶다는 의지를 피력했다. 2005년 송건호 언론상을 수상했으며, 2016년 1월 25일 세상을 떠난 후, 생전에 남긴 모발과 손톱 등이 그해 5.18 기념식에서 구묘역 입구에 안치되었다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://newsimg.sedaily.com/2017/08/02/1OJLELFKR2_1.jpg" alt="" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;역시 역사는 행동하는 사람에 의해 바뀐다는 점!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;더 읽어볼 글&lt;/strong&gt;&lt;br /&gt;
&amp;#8211; &lt;a href="https://ko.wikipedia.org/wiki/%EC%9C%84%EB%A5%B4%EA%B2%90_%ED%9E%8C%EC%B8%A0%ED%8E%98%ED%84%B0"&gt;위르겐 힌츠페터 위키 백과&lt;/a&gt;&lt;br /&gt;
&amp;#8211; &lt;a href="https://namu.wiki/w/%EC%9C%84%EB%A5%B4%EA%B2%90%20%ED%9E%8C%EC%B8%A0%ED%8E%98%ED%84%B0"&gt;위르겐 힌츠페터 나무위키&lt;/a&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=xaXuvtZzSFM:kf5jKtjgUEI:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 06 Aug 2017 15:00:07 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1159#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>시애틀에 있는 세 가지 아마존(Amazon) 명소</title>
	<link>http://channy.creation.net/blog/1157</link>
	<description>&lt;p&gt;시애틀(Seattle)하면 생각 나는 것이 많죠! &amp;#8216;시애틀의 잘 못 이루는 밤, 스페이스니들, 파이크 수산 시장, 스타벅스 1호점&amp;#8217; 등등&amp;#8230; 그 중에서도 시애틀을 대표하는 기업 중 하나인 아마존(Amazon)이 있습니다. 재미있는 건 오피스가 거의 다운 타운에 있는데, 최근에 관광객들에게도 인상적인 몇 가지 볼거리가 늘어나기 시작했습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1158 aligncenter" src="http://channy.creation.net/data/channy/2017/08/04082113/seattle-amazon.jpg" alt="" width="90%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.amazon.com/b?node=16008589011"&gt;Amazon Go &lt;/a&gt;스토어 &amp;#8211; &lt;small&gt;2131 7th Ave, Seattle&lt;/small&gt;&lt;/strong&gt;&lt;br /&gt;
컴퓨터 비전, 딥러닝, 센서 융합 같은 기술을 통해 만든 판매원이 없는 매장입니다. 현재 직원들을 대상으로 베타 테스트를 하고 있는데, 들어가 봤는데 실제로 잘 동작하더라구요. 과일팩 하나 사 가지고 나왔습니다.  아직 들어가지는 못하지만, 밖에서 사진 찍는 건 가능합니다.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/b?node=16008589011"&gt;홈페이지&lt;/a&gt;에서 언제 일반인에게 오픈 하는지 알림 메일을 받으실 수도 있습니다. 아마 여러분이 오실 때 쯤 되면 일반 공개 되면 좋겠네요.&lt;/p&gt;
&lt;p&gt;Update. 2018년 1월 22일 부터 시애틀 상점에 대한 일반인 서비스가 시작되었습니다!&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;편리한 오프라인 고객 경험을 만들어낸 Amazon Go의 핵심인 Just Walk Out 기술에 대해서는 아마존이 상세하게 공개하지 않겠지만, 몇몇 관련 기사에서 약간의 힌트를 얻을 수 있다. &lt;a href="https://www.recode.net/2016/12/5/13842892/amazon-go-grocery-store-no-lines-cashier-paying"&gt;recode는&lt;/a&gt; 2016년 기사에서 아마존의 특허를 인용하며 센서를 통해 수집된 고객 데이터를 바탕으로 AI와 컴퓨터 비전 기술을 통해 매장 고객이 어떤 물건을 집어 들었는지 파악한다고 한다. &lt;a href="https://www.geekwire.com/2016/amazon-go-works-technology-behind-online-retailers-groundbreaking-new-grocery-store/"&gt;GeekWire는&lt;/a&gt; Amazon Go에 RFID 기술은 적용되지 않았음을 확인했으며, &lt;a href="https://www.digitalpulse.pwc.com.au/amazon-go-strategy-retail-grocery/?utm_source=pwcchair&amp;utm_campaign=shared-content&amp;utm_medium=referral"&gt;PwC도&lt;/a&gt; 블루투스 비콘 등 Amazon Go에 적용된 기술을 분석하는 아티클을 공개했다. &lt;a href="https://techcrunch.com/2018/01/21/inside-amazons-surveillance-powered-no-checkout-convenience-store/"&gt;TechCrunch의 최신 기사에 따르면&lt;/a&gt; Amazon Go 천장에는 수많은 RGB 카메라가 달려 있어 고객과 상품의 움직임을 추적하며, 선반에도 저울이 달려 있어 무게 변화를 미세하게 측정한다고 한다. 무인 결제 시스템을 악용한 절도를 걱정했던 &lt;a href="https://www.nytimes.com/2018/01/21/technology/inside-amazon-go-a-store-of-the-future.html"&gt;뉴욕타임즈 기자는&lt;/a&gt; (미리 아마존의 허락을 맡고) 자신이 직접 바닐라 탄산수를 숨겨서 몰래 나왔지만 결제가 됐다는 에피소드를 전하기도 했다.  &amp;#8211; 출처: &lt;a href="http://techneedle.com/archives/33910"&gt;http://techneedle.com/archives/33910&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Amazon Sphere  &lt;small&gt;2117 7th Ave, Seattle, WA&lt;/small&gt;&lt;/strong&gt;&lt;br /&gt;
Amazon Go 바로 옆에는 Spheres라는 3개의 대형 유리 원형 조형물이 만들어지고 있습니다. 직원, 시민, 관광객들이 도심에서 자연을 느낄 수 있도록 거대 열대 우림을 조성하는 프로젝트입니다.&lt;/p&gt;
&lt;p&gt;이를 위해 시애틀 근교 온실에서 2015년 부터 4만종의 다양한 식물을 키우고 있습니다. 얼마 전부터 식물을 옮기는 작업을 하고 있고, 2018년에 오픈되면 시애틀의 새로운 명소가 될듯합니다. &lt;a href="https://www.youtube.com/channel/UCzE5rz2KHTFYAkmMksUpPLA/videos"&gt;Amazon News 유튜브 채널&lt;/a&gt;을 보시면 관련 동영상들이 계속 업데이트 되고 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;스페어 바로 옆 건물은 아마존 미팅 센터가 있습니다. 이 건물은 일반인들도 쉴 수 있는 공간을 제공하구요. 재미있는 건 매일 바나나를 가져 갈 수 있도록 놔 둔다는 점인데요. 누구나 하나씩 가져갈 수 있어요.^^&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.amazon.com/b?node=13270229011"&gt;Amazon Book Store&lt;/a&gt; &amp;#8211; &lt;small&gt;4601 26th Ave NE, Seattle&lt;/small&gt;&lt;/strong&gt;&lt;br /&gt;
이미 2015년에 오픈해서 운영 중인 오프라인 서점입니다. 시내에서 북쪽으로 워싱턴주립대(UW) 근처에 유니버시티 빌리지라는 곳에 있습니다. UW 캠퍼스가 이쁘니까 구경하러 갔다가 가면 좋습니다. 주로 아마존에서 별점 높은 책들이 전시되어 있고, 킨들 같은 디지털 기기들이 전시되어 있는 곳입니다. 이미 많은 분들이 다녀갔고, &lt;a href="https://estimastory.com/2016/03/12/amazonbooks/"&gt;임정욱님&lt;/a&gt;, &lt;a href="https://brunch.co.kr/@lifidea/8"&gt;김진영님&lt;/a&gt; 등 다양한 후기가 있으니 한번 읽어 보시길&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;아마존 북 스토어는 시애틀 뿐만 아니라 뉴욕, 샌디에고, 보스톤 근교 등 &lt;a href="https://www.amazon.com/b?node=13270229011"&gt;8개&lt;/a&gt;가 있는데, 캘리포니아를 중심으로 더 늘어날 예정입니다.&lt;/p&gt;
&lt;p&gt;시애틀을 방문해 보신다면, 한번 방문해 보신다면 어떨까요?&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=FefYCYLyQ9M:q-VBEliGlLU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 03 Aug 2017 23:44:26 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1157#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 기반 추천 오픈 소스 딥러닝 프로젝트 모음</title>
	<link>http://blog.creation.net/apache-mxnet-deep-learning-project</link>
	<description>&lt;p&gt;&lt;a href="http://mxnet.io/"&gt;Apache MXNet&lt;/a&gt; 은 일반 개발자가 손쉽게 딥러닝(Deep Learning) 모델을 구축, 학습 및 실행하는 데 도움을 주는 오픈 소스 라이브러리입니다. &lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;이전 시리즈&lt;/a&gt;에서 MXNet API 및 주요 기능, 활용 방법에 대해 소개했습니다.&lt;/p&gt;
&lt;p&gt;이 글에서는 MXNet을 다양한 유스 케이스에 적용하는 특징적인 오픈 소스 프로젝트를 소개합니다. (참고로 &lt;a href="http://mxnet.io/model_zoo/"&gt;MXNet Model Zoo&lt;/a&gt;에는 다양한 주요 딥러닝 학습 모델 사례가 있으니, 먼저 살펴 보시기 바랍니다!)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#1 — 이미지 객체 인식&lt;/strong&gt;&lt;br /&gt;
이 프로젝트는 하나의 이미지에서 여러개의 객체를 탐지하는  것으로 &lt;a href="https://github.com/zhreshold/mxnet-ssd"&gt;mxnet-ssd&lt;/a&gt; (&lt;a href="https://arxiv.org/abs/1512.02325"&gt;논문 링크&lt;/a&gt;)라는 프로젝트를 개량한 것으로 MXNet의 특징이라고 할 수 있는, 멀티 GPU에서 성능을 향상 시킨 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*69-LDvP5UV3z1kaG9XgWQQ.png" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/precedenceguo/mx-rcnn"&gt;&lt;strong&gt;precedenceguo/mx-rcnn&lt;/strong&gt;&lt;br /&gt;
&lt;/a&gt;&lt;em&gt;mx-rcnn &amp;#8211; Faster R-CNN, an MXNet implementation with distributed implementation and data parallelization&lt;br /&gt;
&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 아래 연구 결과를 기반으로 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ross Girshick: &lt;a href="https://arxiv.org/abs/1504.08083"&gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun: &lt;a href="https://arxiv.org/abs/1506.01497"&gt;Faster R-CNN: Towards real-time object detection with region proposal networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;#2 — 스마트폰용 이미지 분석 프로젝트&lt;/strong&gt;&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*oc9YknlZp6rWnf7DCXL9tQ.gif" alt="" width="200" align="right" hspace="10" /&gt; &lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 입문 마지막 가이드&lt;/a&gt;에서 살펴본 대로,  &lt;a href="https://arxiv.org/abs/1602.07261"&gt;Inception v3&lt;/a&gt; 을 사용하면, 모바일 기기에서도 실시간으로 이미지 분석이 가능합니다.&lt;/p&gt;
&lt;p&gt;아래 프로젝트는 안드로이드 및 iOS에ㅓ 사용할 수 있는 이미지 인식 프로젝트입니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dneprDroid/ImageRecognizer-iOS"&gt;&lt;strong&gt;dneprDroid/ImageRecognizer-iOS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dneprDroid/ImageRecognizer-Android"&gt;&lt;strong&gt;dneprDroid/ImageRecognizer-Android&lt;/strong&gt;&lt;/a&gt;&lt;em&gt;&lt;br /&gt;
&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#3 — 얼굴 인식 및 안면 감지 기능&lt;/strong&gt;&lt;br /&gt;
이 프로젝트는 &lt;a href="https://aws.amazon.com/fr/rekognition/"&gt;Amazon Rekognition&lt;/a&gt;의 얼굴 인식과 유사한 기능을 제공합니다. 좀 더 자세한 구현을 하고 싶은 경우, 좋은 출발점이 될 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image aligncenter" src="https://cdn-images-1.medium.com/max/1455/1*vvCWxc4Z1IOzrT_-u2IATA.png" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tornadomeet/mxnet-face"&gt;&lt;strong&gt;tornadomeet/mxnet-face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 아래 연구 결과를 기반으로 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wu X, He R, Sun Z. &lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Lightened CNN for Deep Face Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rudd E, Günther M, Boult T. MOON: &lt;a href="https://arxiv.org/abs/1603.07027"&gt;A Mixed Objective Optimization Network for the Recognition of Facial Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jiang H, Learned-Miller E. &lt;a href="https://arxiv.org/abs/1606.03473"&gt;Face detection with the faster R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;#4— 자동차 번호판 인식하기 &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 81 % 정확도로 MacBook Pro에서 초당 9 매의 번호판 인식을 수행할 수 있습니다. 약간의 노력을 더 한다면 다른 문자 인식 사용 사례에 적용할 수 있습니다 &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f642.png" alt="&#x1f642;" class="wp-smiley" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/szad670401/end-to-end-for-chinese-plate-recognition"&gt;&lt;strong&gt;szad670401/end-to-end-for-chinese-plate-recognition&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#5 — Sockeye : 기계 번역 프로젝트&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sockeye 프로젝트는 MXNet에 기반한 신경망 기계 번역(Neural Machine Translation)을 위한 시퀀스-시퀀스(sequence-to-sequence) 프레임 워크입니다.  AWS에서 개발하고 있으며, 더 자세한 것은 &lt;a href="https://aws.amazon.com/ko/blogs/korea/train-neural-machine-translation-models-with-sockeye/"&gt;MXNet 기반 Sockeye를 통한 기계 번역 학습 해보기&lt;/a&gt;를 참고하시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/awslabs/sockeye"&gt;&lt;strong&gt;awslabs/sockeye&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;sockeye &amp;#8211; Sequence-to-sequence framework with a focus on Neural Machine Translation based on MXNet&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AWS  기반 배포 방법&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;지금까지 다른 Python 애플리케이션과 마찬가지로 Amazon EC2 인스턴스에서 MXNet 코드를 실행했습니다. AWS에서 애플리케이션을 실행할 수 있는 대체 방법(콘테이너 및 서버리스)이 있으면, 이는 MXNet에 적용할 수 있겠죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#6— Amazon ECS와 코드 도구를 통한 MXNet API 지속적 배포 방식&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 &lt;a href="https://aws.amazon.com/fr/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; 템플릿을 사용하여 MXNet 모델 또는 애플리케이션 코드의 변경 사항을 파이프 라인을 통해 배포, 구성 및 조율하는 자동화 된 워크 플로우를 생성할 수 있습니다. &lt;a href="https://aws.amazon.com/codepipeline/"&gt;CodePipeline&lt;/a&gt;과 &lt;a href="https://aws.amazon.com/codebuild/"&gt;CodeBuild&lt;/a&gt;를 사용하여 지속적 전달(CD) 방식이 가능하고,  몇 분 만에 사용자가 사용할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/awslabs/ecs-mxnet-example"&gt;&lt;strong&gt;awslabs/ecs-mxnet-example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/fr/blogs/ai/deploy-deep-learning-models-on-amazon-ecs/"&gt;Deploy Deep Learning Models on Amazon ECS | AWS AI Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;#7 —  MXNet Lambda 함수로 배포 하기&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AWS Lambda를 통해 MXNet을 사용해 미리 학습된 모델을 통해 이미지 인식 등을 해 볼 수 있는 프로젝트입니다. Serverless Application Model (SAM) 템플릿을 통해 서버리스 API 엔드포인트도 자동으로 구현합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/awslabs/mxnet-lambda"&gt;&lt;strong&gt;awslabs/mxnet-lambda&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/fr/blogs/compute/seamlessly-scale-predictions-with-aws-lambda-and-mxnet/"&gt;Seamlessly Scale Predictions with AWS Lambda and MXNet | AWS Compute Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;지금까지 다양한 MXNet 기반의 추천 오픈 소스 프로젝트를 살펴 보았습니다. 혹시 더 추천해 주실만한 프로젝트가 있으면 알려주세요!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=xI70O930_EE:tsPOh-lNd5M:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 02 Aug 2017 17:21:48 +0000</pubDate>
	<comments>http://blog.creation.net/apache-mxnet-deep-learning-project#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>Apache MXNet에 대한 모든 것!</title>
	<link>http://channy.creation.net/blog/1155</link>
	<description>&lt;p&gt;아마존의 CTO인 Werner Vogels 박사는 &lt;a href="http://www.allthingsdistributed.com/2016/11/mxnet-default-framework-deep-learning-aws.html"&gt;MXNet – Deep Learning Framework of Choice at AWS&lt;/a&gt;라는 글에서 확장 능력, 개발 속도, 이동성 등의 다양한 요인을 비추어 볼 때, MXNet이 가장 좋은 인공 지능 애플리케이션 개발을 위한 딥러닝 프레임웍이라고 판단하고, 이를 기반한 딥러닝 서비스 개발 지원 및 오픈 소스 지원에 대한 의지를 피력한 바 있습니다.&lt;/p&gt;
&lt;p&gt;이 글은 다양한 오픈 소스 딥러닝 프레임웍 중에 아마존이 선택한 Apache MXNet에 관한 다양한 한국어 자료들을 모아서 제공하는 것을 목적으로 합니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://blog.creation.net/data/tisotry/2017/07/31141306/apache-mxnet.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Apache MxNet은 개발자들에게 친숙한 심볼릭(Symbolic)과 명령형(imperative) 프로그래밍의 혼합 방식을 지원할 뿐만 아니라 CPU와 GPU 연산을 지원하고, 특히 GPU 클러스터에 최적화된 엔진을 사용해서 성능이 뛰어납니다.&lt;/p&gt;
&lt;p&gt;또한, 실무적으로 많이 사용하는 Python, C++, R, Scala, Julia, Matlab, and JavaScript을 지원하고, 모바일 기기 부터 서버까지 다양한 디바이스를 지원하여 산업계에서 응용하기에 매우 적합한 딥러닝 프레임워크입니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apache MXNet 입문 가이드&lt;/strong&gt;&lt;br /&gt;
이 시리즈는 AWS 테크 에반젤리스트인 Julien Simon이 연재한 &lt;a href="https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab?source=user_profile---------17-----------"&gt;MXNet 관련 글 모음&lt;/a&gt;의 번역 편집본으로 최근 각광 받고 있는 Deep Learning 라이브러리인 &lt;a href="http://mxnet.io/"&gt;Apache MXnet&lt;/a&gt;을 개괄적으로 설명하려고 합니다.&lt;/p&gt;
&lt;p&gt;이 글은 간단한 코드를 이해하는 개발자라면 기계 학습과 인공 지능을 잘 알지 못하는 분이라도 쉽게 따라올 수 있도록 했습니다. 너무 겁먹지 않으셔도 됩니다.&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;동영상&lt;/strong&gt;&lt;br /&gt;
&lt;a href="http://hunkim.github.io/ml/"&gt;모두를 위한 딥러닝&lt;/a&gt; 강의로 유명한 홍콩과기대 김성훈 교수와 MXNet 코드 개발자인 Xingjian Shi가 함께 MXNet의 장점과 함께 간단한 딥러닝 학습 문제를 데모로 보여 드립니다. (&lt;a href="https://www.slideshare.net/awskorea/2-mx-net"&gt;슬라이드&lt;/a&gt;)  모두를 위한 딥러닝을 청취하신 분들이라면, Lab 강의에 대한 &lt;a href="https://github.com/hunkim/DeepLearningZeroToAll/tree/master/mxnet"&gt;MXNet 소스 코드&lt;/a&gt;를 참고하셔도 됩니다!&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Apache MXNet에 대해 간단하게 소개하고, AWS에서 Deep Learning AMI을 이용하여 Amazon EC2 인스턴스에서 MXNet을 구동하고, 테스트하는 방법을 살펴 봅니다. 또한, 분산 딥러닝 클러스터 생성 템플릿으로 멀티 GPU에서 구동하는 방법도 살펴 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;사용자 모임&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apache MXNet에 대한 관심이 늘어나고 있고, 배우려는 분들과 질문/답변을 할 수 있도록 페이스북에 그룹을 만들었습니다. 관심 있는 분들 참여해 주시길&amp;#8230;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/groups/mxnetkr/"&gt;https://www.facebook.com/groups/mxnetkr/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;더 자세한  정보&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/mxnet"&gt;AWS &amp;#8211; Apache MXNet 소개 페이지&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/blogs/korea/aws-deep-learning-framework-mxnet/"&gt;AWS로 딥 러닝을 위한 프레임워크 MxNet 활용하기 | AWS 블로그&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/blogs/korea/excited-about-mxnet-joining-apache/" rel="bookmark"&gt;MXNet, Apache 재단 오픈 소스 프로젝트 참여 | AWS 블로그&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/blogs/korea/train-neural-machine-translation-models-with-sockeye/" rel="bookmark"&gt;MXNet 기반 Sockeye를 통한 기계 번역 학습 해보기 | AWS 블로그&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.popit.kr/mxnet%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%EB%A5%98-%EC%95%B1-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0/"&gt;MXNet을 활용한 이미지 분류 앱 개발하기 | Popit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;더 자세한 것은 AWS 블로그 &lt;a href="https://aws.amazon.com/ko/blogs/korea/category/mxnet/"&gt;MXNet 카테고리&lt;/a&gt;를 참고하셔도 됩니다.&lt;/p&gt;
&lt;p&gt;앞으로 이 글에는 &lt;a href="https://chatbotslife.com/training-mxnet-part-1-mnist-6f0dc4210c62"&gt;MXNet 기반 모델 학습 시리즈&lt;/a&gt; 한국어 번역 및 &lt;a href="https://aws.amazon.com/ko/blogs/ai/"&gt;Amazon AI 블로그&lt;/a&gt;의 MXNet 관련 글 모음 등 다양한 정보를 소개할 예정입니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WyM00RfUUZU:FsM0kCPx1pU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 31 Jul 2017 23:04:31 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1155#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기</title>
	<link>http://blog.creation.net/mxnet-part-6-realtime-object-detection</link>
	<description>&lt;p&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;5편&lt;/a&gt;에서는 이미지의 객체 검출을 위해 유명한 세 가지 학습 모델을 사용하여, 간단한 MXNet 소스 코드를 사용하여 몇 가지 이미지를 테스트해보았습니다.&lt;/p&gt;
&lt;p&gt;우리가 배운 것 중 하나는 각 모델이 서로 다른 메모리 요구 사항이 있다는 것입니다. 가장 메모리를 적게 먹는 것은 Inception v3으로 &amp;#8220;43MB&amp;#8221;만 사용입니다. 정말 이게 잘 될까? 예를 들어, &amp;#8220;라스베리파이(Raspberry Pi)&amp;#8221; 같은 작은 디바이스에서 실행할 수 있을까라는 질문을 해볼 수 있을 텐데요. 이번 글에서 함께 알아 보시죠!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mxnet.io/get_started/raspbian_setup.html"&gt;공식 가이드&lt;/a&gt;가 있지만, 몇 가지 단계가 빠진 것을 발견해서 아래 내용을 따라가면 최신 Raspbian을 실행하는 Raspberry Pi 3에서 제대로 작동합니다.&lt;/p&gt;
&lt;pre&gt;$ uname -a
 Linux raspberrypi 4.4.50-v7+ #970 SMP Mon Feb 20 19:18:29 GMT 2017 armv7l GNU/Linux&lt;/pre&gt;
&lt;p&gt;맨 먼저 필요한 라이브러리를 추가합니다.&lt;/p&gt;
&lt;pre&gt;$ sudo apt-get update
$ sudo apt-get -y install git cmake build-essential g++-4.8 c++-4.8 liblapack* libblas* libopencv* python-opencv libssl-dev screen&lt;/pre&gt;
&lt;p&gt;그런 다음 MXNet 저장소를 복제하고, 최신 버전을 가져옵니다. 마지막 단계를 놓치지 마세요.&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/dmlc/mxnet.git --recursive
$ cd mxnet
# List tags: v0.9.3a is the latest at the time of writing
$ git tag -l
$ git checkout tags/v0.9.3a&lt;/pre&gt;
&lt;p&gt;MXNet은 Amazon S3에서 데이터를 로드하고 저장할 수 있으므로 나중에 이 기능을 사용하면 편리 할 것입니다. MXNet은 또한 HDFS를 지원하지만 Hadoop을 로컬에 설치해야 하므로 좀 힘들겠죠?&lt;/p&gt;
&lt;p&gt;make를 실행할 수 있지만 Pi의 제한된 처리 능력을 감안할 때, 빌드는 약간 시간이 걸릴 것입니다. SSH 세션이 만료되면 문제가 있을 수 있으니 &lt;a href="https://www.gnu.org/software/screen/"&gt;Screen&lt;/a&gt;을 사용하면 좋습니다.&lt;/p&gt;
&lt;p&gt;약간 속도를 높이기 위해 2 코어(4개 중)에서 병렬 실행을 실행할 수 있습니다. 더 많은 코어를 사용하면, 응답을 안할 수가 있으니 꼭 2개만 사용하시기 바랍니다.&lt;/p&gt;
&lt;pre&gt;$ export USE_S3=1
$ screen make -j2&lt;/pre&gt;
&lt;p&gt;이것은 약 1 시간 정도 걸립니다. 마지막 단계는 라이브러리와 Python 바인딩을 설치하는 것입니다.&lt;/p&gt;
&lt;pre&gt;$ cd python
 $ sudo python setup.py install
 $ python
 Python 2.7.9 (default, Sep 17 2016, 20:26:04)
 [GCC 4.9.2] on linux2
 Type "help", "copyright", "credits" or "license" for more information.
 &amp;gt;&amp;gt;&amp;gt; import mxnet as mx
 &amp;gt;&amp;gt;&amp;gt; mx.__version__
 '0.9.3a'&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;모델 로딩하기&lt;/strong&gt;&lt;br /&gt;
일단 모델 파일을 Pi에 복사 한 후에는 실제 파일을 로드 할 수 있어야합니다. 5편에서 쓴 똑같은 코드를 재사용할 수 있습니다. Pi는 약 580MB의 여유 메모리가 있는 CLI 모드를 지원합니다. 모든 데이터는 32GB SD 카드에 저장됩니다. 이제 VGG16를 로딩해봅시다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; vgg16,categories = init("vgg16")
 terminate called after throwing an instance of 'std::bad_alloc'
 what(): std::bad_alloc&lt;/pre&gt;
&lt;p&gt;앗! VGG16이 너무 커서 메모리에 맞지 않네요. ResNet-152를 사용해 봅시다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; resnet152,categories = init("resnet-152")
 Loaded in 11056.10 milliseconds

&amp;gt;&amp;gt; print predict("kreator.jpg",resnet152,categories,5)
 Predicted in 7.98 milliseconds
 [(0.87835813, 'n04296562 stage'), (0.045634001, 'n03759954 microphone, mike'), (0.035906471, 'n03272010 electric guitar'), (0.021166906, 'n04286575 spotlight, spot'), (0.0054096784, 'n02676566 acoustic guitar')]&lt;/pre&gt;
&lt;p&gt;ResNet-152는 약 10 초 만에 성공적으로 로딩되고, 10 밀리초 이내에 예측이 가능합니다. 이제 Inception v3으로 넘어 가 보죠.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; inceptionv3,categories = init("Inception-BN")
 Loaded in 2137.62 milliseconds

&amp;gt;&amp;gt; print predict("kreator.jpg",resnet152,categories,5)
 Predicted in 2.35 milliseconds
 [(0.4685601, 'n04296562 stage'), (0.40474886, 'n03272010 electric guitar'), (0.073685646, 'n04456115 torch'), (0.011639798, 'n03250847 drumstick'), (0.011014056, 'n02676566 acoustic guitar')]&lt;/pre&gt;
&lt;p&gt;Pi와 같은 제한된 장치에서는 모델 차이가 훨씬 더 분명하게 보입니다! Inception v3은 훨씬 빠른 속도로 로딩하고, 몇 밀리 초 안에 예측이 가능합니다. 모델이 로딩 될 경우에도 실제 응용 프로그램을 실행하는 충분한 RAM이 PI에 남아있어야 한다는 점은 확인하기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pi 카메라를 사용하여 이미지 캡처하기&lt;/strong&gt;&lt;br /&gt;
Raspberry Pi에 추가 할 수 있는 제일 좋은 디바이스는 카메라 모듈입니다. 진짜 간단합니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; inceptionv3,categories = init("Inception-BN")

&amp;gt;&amp;gt;&amp;gt; import picamera
&amp;gt;&amp;gt;&amp;gt; camera = picamera.PiCamera()
&amp;gt;&amp;gt;&amp;gt; filename = '/home/pi/cap.jpg'

&amp;gt;&amp;gt;&amp;gt; print predict(filename, inceptionv3, categories, 5)&lt;/pre&gt;
&lt;p&gt;아래는 간단한 예입니다.&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*ZRHWR2Bzb-S0mccRVTQcwQ.jpeg" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;Predicted in 12.90 milliseconds
 [(0.95071173, 'n04074963 remote control, remote'), (0.013508897, 'n04372370 switch, electric switch, electrical switch'), (0.013224524, 'n03602883 joystick'), (0.00399205, 'n04009552 projector'), (0.0036674738, 'n03777754 modem')]&lt;/pre&gt;
&lt;p&gt;잘 동작하죠?  이제 Amazon AI 서비스를 추가 해보면 어떨까요.  얼마 전에 만든 Python 스크립트 (&lt;a href="https://medium.com/@julsimon/a-hands-on-look-at-the-amazon-rekognition-api-e30e19e7d88b"&gt;문서&lt;/a&gt;, &lt;a href="https://github.com/juliensimon/aws/tree/master/rekognition"&gt;코드&lt;/a&gt;)를 사용하여 &lt;a href="https://aws.amazon.com/rekognition/"&gt;Amazon Rekognition&lt;/a&gt;을 통해 같은 사진을 실행해 볼 수 있습니다.&lt;/p&gt;
&lt;pre&gt;$ ./rekognitionDetect.py jsimon-public cap.jpg copy
 Label Remote Control, confidence: 94.7508468628&lt;/pre&gt;
&lt;p&gt;인식된 레이블을  음성으로 전달해 본다면 어떨까요? &lt;a href="https://medium.com/@julsimon/amazon-polly-hello-world-literally-812de2c620f4"&gt;Amazon Polly를 추가&lt;/a&gt;하는 것도 쉽습니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Amazon Rekognition 및 Amazon Polly는 AWS가 제공하는 딥러닝(Deep Learning) 기술을 기반으로 만들어진 완전 관리(Fully-managed) 서비스입니다.  개발자라면 누구나 쉽게 사용 가능하며 학습 모델이나 인프라에 대해 걱정할 필요가 없고, 단지 API를 호출만 하면 됩니다. 다양한 이미지 인식 기능과 많은 언어의 음성 합성 기능을 제공합니다.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;MXnet 기반의 Inception v3 모델을 사용하여 실시간 개체 감지를 수행하고, Amazon Polly에서 본 내용을 설명하는 Raspberry Pi의 비디오를 한번 살펴 보세요!&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;지금까지 MXNet에 대한 API와 이를 사용한 이미지 인식을 위해 많은 것을 살펴 보았습니다.  이미지내 객체 감지를 위해 합성곱 신경망(Convolutional Neural Networks)에 초점을 맞추었지만, MXNet에는 훨씬 더 많은 학습 모델이 있고, 추가로 새로운 시리즈를 통해 배울 수 있을 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;이 시리즈는 AWS 테크 에반젤리스트인 Julien Simon이 연재한 &lt;a href="https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab?source=user_profile---------17-----------"&gt;MXNet 관련 글 모음&lt;/a&gt;의 번역 편집본으로 최근 각광 받고 있는 Deep Learning 라이브러리인 &lt;a href="http://mxnet.io/"&gt;Apache MXnet&lt;/a&gt;  을 개괄적으로 설명하려고 합니다.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=X2Ug4qdnPKI:uYWVGLOR4Q4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sat, 29 Jul 2017 18:43:29 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기</title>
	<link>http://blog.creation.net/mxnet-part-5-vgc16-resnet152</link>
	<description>&lt;p&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;4편&lt;/a&gt;에서는 사전 학습된 Inception v3 모델을 사용하여, 이미지 내 분류를 검색하는 것이 얼마나 쉬운지 확인했습니다. 이 글에서는 두 개의 유명한 &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;합성곱 신경망&lt;/a&gt;(Convolutional neural network, CNN)인 VGG19와 ResNet-152을 사용해 보고, Inception v3 모델과 비교해 보겠습니다.&lt;/p&gt;
&lt;p&gt;CNN이라는 어려운 단어가 나왔지만, 과정은 크게 다르지 않습니다. 레이어를 더 많이 늘린 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://cdn-images-1.medium.com/max/1455/1*V_lnH58ZtxQyxXCqHYt6DQ.png" alt="" /&gt;&lt;br /&gt;
&lt;small&gt;Architecture of a CNN (Source: Nvidia)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VGG16&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2014 년에 발표된 VGG16은 &lt;strong&gt;16 개의 레이어&lt;/strong&gt;로 구성된 모델입니다 (&lt;a href="https://arxiv.org/abs/1409.1556"&gt;연구 논문&lt;/a&gt;). 객체 분류에서 7.4 %의 오류율을 달성하여 &lt;a href="http://image-net.org/challenges/LSVRC/2014/"&gt;2014 년 ImageNet Challenge&lt;/a&gt;에서 우승했습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ResNet-152&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2015 년에 발표된 ResNet-152는 &lt;strong&gt;152 개의 레이어&lt;/strong&gt; (&lt;a href="https://arxiv.org/abs/1512.03385"&gt;연구 논문&lt;/a&gt;)로 구성된 모델입니다. 이미지 내 객체 탐지에 대한 오류율 3.57 %를 달성함으로써 &lt;a href="http://image-net.org/challenges/LSVRC/2015/"&gt;2015 ImageNet Challenge&lt;/a&gt;에서 우승했습니다.  이건 진짜 놀라운 결과인데, 인간이 대체적으로 5 %의 오류율을 가지는데 (100개를 보면 5개를 긴가민가하게 생각하는) 사람 보다 더 똑똑하다고 볼 수 있겠죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 다운로드 하기&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;MXNet zoo를 다시 방문 할 시간입니다! Inception v3와 마찬가지로 모델 정의와 매개 변수를 다운로드해야합니다. 세 모델 모두 동일한 카테고리에 대해 교육을 받았으므로 synset.txt 파일을 다시 사용할 수 있습니다.&lt;/p&gt;
&lt;pre&gt;$ wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json
$ wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params
$ wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json
$ wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;모델 로딩하기&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;세 가지 모델은 224 X 224의 전형적인 이미지 크기의 ImageNet 데이터 셋을 기반으로 학습하기 때문에 우리가 이전에 사용한 코드를 재사용 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;우리가 바꾸어야하는 것은 모델 이름입니다 : loadModel()과 init() 함수에 매개 변수를 추가합니다.&lt;/p&gt;
&lt;pre&gt;def loadModel(modelname):
        sym, arg_params, aux_params = mx.model.load_checkpoint(modelname, 0)
        mod = mx.mod.Module(symbol=sym)
        mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])
        mod.set_params(arg_params, aux_params)
        return mod

def init(modelname):
        model = loadModel(modelname)
        cats = loadCategories()
        return model, cats&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;모델별 예측 비교하기&lt;/strong&gt;&lt;br /&gt;
이전에 사용한 샘플 이미지를 기반으로 이번 두 가지 모델을 비교해 보겠습니다.&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*KcUFMWb7KKTvOd_fgrXydA.jpeg" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;*** VGG16
[(0.58786136, 'n03272010 electric guitar'), (0.29260877, 'n04296562 stage'), (0.013744719, 'n04487394 trombone'), (0.013494448, 'n04141076 sax, saxophone'), (0.00988709, 'n02231487 walking stick, walkingstick, stick insect')]&lt;/pre&gt;
&lt;p&gt;상위 2개 카테고리는 잘 맞는데, 나머지 3개는 잘못되었습니다. 마이크 받침대의 수직 모양에서 혼란스러워하네요.&lt;/p&gt;
&lt;pre&gt;*** ResNet-152
[(0.91063803, 'n04296562 stage'), (0.039011702, 'n03272010 electric guitar'), (0.031426914, 'n03759954 microphone, mike'), (0.011822623, 'n04286575 spotlight, spot'), (0.0020199812, 'n02676566 acoustic guitar')]&lt;/pre&gt;
&lt;p&gt;상위 카테고리에서 매우 높습니다. 나머지 4 개 모두 의미가 있습니다.&lt;/p&gt;
&lt;pre&gt;*** Inception v3
[(0.58039135, 'n03272010 electric guitar'), (0.27168664, 'n04296562 stage'), (0.090769522, 'n04456115 torch'), (0.023762707, 'n04286575 spotlight, spot'), (0.0081428187, 'n03250847 drumstick')]&lt;/pre&gt;
&lt;p&gt;상위 2 개 카테고리의 VGG16과 매우 유사한 결과. 나머지 세 개는 역시 잘 모르겠네요.&lt;/p&gt;
&lt;p&gt;다른 사진으로 해볼까요?&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*03jvGuldTuyRbYCFwwRP7A.jpeg" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;*** VGG16
[(0.96909302, 'n04536866 violin, fiddle'), (0.026661994, 'n02992211 cello, violoncello'), (0.0017284016, 'n02879718 bow'), (0.00056815811, 'n04517823 vacuum, vacuum cleaner'), (0.00024804732, 'n04090263 rifle')]

*** ResNet-152
[(0.96826887, 'n04536866 violin, fiddle'), (0.028052919, 'n02992211 cello, violoncello'), (0.0008367821, 'n02676566 acoustic guitar'), (0.00070532493, 'n02787622 banjo'), (0.00039021231, 'n02879718 bow')]

*** Inception v3
[(0.82023674, 'n04536866 violin, fiddle'), (0.15483995, 'n02992211 cello, violoncello'), (0.0044540241, 'n02676566 acoustic guitar'), (0.0020963412, 'n02879718 bow'), (0.0015099624, 'n03447721 gong, tam-tam')]&lt;/pre&gt;
&lt;p&gt;세 가지 모델 모두 상위 카테고리에서 매우 높은 점수를 받았습니다. 바이올린의 모양이 신경망에 대해 매우 모호한 패턴이라고 가정 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;몇 가지 샘플에서 사용 여부를 결정해서는 안됩니다. 사전 학습된 모델을 찾고 있다면 반드시 학습 데이터 셋을 잘 살펴보고, 자신의 데이터에 대한 테스트를 실행하고 사용할지 여부를 정해야합니다!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;기술적 성능 비교&lt;/strong&gt;&lt;br /&gt;
위의 연구 논문에서 많은 모델의 성능에 대한 벤치 마크 결과를 보실 수 있습니다. 개발자의 경우, 가장 중요한 두 가지 요인은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;모델에 필요한 메모리 양은 얼마인가?&lt;/li&gt;
&lt;li&gt;얼마나 빨리 예측 가능한가?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;첫 번째 질문에 답하기 위해 우리는 매개 변수 파일의 크기를 보고 추측을 할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VGG16: 528MB (about 140 million parameters)&lt;/li&gt;
&lt;li&gt;ResNet-152: 230MB (about 60 million parameters)&lt;/li&gt;
&lt;li&gt;Inception v3: 43MB (about 25 million parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;보시다시피 더 적은 매개 변수로 더 많은 레이어로 심층 네트워크를 사용하는 것입니다. 이것은 교육 시간 단축(네트워크가 매개 변수를 덜 학습하므로)과 메모리 사용량 감소라는 두 가지 이점이 있습니다.&lt;/p&gt;
&lt;p&gt;두 번째 질문은 조금 복잡해서 배치 크기와 같은 다양한 매개 변수에 따라 달라집니다. 예측 호출 및 실행 시간에 중점을 두고 예제를 다시 실행 해 봅시다.&lt;/p&gt;
&lt;pre&gt;t1 = time.time()
model.forward(Batch([array]))
t2 = time.time()
t = 1000*(t2-t1)
print("Predicted in %2.2f millisecond" % t)&lt;/pre&gt;
&lt;p&gt;결과는 다음과 같습니다. (몇 번 호출한 뒤 평균값입니다.)&lt;/p&gt;
&lt;pre&gt;*** VGG16
Predicted in 0.30 millisecond
*** ResNet-152
Predicted in 0.90 millisecond
*** Inception v3
Predicted in 0.40 millisecond&lt;/pre&gt;
&lt;p&gt;자! 이제 요약을 해보죠. ResNet-152는 세 가지 네트워크 중에서 가장 우수한 정확도를 가졌지만, 2-3 배 더 느립니다.&lt;br /&gt;
VGG16은 레이어 수가 적기 때문에 가장 빠릅니다. 그러나, 높은 메모리 사용과 최악의 정확도를 가지고 있습니다.&lt;/p&gt;
&lt;p&gt;Inception v3은 더 빠른 정확성과 가장 보수적인 메모리 사용을 제공하는 동시에 거의 빠릅니다. 이 마지막 점은 실시간 분석과 같은 제한된 환경에서는 좋은 후보가 됩니다. 이에 대한 자세한 부분은 마지막 6편에서 다뤄 보겠습니다.&lt;/p&gt;
&lt;p&gt;다음 글:  &lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;코드 전체 보기&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=5nzXCYgNenI:evKyMVihp-A:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Fri, 28 Jul 2017 15:21:35 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)</title>
	<link>http://blog.creation.net/mxnet-part-4-inception-v3</link>
	<description>&lt;p&gt;이전 글에서는 처음으로 신경망을 구축하고 학습하는 방법을 배웠습니다. 이제 좀 더 실질적인 문제 해결을 할 수 있는 사례를 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;우선 최근에 사용되는 딥러닝 학습 모델은 매우 복잡하다는 사실을 알고 계셔야 합니다. 수 백개의 레이어가 있으며 막대한 양의 데이터를 학습하는 데 며칠이 걸릴수 있으며, 이러한 모델을 만들고 조정하는 데는 많은 전문 지식이 필요합니다.&lt;/p&gt;
&lt;p&gt;매우 다행인 것인 이러한 모델을 사용하는 것은 생각 보다 간단하며 몇 줄의 소스 코드만 가지고 할 수 있습니다. 이 글에서는 &lt;a class="markup--anchor markup--p-anchor" href="https://arxiv.org/abs/1512.00567" target="_blank" rel="nofollow noopener"&gt;Inception v3&lt;/a&gt;이라는 이미지 분류를 위해 미리 학습된 모델을 살펴 볼 것입니다.&lt;/p&gt;
&lt;p&gt;2015년 12월에 나온 Inception v3은 &lt;a class="markup--anchor markup--p-anchor" href="https://arxiv.org/abs/1409.4842" target="_blank" rel="nofollow noopener"&gt;GoogleNet&lt;/a&gt; 모델(&lt;a class="markup--anchor markup--p-anchor" href="http://image-net.org/challenges/LSVRC/2014/" target="_blank" rel="nofollow noopener"&gt;2014 ImageNet Challenge&lt;/a&gt;에서 우승 한 모델)을 발전 시킨 것입니다. 연구 논문의 세부 사항에 대해서는 언급하지 않겠지만, 결론적으로 Inception v3는 당시에 사용 가능한 최고의 학습 모델보다 15-25% 정확도가 높으며, 연산에 있어 6배 저렴하고 최소 20% 미만의 매개 변수를 사용합니다 (즉, 모델 사용에 필요한 RAM 사용량이 적습니다.)&lt;/p&gt;
&lt;p&gt;딥러닝을 인기있는 모델로 끌어 올린 대단한 물건인데, 이를 MXNet으로 한번 작동시켜 봅시다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MXNet Zoo 학습 모델&lt;/strong&gt;&lt;br /&gt;
&lt;a class="markup--anchor markup--p-anchor" href="http://mxnet.io/model_zoo/" target="_blank" rel="nofollow noopener"&gt;Model Zoo&lt;/a&gt;는 MXNet에서 손쉽게 사용할 수 있도록 미리 학습된 모델 모음입니다. 모델 정의, 모델 매개 변수 (즉, 뉴런 가중치) 및 사용 가이드 등으로 구성되어 있습니다.&lt;/p&gt;
&lt;p&gt;여기서 ImageNet에 해당 하는 모델 정의와 매개 변수 파일을 다운로드하세요. (파일 이름을 변경해야 할 수도 있음). 첫 번째 파일을 열면 모든 레이어의 정의가 표시되어 있습니다. 두 번째 파일은 바이너리 파일입니다.&lt;/p&gt;
&lt;pre&gt;$ wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json
$ wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params
$ mv Inception-BN-0126.params Inception-BN-0000.params&lt;/pre&gt;
&lt;p&gt;이 모델은 ImageNet 데이터 세트에서 학습되었으므로 해당 이미지 카테고리 (1000개)도 다운로드해야 합니다.&lt;/p&gt;
&lt;pre&gt;$ wget http://data.dmlc.ml/models/imagenet/synset.txt

$ wc -l synset.txt
 1000 synset.txt

$ head -5 synset.txt
 n01440764 tench, Tinca tinca
 n01443537 goldfish, Carassius auratus
 n01484850 great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
 n01491361 tiger shark, Galeocerdo cuvieri
 n01494475 hammerhead, hammerhead shark&lt;/pre&gt;
&lt;p&gt;다운로드 다 받으셨나요? 그러면 이제 모델을 가져와서 작업을 시작해 봅시다!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 로딩하기&lt;br /&gt;
&lt;/strong&gt;우리가 해야 할 일을 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;1. 저장 상태에서 모델 로딩하기 : MXNet에서는 이것을 체크 포인트라고 부릅니다.  우리는 입력 Symbol과 모델 매개 변수를 반환 받습니다.&lt;/p&gt;
&lt;pre&gt;import mxnet as mx
sym, arg_params, aux_params = mx.model.load_checkpoint('Inception-BN', 0)&lt;/pre&gt;
&lt;p&gt;2. 새로운 모듈을 생성하고 그것을 입력 심볼로 할당합니다. 모델을 어디에서 실행할지를 나타내는 컨텍스트 매개 변수를 지정할 수도 있습니다. 기본값은 cpu (0)이지만 gpu (0)를 사용하여 GPU에서 실행할 수도 있습니다.&lt;/p&gt;
&lt;pre&gt;mod = mx.mod.Module(symbol=sym)&lt;/pre&gt;
&lt;p&gt;3. 입력 심볼에 입력 데이터를 바인딩합니다. 이름은 네트워크의 입력 레이어에 있는 이름이기 때문에 &amp;#8216;data&amp;#8217;라고 합시다. (JSON 파일의 처음 몇 줄을보십시오). &amp;#8216;data&amp;#8217;의 크기를 1 x 3 x 224 x 224로 정의합니다. 당황하지 마세요 &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f609.png" alt="&#x1f609;" class="wp-smiley" /&gt; &amp;#8216;224 x 224&amp;#8217;는 이미지 해상도로 모델 학습 방법입니다. &amp;#8216;3&amp;#8217;은 채널 수입니다: 빨강, 초록, 파랑​​(순서대로). &amp;#8216;1&amp;#8217;은 배치 크기입니다. 한 번에 하나의 이미지를 예측합니다.&lt;/p&gt;
&lt;pre&gt;mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])&lt;/pre&gt;
&lt;p&gt;4. 모델 매개 변수를 설정합니다.&lt;/p&gt;
&lt;pre&gt;mod.set_params(arg_params, aux_params)&lt;/pre&gt;
&lt;p&gt;이게 전부입니다. 코드 네 줄! ㅋㅋ 이제 몇 가지 데이터를 밀어 넣고 무슨 일이 일어나는지 살펴 보겠습니다. (데이터 먼저 준비하구요.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;데이터 준비하기&lt;br /&gt;
&lt;/strong&gt;사실 개발자라면 1970년대 부터 하던 일인데, 데이터베이스 부터 기계 학습, 딥러닝에 이르기까지 빠지지 않는 게 데이터 준비네요. 좀 귀찮은 일이긴 하지만 꼭 필요하니 시작해 봅시다.&lt;/p&gt;
&lt;p&gt;이 모델은 하나의 224 x 224 이미지의 빨강, 녹색 및 파란 채널을 유지하는 4차원 NDArray를 사용합니다. 우리는 인기있는 OpenCV 라이브러리를 사용하여 입력 이미지로 부터 데이터를 추출해서 NDArray에 넣을 것입니다. OpenCV를 설치하지 않은 경우 &amp;#8220;pip install opencv-python&amp;#8221;을 실행하면 대부분의 경우 충분합니다. &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f642.png" alt="&#x1f642;" class="wp-smiley" /&gt;&lt;/p&gt;
&lt;p&gt;단계는 다음과 같습니다.&lt;/p&gt;
&lt;p&gt;1. 이미지 읽기 : BGR 순서 (파란색, 녹색 및 빨간색)의 세 채널을 사용하여 (이미지 높이, 이미지 너비, 3) 모양의 수적으로 배열을 반환합니다.&lt;/p&gt;
&lt;pre&gt;img = cv2.imread(filename)&lt;/pre&gt;
&lt;p&gt;2. 이미지를 RGB로 변환합니다.&lt;/p&gt;
&lt;pre&gt;img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)&lt;/pre&gt;
&lt;p&gt;3. 이미지를 224 x 224 크기로 조정합니다.&lt;/p&gt;
&lt;pre&gt;img = cv2.resize(img, (224, 224,))&lt;/pre&gt;
&lt;p&gt;4. 배열을 (이미지 높이, 이미지 너비, 3)에서 (3, 이미지 높이, 이미지 너비)로 바꿉니다.&lt;/p&gt;
&lt;pre&gt;img = np.swapaxes(img, 0, 2)
img = np.swapaxes(img, 1, 2)&lt;/pre&gt;
&lt;p&gt;5. 네 번째 차원을 추가하고 NDArray를 정의합니다.&lt;/p&gt;
&lt;pre&gt;img = img[np.newaxis, :]
array = mx.nd.array(img)
&amp;gt;&amp;gt;&amp;gt; print array.shape
 (1L, 3L, 224L, 224L)&lt;/pre&gt;
&lt;p&gt;자! 이제 실제 해볼까요? 아래에 이미지 파일이 하나 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*sPdrfGtDd_6RQfYvD5qcyg.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;Input picture 448&amp;#215;336 (Source: metaltraveller.com)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;이미지 처리가 끝나면 그림 크기가 조정되어 array[0]에 저장된 RGB 채널로 분할됩니다 (아래 그림을 생성하는 데 사용 된 코드가&lt;br /&gt;
&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*yqdl78KIugYepzJ4-lMY8g.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;array[0][0] : 224&amp;#215;224 red channel&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*sitDwoAzPDLrav0dXQrcbA.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;array[0][1] : 224&amp;#215;224 green channel&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*h1RyEPvd2fqIgd2jkfES-w.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;array[0][2] : 224&amp;#215;224 blue channel&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;일괄 처리 크기가 1보다 크면, array[1]에 두 번째 이미지, array[2]에 세 번째 이미지 등이 나눠집니다. 이제 예측을 해볼까요?&lt;br /&gt;
&lt;strong&gt;예측하기&lt;/strong&gt;&lt;br /&gt;
파트 3에서 Module 객체에서 데이터를 일괄적으로 모델에 공급해야 한다는 것을 기억하실 거에요. 일반적으로 데이터 반복기(Iterator)를 사용하는 것이 일반적인 방법입니다. (특히, NDArrayIter 객체 사용).&lt;/p&gt;
&lt;p&gt;여기서 이미지 &lt;strong&gt;하나만&lt;/strong&gt; 예측하고 싶습니다. 데이터 반복기를 사용할 수는 있겠지만 너무 오버하는 것이 될 수 있습니다. 대신 데이터 속성이 참조될 때 입력 NDArray를 반환하여, 가짜 반복기 역할을 할 Batch라는 이름의 튜플(tuple)을 만들 것입니다.&lt;/p&gt;
&lt;pre&gt;from collections import namedtuple
Batch = namedtuple('Batch', ['data'])&lt;/pre&gt;
&lt;p&gt;이제 &amp;#8220;Batch&amp;#8221;를 모델에 전달하고 예측할 수 있습니다.&lt;/p&gt;
&lt;pre&gt;mod.forward(Batch([array]))&lt;/pre&gt;
&lt;p&gt;모델은 1000 개 카테고리에 해당하는 &lt;strong&gt;1000개의 확률&lt;/strong&gt;을 갖는 NDArray를 출력합니다. 일괄 처리 크기가 1이므로 한 줄 밖에 없습니다.&lt;/p&gt;
&lt;pre&gt;prob = mod.get_outputs()[0].asnumpy()

&amp;gt;&amp;gt;&amp;gt; prob.shape
 (1, 1000)&lt;/pre&gt;
&lt;p&gt;이것을 squeeze()를 사용하여 배열로 바꾼 후, argsort()를 사용하여 &lt;strong&gt;내림차순&lt;/strong&gt;으로 정렬된 두 번째 배열 &lt;strong&gt;인덱스&lt;/strong&gt;를 만듭니다.&lt;/p&gt;
&lt;pre&gt;prob = np.squeeze(prob)

&amp;gt;&amp;gt;&amp;gt; prob.shape
(1000,)
&amp;gt;&amp;gt; prob
[ 4.14978594e-08 1.31608676e-05 2.51907986e-05 2.24045834e-05
2.30327873e-06 3.40798979e-05 7.41563645e-06 3.04062659e-08 etc.

sortedprob = np.argsort(prob)[::-1]

&amp;gt;&amp;gt; sortedprob.shape
(1000,)&lt;/pre&gt;
&lt;p&gt;현재 모델에 따르면, 사진의 가장 가능성이 큰 카테고리는 #546이며 확률은 58%입니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt; sortedprob
[546 819 862 818 542 402 650 420 983 632 733 644 513 875 776 917 795
etc.
&amp;gt;&amp;gt; prob[546]
0.58039135&lt;/pre&gt;
&lt;p&gt;이 카테고리의 이름을 찾아 보겠습니다. synset.txt 파일을 사용하여 카테고리 목록을 작성하고, #546를 목록에서 찾을 수 있습니다.&lt;/p&gt;
&lt;pre&gt;synsetfile = open('synset.txt', 'r')
categorylist = []
for line in synsetfile:
categorylist.append(line.rstrip())

&amp;gt;&amp;gt;&amp;gt; categorylist[546]
'n03272010 electric guitar'&lt;/pre&gt;
&lt;p&gt;어떠세요? 전기 기타(electric quitar)라고 나오죠? 두번째를 볼까요?&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; prob[819]
0.27168664
&amp;gt;&amp;gt;&amp;gt; categorylist[819]
'n04296562 stage&lt;/pre&gt;
&lt;p&gt;무대(stage)라고 나옵니다. 어떠세요? 그냥 됩니다. 우리는 이제 이미지 내의 분류를 위해 사전 학습된 모델을 사용하는 방법을 사용해 보았습니다. 코드는 고작 4줄이었고 주로 했던 일은 데이터 준비였습니다.&lt;/p&gt;
&lt;p&gt;맨 아래 전체 코드가 있습니다. 다음에는 이미지 분석을 위해 좀 더 잘 학습된 모델인 VGG16과 ResNet-152를 살펴 보도록 하겠습습니다. 잘 따라 와 주세요~&lt;/p&gt;
&lt;p&gt;다음 글: &lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;코드 전체 보기&lt;/strong&gt;&lt;br /&gt;
&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=eKx26a-yUeI:ZpdSVZJGlT4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Fri, 28 Jul 2017 14:28:58 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>친정인 ‘카카오뱅크’ 오픈 소식에 계좌 개설!</title>
	<link>http://channy.creation.net/blog/1152</link>
	<description>&lt;p&gt;&lt;a href="http://www.kakaobank.com/"&gt;카카오 뱅크&lt;/a&gt; 오픈 소식에 오전에 사람들이 좀 몰려서 일시 장애가 있었던 듯 싶으나 은행계 시스템이란게 초기 계좌 개설 같은 DB 쓰기 용량이 한계가 있으니, 그건 어느 정도 이해하고… 일반인들의 경우도 약간의 노이즈 마케팅이 될 수 있으니 오케이! (기사에 따르면, &lt;a href="http://naver.me/xQ3llS2W"&gt;6시간만에 6만 계좌 개설&lt;/a&gt;됐음. 케이뱅크의 경우, 15시간 동안 1만 5천계좌에 비하면 엄청난 관심이네요.)&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channy.creation.net/data/channy/2017/07/27155709/kakao-bank-account.jpg" alt="" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;1. 계좌 개설 방법은 비교적 간단(?)하지만, 중간에 타행 계좌에 1원 입금해 주고, 계좌 이체한 사람 이름 확인하는 과정에서 멘붕. 갑자기 타행 계좌 조회해야 하는 불상사가 생겨서 당황했는데, 필수 조건이기 때문에 타행 조회 꼭 준비하셔야 합니다.&lt;/p&gt;
&lt;p&gt;2. Bad gateway 오류가 계속 나는데도 불구하고 무시 전략으로 롤백을 믿고 강행한 덕에 계좌 개설, 체크 카드 신청 및 카톡 무료 이모티콘 다운로드까지 일사 천리 10분안에 진행 완료. 계좌번호 처음이 삼삼하게 3333으로 시작하는데, 계좌 번호도 자기가 선택할 수 있었으면 좋겠네요.&lt;/p&gt;
&lt;p&gt;3. 중간에 오류가 나면 전화나 카톡 고객 센터를 연결해 주는 친절한 메시지는 나쁘지 않은 듯. 웹 사이트에는 오로지 고객 센터 도움말과 상담 기능만 있으며, 모든 업무 처리는 앱으로만 가능해서 PC를 버리는 선택과 집중은 개발 리소스가 적을텐데 좋은 전략입니다.&lt;/p&gt;
&lt;p&gt;구, 다음 출신들이 카카오뱅크에 가서 고생을 많이 했는데 서비스 오픈 하느라 고생많으셨을듯. 이게 일반 웹 서비스가 아니라, 레거시 은행 기간망이랑 연동하면서 만들어야 하는 거라 어려움이 있었을 텐데요. 유닉스가 아니라 리눅스 계열 X86으로 바꾸고, 오픈 소스를 쓰면서 만드는 첫번째 실험이라 앞으로 잘되길 바랍니다.&lt;/p&gt;
&lt;p&gt;개인적으로 공인 인증서 (발급/인증) 및 OTP  같은 복잡한 절차 없이 은행 업무를 볼 수 있다는 것만으로도 충분히 도움이 될 것 같네요. 앞으로 꼭 성공하길!~&lt;/p&gt;
&lt;div class="intro_main"&gt;
&lt;h3 class="tit_main"&gt;같지만 다른 은행 카카오뱅크&lt;/h3&gt;
&lt;ul class="list_store"&gt;
&lt;li&gt;&lt;a class="link_store" href="https://www.kakaobank.com/download/android?utm_source=channyblog&amp;utm_medium=channyblog&amp;utm_campaign=channyblog" target="_blank" rel="noopener"&gt;Google Play 다운로드&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link_store" href="https://www.kakaobank.com/download/ios?utm_source=channyblog&amp;utm_medium=channyblog&amp;utm_campaign=channyblog" target="_blank" rel="noopener"&gt;App Store 다운로드&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WQZ7pbbxJr8:9wt_7j-_cPs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 27 Jul 2017 07:05:15 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1152#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (3) – Module API</title>
	<link>http://blog.creation.net/mxnet-part-3-module-api</link>
	<description>&lt;p&gt;2편에서는 Symbols를 사용하여 NDArrays에 저장된 데이터를 처리하는 연산 그래프를 정의하는 방법을 설명했습니다. 이 글에서는 Symbol 및 NDArrays에서 배운 것을 사용하여 일부 데이터를 준비하고 신경망을 구성합니다. 그런 다음 &lt;a href="http://mxnet.io/api/python/module.html"&gt;Module API&lt;/a&gt;를 사용하여 신경망 기반 데이터를 학습하고 결과를 예측해보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;데이터셋 정의&lt;/strong&gt;&lt;br /&gt;
(가상) 데이터 세트는 1000개의 데이터 샘플로 구성합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;각 샘플에는 100개의 &lt;strong&gt;기능(Feature)&lt;/strong&gt;이 있습니다.&lt;/li&gt;
&lt;li&gt;각 기능은 &lt;strong&gt;0에서 1 사이의 float 값&lt;/strong&gt;으로 표현됩니다.&lt;/li&gt;
&lt;li&gt;샘플은 &lt;strong&gt;10개의 카테고리&lt;/strong&gt;로 나뉩니다. 네트워크의 목적은 주어진 샘플에 대한 올바른 카테고리를 예측하는 것입니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;학습(Training)&lt;/strong&gt;을 위해 800개의 샘플을 사용하고 &lt;strong&gt;검증(Validation)&lt;/strong&gt;을 위해 200개의 샘플을 사용할 것입니다.&lt;/li&gt;
&lt;li&gt;학습 및 검증을 위해 &lt;strong&gt;배치(Batch) 크기 10&lt;/strong&gt;을 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;아래는 데이터 정의를 위한 간단한 MXNet 코드입니다.&lt;/p&gt;
&lt;pre id="df70" name="df70" class="graf graf--pre graf-after--li"&gt;import mxnet as mx
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

sample_count = 1000
train_count = 800
valid_count = sample_count - train_count

feature_count = 100
category_count = 10
batch=10&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;데이터셋 생성&lt;/strong&gt;&lt;br /&gt;
균등 분포를 사용하여 1000 개의 샘플을 생성합시다. 이를 &amp;#8216;X&amp;#8217;라는 이름의 NDArray에 저장합니다. (즉, &lt;strong&gt;1000라인, 100 컬럼&lt;/strong&gt;)&lt;/p&gt;
&lt;pre id="ece8" name="ece8" class="graf graf--pre graf-after--p"&gt;X = mx.nd.uniform(low=0, high=1, shape=(sample_count,feature_count))

&amp;gt;&amp;gt;&amp;gt; X.shape
(1000L, 100L)
&amp;gt;&amp;gt;&amp;gt; X.asnumpy()
array([[ 0.70029777,  0.28444085,  0.46263582, ...,  0.73365158,
         0.99670047,  0.5961988 ],
       [ 0.34659418,  0.82824177,  0.72929877, ...,  0.56012964,
         0.32261589,  0.35627609],
       [ 0.10939316,  0.02995235,  0.97597599, ...,  0.20194994,
         0.9266268 ,  0.25102937],
       ...,
       [ 0.69691515,  0.52568913,  0.21130568, ...,  0.42498392,
         0.80869114,  0.23635457],
       [ 0.3562004 ,  0.5794751 ,  0.38135922, ...,  0.6336484 ,
         0.26392782,  0.30010447],
       [ 0.40369365,  0.89351988,  0.88817406, ...,  0.13799617,
         0.40905532,  0.05180593]], dtype=float32)&lt;/pre&gt;
&lt;p&gt;각 1000개의 샘플에 대한 카테고리는 0-9 범위의 정수로 표시됩니다. 랜덤하게 생성되어 &amp;#8216;Y&amp;#8217;라는 NDArray에 저장됩니다.&lt;/p&gt;
&lt;pre id="60cf" name="60cf" class="graf graf--pre graf-after--p"&gt;Y = mx.nd.empty((sample_count,))
for i in range(0,sample_count-1):
  Y[i] = np.random.randint(0,category_count)

&amp;gt;&amp;gt;&amp;gt; Y.shape
(1000L,)
&amp;gt;&amp;gt;&amp;gt; Y[0:10].asnumpy()
array([ 3.,  3.,  1.,  9.,  4.,  7.,  3.,  5.,  2.,  2.], dtype=float32)&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;데이터셋 나누기&lt;/strong&gt;&lt;br /&gt;
다음 단계로 학습과 검증을 위해 데이터셋을 80/20으로 나눠야합니다. 이를 위해 NDArray.crop 함수를 사용합니다. 여기서 데이터 세트는 완전 무작위이므로 학습을 위해 상위 80%를 사용하고, 유효성을 검사하기 위해 하위 20%를 사용합니다. 실제 정식으로 할 때는 순차적으로 생성 된 데이터에 잠재적인 편향을 피하기 위해 데이터셋을 먼저 뒤섞어 놓아야 할 것입니다.&lt;/p&gt;
&lt;pre id="6487" name="6487" class="graf graf--pre graf-after--p"&gt;X_train = mx.nd.crop(X, begin=(0,0), end=(train_count,feature_count-1))

X_valid = mx.nd.crop(X, begin=(train_count,0), end=(sample_count,feature_count-1))

Y_train = Y[0:train_count]

Y_valid = Y[train_count:sample_count]&lt;/pre&gt;
&lt;p&gt;자, 이제 데이터 준비가 끝났습니다. 다음 단계로 넘어가 볼까요?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;신경망 바인딩&lt;/strong&gt;&lt;br /&gt;
우리가 만든 네트워크는 매우 간단합니다. 각 레이어를 살펴 보겠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;입력 레이어는 &amp;#8216;data&amp;#8217;라는 심볼로 표현됩니다. 나중에 실제 입력 데이터에 바인딩 할 것입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="70f8" name="70f8" class="graf graf--pre graf-after--li"&gt;data = mx.sym.Variable('data')&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fc1에서 첫 번째 숨겨진 레이어는 &lt;strong&gt;64개 연결된 신경(Neurons)&lt;/strong&gt;으로 구성됩니다. 즉, 입력 레이어의 각 기능은 64개의 모든 신경에 연결됩니다. 이를 위해 Symbol.FullyConnected 함수를 사용합니다.이 함수는 수동으로 각 연결을 만드는 것보다 훨씬 편리합니다!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="5582" name="5582" class="graf graf--pre graf-after--li"&gt;fc1 = mx.sym.FullyConnected(data, name='fc1', num_hidden=64)&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fc1의 각 결과 출력은 &lt;a href="https://en.wikipedia.org/wiki/Activation_function"&gt;활성화 함수&lt;/a&gt;를 통해 수행합니다. 활성화 함수란 Sigmod처럼 어디에 속하는지 분류하기 위해 일정 값을 두고 그 값을 넘어야 성공 혹은 참으로 분류하는 함수 입니다. 여기서 우리는 &lt;a class="markup--anchor markup--li-anchor" href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" target="_blank" rel="nofollow noopener"&gt;Rectified Linear Unit&lt;/a&gt; , 일명 &amp;#8216;ReLU&amp;#8217;를 사용합니다.  (역자주: ReLU는 신경망 학습에서 매우 중요한 성능을 높이는 판단 기준을 제안한 것으로, 출력이 0보다 작을 때는 0을 사용하고, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법입니다. 음수에 대해서는 값이 바뀌지만, 양수에 대해서는 값을 바꾸지 않습니다.)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="7f79" name="7f79" class="graf graf--pre graf-after--li"&gt;relu1 = mx.sym.Activation(fc1, name='relu1', act_type="relu")&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fc2에서 두 번째 숨겨진 레이어는 연결된 10개의 뉴런으로 구성되며 10개의 카테고리에 매핑됩니다. 각 뉴런은 임의의 부동 소수점 값을 출력합니다. 10개의 값 중 가장 큰 값은 데이터 샘플의 가장 가능성 있는 카테고리를 나타냅니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="6cfc" name="6cfc" class="graf graf--pre graf-after--li"&gt;fc2 = mx.sym.FullyConnected(relu1, name='fc2', num_hidden=category_count)&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;출력 레이어는 fc2 레이어에서 10개의 값에 Softmax 함수를 적용합니다.이 값은 0과 1사이의 10 개의 값으로 변환되어 1을 가산합니다. 각 값은 각 카테고리의 예상 확률을 나타내며 가장 가능성 있는 큰 카테고리를 가리키게 됩니다. (역자주: Softmax 함수는 자연수 N개의 값에서, n 번째 값의 중요도를 찾는 함수로서 각각의 값의 편차를 확대시켜 큰 값은 상대적으로 더 크게, 작은 값은 상대적으로 더 작게 만든 다음 정규화 시키는 함수이다.)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="e2c5" name="e2c5" class="graf graf--pre graf-after--li"&gt;out = mx.sym.SoftmaxOutput(fc2, name='softmax')
mod = mx.mod.Module(out)&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;데이터 반복자(Iterator) 만들기&lt;/strong&gt;&lt;br /&gt;
Part 1에서는 한 번에 학습되지 않은 하나의 샘플을 가진 신경망를 살펴 보았습니다. 성능 관점에서 볼 때 매우 비효율적이죠. 대신에 고정 된 수의 샘플인 배치(Batch)를 사용합니다.&lt;/p&gt;
&lt;p&gt;이러한 배치를 네트워크에 전달하려면 NDArrayIter 함수를 사용하여 반복기(Interna)를 만들어야합니다. 그 파라미터는 학습 데이터, 카테고리 (MXNet에서는 라벨(Label)이라고 부름) 및 배치 크기입니다.&lt;/p&gt;
&lt;p&gt;아래 처럼, 한 번에 10개의 샘플과 10개의 레이블로 데이터셋을 반복 할 수 있습니다. 그런 다음 reset() 함수를 호출하여 반복기 상태를 원래로 복원합니다.&lt;/p&gt;
&lt;pre id="9cbc" name="9cbc" class="graf graf--pre graf-after--p"&gt;train_iter = mx.io.NDArrayIter(data=X_train,label=Y_train,batch_size=batch)
&amp;gt;&amp;gt;&amp;gt; for batch in train_iter:
...   print batch.data
...   print batch.label
...
[&amp;lt;NDArray 10x99 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10x99 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10x99 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
&lt;em class="markup--em markup--pre-em"&gt;&amp;lt;edited for brevity&amp;gt;
&amp;gt;&amp;gt;&amp;gt; &lt;/em&gt;train_iter.reset()&lt;/pre&gt;
&lt;p&gt;우리가 만든 데이터셋을 통한 네트워크는 이제 학습 준비가 끝났습니다!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 학습하기&lt;/strong&gt;&lt;br /&gt;
먼저 입력 심볼을 실제 데이터셋 (샘플 및 레이블)에 바인딩합니다. 반복기를 통해 쉽게 할 수 있겠죠?&lt;/p&gt;
&lt;pre id="dbdb" name="dbdb" class="graf graf--pre graf-after--p"&gt;mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)&lt;/pre&gt;
&lt;p&gt;다음으로, 네트워크에서 뉴런 가중치를 초기화 해 봅시다. 이것은 실제로 매우 중요한 단계입니다. &amp;#8220;올바른&amp;#8221;기술로 초기화하면 네트워크가 훨씬 빨리 학습하는 데 도움이 됩니다. Xavier 초기화(이것을 만든 Xavier Glorot (&lt;a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="nofollow noopener"&gt; PDF&lt;/a&gt;)의 이름을 따서 명명)는 이러한 기술 중 하나입니다.&lt;/p&gt;
&lt;pre id="7dc8" name="7dc8" class="graf graf--pre graf-after--p"&gt;# Allowed, but not efficient
mod.init_params()
# Much better
mod.init_params(initializer=mx.init.Xavier(magnitude=2.))&lt;/pre&gt;
&lt;p&gt;다음으로 최적화 매개 변수를 정의해야 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;우리는 기계 학습 및 딥러닝 애플리케이션에 오랫동안 사용해온 &lt;a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"&gt;Stochastic Gradient Descent 알고리즘&lt;/a&gt; (일명 SGD)을 사용하고 있습니다.&lt;/li&gt;
&lt;li&gt;우리는 학습 속도를 SGD의 일반적 값인 0.1로 설정하고 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="fa49" name="fa49" class="graf graf--pre graf-after--li"&gt;mod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 0.1), ))&lt;/pre&gt;
&lt;p&gt;이제 신경망에 대한 학습을 시작합니다. 50번의 학습 단계(Epochs)를 반복합니다. 즉, 전체 데이터셋이 네트워크를 통해 50회 (10개 샘플 배치 처리)로 진행합니다.&lt;/p&gt;
&lt;pre id="3abf" name="3abf" class="graf graf--pre graf-after--p"&gt;mod.fit(train_iter, num_epoch=50)
INFO:root:Epoch[0] Train-accuracy=0.097500
INFO:root:Epoch[0] Time cost=0.085
INFO:root:Epoch[1] Train-accuracy=0.122500
INFO:root:Epoch[1] Time cost=0.074
INFO:root:Epoch[2] Train-accuracy=0.153750
INFO:root:Epoch[2] Time cost=0.087
INFO:root:Epoch[3] Train-accuracy=0.162500
INFO:root:Epoch[3] Time cost=0.082
INFO:root:Epoch[4] Train-accuracy=0.192500
INFO:root:Epoch[4] Time cost=0.094
INFO:root:Epoch[5] Train-accuracy=0.210000
INFO:root:Epoch[5] Time cost=0.108
INFO:root:Epoch[6] Train-accuracy=0.222500
INFO:root:Epoch[6] Time cost=0.104
INFO:root:Epoch[7] Train-accuracy=0.243750
INFO:root:Epoch[7] Time cost=0.110
INFO:root:Epoch[8] Train-accuracy=0.263750
INFO:root:Epoch[8] Time cost=0.101
INFO:root:Epoch[9] Train-accuracy=0.286250
INFO:root:Epoch[9] Time cost=0.097
INFO:root:Epoch[10] Train-accuracy=0.306250
INFO:root:Epoch[10] Time cost=0.100
...
INFO:root:Epoch[20] Train-accuracy=0.507500
...
INFO:root:Epoch[30] Train-accuracy=0.718750
...
INFO:root:Epoch[40] Train-accuracy=0.923750
...
INFO:root:Epoch[50] Train-accuracy=0.998750
INFO:root:Epoch[50] Time cost=0.077&lt;/pre&gt;
&lt;p&gt;훈련의 정확도는 빠르게 상승하여 50 단계 후에 99+%에 도달합니다. 우리가 만든 네트워크에서 학습 세트를 잘 수행을 했으며, 아주 좋은 결과를 보입니다. 이제 유효성 검사를 수행해 보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 유효성 검사하기&lt;/strong&gt;&lt;br /&gt;
이제 우리는 네트워크에 새로운 데이터 샘플, 즉 학습에 사용하지 않은 데이터 20%를 통해 유효성 검사를 해보겠습니다. 유효성 검사 샘플과 라벨을 사용하여 반복기를 먼저 만듭니다.&lt;/p&gt;
&lt;pre id="7809" name="7809" class="graf graf--pre graf-after--p"&gt;pred_iter = mx.io.NDArrayIter(data=X_valid,label=Y_valid, batch_size=batch)&lt;/pre&gt;
&lt;p&gt;그 다음으로 &lt;code&gt;Module.iter_predict()&lt;/code&gt; 함수를 사용하여 네트워크를 통해 20% 샘플을 실행합니다. 이제 예상 라벨과 실제 라벨을 비교합니다. 유효성 점수와 검증 정확도를 추적하는데, 이는 네트워크가 유효성 검증셋의 수행 결과가 잘 되었는지 알아 볼 수 있습니다.&lt;/p&gt;
&lt;pre id="4d78" name="4d78" class="graf graf--pre graf-after--p"&gt;pred_count = valid_count
correct_preds = total_correct_preds = 0

for preds, i_batch, batch in mod.it