<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[행복한 유파]]></title><description><![CDATA[Ruby on Rails, Swift, React, Docker, Mesos, Kuernetes, Google Cloud Platform, Ubuntu, MacOS]]></description><link>http://iamartin-gh.herokuapp.com/</link><generator>Ghost 0.11</generator><lastBuildDate>Sun, 12 May 2019 19:18:15 GMT</lastBuildDate><atom:link href="http://iamartin-gh.herokuapp.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[How To Secure HAProxy with Let's Encrypt on Ubuntu 14.04]]></title><description><![CDATA[<blockquote>
  <p>HAProxy에서 Let's Encrypt를 적용하기 위하여 study 목적으로 번역한 내용이여서 검증되지 않는 내용을 포함할 수도 있습니다. 계속 내용을 수정할 예정입니다.   </p>
</blockquote>

<p><strong>원문</strong> : <a href="https://www.digitalocean.com/community/tutorials/how-to-secure-haproxy-with-let-s-encrypt-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-secure-haproxy-with-let-s-encrypt-on-ubuntu-14-04</a></p>

<p><code>Let's Encrypt</code>는 무료 TLS / SSL 인증서를 쉽게 얻고 설치할 수있는 새로운 인증 기관 (CA)이므로 웹 서버에서 암호화 된 HTTPS를 사용할 수</p>]]></description><link>http://iamartin-gh.herokuapp.com/how-to-secure-haproxy-with-lets-encrypt-on-ubuntu-14-04-2/</link><guid isPermaLink="false">1ce0b89b-2075-409e-8653-303bce276c11</guid><category><![CDATA[HAProxy]]></category><category><![CDATA[Let's Encrypt]]></category><category><![CDATA[Ubnuntu 14.04]]></category><category><![CDATA[Security]]></category><category><![CDATA[DigitalOcean]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Sun, 30 Apr 2017 16:35:42 GMT</pubDate><media:content url="http://i.imgur.com/5gejlKK.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://i.imgur.com/5gejlKK.jpg" alt="How To Secure HAProxy with Let's Encrypt on Ubuntu 14.04"><p>HAProxy에서 Let's Encrypt를 적용하기 위하여 study 목적으로 번역한 내용이여서 검증되지 않는 내용을 포함할 수도 있습니다. 계속 내용을 수정할 예정입니다.   </p>
</blockquote>

<p><strong>원문</strong> : <a href="https://www.digitalocean.com/community/tutorials/how-to-secure-haproxy-with-let-s-encrypt-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-secure-haproxy-with-let-s-encrypt-on-ubuntu-14-04</a></p>

<p><code>Let's Encrypt</code>는 무료 TLS / SSL 인증서를 쉽게 얻고 설치할 수있는 새로운 인증 기관 (CA)이므로 웹 서버에서 암호화 된 HTTPS를 사용할 수 있습니다. 필요한 단계를 대부분 자동화하는 시도인 소프트웨어 클라이언트, letsencrypt를 제공함으로써 프로세스를 단순화합니다. 현재 <code>Let's Encrypt</code>는 공개 베타 버전이므로 인증서를 얻고 설치하는 전체 과정은 Apache 웹서버에서만 완전히 자동화됩니다. 그러나 Let's Encrypt를 사용하면 무료 SSL 인증서를 쉽게 얻을 수 있습니다. 이 인증서는 웹서버 소프트웨어 선택에 관계없이 수동으로 설치할 수 있습니다.</p>

<p>이 튜토리얼에서는 <code>Let's Encrypt</code>를 사용하여 무료 SSL 인증서를 얻고 Ubuntu 14.04에서 HAProxy와 함께 사용하는 방법을 보여줍니다. 또한 SSL 인증서를 자동으로 갱신하는 방법을 알려드립니다.</p>

<p><img src="https://assets.digitalocean.com/articles/letsencrypt/haproxy-letsencrypt.png" alt="How To Secure HAProxy with Let's Encrypt on Ubuntu 14.04"></p>

<p>이 자습서를 수행하기 전에 몇 가지가 필요합니다.</p>

<p>Ubuntu 14.04 서버에 sudo 권한이 있는 root가 아닌 사용자 계정이 있어야 합니다. Ubuntu 14.04의 초기 서버 설정에서 1-3 단계를 수행하여 이러한 사용자 계정을 설정하는 방법을 배울 수 있습니다. <a href="https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-14-04">initial server setup for Ubuntu 14.04</a></p>

<p>인증서를 사용할 등록된  domain name을 소유하거나 제어해야 합니다. 등록된 domain name이 없다면 많은 domain name 등록 기관 중 하나 (예 : Namecheap, GoDaddy 등)에 등록할 수 있습니다.</p>

<p>아직 도메인을 서버의 public IP 주소로 가리키는 A 레코드를 작성하지 않았는지 확인하세요. 이것은 <code>Let's Encrypt</code>가 인증서를 발급하는 도메인을 소유하고 있는지 확인하는 방법때문에 필요합니다. 예를 들어, example.com에 대한 인증서를 얻기위한 유효성 검사 프로세스가 작동하려면 해당 도메인이 서버로 확인되어야 합니다. 우리의 설치 프로그램은 example.com과 www.example.com을 도메인 이름으로 사용하므로 DNS 기록이 모두 필요합니다.</p>

<p>모든 필수 구성 요소를 준비했으면 <code>Let's Encrypt</code> 클라이언트 소프트웨어 설치로 넘어 갑니다.</p>

<h4 id="1letsencryptclient">1단계 — Let's Encrypt Client 설치</h4>

<p><code>Let's Encrypt</code>를 사용하여 SSL 인증서를 얻는 첫 번째 단계는 서버에 <code>letsencrypt</code> 소프트웨어를 설치하는 것입니다. 현재, <code>Let's Encrypt</code>를 설치하는 가장 좋은 방법은 공식 GitHub 저장소에서 단순히 복제하는 것입니다. 앞으로는 패키지 관리자를 통해 제공 될 것입니다.</p>

<h6 id="gitbc">Git과 bc 설치</h6>

<p>이제 Git과 bc를 설치하여 <code>Let's Encrypt</code> 저장소를 복제 할 수 있습니다.
아래 명령으로 서버의 패키지 관리자를 업데이트하십시오.</p>

<pre><code>$ sudo apt-get update
</code></pre>

<p>그런 다음 apt-get을 사용하여 git 및 bc 패키지를 설치하십시오</p>

<pre><code>$ sudo apt-get -y install git bc
</code></pre>

<p>git와 bc를 설치하면 GitHub에서 저장소를 복제하여 쉽게 <code>letsencrypt</code>를 다운로드할 수 있습니다.</p>

<h6 id="cloneletsencrypt">Clone Let's Encrypt</h6>

<p>이제 다음 명령으로 <code>/opt</code>에 <code>Let's Encrypt</code> 저장소를 복제할 수 있습니다.</p>

<pre><code>$ sudo git clone https://github.com/letsencrypt/letsencrypt /opt/letsencrypt
</code></pre>

<p>이제 <code>/opt/letsencrypt</code> 디렉토리에 <code>letsencrypt</code> 저장소 사본이 있어야 합니다.</p>

<h4 id="2">2단계 - 인증서 얻기</h4>

<p><code>Let's Encrypt</code>는 다양한 플러그인을 통해 SSL 인증서를 얻는 다양한 방법을 제공합니다. 다른 튜토리얼에서 다루는 아파치 플러그인과는 달리, 대부분의 플러그인은 웹서버를 수동으로 구성해야하는 인증서를 얻는데 도움이 됩니다. 인증서를 가져오고 설치하지 않는 플러그인은 서버에 인증서를 발급해야하는지 여부를 인증하는데 사용되므로 "인증 자"라고 합니다.</p>

<p>SSL 인증서를 얻기 위해 <code>Standalone plugin</code>을 사용하는 방법을 알려 드리겠습니다.</p>

<h6 id="80">80번 포트가 열려있는 지 확인</h6>

<p><code>Standalone plugin</code>은 SSL 인증서를 얻을 수있는 매우 간단한 방법을 제공합니다. 서버에서 <code>80번 포트</code>의 작은 웹서버를 임시로 실행하면 인증서를 발급하기 전에 암호화 CA가 서버 ID에 연결하여 유효성을 검사할 수 있습니다. 따라서 이 방법을 사용하려면 <code>80번 포트</code>를 사용하고 있지 않아야 합니다. 즉, 이 플러그인을 사용하기 전에 <code>80번 포트</code>(예: http)을 사용하는 일반 웹서버를 중지하십시오.</p>

<p>예를 들어 <code>HAProxy</code>를 사용하는 경우 다음 명령을 실행하여 <code>HAProxy</code>를 중지할 수 있습니다.</p>

<pre><code>$ sudo service haproxy stop
</code></pre>

<p><code>80번 포트</code>이 사용 중인지 확실하지 않을 경우 다음 명령을 실행할 수 있습니다.</p>

<pre><code>netstat -na | grep ':80.*LISTEN'  
</code></pre>

<p>이 명령을 실행했을 때 출력이 없으면 <code>Standalone plugin</code>을 사용할 수 있습니다.</p>

<h6 id="letsencrypt">Let's Encrypt 실행</h6>

<p><code>Let's Encrypt</code>를 사용하기 전에 <code>letsencrypt</code> 디렉토리로 변경하세요.</p>

<pre><code>$ cd /opt/letsencrypt
</code></pre>

<p>이제 다음 명령을 실행하여 <code>Standalone plugin</code>을 사용하세요.</p>

<pre><code>$ ./letsencrypt-auto certonly --standalone
</code></pre>

<blockquote>
  <p><code>Let's Encrypt</code> 소프트웨어는 수퍼 유저 권한이 필요하므로 최근에 sudo를 사용하지 않았다면 암호를 입력해야합니다.</p>
</blockquote>

<p><code>letsencrypt</code>가 초기화된 후 몇 가지 정보를 묻는 메시지가 나타납니다. 이전에 <code>Let's Encrypt</code>를 사용했는지 여부에 따라 달라질 수 있지만 처음부터 다시 시작하겠습니다.</p>

<p>프롬프트에서 알림 및 손실된 키 복구에 사용될 전자 메일 주소를 입력하십시오.</p>

<p><img src="https://assets.digitalocean.com/articles/letsencrypt/le-email.png" alt="How To Secure HAProxy with Let's Encrypt on Ubuntu 14.04"></p>

<p>그러면 <code>Let's Encrypt</code> Subscribe 계약에 동의해야합니다. 동의(Agree)을 선택하십시오.
<img src="https://assets.digitalocean.com/articles/letsencrypt/le-agreement.png" alt="How To Secure HAProxy with Let's Encrypt on Ubuntu 14.04"></p>

<p>그런 다음 도메인 이름을 입력하십시오. 단일 인증서를 여러 도메인 이름 (예: example.com 및 www.example.com)과 함께 사용하려면 모든 인증서를 포함해야합니다.
<img src="https://assets.digitalocean.com/articles/letsencrypt/le-domain.png" alt="How To Secure HAProxy with Let's Encrypt on Ubuntu 14.04"></p>

<p>모든 것이 성공하면 다음과 같은 출력 메시지가 나타납니다.</p>

<pre><code>Output:  
IMPORTANT NOTES:  
 - If you lose your account credentials, you can recover through
   e-mails sent to sammy@digitalocean.com
 - Congratulations! Your certificate and chain have been saved at
   /etc/letsencrypt/live/example.com/fullchain.pem. Your
   cert will expire on 2016-03-15. To obtain a new version of the
   certificate in the future, simply run Let's Encrypt again.
 - Your account credentials have been saved in your Let's Encrypt
   configuration directory at /etc/letsencrypt. You should make a
   secure backup of this folder now. This configuration directory will
   also contain certificates and private keys obtained by Let's
   Encrypt so making regular backups of this folder is ideal.
 - If like Let's Encrypt, please consider supporting our work by:

   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le
</code></pre>

<p>예제 출력에서 강조 표시된 인증서의 경로와 만기 날짜를 기록해 두는 것이 좋습니다.</p>

<blockquote>
  <p>도메인이 CloudFlare와 같은 DNS 서비스를 통해 라우팅하는 경우 인증서를 얻을 때까지 일시적으로 비활성화해야합니다.</p>
</blockquote>

<h6 id="">인증서 파일</h6>

<p>인증서를받은 후 다음과 같은 PEM 인코딩 파일을 갖게 됩니다.</p>

<ul>
<li>cert.pem : 도메인 인증서</li>
<li>chain.pem : Let's Encrypt 체인 인증서</li>
<li>fullchain.pem : cert.pem 및 chain.pem 결합</li>
<li>privkey.pem : 인증서의 private key</li>
</ul>

<p>방금 작성한 인증서 파일의 위치를 알고 있으므로 웹 서버 구성에서 사용할 수 있습니다. 파일 자체는 <code>/etc/letsencrypt/archive</code>의 서브 디렉토리에 있습니다. 그러나 <code>Let's Encrypt</code>는 <code>/etc/letsencrypt/live/your_domain_name</code> 디렉토리의 가장 최근 인증서 파일에 대한 심볼릭 링크를 만듭니다.</p>

<p>다음 명령을 실행하여 파일이 존재하는지 확인할 수 있습니다 (도메인 이름으로 대체):</p>

<pre><code>$ sudo ls /etc/letsencrypt/live/your_domain_name
</code></pre>

<p>출력은 위에서 언급 한 네 개의 인증서 파일이어야 합니다.</p>

<h6 id="fullchainpemprivkeypem">Fullchain.pem과 Privkey.pem을 결합</h6>

<p>SSL 종료를 수행하도록 HAProxy를 구성할 때 자체와 최종 사용자간의 트래픽을 암호화하므로 fullchain.pem과 privkey.pem을 단일 파일로 결합해야 합니다.</p>

<p>먼저, 결합된 파일을 저장할 디렉토리 <code>/etc/haproxy/certs</code>를 만듭니다.</p>

<pre><code>$ sudo mkdir -p /etc/haproxy/certs
</code></pre>

<p>다음으로, 이 cat 명령으로 결합된 파일을 만듭니다. (강조 표시된 example.com을  domain name으로 대체하십시오).</p>

<pre><code>$ DOMAIN='example.com' sudo -E bash -c 'cat /etc/letsencrypt/live/$DOMAIN/fullchain.pem /etc/letsencrypt
</code></pre>

<p>다음 명령을 사용하여 private key가 들어있는 결합된 파일에 안전하게 액세스하십시오.</p>

<pre><code>$ sudo chmod -R go-rwx /etc/haproxy/certs
</code></pre>

<p>이제 우리는 SSL 인증서와 개인 키를 <code>HAProxy</code>와 함께 사용할 준비가 되었습니다.</p>

<h4 id="3haproxy">3단계 - HAProxy 설치</h4>

<p>이 단계는 <code>HAProxy</code>의 설치를 포함합니다. 서버에 이미 설치되어있는 경우, 이 단계를 건너 뜁니다.</p>

<p>우리는 기본 우분투 저장소에 없는 <code>HAProxy 1.6</code>을 설치합니다. 그러나 PPA를 사용하는 경우 다음 명령을 사용하여 패키지 관리자를 사용하여 <code>HAProxy 1.6</code>을 설치할 수 있습니다.</p>

<pre><code>$ sudo add-apt-repository ppa:vbernat/haproxy-1.6
</code></pre>

<p>로드 밸런서에서 로컬 패키지 색인을 업데이트하고 다음을 입력하여 <code>HAProxy</code>를 설치하십시오.</p>

<pre><code>$ sudo apt-get update
$ sudo apt-get install haproxy
</code></pre>

<p><code>HAProxy</code>가 설치되었지만 구성해야합니다.</p>

<h4 id="4haproxy">단계 4 - HAProxy 설정</h4>

<p>이 섹션에서는 SSL 설정으로 기본 <code>HAProxy</code>를 구성하는 방법을 보여줍니다. 또한 <code>Let's Encrypt</code> 인증서를 자동 갱신할 수 있도록 <code>HAProxy</code>를 구성하는 방법을 설명합니다.</p>

<p>텍스트 편집기에서 <code>haproxy.cfg</code>을 편집합니다.</p>

<pre><code>$ sudo nano /etc/haproxy/haproxy.cfg
</code></pre>

<p>이 파일은 다음 몇 섹션에서 편집 할 때 열어 두세요.</p>

<h6 id="globalsection">Global Section</h6>

<p><code>Global</code> Section 아래에 몇 가지 기본 설정을 추가해 보겠습니다.</p>

<p>가장 먼저해야 할 일은 maxconn을 적당한 수로 설정하는 것입니다. <br>
이는 HAProxy가 허용하는 동시 연결 수에 영향을 주지만,이는 QoS에 영향을 미치고 웹 서버가 너무 많은 요청을 처리하지 못하는 것을 막을 수 있습니다. 귀하의 환경에 적합한 것을 찾으려면 play around가 필요합니다. 합리적인 값을 가진 다음 줄을 <code>Global</code> Section 추가하십시오.</p>

<pre><code>   maxconn 2048
</code></pre>

<p>그런 다음 이 줄을 추가하여 생성되는 임시 DHE 키의 최대 크기를 구성하십시오.</p>

<pre><code>   tune.ssl.default-dh-param 2048
</code></pre>

<h6 id="defaultssection">Defaults Section</h6>

<p><code>defaults</code> 섹션에 다음 행을 추가하십시오.</p>

<pre><code>   option forwardfor
   option http-server-close
</code></pre>

<p>forwardfor 옵션은 HAProxy가 각 요청에 <code>X-Forwarded-For</code> 헤더를 추가하도록 설정하고 <code>http-server-close</code> 옵션은 연결을 닫지만 keep-alives를 유지함으로써 HAProxy와 사용자 간의 대기 시간을 줄입니다.</p>

<h6 id="frontendsections">Frontend Sections</h6>

<p>이제 Frontend Sections을 정의할 준비가 되었습니다. <br>
추가하고자하는 첫 번째 작업은 들어오는 HTTP 연결을 처리하고 기본 백엔드로 전송하는 프론트 엔드입니다 (나중에 정의 할 것입니다). 파일의 끝에서 <strong>www-http</strong>라는 프론트 엔드를 추가합시다. <code>haproxy_public_IP</code>를 HAProxy 서버의 공용 IP 주소로 바꾸십시오.</p>

<pre><code>frontend www-http  
   bind haproxy_www_public_IP:80
   reqadd X-Forwarded-Proto:\ http
   default_backend www-backend
</code></pre>

<p>다음으로 프론트 엔드를 추가하여 들어오는 HTTPS 연결을 처리합니다. 파일의 끝에 www-https라는 프론트 엔드를 추가하십시오. <code>haproxy_www_public_IP</code>를 HAProxy 서버의 공인 IP로 바꾸십시오. 또한 <code>example.com</code>을 도메인 이름 (이전에 생성 한 인증서 파일과 일치해야 함)으로 바꿔야합니다.</p>

<pre><code>frontend www-https  
   bind haproxy_www_public_IP:443 ssl crt /etc/haproxy/certs/example.com.pem
   reqadd X-Forwarded-Proto:\ https
   acl letsencrypt-acl path_beg /.well-known/acme-challenge/
   use_backend letsencrypt-backend if letsencrypt-acl
   default_backend www-backend
</code></pre>

<p>이 프론트 엔드는 ACL(<code>letsencrypt-acl</code>)을 사용하여 유효화 요청 (<code>/.well-known/acme-challenge</code>)을 <code>letsencrypt-backend</code> 백엔드에 보냅니다. 그러면 HAProxy 서비스를 중지하지 않고 인증서를 갱신할 수 있습니다. 다른 모든 요청은 웹 응용 프로그램이나 사이트를 제공 할 백엔드인 <code>www-backend</code>로 전달됩니다.</p>

<h6 id="backendsections">Backend Sections</h6>

<p>프론트 엔드 구성을 완료 한 후, 다음 행을 추가하여 <code>www-backend</code> 백엔드를 추가하십시오. 강조 표시된 단어를 웹 서버의 개별 IP 주소로 바꾸십시오 (보유하고 있는 백엔드 서버 수와 일치하도록 서버 행 수 조정).</p>

<pre><code>backend www-backend  
   redirect scheme https if !{ ssl_fc }
   server www-1 www_1_private_IP:80 check
   server www-2 www_2_private_IP:80 check
</code></pre>

<p>이 백엔드에서 수신하는 모든 트래픽은 HTTP(80번 포트)를 통해 서버 항목간에 균형을 유지합니다.
마지막으로, 다음 행을 추가하여 <code>letsencrypt-backend</code> 백엔드를 추가하십시오.</p>

<pre><code>backend letsencrypt-backend  
   server letsencrypt 127.0.0.1:54321
</code></pre>

<p>이 백엔드는 인증서 요청 및 갱신에 사용되는 Let's Encrypt ACME 문제만 처리하여  <code>54321</code> 포트의 로컬 호스트에 트래픽을 전송합니다. <br>
우리는 Let's Encrypt SSL 인증서를 갱신 할 때 <code>80</code> 및 <code>443</code>대신 이 포트를 사용합니다.
이제 우리는 HAProxy를 시작할 준비가되었습니다:</p>

<pre><code>$ sudo service haproxy restart
</code></pre>

<blockquote>
  <p>haproxy.cfg 구성 파일에 문제가있는 경우이 GitHub Gist에서 예제를 확인하십시오.</p>
</blockquote>

<p>Let's Encrypt TLS/SSL 인증서가 설치되었으며 자동 갱신 스크립트를 설정할 준비가되었습니다. 이 시점에서 웹 브라우저에서 도메인을 방문하여 TLS/SSL 인증서가 작동하는지 테스트해야합니다.</p>

<h4 id="5">5단계 - 자동 갱신 설정</h4>

<p>Let's Encrypt Certificate는 90일동안 유효하지만 오류가 허용되도록 60 일마다 인증서를 갱신하는 것이 좋습니다. 이 글을 쓰는 시점에서 클라이언트의 기능으로 자동 갱신을 계속 사용할 수는 없지만 Let's Encrypt 클라이언트를 다시 실행하여 인증서를 수동으로 갱신할 수 있습니다.</p>

<p>인증서가 구형이 되지않도록 하는 실질적인 방법은 갱신 프로세스를 자동으로 처리하는 cron 작업을 만드는 것입니다. 앞에서 사용했던 대화형 메뉴 기반 프로세스를 피하기 위해 cron 작업에서 Let's Encrypt 클라이언트를 호출할 때 다른 매개 변수를 사용합니다.</p>

<p>앞에서 사용된 Standalone plugin을 사용하지만 포트 <code>54321</code>을 사용하도록 구성하여 HAProxy (포트 <code>80</code> 및 <code>443</code>에서 수신 대기)와 충돌하지 않도록합니다. 이렇게 하려면 다음 명령을 사용합니다 (강조 표시된 <code>example.com</code> 도메인 둘 다 도메인 이름으로 대체).</p>

<pre><code>$ cd /opt/letsencrypt
$ ./letsencrypt-auto certonly --agree-tos --renew-by-default --standalone-supported-challenges http-01 --http-01-port 54321 -d example.com -d www.example.com
</code></pre>

<p>성공하면 다음과 같이 결합된 인증서 파일을 새로 만들어야 합니다 (example.com을 도메인 이름으로 바꿉니다).</p>

<pre><code>$ DOMAIN='example.com' sudo -E bash -c 'cat /etc/letsencrypt/live/$DOMAIN/fullchain.pem /etc/letsencrypt/live/$DOMAIN/privkey.pem &gt; /etc/haproxy/certs/$DOMAIN.pem'
</code></pre>

<p>그런 다음 HAProxy를 다시로드하여 새 인증서를 사용하십시오.</p>

<pre><code>$ sudo service haproxy reload
</code></pre>

<p>이제 인증서를 갱신해야하는 명령을 알았으므로 스크립트와 cron 작업을 사용하여이 프로세스를 자동화할 수 있습니다.</p>

<h6 id="letsencrypt">Let's Encrypt 구성 파일 생성</h6>

<p>계속하기 전에 <code>/usr/local/etc/le-renew-haproxy.ini</code>에 Let's Encrypt 구성 파일을 작성하여 갱신 프로세스를 간소화합시다.</p>

<pre><code>$ sudo cp /opt/letsencrypt/examples/cli.ini /usr/local/etc/le-renew-haproxy.ini
</code></pre>

<p>이제 편집을 위해 파일을 엽니다.</p>

<pre><code>sudo nano /usr/local/etc/le-renew-haproxy.ini  
</code></pre>

<p>그런 다음 <code>전자 메일</code> 및 <code>도메인</code> 라인의 주석 처리를 제거하고 사용자 자신의 정보로 업데이트하십시오. 주석이 제거된 파일은 다음과 같이 보일 것입니다:</p>

<pre><code>rsa-key-size = 4096

email = you@example.com

domains = example.com, www.example.com  
</code></pre>

<p>그런 다음 <code>standalone-supported-challenges</code> 라인의 주석 처리를 제거하고 해당 값을 <code>http-01</code>로 바꿉니다. </p>

<pre><code>standalone-supported-challenges = http-01  
</code></pre>

<p>이제 명령에서 도메인 이름을 지정하는 대신 Let's Encrypt 구성 파일을 사용하여 공백을 채울 수 있습니다. 구성 파일이 올바른 것으로 가정하면 이 명령을 사용하여 인증서를 갱신할 수 있습니다.</p>

<pre><code>cd /opt/letsencrypt  
./letsencrypt-auto certonly --renew-by-default --config /usr/local/etc/le-renew-haproxy.ini --http-01-port 54321
</code></pre>

<blockquote>
  <p>le-renew-haproxy.ini 구성 파일에 문제가있는 경우이 GitHub Gist에서 예제를 확인하십시오. <br>
  이제 인증서를 갱신하는 데 사용할 수있는 스크립트를 작성해 보겠습니다.</p>
</blockquote>

<h6 id="">갱신 스크립트 만들기</h6>

<p>갱신 프로세스를 자동화하기 위해 제공된 도메인의 인증서 만료 날짜를 확인하고 만료일이 30일보다 짧은 경우 갱신을 요청하는 쉘 스크립트를 사용합니다. 이 스크립트는 일주일에 한 번 실행되도록 예약됩니다. 이렇게 하면 cron작업이 실패하더라도 매주 다시 시도 할 수있는 30 일짜리 창이 있습니다.</p>

<p>먼저 스크립트를 다운로드하여 실행 가능하게 만드십시오. 스크립트를 다운로드하기 전에 스크립트의 내용을 자유롭게 검토하십시오.</p>

<pre><code>$ sudo curl -L -o /usr/local/sbin/le-renew-haproxy https://gist.githubusercontent.com/thisismitch/7c91e9b2b63f837a0c4b/raw/700cfe953e5d5e71e528baf20337198195606630/le-renew-haproxy
$ sudo chmod +x /usr/local/sbin/le-renew-haproxy
</code></pre>

<p><code>le-renew-haproxy</code> 스크립트는 갱신을 확인하려는 인증서의 domain name을 인수로 취합니다. 갱신이 아직 필요하지 않으면 주어진 인증서 만료일까지 남은 일수를 출력합니다.</p>

<blockquote>
  <p><code>/usr/local/etc/le-renew-haproxy.ini</code> 파일이 없으면 스크립트가 실행되지 않습니다. 또한 구성 파일에 지정된 첫 번째 도메인이 원래 인증서를 만들 때 지정한 첫 번째 도메인과 동일해야합니다.</p>
</blockquote>

<p>지금 스크립트를 실행하면이 인증서가 만료되는 데 남은 일수를 볼 수 있습니다.</p>

<pre><code>$ sudo le-renew-haproxy
</code></pre>

<pre><code>output  
Checking expiration date for example.com...  
The certificate is up to date, no need for renewal (89 days left).  
</code></pre>

<p>다음으로, 매주 이 명령을 실행할 새로운 작업을 작성하기 위해 crontab을 편집합니다. 루트 사용자의 crontab을 편집하려면 다음을 실행하십시오.</p>

<pre><code>$ sudo crontab -e
</code></pre>

<p>다음 내용을 모두 한 줄에 포함하십시오.</p>

<pre><code>30 2 * * 1 /usr/local/sbin/le-renew-haproxy &gt;&gt; /var/log/le-renewal.log  
</code></pre>

<p>저장 및 종료합니다. 매주 월요일 오전 2시30분에 <code>le-renew-haproxy</code> 명령을 실행할 새 크론 작업이 생성됩니다. 명령에 의해 생성된 출력은 <code>/var/log/le-renewal.log</code>에 있는 로그 파일로 저장됩니다.</p>

<h4 id="">결론</h4>

<p>HAProxy는 이제 TLS / SSL 암호화 인증서를 사용하여 HTTPS 트래픽을 안전하게 제공합니다.</p>]]></content:encoded></item><item><title><![CDATA[북마크]]></title><description><![CDATA[<h4 id="atom">Atom</h4>

<ul>
<li>Automates closing of HTML Tags : <a href="https://github.com/mattberkowitz/autoclose-html">https://github.com/mattberkowitz/autoclose-html</a></li>
<li>Firefox user agent string reference
<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent/Firefox">https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent/Firefox</a>  </li>
<li>Detect Mobile Browsers <br>
<a href="http://detectmobilebrowsers.com/">http://detectmobilebrowsers.com/</a></li>
</ul>]]></description><link>http://iamartin-gh.herokuapp.com/bookmark/</link><guid isPermaLink="false">81b6cca0-7802-4c8c-8930-bdf2f0344f46</guid><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Sun, 04 Dec 2016 14:00:54 GMT</pubDate><content:encoded><![CDATA[<h4 id="atom">Atom</h4>

<ul>
<li>Automates closing of HTML Tags : <a href="https://github.com/mattberkowitz/autoclose-html">https://github.com/mattberkowitz/autoclose-html</a></li>
<li>Firefox user agent string reference
<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent/Firefox">https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent/Firefox</a>  </li>
<li>Detect Mobile Browsers <br>
<a href="http://detectmobilebrowsers.com/">http://detectmobilebrowsers.com/</a></li>
</ul>]]></content:encoded></item><item><title><![CDATA[Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기]]></title><description><![CDATA[<p><code>Windows</code>에서 <code>Ruby on Rails</code> 개발환경을 구축하기란 그렇게 만만하지 않습니다. 그래서 대부분 MacOS나 Linux를 사용하지요. <br>
하지만 아쉽게도 업무에서 Windows가 설치된 노트북을 사용한다면 이야기는 다르지요.
이제부터 간단하게 Windows에서 Rails 개발환경을 구축해 보겠습니다. <br>
지금까지 다양한 방법으로 시도했지만 이 방법이 저에겐 최선이었습니다.</p>

<p>지금까지 시도한 방법 중 가장 간단한 방법은 <code>VMWare Workstaion</code>이나 <code>VirtualBox</code></p>]]></description><link>http://iamartin-gh.herokuapp.com/babun-vagrant-rails-dev/</link><guid isPermaLink="false">dee2f898-d2b3-4a68-9f4b-d4991adbf18c</guid><category><![CDATA[Windows]]></category><category><![CDATA[babun]]></category><category><![CDATA[install]]></category><category><![CDATA[rails]]></category><category><![CDATA[development]]></category><category><![CDATA[설치]]></category><category><![CDATA[Ruby On Rails]]></category><category><![CDATA[VirtualBox]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Wed, 23 Nov 2016 15:53:23 GMT</pubDate><media:content url="http://i.imgur.com/xsj9lOI.png" medium="image"/><content:encoded><![CDATA[<img src="http://i.imgur.com/xsj9lOI.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"><p><code>Windows</code>에서 <code>Ruby on Rails</code> 개발환경을 구축하기란 그렇게 만만하지 않습니다. 그래서 대부분 MacOS나 Linux를 사용하지요. <br>
하지만 아쉽게도 업무에서 Windows가 설치된 노트북을 사용한다면 이야기는 다르지요.
이제부터 간단하게 Windows에서 Rails 개발환경을 구축해 보겠습니다. <br>
지금까지 다양한 방법으로 시도했지만 이 방법이 저에겐 최선이었습니다.</p>

<p>지금까지 시도한 방법 중 가장 간단한 방법은 <code>VMWare Workstaion</code>이나 <code>VirtualBox</code>에 리눅스를 설치하고 ssh로 접속하여 vi로 개발하는 것입니다.
하지만 Atom 에디터도 쓰고 싶고 간단하게 Git Desktop도 쓰면서 익숙한 Windows 환경에서 개발하는 것이 정신건강에도 이롭고 초보자에게도 훨씬 스트레스도 덜 할 것 같습니다. <br>
참고로 VMWare Workstaion에 MacOS를 올려서 사용도 해봤습니다. <br>
아이러니하게도 VMWare Workstaion에서 돌리는 MacOS가 저의 2015 Mac mini보다 빠른 것같은 착각도 느꼈습니다.   </p>

<p>그럼 준비물은 아래와 같습니다.</p>

<ul>
<li>Windows가 설치된 노트북 (Windows 7)</li>
<li>Babun <br>
<mark>전 Babun을 무척 사랑합니다. Windows에서 사용할 수 있는 가장 아름다운 ssh client입니다.</mark></li>
<li>Vagrant 1.8.7</li>
<li>VirtualBox 5.1.10</li>
<li>Atom</li>
</ul>

<h5 id="babun">Babun 설치</h5>

<p><a href="http://babun.github.io/">http://babun.github.io/</a> 에 접속하여 설치하면 됩니다. <br>
전 Babun은 Windows에게는 축복이라고 생각합니다. <del>저만  그렇게 생각할 수도</del></p>

<p><img src="http://i.imgur.com/bRp6tmH.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
Windows의 명령프롬프트와 다르게 아주 아름답습니다. iTerm처럼요. <br>
아래의 Babun은 Melso Powerline Font와 oh-my-zsh 테마인 cobalt2 조합니다. 
<img src="http://i.imgur.com/ETUHFGF.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
Windows에서도 아래와 같은 화면을 볼 수 있습니다. <br>
<img src="http://i.imgur.com/xRmeYfd.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<h5 id="vagrant">Vagrant 설치</h5>

<p><a href="https://www.vagrantup.com/">https://www.vagrantup.com/</a> 에 접속하여 설치합니다. <br>
Windows는 패키지 설치만큼은 편안하고 확실합니다. 물론 MacOS보다는 아니지만. <br>
<img src="http://i.imgur.com/xsj9lOI.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<h5 id="virtualbox">VirtualBox 설치</h5>

<p><a href="https://www.virtualbox.org/">https://www.virtualbox.org/</a> 에 접속하여 설치합니다. <br>
<img src="http://i.imgur.com/grQvN5r.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<h5 id="vagrant">Vagrant 설정</h5>

<p>이제 준비가 다 되었으니 본격적으로 설정을 합니다.
우선 전 D드라이브에 rails란 디렉토리를 만들고 Rails 개발을 이곳에서 진행할 예정입니다.
<img src="http://i.imgur.com/aQ7RF04.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>Babun을 실행 후 rails 디렉토리로 이동합니다.  </p>

<pre><code>cd /cygdrive/d/rails  
</code></pre>

<p><img src="http://i.imgur.com/RraIFAG.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>vagrant init을 실행하여 <code>Vagrantfile</code>을 생성합니다.  </p>

<pre><code>vagrant init  
</code></pre>

<p>Atom에서 <code>Vagrantfile</code>을 열고 아래와 같이 수정합니다. <br>
<img src="http://i.imgur.com/BPf0phQ.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<pre><code># -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|  
  # Use Ubuntu 16.04 Xenial Xerus 64-bit as our operating system
  config.vm.box = "ubuntu/xenial64"

  # Configurate the virtual machine to use 2GB of RAM
  config.vm.provider :virtualbox do |vb|
    vb.customize ["modifyvm", :id, "--memory", "2048"]
  end

  # Forward the Rails server default port to the host
  config.vm.network :forwarded_port, guest: 3000, host: 3000
end  
</code></pre>

<p>간단하게 설명하면 OS로는 Ubuntu 16.04를 사용하고 메모리는 2GB 그리고 포트는 3000번을 사용합니다.</p>

<p>Vagrant를 실행합니다.  </p>

<pre><code>vagrant up  
</code></pre>

<p><img src="http://i.imgur.com/IWBrDIg.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
기존에 한번 실행했던 것을 다시 실행해서 처름 실행할 때와 메시지가 다르긴 한데 특별한 오류 없이 실행되면 아래와 같은 메시지로 보게됩니다.
<img src="http://i.imgur.com/7IVdj59.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>중요한 건 Ubuntu의 /vagrant와 Windows D:/rails가 공유된다는 사실^^
정말 유용하죠. 이것때문에 Windows에서 Atom으로 코딩을 하고 Ubuntu에서 <code>rails generate</code>나 <code>rails migrate</code>하고 <code>rails server</code> 로 띄워서 확인할 수 있습니다.</p>

<p><code>vagrant ssh</code>로 Ubuntu에 접속해 봅니다.
<img src="http://i.imgur.com/uDJ2y5A.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>아참 우리의 VirtualBox는 잘 계신지 확인해 봐야죠.
<img src="http://i.imgur.com/uFjPt61.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
잘 실행되고 계시네요.</p>

<h5 id="rubyonrails">Ruby On Rails 개발 환경 설정</h5>

<p>이제 Rails 개발 환경을 설정합니다.
개발 환경 설정은 GoRails를 참고하였습니다. GoRails 만세!!! <br>
<a href="https://gorails.com/setup/ubuntu/16.04">https://gorails.com/setup/ubuntu/16.04</a></p>

<pre><code># package 설치
sudo apt-get update  
sudo apt-get install git-core curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev python-software-properties libffi-dev

cd  
git clone https://github.com/rbenv/rbenv.git ~/.rbenv  
echo 'export PATH="$HOME/.rbenv/bin:$PATH"' &gt;&gt; ~/.bashrc  
echo 'eval "$(rbenv init -)"' &gt;&gt; ~/.bashrc  
exec $SHELL

# rbenv 설치
git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build  
echo 'export PATH="$HOME/.rbenv/plugins/ruby-build/bin:$PATH"' &gt;&gt; ~/.bashrc  
exec $SHELL

rbenv install 2.3.1  
rbenv global 2.3.1  
ruby -v

echo "gem: --no-ri --no-rdoc" &gt; ~/.gemrc

# bundler 설치
gem install bundler

# node.js 설정
curl -sL https://deb.nodesource.com/setup_4.x | sudo -E bash -  
sudo apt-get install -y nodejs

# rails 설치
gem install rails  
rbenv rehash  
rails -v  
</code></pre>

<p><img src="http://i.imgur.com/94njcjo.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>추가적으로 필요하다면 Git 설정을 합니다.</p>

<pre><code>git config --global color.ui true  
git config --global user.name "YOUR NAME"  
git config --global user.email "YOUR@EMAIL.com"  
ssh-keygen -t rsa -b 4096 -C "YOUR@EMAIL.com"  
</code></pre>

<p>아래 명령으로 id_rsa.pub 내용을 확인하고 복사합니다.</p>

<pre><code>cat ~/.ssh/id_rsa.pub  
</code></pre>

<p>GitHub에 접속하여 id_rsa.pub 내용을 등록합니다. <br>
<img src="http://i.imgur.com/X0xBiq8.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
<img src="http://i.imgur.com/MkzidM5.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
<img src="http://i.imgur.com/jZ53QRT.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>정상적으로 등록되었는지 아래 방법으로 테스트합니다.
<img src="http://i.imgur.com/RHEWKWA.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<h5 id="">테스트</h5>

<p>이제 GitHub에 있는 rails 예제 코드를 clone하여 실행해 봅니다.
참고로 여기에서 사용할 예제 코드는 Michael Hartl의 <a href="https://www.railstutorial.org/">RUBY ON RAILS TUTORIAL (RAILS 5)</a>입니다. <br>
8장까지 공부한 코드를 GitHub에 올렸고 그 예제 코드를 다운받아 테스트해 보겠습니다.</p>

<pre><code>cd /vagrant  
git clone https://github.com/jeongdeoksu/sample_app.git  
</code></pre>

<p><img src="http://i.imgur.com/rgLlOHa.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기">
<img src="http://i.imgur.com/1PuQbmL.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>Gemfile 중 postgresql을 사용하지 않을 것이므로 주석처리합니다. <br>
(참고로 Heroku에 배포하기 위해 pg gem을 사용합니다)
<img src="http://i.imgur.com/luJlvdi.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>필요한 Gem을 설치하고 DB를 설정합니다.</p>

<pre><code>bundle install  
rails db:create  
rails db:migrate  
</code></pre>

<p><code>rails server</code>를 실행합니다.
<strong>tmux</strong>에서 <code>rails server</code>를 실행하면 더욱 편리합니다. </p>

<pre><code>rails server -b 0.0.0.0  
</code></pre>

<p><img src="http://i.imgur.com/osZ1Os3.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>Windows에서 아래와 같이 접속해 봅니다. <br>
<img src="http://i.imgur.com/Glumr8Z.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>그리고 Atom에서 <code>D:\rails\sample_app</code>을 열어서<mark>(Add Project Foler 메뉴로 등록)</mark> 코딩을 계속합니다.
<img src="http://i.imgur.com/uROUh9f.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<blockquote>
  <p>Atom Editor에 관한 자세한 사용법은 gomugom님 블로그의 <a href="https://gomugom.github.io/atom-packages/">Atom Editor 영업하는 글</a>을 참고하세요. Atom에 관한 좋은 글입니다.</p>
</blockquote>

<h5 id="">몇가지 설정하면 편리한 것들</h5>

<p>Babun 홈의 .zshrc에 아래의 코드를 추가하여 <code>/cygdrive/d/rails</code>로의 이동을 편리하게 합니다.  </p>

<pre><code>vi .zshrc

# 아래 내용 추가
export RAILS_PATH="/cygdrive/d/rails"  
alias rails="cd ${RAILS_PATH}"  
</code></pre>

<p>이제 rails만 입력하면 <code>/cygdrive/d/rails</code>로 이동한다.
<img src="http://i.imgur.com/M8M7GZq.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<p>Vagant로 실행시킨 Ubuntu의 .profile에 아래 코드 추가합니다.</p>

<pre><code>vi .profile

# 아래 내용 추가 
export SAMPLE_APP_PATH="/vagrant/sample_app"  
alias samapp="cd ${SAMPLE_APP_PATH}"  
</code></pre>

<p><code>samapp</code> 을 입력하여 <code>/vagrant/sample_app</code> 디렉토리로 이동합니다.
<img src="http://i.imgur.com/WBeFBGU.png" alt="Windows에서 Babun, Vagrant 그리고 Virtualbox를 이용한 Rails 개발환경 구축하기"></p>

<h5 id="">마무리</h5>

<p>Windows가 부팅되면 Babun을 실행시키고 <br>
rails 명령을 입력하여 <code>/cygdrive/d/rails</code> 이동하여 <code>vagrant up</code>으로 <br>
Ubuntu 를 실행시킵니다. <br>
<code>vagrant ssh</code>로 접속한 후 <code>samapp</code>으로 이동하여 rails 관련 작업을 합니다. <br>
그리고 <code>Atom</code>을 실행하여 코딩합니다.</p>

<p>뭐 MacOS에 비해 조금은 불편하지만 완전 못쓸 정도는 아닙니다. 나름 제 생각에는 쾌적하다고 생각합니다. 물론 이 모든 것이 Babun이 있어서가 아닐까요?  </p>

<blockquote>
  <p>복기하면서 쓰다 보니 조금 정신없는 글이 되었네요. 물론 Rails 개발 환경 설치도  Chef로 자동화 할 수 있는 방법이 GoRails에 있지만 생각보다 잘 안되어서 시간 날때 다시 한번 해봐야곘습니다. <br>
  <a href="https://gorails.com/guides/using-vagrant-for-rails-development">Rails + Vagrant + Chef 참조</a></p>
</blockquote>]]></content:encoded></item><item><title><![CDATA[GCP Netwoking 101에 대한 이해]]></title><description><![CDATA[<p><code>Networking 101</code> codelab은 기본적인 네트워크 모니터링 툴 뿐만 아니라 Google Cloud Platform(이하 GCP) 네트워크 구조 전반에 대한 설명과 실습이 포함되어 있습니다. <br>
GCP  네크워크를 이해하는데 좋은 출발점이 될 수 있습니다.</p>

<p><a href="https://codelabs.developers.google.com/codelabs/cloud-networking-101/index.html?index=..%2F..%2Findex#0">https://codelabs.developers.google.com/codelabs/cloud-networking-101/index.html?index=..%2F..%2Findex#0</a></p>

<h4 id="networking101codelab">Networking 101 codelab 구성도</h4>

<p><img src="http://i.imgur.com/gp8geRL.png" alt="Imgur"></p>

<h4 id="">무엇을 배울 수</h4>]]></description><link>http://iamartin-gh.herokuapp.com/gcp-netwoking-101-codelab/</link><guid isPermaLink="false">1d764e20-2495-4bca-9919-9eb44e0ff47a</guid><category><![CDATA[google cloud platform]]></category><category><![CDATA[Network]]></category><category><![CDATA[Networking 101]]></category><category><![CDATA[codelab]]></category><category><![CDATA[tools]]></category><category><![CDATA[Load Balancer]]></category><category><![CDATA[firewall]]></category><category><![CDATA[TCP keepalive]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Thu, 10 Nov 2016 04:48:22 GMT</pubDate><media:content url="http://i.imgur.com/NC15TdG.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://i.imgur.com/NC15TdG.jpg" alt="GCP Netwoking 101에 대한 이해"><p><code>Networking 101</code> codelab은 기본적인 네트워크 모니터링 툴 뿐만 아니라 Google Cloud Platform(이하 GCP) 네트워크 구조 전반에 대한 설명과 실습이 포함되어 있습니다. <br>
GCP  네크워크를 이해하는데 좋은 출발점이 될 수 있습니다.</p>

<p><a href="https://codelabs.developers.google.com/codelabs/cloud-networking-101/index.html?index=..%2F..%2Findex#0">https://codelabs.developers.google.com/codelabs/cloud-networking-101/index.html?index=..%2F..%2Findex#0</a></p>

<h4 id="networking101codelab">Networking 101 codelab 구성도</h4>

<p><img src="http://i.imgur.com/gp8geRL.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h4 id="">무엇을 배울 수 있을까?</h4>

<ul>
<li>네트워크 관련 오픈 소스 툴을 사용하여 접속 이상유무와 성능을 측정하는 방법</li>
<li>Google Compute Engine이 속한 각각의 Region과 Zone 사이의 latency를 측정하는 방법</li>
<li>기본적인 로드 발란싱과 방화벽을 설정하는 방법</li>
<li>네트워크 관련 에러들을 troubleshoting하는 방법</li>
</ul>

<h4 id="codelab">Codelab 준비</h4>

<h6 id="project">신규 Project 생성</h6>

<p>GCP에 접속하여 신규 Project를 생성합니다. 여기서는 <code>my-networking-101-codelab</code>으로 신규 Project를 생성했습니다.</p>

<p><img src="http://i.imgur.com/7Fm1gu4.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>그리고 <code>Compute Engine API</code>를 사용하기 위하여 GCP Console에서 API Manager로 이동하여 <code>Compute Engine API</code>을 Enable 합니다.</p>

<p><img src="http://i.imgur.com/n6ZKChy.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/ariR4Ih.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/0a2A6oe.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h6 id="projectjumphost">Project 설정 및 jump host 생성</h6>

<p>방금 생성한 Project로 기본 project를 설정하고 jump host를 생성합니다.</p>

<pre><code>$ gcloud config set project &lt;your-project-name&gt;
$ gcloud compute instances create jumphost --scopes cloud-platform --zone us-central1-f --metadata startup-script-url=gs://nw101/startupscript.sh
</code></pre>

<p><code>startupscript.sh</code></p>

<script src="https://gist.github.com/iamartin-gh/d0986485f59618057501e7a4252224f9.js"></script>

<p><img src="http://i.imgur.com/BXxFl9w.png" alt="GCP Netwoking 101에 대한 이해"></p>

<blockquote>
  <p>jump host는 앞으로 각 region에 생성할 VM 모두에게 접근할 수 있는 VM 인스턴스이며 jump host 생성 시 실행되는 스크립트(<code>startupscript.sh</code>)는 codelab에 필요한 VM 인스턴스와 네트워크를 생성합니다. </p>
</blockquote>

<p><img src="http://i.imgur.com/wDxTfjZ.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h6 id="vmtool">생성한 VM 인스턴스에 필요한 Tool 설치</h6>

<p><img src="http://i.imgur.com/wh4EkN5.png" alt="GCP Netwoking 101에 대한 이해"></p>

<pre><code>$ gcloud compute ssh --zone &lt;vm-zone&gt; [vm-name] (e.g. gcloud compute ssh --zone us-central1-f us-vmc)

# Debian
$ sudo apt-get -y update
$ sudo apt-get -y install traceroute mtr tcpdump iperf whois host dnsutils

# CentOS
$ sudo yum check-update 
$ sudo yum -y install epel-release traceroute mtr tcpdump whois bind-utils
$ sudo yum -y install iperf
</code></pre>

<h5 id="vmlatency">VM 인스턴스 간의 latency 확인</h5>

<p>latency 확인을 위한 ping 사용법입니다.  </p>

<pre><code>$ ping -i0.2 us-vm2 #(sends a ping every 200ms)
$ sudo ping -i0.05 us-vm2 -c 1000 #(sends a ping every 50ms, 1000 times)
$ sudo ping -f -i0.05 us-vm2 #(flood ping, adds a dot for every sent packet, and removes one for every received packet)
$ # careful with flood ping without interval, it will send packets as fast as possible, which within the same zone is very fast
$ sudo ping -i0.05 us-vm2 -c 100 -s 1400 #(send larger packets, does it get slower?)
</code></pre>

<p><strong>재미있는 내용</strong> </p>

<pre><code>VM 1: us-vm1 (Council Bluffs, Iowa)  
VM 2: eu-vm (St. Ghislain, Belgium)  
Distance as the crow flies: 7197.57 km  
Ideal latency: 7197.57 km / 202562 km/s * 1000 ms/s * 2 = 71.07 ms  
</code></pre>

<p>먼저 전제는 <mark>광케이블의 신호가 초당 202562km를 이동</mark>한다는 것입니다.
이제 위의 Ideal latency 계산식을 한번 살펴보면, <br>
위 계산식은 초당(s) 202562km를 이동하므로 거리(7197.57km)를 이 값으로 나누고 ms단위로 계산하기 위해서 1000ms/s 를 곱하고 신호가 왕복이니까 2를 곱합니다. 이렇게 나온 latency 값이 71.07ms입니다.</p>

<p>실제로 측정된 latency가 아래와 같으므로 회선 상태 등을 고려하면 근접한 결과라고 할 수 있습니다.   </p>

<pre><code>Observed latency: 100.88 ms (minimum counts)  
</code></pre>

<blockquote>
  <p>GCP region과 zone 정보 : <a href="https://cloud.google.com/compute/docs/regions-zones/regions-zones#available">https://cloud.google.com/compute/docs/regions-zones/regions-zones#available</a></p>
</blockquote>

<p><img src="http://i.imgur.com/nYeHtia.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>위의 표를 보면 <code>Saint Ghislain</code> (europe-west1/벨기에)와 <code>Changhua County</code> ( asia-east1/타이완) 간의 거리가 더 가까운데 latency는 더 깁니다. <br>
이유는 europe-west1 region과 asia-east1 region 간의 직접적인 연결이 없기 때문입니. 결국 asia-east1에서 europe-west1 이동하려면 us-central1 거쳐서 가야합니다. <br>
이런 이유로 오직 하나의 글로벌한 서비스를 한다면 미국에 위치하는 게 latency가 가장 좋은 결과를 가져옵니다.</p>

<h6 id="traceroute">traceroute</h6>

<p>traceroute는 호스트간의 <code>Layer 3</code> (routing layer) hop을 보여줍니다.  </p>

<blockquote>
  <p>hop아린 데이타통신망에서 각 패킷이 매 노드(또는 라우터)를 건너가는 양상을 비유적으로 표현</p>
</blockquote>

<pre><code>$ traceroute www.icann.org
</code></pre>

<h6 id="mtr">mtr</h6>

<p>콘솔에서 호스트간의 <code>Layer 3</code> (routing layer) hop을 그래픽적으로 보여줍니다.</p>

<pre><code>$ mtr www.icann.org
</code></pre>

<p><img src="http://i.imgur.com/AgUJRbu.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h5 id="iperf">iperf을 이용한 네트워크 성능 측정</h5>

<p>eu-vm에서 서버 모드로 실행 (TCP)  </p>

<pre><code>$ iperf -s #run in server mode
</code></pre>

<p><img src="http://i.imgur.com/hA1T5pI.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>us-vm1에서 클라이언트 모드 실행하여 eu-vm에 접속 (TCP)  </p>

<pre><code>$ iperf -c eu-vm #run in client mode, connecting to eu-vm
</code></pre>

<p><img src="http://i.imgur.com/b6Qs6Un.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>Region간의 bandwidth는 <mark>Core 당 2 Gbit/s</mark> 의 제약이 있습니다.</p>

<p>eu-vm에서 서버 모드로 실행 (UDP)  </p>

<pre><code>$ iperf -s -u #iperf server side (-u UDP 포트)
</code></pre>

<p><img src="http://i.imgur.com/oGVAnpT.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>us-vm1에서 클라이언트 모드로 실행 (UDP)  </p>

<pre><code>$ iperf -c eu-vm -u -b 2G #iperf client side - send 2 Gbit/s
</code></pre>

<p><img src="http://i.imgur.com/mtqEDS3.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>위에서 측정한 네트워크 성능을 보면 TCP보다 UDP가 더 빠름을 알수 있습니다.</p>

<p>eu-vm에서 서버 모드로 실행  </p>

<pre><code>$ iperf -s
</code></pre>

<p>us-vm1에서 클라이언트 모드로 실행  </p>

<blockquote>
  <p>-P number of parallel client threads to run</p>
</blockquote>

<pre><code>$ iperf -c eu-vm -P 20
</code></pre>

<p><img src="http://i.imgur.com/RoLyApw.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>위의 성능 측정결과를 보면 Bandwith의 합이 달성 가능한 최대 Bandwith에 근접한 것을 알수 있다.</p>

<h5 id="interactivetcpdump">Interactive하게 tcpdump 실행 </h5>

<pre><code>$ sudo tcpdump -c 1000 -i eth0 not tcp port 22
</code></pre>

<p><strong>옵션</strong> <br>
-c : 카운트 1000개를 보여줌 <br>
-i : 모니터링 하고자 하는 interface 이름, 여기서는 eth0에서 들어오는 패킷의 정보를 보여준다.  </p>

<p>us-vm1에서 apache 웹서버를 설치 후 tcpdump를 실행합니다.  </p>

<pre><code>$ sudo apt-get -y install apache2

$ sudo tcpdump -i eth0 -c 1000 -s 1460 -w webserver.pcap tcp port 80
</code></pre>

<p><strong>옵션</strong> <br>
-s 1460 : 단지 header 만이 아니라 모든 패킷을 수집 <br>
-w 파일명 : 수집한 패킷을 지정한 파일에 저장  </p>

<p>us-vm2에서 <code>curl</code> 명령으로 apache 서버에 접속합니다.  </p>

<pre><code>$ curl us-vm1
$ curl us-vm1/404.php
</code></pre>

<h6 id="packetcapturewebserverpcap">Packet capture 파일인 webserver.pcap 분석</h6>

<pre><code>$ sudo tcpdump -nr webserver.pcap
</code></pre>

<p>단지 protocol, source 그리고 destination 정보를 보여준다.
<img src="http://i.imgur.com/zM4wAOE.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h6 id="cloudshark">CloudShark 이용하여 분석하기</h6>

<p>해당 project를 설정하고 저장된 Packet capture 파일을 복사합니다.</p>

<pre><code>$ gcloud config set project &lt;your-project-name&gt; #only if not done before
$ gcloud compute copy-files us-vm1:~/webserver.pcap webserver.pcap --zone us-central1-f
</code></pre>

<p>노트북에 Google Cloud SDK가 설치되어 있지 않다면 Google Storage에 저장합니다.</p>

<pre><code>$ gcloud compute copy-files us-vm1:~/webserver.pcap webserver.pcap --zone us-central1-f
</code></pre>

<p><img src="http://i.imgur.com/dwt1He8.png" alt="GCP Netwoking 101에 대한 이해"></p>

<pre><code>$ gsutil mb gs://username-lab #replace username with a unique username
$ gsutil cp webserver.pcap gs://username-lab/
</code></pre>

<p><img src="http://i.imgur.com/XWWQxlJ.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p><a href="https://www.cloudshark.org/">CloudShark</a>를 통해 분석합니다.
<img src="http://i.imgur.com/CkDZfP1.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/VJ3ZqqC.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h5 id="basicfirewallingandloadbalancing">Basic firewalling and load balancing</h5>

<h6 id="">방화벽 추가</h6>

<p>us-vmc에 nginx 설치 후에 81번 포트에 응답하도록 설정한다.  </p>

<pre><code>$ sudo su - 
# apt-get -y install nginx
# echo "server { listen 81; root /usr/share/nginx/html; }" &gt; /etc/nginx/sites-enabled/default
# service nginx restart
# curl localhost:81
# exit
$
</code></pre>

<p><code>81번 포트</code>에 대한 방화벽 룰 추가</p>

<ul>
<li>us-vmc 인스턴스에 대하여 nginx-81이라는 태그를 추가합니다.</li>
<li>nginx-81이라는 방화벽 룰을 생성합니다.</li>
</ul>

<pre><code>$ gcloud compute instances add-tags us-vmc --tags nginx-81 --zone us-central1-c
$ gcloud compute firewall-rules create nginx-81 --allow tcp:81 --network codelab --source-ranges 0.0.0.0/0 --target-tags nginx-81
</code></pre>

<h6 id="testnetworkloadbalancer13">Test Network Load Balancer 1/3</h6>

<p>각각의 instance에 apache webserver를 설치하는 스크립트를 실행합니다.</p>

<p>Debian  </p>

<pre><code>for a in us-vm1 us-vmc eu-vm asia-vm ; do  
  gcloud compute ssh --command "sudo apt-get -y install apache2" --zone `gcloud compute instances list $a --format text | grep zone: | awk '{print $2}'` $a; 
done  
</code></pre>

<p>CentOS  </p>

<pre><code>gcloud compute ssh --ssh-flag="-t" --command "sudo yum -y install httpd; sudo service httpd start" --zone us-central1-f us-vm2  
</code></pre>

<p>CentOS의 경우 apache DocumentRoot로 되어 있는 /var/www/html(비어있음)에 새로운 index.html을 만들지 않으면 /usr/share/httpd/noindex/index.html이 로드됩니다. <br>
문제는 load balancer에서 아래와 같이 오류로 인식한다
<img src="http://i.imgur.com/DojdtUy.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>오류를 해결하려면 /var/www/html에 index.html을 만듭니다.</p>

<p>codelab 네트워크에 있는 VM에서 아래 명령어 실행합니다.  </p>

<pre><code>for a in us-vm1 us-vm2 us-vmc eu-vm asia-vm ; do curl $a; done  
</code></pre>

<h6 id="usloadbalancer">US loadbalancer 생성</h6>

<p>80 포트에 대하여 방화벽을 오픈합니다.  </p>

<pre><code>$ gcloud compute firewall-rules create http --allow tcp:80 --network codelab
</code></pre>

<p>load balancer 설정</p>

<ul>
<li>http healthy 체크 : Path / 에 대하여 80 포트 헬스 체크</li>
<li>load balancer는 하나의 region(여기서는 US)에 있는 인스턴스들의 target-pool로 구성됩니다. 우선 target-pool을 생성하고 인스턴스를 추가합니다.</li>
<li>forwarding-rule을 추가합니다.</li>
</ul>

<pre><code>$ gcloud compute http-health-checks create basic-check
$ gcloud compute target-pools create apaches --region us-central1 --health-check basic-check
$ gcloud compute target-pools add-instances apaches --instances us-vm1,us-vm2 --zone us-central1-f
$ gcloud compute target-pools add-instances apaches --instances us-vmc --zone us-central1-c
$ gcloud compute forwarding-rules create interwebs --region us-central1 --port-range 80 --target-pool apaches
</code></pre>

<h6 id="testnetworkloadbalancer23">Test Network Load Balancer 2/3</h6>

<p>us-vm1, us-vmc, eu-vm, asia-vm 인스턴스에 대하여 index.html 에 host명을 입력하여 웹 접속 시 호스트명이 표시되게 설정합니다.  </p>

<pre><code>$ for a in us-vm1 us-vmc eu-vm asia-vm ; do gcloud compute ssh --command "sudo hostname | sudo tee /var/www/html/index.html &gt; /dev/null" --zone `gcloud compute instances list $a --format text | grep zone: | awk '{print $2}'` $a; done
</code></pre>

<h6 id="testnetworkloadbalancer33">Test Network Load Balancer 3/3</h6>

<p>eu-vm, asia-vm에 대한 target-instance 생성합니다.  </p>

<pre><code>$ gcloud compute target-instances create eu-target --instance eu-vm --zone europe-west1-d
$ gcloud compute forwarding-rules create interwebs-eu --region europe-west1 --port-range 80 --target-instance eu-target --target-instance-zone europe-west1-d
$ gcloud compute target-instances create asia-target --instance asia-vm --zone asia-east1-c
$ gcloud compute forwarding-rules create interwebs-asia --region asia-east1 --port-range 80 --target-instance asia-target --target-instance-zone asia-east1-c
</code></pre>

<h6 id="testhttploadbalancer">Test HTTP Load Balancer</h6>

<p>아래 아키텍쳐가 글로벌하게 웹서비스를 구성하는 방법이다.
<img src="http://i.imgur.com/HmCOMf6.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>요청이 들어와서 instance에 전달되는 순서와 반대로 서비스를 구성하면 됩니다.</p>

<ol>
<li>Instance Group  </li>
<li>Backend Service  </li>
<li>URL Map  </li>
<li>Target Proxy  </li>
<li>Global Forwarding Rule  </li>
</ol>

<h6 id="instancegroup">Instance Group</h6>

<p>모든 zone에 대하여 unmanaged instance group(non-autoscaled) <code>instance-groups unmanaged create</code> 을 생성합니다. <br>
생성된 instance group에 해당 zone에 있는 instance를 추가 <code>instance-groups unmanaged add-instances</code> 합니다.
instance group에 대하여 named port를 http:80으로 설정 <code>instance-groups unmanaged set-named-ports</code> 합니다.  </p>

<pre><code>$ gcloud compute instance-groups unmanaged create us-f --zone us-central1-f
$ gcloud compute instance-groups unmanaged create us-c --zone us-central1-c
$ gcloud compute instance-groups unmanaged create eu --zone europe-west1-d
$ gcloud compute instance-groups unmanaged create asia --zone asia-east1-c
$ gcloud compute instance-groups unmanaged add-instances us-f --instances us-vm1,us-vm2 --zone us-central1-f
$ gcloud compute instance-groups unmanaged add-instances us-c --instances us-vmc --zone us-central1-c
$ gcloud compute instance-groups unmanaged add-instances eu --instances eu-vm --zone europe-west1-d
$ gcloud compute instance-groups unmanaged add-instances asia --instances asia-vm --zone asia-east1-c
$ gcloud compute instance-groups unmanaged set-named-ports us-f --named-ports http:80 --zone us-central1-f
$ gcloud compute instance-groups unmanaged set-named-ports us-c --named-ports http:80 --zone us-central1-c
$ gcloud compute instance-groups unmanaged set-named-ports eu --named-ports http:80 --zone europe-west1-d
$ gcloud compute instance-groups unmanaged set-named-ports asia --named-ports http:80 --zone asia-east1-c
</code></pre>

<h6 id="backendservice">Backend Service</h6>

<p>global-bs란 이름의 backend service 를 생성합니다. <br>
backend service에 instance group을 추가합니다. <br>
global-map란 이름의 url-map 생성합니다.</p>

<pre><code>$ gcloud compute backend-services create global-bs --protocol http --http-health-checks basic-check
$ gcloud compute backend-services add-backend global-bs --instance-group us-f --instance-group-zone us-central1-f
$ gcloud compute backend-services add-backend global-bs --instance-group us-c --instance-group-zone us-central1-c
$ gcloud compute backend-services add-backend global-bs --instance-group eu --instance-group-zone europe-west1-d
$ gcloud compute backend-services add-backend global-bs --instance-group asia --instance-group-zone asia-east1-c
$ gcloud compute url-maps create global-map --default-service global-bs
</code></pre>

<h6 id="urlmap">URL Map</h6>

<p>요청이 load balancer로 들어올때 <code>URL Map</code>의 설정에 따라 backend로 라우팅을 합니다. <br>
이때 <code>URL Map</code>은 목적지의 URL에 있는 host값(example.com)과 path값(/path)을 가지고 그 요청에 해당되는 적절한 backend service로 전달합니다. 
<img src="http://i.imgur.com/RqC1nNi.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/wXAsvq0.png" alt="GCP Netwoking 101에 대한 이해" title=""> <br>
아래 그림은 실제로 요청에 포함된 path 값에 따라 어떻게 backend service로 전달되는지 알수 있습니다.
<img src="http://i.imgur.com/SP2gMhb.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>요청에 포함된 특정 path와 backend service를 연결하는 방법은 아래와 같습니다.</p>

<pre><code>gcloud compute url-maps add-path-matcher URL_MAP \  
    --default-service BACKEND_SERVICE \
    --path-matcher-name PATH_MATCHER \
    [--path-rules PATH=SERVICE]
--path-rules /video=video-service,/video/*=video-service
https://cloud.google.com/compute/docs/load-balancing/http/url-map  
</code></pre>

<h6 id="targetproxy">target proxy</h6>

<p>global-proxy란 이름의 target proxy 를 생성하고 GLOBAL IP에 대하여 forwarding rule을 설정합니다. <br>
target proxy는 하나 이상의 global forwarding rule을 참조하여 들어오는 HTTP와 HTTPS 요청을 URL Map으로 라우팅한다.</p>

<pre><code>gcloud compute target-http-proxies create [HTTP_PROXY] \  
 — url-map URL_MAP [ — description [DESCRIPTION]]
gcloud compute target-https-proxies create [HTTPS_PROXY] \  
 — url-map URL_MAP — ssl-certificate [SSL_CERTIFICATES] \
 [ — description [DESCRIPTION]]
$ gcloud compute target-http-proxies create global-proxy --url-map global-map
$ gcloud compute forwarding-rules create global-lb --global --target-http-proxy global-proxy --ports 80
</code></pre>

<h6 id="globalforwardingrules">Global forwarding rules</h6>

<p>global forwarding rule은 서비스하는 site의 DNS Record에 사용할 수 있는 global IP address 하나를 제공합니다. 또한 target proxy와 URL map의 load balancing 설정에 포함된  IP address, port, and protocol 등을 통해 instance group으로 라우팅합니다. <br>
Global forwarding rule은 단지 HTTP/HTTPS load balancer에만 사용할 수 있고 하나의 global forwarding rule은 하나의 포트로만 전달할 수 있습니다.  </p>

<pre><code>gcloud compute forwarding-rules create  
 FORWARDING_RULE — global
 [ — address ADDRESS]
 [ — description DESCRIPTION]
 [ — ip-protocol IP_PROTOCOL]
 [ — port-range PORT]
 [ — target-http-proxy TARGET_HTTP_PROXY | — target-https-proxy TARGET_HTTPS_PROXY ]
</code></pre>

<p><img src="http://i.imgur.com/zv21Oec.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/S49Stri.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/zRDqqqt.png" alt="GCP Netwoking 101에 대한 이해">
<img src="http://i.imgur.com/Eyf9onD.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h6 id="apache">apache 로그 설정 변경</h6>

<p>apache의 access.log를 확인합니다.  </p>

<pre><code>$ tail -10 /var/log/apache2/access.log
</code></pre>

<p>130.211.1.214 — — [30/Sep/2015:11:08:28 +0000] “GET / HTTP/1.1” 200 316 “-” “GoogleHC/1.0”</p>

<p>Debian VMs (all VMs but us-vm2)  </p>

<pre><code>$ sed -e 's/%h/%{X-Forwarded-For}i/' /etc/apache2/apache2.conf | sudo tee /etc/apache2/apache2.conf &gt; /dev/null
$ sudo service apache2 reload
</code></pre>

<p>CentOS VM (us-vm2)  </p>

<pre><code>$ sudo sed -e 's/%h/%{X-Forwarded-For}i/' /etc/httpd/conf/httpd.conf | sudo tee /etc/httpd/conf/httpd.conf &gt; /dev/null
$ sudo service httpd reload
</code></pre>

<h4 id="httpsloadbalancer"> HTTPS load balancer 테스트</h4>

<p>self-signed certificate 생성</p>

<ul>
<li>generate a private key</li>
<li>generate a signing request for the public certificate matching the key</li>
<li>sign the request using the key we just created</li>
</ul>

<pre><code>$ mkdir ssl
$ cd ssl
$ openssl genrsa -out my.key 2048
$ openssl req -new -key my.key -out my.csr #Enter data at each prompt except PW
$ openssl x509 -req -days 365 -in my.csr -signkey my.key -out my.crt
</code></pre>

<p>ssl certificate 생성  </p>

<pre><code>$ sudo gcloud compute ssl-certificates create ssl --certificate my.crt --private-key my.key
</code></pre>

<p>target https proxy와  forwarding rule 생성  </p>

<pre><code>$ gcloud compute target-https-proxies create ssl-proxy --url-map global-map --ssl-certificate ssl
$ gcloud compute forwarding-rules create ssl-lb --global --target-https-proxy ssl-proxy --port 443
</code></pre>

<h6 id="">테스트</h6>

<p>eu-vm에 접속하여 아래 명령으로 접속 시 가장 가까운 eu-vm으로 접속합니다.  </p>

<pre><code>curl Global external IP address  
</code></pre>

<p>아래와 같이 eu-vm으로만 연결되고 있습니다.
<img src="http://i.imgur.com/8I4ohIO.png" alt="GCP Netwoking 101에 대한 이해"></p>

<p>eu-vm의 apache 웹서버를 종료하고 다시 접속을 해봅니다. <br>
아래와 같이 asia-vm, us-vm1, us-vm2 로 골고루 접속되는 것을 알수 있다. 지리적인 위치가 asia와 us가 비슷한 듯 보인다.
<img src="http://i.imgur.com/JcFYfVo.png" alt="GCP Netwoking 101에 대한 이해"></p>

<h6 id="tcpkeepalivesettings">TCP keepalive settings</h6>

<p>GCP firewall은 connection table에 단지 10분동안 idle TCP connection을 유지합니다. <br>
다시 말하면 10분안에 패킷이 들어오거나 나가는 패킷이 없다면 firewall에서 session이 제거됩니다.
Compute Engine instance에 ssh로 접속하고 10분동안 어떤 작업도 하지 않는다면 10분 후에는 GCP firewall에서 연결을 끊어버리고 터미널은 hang이 걸리게 됩니다. <br>
아래 설정은 TCP keepalive을 정기적으로 (1분) 세션을 유지 할수 있도록 설정을 변경합니다.</p>

<pre><code>sudo /sbin/sysctl -w net.ipv4.tcp_keepalive_time=60 net.ipv4.tcp_keepalive_intvl=60 net.ipv4.tcp_keepalive_probes=5  
</code></pre>]]></content:encoded></item><item><title><![CDATA[Kubernetes Dashboard 설치합니다.]]></title><description><![CDATA[<p>Kubernetes가 잘 동작하기는 하는데 계속 명령어로 상태를 확인하기도 그렇고 뭔가 한눈에 보이면 굉장히 편할 것 같기도 해서 Dashboard를 설치해 보기로 했습니다. <br>
Dashboard 설치도 초간단합니다.</p>

<p><a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard">https://github.com/kubernetes/dashboard#kubernetes-dashboard</a></p>

<p>먼저 master에 ssh로 접속합니다. 그리고 Dashboard가 설치되어 있는지 아래 명령으로 확인합니다.</p>

<pre><code>gcloud compute ssh kube-master-1  
kubectl get pods --all-namespaces | grep</code></pre>]]></description><link>http://iamartin-gh.herokuapp.com/kubernetes-dashboard-install/</link><guid isPermaLink="false">d49de349-8929-4571-975f-a0d585e5d5c6</guid><category><![CDATA[install]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[dashboard]]></category><category><![CDATA[weavescpoe]]></category><category><![CDATA[weave cloud]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Tue, 08 Nov 2016 13:52:25 GMT</pubDate><content:encoded><![CDATA[<p>Kubernetes가 잘 동작하기는 하는데 계속 명령어로 상태를 확인하기도 그렇고 뭔가 한눈에 보이면 굉장히 편할 것 같기도 해서 Dashboard를 설치해 보기로 했습니다. <br>
Dashboard 설치도 초간단합니다.</p>

<p><a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard">https://github.com/kubernetes/dashboard#kubernetes-dashboard</a></p>

<p>먼저 master에 ssh로 접속합니다. 그리고 Dashboard가 설치되어 있는지 아래 명령으로 확인합니다.</p>

<pre><code>gcloud compute ssh kube-master-1  
kubectl get pods --all-namespaces | grep dashboard  
</code></pre>

<p>설치가 되어있지 않다면 아래 명령으로 Dashboard를 설치합니다.</p>

<pre><code>kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml  
</code></pre>

<p><img src="http://i.imgur.com/KYyIT8G.png" alt="Imgur"></p>

<p>kubernetes-dashboard의 서비스 포트를 확인합니다.  </p>

<pre><code>kubectl describe svc kubernetes-dashboard -n kube-system  
</code></pre>

<p><img src="http://i.imgur.com/1AxPEm3.png" alt="Imgur">
<img src="http://i.imgur.com/5bAkaCC.png" alt="Imgur"></p>

<p>31023 포트를 사용하는 군요. 방화벽에서 해당 포트를 오픈합니다. <br>
<a href="http://blog.iamartin.com/kubernetes-install/">방화벽 포트 오픈은 Kubernetes 설치를 참조합니다.</a></p>

<p>웹브라우저에서 접속하면 아래와 같이 Dashboard가 펼쳐집니다.</p>

<pre><code>http://104.198.120.244:31023/  
</code></pre>

<p><img src="http://i.imgur.com/ddGgPk7.png" alt="Imgur"></p>

<p>추가적으로 <code>Weave Scope</code>라는 재미있는 기능을 설치해 보도록 하겠습니다.
Kubernetes Dashboard와 기능은 유사하지만 시각적으로 보기좋게 표현해줘서 정말 편하고 재미있습니다.</p>

<p><a href="https://cloud.weave.works/login">Weave Cloud</a>에 로그인합니다. 계정이 없다면 <a href="https://cloud.weave.works/signup">Weave Cloud 계정을 등록</a>합니다.
<img src="http://i.imgur.com/gnTFYXI.png" alt="Imgur"></p>

<p>로그인 후 새로운 인스턴스를 생성합니다. 여기에서는 kubernetes-demo로 하였습니다.
<img src="http://i.imgur.com/ZwVy8yQ.png" alt="Imgur"></p>

<p>우측 상단의 Service Token을 확인합니다. Service Token은 <code>Weave Scope</code> 설치 시 필요합니다.
<img src="http://i.imgur.com/bprasd2.png" alt="Imgur"></p>

<p>이제 아래의 명령으로 <code>Weave Scope</code>를 설치합니다.
설치에 관한 자세한 내용은 아래 링크를 확인합니다. <br>
<a href="https://www.weave.works/documentation/scope-latest-installing/#k8s">https://www.weave.works/documentation/scope-latest-installing/#k8s</a></p>

<pre><code>$ kubectl apply -f 'https://cloud.weave.works/launch/k8s/weavescope.yaml?service-token=&lt;token&gt;'
</code></pre>

<p><img src="http://i.imgur.com/ZIYWwbh.png" alt="Imgur"></p>

<p>일반적으로 node 수만큼 <code>weavescope-probe</code>가 생성됩니다.</p>

<p><img src="http://i.imgur.com/Zwq65rQ.png" alt="Imgur"></p>

<p>그리고 아래와 같이 Probe가 정상적으로 등록됩니다.
<img src="http://i.imgur.com/AOMdsry.png" alt="Imgur"></p>

<p><mark>VIEW INSTANCE</mark>를 선택하여 View로 들어갑니다.</p>

<p>host와 container 그리고 pod 별로 잘 시각화해서 보여줍니다.</p>

<p><img src="http://i.imgur.com/o6DXfrQ.png" alt="Imgur">
<img src="http://i.imgur.com/bVmpeLc.png" alt="Imgur">
<img src="http://i.imgur.com/MDYsAou.png" alt="Imgur"></p>

<p>CPU와 메모리 사용률도 보이고 이렇게 보니 뭔가 있어보이기도 하네요. <br>
그리고 이렇게 그룹별로 보이니 상호연관성도 확인하기 쉽고 흐름도 어떻게 되는 지 알기쉽습니다. 하지만 딱 거기까지네요. </p>]]></content:encoded></item><item><title><![CDATA[HAProxy를 이용한 Internal Load Balancer 구성 따라하기]]></title><description><![CDATA[<h6 id="">유의사항</h6>

<p>아래의 원문을 스터디 목적으로 따라하면서 작성한 문서여서 용어도 부정확하고 읽기에 거북할 수 있습니다.  </p>

<p><strong>원문</strong> : <a href="https://cloud.google.com/solutions/internal-load-balancing-haproxy">https://cloud.google.com/solutions/internal-load-balancing-haproxy</a></p>

<h6 id="">따라하면서 배울 수 있는 것은 무엇인가?</h6>

<ul>
<li><code>load balancer</code>가 무엇인지 대충은 알 수 있습니다.   </li>
<li>Goole Compute Engine에서 gcloud명령으로 instance를 배포하는 방법과 instance 배포 시 외부 IP를 할당하지 않는 방법을</li></ul>]]></description><link>http://iamartin-gh.herokuapp.com/gcp-haproxy-internal-lb/</link><guid isPermaLink="false">019155e8-c98e-47f8-8398-30ab782934f0</guid><category><![CDATA[gcp]]></category><category><![CDATA[HAProxy]]></category><category><![CDATA[Internal Load Balancer]]></category><category><![CDATA[systemd]]></category><category><![CDATA[Google Compute Engine]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Sun, 06 Nov 2016 15:19:49 GMT</pubDate><content:encoded><![CDATA[<h6 id="">유의사항</h6>

<p>아래의 원문을 스터디 목적으로 따라하면서 작성한 문서여서 용어도 부정확하고 읽기에 거북할 수 있습니다.  </p>

<p><strong>원문</strong> : <a href="https://cloud.google.com/solutions/internal-load-balancing-haproxy">https://cloud.google.com/solutions/internal-load-balancing-haproxy</a></p>

<h6 id="">따라하면서 배울 수 있는 것은 무엇인가?</h6>

<ul>
<li><code>load balancer</code>가 무엇인지 대충은 알 수 있습니다.   </li>
<li>Goole Compute Engine에서 gcloud명령으로 instance를 배포하는 방법과 instance 배포 시 외부 IP를 할당하지 않는 방법을 알수 있습니다.</li>
<li>image와 template을 생성하여 instance를 배로하는 방법을 알 수 있습니다.      </li>
<li>instance groups를 만드는 방법을 알 수 있습니다. <mark>(중요)</mark></li>
<li>가장 기본적인 HAProxy 설정 방법을 알 수 있습니다.</li>
<li>instance-instance ssh 접속 방법을 알 수 있습니다.</li>
<li>systemd를 이용한 서비스 자동 재시작 방법을 알 수 있습니다.</li>
</ul>

<p><code>load balancer</code>는 들어오는 네트워크 트래픽을 받아서 애플리케이션 서버 그룹에 배포합니다. 
예를 들면 Google Compute Engine Load Balancing service는 인터넷으로 들어오는 네트워크 트래픽을 Compute Engine instances 그룹으로 배포합니다. <br>
아래 그림은 Compute Engine Network Load Balancing이 어떻게 운영되는지 보여줍니다.</p>

<p><img src="http://i.imgur.com/brzTpbt.png" alt="Imgur"></p>

<p>Internal load balancer는 인터넷에 노출되지 않은 private network에 트래픽을 분배합니다. <br>
Internal load balancer는 모든 트래픽이 private network에서 흐르는 구조에서도 유용하지만 Frontend Server가 요청을 받아서 private network에 있는 Backend server들에게 보내주는 복잡한 웹 애플리케이션에 더 적합하고 유용합니다. <br>
Compute Engine에서는 Software load balancer를 사용하여 Internal load balancer를 구성할 수 있으며 여기서는 Compute Engine instance에 open source software load balancer인 <code>HAProxy</code>를 사용하여 구성합니다. <br>
아래 그림은 Google Cloud Platform에서 Internal load balancer로 HAProxy를 어떻게 사용하는지 보여줍니다.</p>

<p><img src="http://i.imgur.com/5b7Rdvh.png" alt="Imgur"></p>

<p>위의 그림처럼 instance 위에서 Internal load balancer로 HAProxy를 운영하면 Internal load balancer가 <mark>단일 장애 지점(single point of failure)</mark>이 되므로 이를 완화하기 위하여 아래와 같이 instance 자체에 문제가 발생할 때 해당 instance를 새롭게 배포하고 HAProxy Service에 오류 발생하면 서비스를 재시작할 수 있도록 기능을 추가합니다.</p>

<ul>
<li>Instance Group Manager가 HAProxy의 Compute Engine <strong>instance</strong>의 상태를 모니터링하고 있다가 해당 instance가 멈추면 새로 배포합니다.</li>
<li>HAProxy load balancer <strong>service</strong>가 항상 동작하도록 보증하기 위해서 systemd를 이용해 오류 이벤트가 발생하면 HAProxy service를 재시작합니다.</li>
</ul>

<h4 id="">목표</h4>

<ul>
<li>3대의 Apache Backend Server 구성</li>
<li>HAProxy가 운영되고 있는 Compute Engine instance를 managed instance group로 구성</li>
<li>테스트를 위해 외부 IP를 가지는 Compute Engine micro instance 구성</li>
<li>backend Apache server들에게 트래픽을 전달할 수 있도록 HAProxy 설정</li>
</ul>

<blockquote>
  <p>참고</p>
  
  <ul>
  <li><p>Unmanaged Group <br>
  인스턴스를 수동으로 묶어 놓은 그룹. 인스턴스의 크기나 종류에 상관 없이 그냥 묶어만 놓은 것이며 수동으로 아무 종류의 인스턴스나 추가해넣을 수 있기 때문에 오토 스케일링은 불가능합니다. 비관리 그룹을 이해하려면 반대로 관리 그룹 개념을 이해하면 쉽습니다.  </p></li>
  <li><p>Managed Group <br>
  <mark>템플릿을 만들어 인스턴스 생성을 클라우드가 담당하게 하며</mark> 인스턴스는 사용자가 직접 추가할 수 없습니다. 템플릿이란 인스턴스를 만들기 위한 틀과 같은 것으로 VM의 크기, OS 이미지 등의 설정을 미리 정해놓으면, 클라우드가 인스턴스를 생성할 때 해당 템플릿이 정의한 설정에 따라서 인스턴스를 추가하게 됩니다.</p></li>
  </ul>
</blockquote>

<h4 id="">사전에 해야 할 일</h4>

<p><a href="https://console.cloud.google.com/project">Google Cloud Platform Console</a>에서 project를 새로 생성합니다.
<img src="http://i.imgur.com/Lc5OHIb.png" alt="Imgur"></p>

<p>Google Compute Engine API와 Google Compute Engine Instance Groups API를 사용 가능(ENABLE)하게 설정합니다. <br>
<img src="http://i.imgur.com/XZzVwek.png" alt="Imgur"></p>

<p><a href="https://cloud.google.com/sdk/docs/">Google Cloud SDK</a>를 설치한 후 project 및 region, zone을 설정합니다.</p>

<pre><code>me@local:~$ gcloud config set project project-id  
me@local:~$ gcloud config set compute/zone my-zone  
</code></pre>

<p><img src="http://i.imgur.com/jorVwkI.png" alt="Imgur"></p>

<h4 id="backendapacheservergroup">backend Apache server Group 생성</h4>

<h6 id="apacheservercustominstance">Apache Server가 포함된 custom instance 생성</h6>

<p>debian-8을 기반으로하는 Compute Engine instance 생성합니다.  </p>

<pre><code>gcloud compute instances create apache-base --image debian-8  
</code></pre>

<p><img src="http://i.imgur.com/LirtdKq.png" alt="Imgur"></p>

<p>생성된 instance에 Apache2를 설치합니다.</p>

<pre><code>me@local:~$ gcloud compute ssh apache-base  
apache-base:~$ sudo apt-get update &amp;&amp; sudo apt-get install -y apache2  
apache-base:~$ exit  
</code></pre>

<p><img src="http://i.imgur.com/YN8Sy2B.png" alt="Imgur">
boot disk를 남겨두고 생성했던 instance를 삭제합니다.  </p>

<pre><code>me@local:~$ gcloud compute instances delete apache-base --keep-disks boot  
</code></pre>

<p><img src="http://i.imgur.com/5M1d70O.png" alt="Imgur"></p>

<p>아래와 같이 instance는 삭제되었지만 apahce-base의 disk는 그대로 있습니다.
<img src="http://i.imgur.com/CCwMjrI.png" alt="Imgur"></p>

<p>위의 boot 디스크로 부터 이미지를 생성합니다.</p>

<pre><code>me@local:~$ gcloud compute images create apache-base-image --source-disk apache-base  
</code></pre>

<p><img src="http://i.imgur.com/wesHrAD.png" alt="Imgur"></p>

<h6 id="customimagebackendapacheserver3">custom image로 부터 backend Apache Server 3대 생성</h6>

<pre><code>me@local:~$ gcloud compute instances create apache1 apache2 apache3 --image \  
apache-base-image --no-address --metadata startup-script='#! /bin/bash  
HOSTNAME=$(hostname)  
sudo cat &lt;&lt; EOF | sudo tee /var/www/html/index.html  
This is $HOSTNAME  
EOF'  
</code></pre>

<ul>
<li>--no-address : 대상 instance가 외부 IP 가지지 않음</li>
<li>--metadata : startup script 포함
statrup script는 /var/www/html/index.html에 자신의 호스트 이름을 등록하여 각 backend apache server를 구별할 수 있도록 설정합니다.</li>
</ul>

<p><img src="http://i.imgur.com/6ZhKbVX.png" alt="Imgur"></p>

<h4 id="microinstance">테스트를 위한 micro instance 생성</h4>

<p>바로 전에 생성한 backend server들과 생성할 Internal load balancer는 외부 IP가 없기때문에 외부 IP를 가지고 있는 micro instance를 생성하여 backend server들과 load balancer를 테스트합니다.</p>

<p>f1-micro 타입의 test instance 생성합니다.  </p>

<pre><code>me@local:~$ gcloud compute instances create test-instance --machine-type f1-micro  
</code></pre>

<p><img src="http://i.imgur.com/6RJZp1D.png" alt="Imgur"></p>

<p>SSH로 test-instance 접속하여 curl 명령으로 backend web server를 확인합니다.</p>

<pre><code>gcloud compute ssh test-instance  
me@test-instance:~$ curl apache1  
me@test-instance:~$ curl apache2  
me@test-instance:~$ curl apache3  
me@test-instance:~$ exit  
</code></pre>

<p><img src="http://i.imgur.com/VYyDH6Q.png" alt="Imgur"></p>

<h4 id="internalloadbalancer">Internal load balancer 생성</h4>

<p>Internal load balance의 단일 장애 지점의 위험을 줄이기 위해 Compute Engine instance와 HAProxy service에 대하여 서비스를 지속적으로 유지할 수 있는 방안을 마련할 필요가 있습니다.</p>

<p>Compute Engine instance가 계속 가동되는 것을 보장하기 위하여 <code>managed instance group</code>를 사용합니다. <br>
HAProxy instance가 <code>managed instance group</code>에 있으면 Instance Group Manage는 서버의 상태를 체크하여 만약 중지되면 새로 생성합니다.  </p>

<blockquote>
  <p>Instance Group Manager는 gcloud compute instances 명령 (또는 Compute Engine API)으로 해당 instance를 중지시키거나 삭제할 때 조차도 다시 생성됩니다. <br>
  그러나 gcloud compute instance-groups managed delete-instances 명령 (또는 Instance Group Manager API)을 사용하여 해당 instance를 삭제한다면 더 이상 다시 생성되지 않습니다.</p>
</blockquote>

<h6 id="haproxyinstancebaseimage">HAProxy instance base image 생성</h6>

<p>debian-8을 기반으로 Compute Engine instance 생성합니다.  </p>

<pre><code>me@local:~$ gcloud compute instances create haproxy-base --image debian-8  
</code></pre>

<p><img src="http://i.imgur.com/Fn0gbDH.png" alt="Imgur"></p>

<p>HAProxy를 설치합니다.  </p>

<pre><code>me@local:~$ gcloud compute ssh haproxy-base  
haproxy-base:~$ sudo apt-get update &amp;&amp; sudo apt-get -y install haproxy  
</code></pre>

<p><img src="http://i.imgur.com/L7UK1Fl.png" alt="Imgur"></p>

<p>아래와 같이 backend server 리스트를 HAProxy configuration에 추가를 통해 HAProxy를 설정합니다.</p>

<pre><code>haproxy-base:~$ echo -e "\n\n# Listen for incoming traffic  
listen apache-lb *:80  
    mode http
    balance roundrobin
    option httpclose
    option forwardfor
    server apache1 internal-ip:80 check
    server apache2 internal-ip:80 check
    server apache3 internal-ip:80 check" | sudo tee -a /etc/haproxy/haproxy.cfg
</code></pre>

<blockquote>
  <p>위 설정 중 internal-ip는 backend server의 Internal IP를 설정합니다.</p>
</blockquote>

<p><img src="http://i.imgur.com/Rh41il4.png" alt="Imgur"></p>

<p><img src="http://i.imgur.com/reLPMkk.png" alt="Imgur"></p>

<h6 id="haproxyservice">HAProxy service 가 언제나 재시작하도록 설정</h6>

<pre><code>haproxy-base:~$ sudo vi /lib/systemd/system/haproxy.service  
</code></pre>

<p>아래 설정을 확인하여 없으면 추가합니다.</p>

<pre><code>[Service]
Environment=CONFIG=/etc/haproxy/haproxy.cfg  
...
Restart=always  
</code></pre>

<p><img src="http://i.imgur.com/IbPfGgf.png" alt="Imgur"></p>

<h6 id="customimage">Custom image 생성</h6>

<p>boot disk를 남겨두고 생성했던 instance를 삭제합니다.  </p>

<pre><code>me@local:~$ gcloud compute instances delete haproxy-base --keep-disks boot  
</code></pre>

<p>source disk로부터 custom image를 생성합니다.  </p>

<pre><code>me@local:~$ gcloud compute images create haproxy-base-image --source-disk haproxy-base  
</code></pre>

<p><img src="http://i.imgur.com/VlpcxkL.png" alt="Imgur"></p>

<h6 id="haproxyinstancetemplate">HAProxy instance template 생성</h6>

<pre><code>me@local:~$ gcloud compute instance-templates create haproxy-template \  
 --image haproxy-base-image --no-address
</code></pre>

<p><img src="http://i.imgur.com/CLAQVSu.png" alt="Imgur"></p>

<blockquote>
  <p>--no-address옵션으로 template를 생성하면 외부 IP를 할당하지 않습니다.</p>
</blockquote>

<h6 id="haproxymanagedinstancegroup">haproxy란 이름으로 managed instance group 생성</h6>

<pre><code>me@local:~$ gcloud compute instance-groups managed create haproxy \  
--base-instance-name haproxy-server --size 1 --template haproxy-template
</code></pre>

<blockquote>
  <p>--base-instance-name 옵션은 Instance Group Manager가 instance를 생성할 때 할당되는 이름를 설정합니다.
  통상적으로 haproxy-server-xxxx 이런 형식의 이름이 생성됩니다.</p>
</blockquote>

<p><img src="http://i.imgur.com/R8we0cU.png" alt="Imgur"></p>

<h4 id="haproxyloadbalancer">HAProxy 이름 확인 및 load balancer 테스트</h4>

<p>HAProxy instance의 이름을 확인 후 test-instance에 SSH로 접속합니다.</p>

<pre><code>me@local:~$ gcloud compute instances list | grep -oE "haproxy-server-[a-z0-9]{4}"  
me@local:~$ gcloud compute ssh test-instance  
</code></pre>

<p>test-instance에서 curl 명령으로 웹서버가 정상적으로 동작하는지 확인합니다.  </p>

<pre><code>me@test-instance:~$ curl haproxy-server-xxxx  
</code></pre>

<p><img src="http://i.imgur.com/jgiNA45.png" alt="Imgur"></p>

<blockquote>
  <p>haproxy.cfg 설정에 balance를 roundrobin으로 되어 있어서 curl 명령어 실행 시 apache1 부터 차례로 호출되는 것을 볼수 있습니다.</p>
</blockquote>

<h4 id="loadbalancer">load balancer 복구 테스트</h4>

<h6 id="haproxyinstance">HAProxy instance 테스트</h6>

<p>gcloud compute instances 명령으로 HAProxy instance를 중지시키고 haproxy-server-xxxx란 이름으로 HAProxy instance가 교체되는 지 확인합니다.</p>

<pre><code>me@local:~$ gcloud compute instances stop haproxy-server-xxxx  
</code></pre>

<p><img src="http://i.imgur.com/IyuXZrB.png" alt="Imgur"></p>

<p>몇분을 기다린 후 HAProxy instance가 정상적으로 동작하는 지 확인합니다.</p>

<pre><code>me@local:~$ gcloud compute instances list  
</code></pre>

<p><img src="http://i.imgur.com/TKOw7Gt.png" alt="Imgur">
<img src="http://i.imgur.com/bNMtl1l.png" alt="Imgur"></p>

<p>test-instance에 접속해서 HAProxy instance가 정상적으로 동작하는 지 확인합니다. <br>
<img src="http://i.imgur.com/78PSx6i.png" alt="Imgur"></p>

<h6 id="haproxyservice">HAProxy service 테스트</h6>

<p>HAProxy service를 테스트하기 위해서 test-instance에 SSH로 접속 후 HAProxy instance로 SSH 접속합니다. <a href="https://cloud.google.com/compute/docs/instances/connecting-to-instance#sshbetweeninstances">instance to instance ssh</a> 접속을 하기 위해서는 아래 과정을 거쳐야 합니다.</p>

<p>아래와 같이 instance to instance ssh 설정을 합니다.</p>

<pre><code>me@local:~$ eval `ssh-agent`  
me@local:~$ ssh-add ~/.ssh/google\_compute\_engine  
</code></pre>

<p>ssh-flag 옵션을 사용하여 test-instance에 접속합니다.  </p>

<pre><code>me@local:~$ gcloud compute ssh test-instance --ssh-flag="-A"  
</code></pre>

<p>test-instance에서 haproxy-server-xxxx instance으로 SSH 접속을 합니다.  </p>

<pre><code>me@test-instance:~$ ssh haproxy-server-xxxx  
</code></pre>

<p><img src="http://i.imgur.com/HGSc7N6.png" alt="Imgur"></p>

<p>haproxy server가 Active 상태인지 확인합니다.  </p>

<pre><code>me@haproxy-server-xxxx:~$ sudo service haproxy status | grep Active  
</code></pre>

<p>HAProxy server process를 강제로 종료시킵니다.  </p>

<pre><code>me@haproxy-server-xxxx:~$ sudo pkill haproxy  
</code></pre>

<p>HAProxy service가 다시 실행되었는지 확인합니다.  </p>

<pre><code>me@haproxy-server-xxxx:~$ sudo service haproxy status | grep Active  
</code></pre>

<p><img src="http://i.imgur.com/tion6Ky.png" alt="Imgur"></p>

<p><strong>수정이 필요하거나 오류가 있는 곳을 알려주시면 친절하게 수정하도록 하겠습니다.</strong></p>

<p>GCP에 Internal Load Balancer가 베타버전으로 공개되었다고 합니다. <br>
이제는 HAProxy를 직접 설치할 일이 많이 줄어들 것 같습니다.</p>

<p><a href="https://cloud.google.com/compute/docs/load-balancing/internal/">https://cloud.google.com/compute/docs/load-balancing/internal/</a></p>]]></content:encoded></item><item><title><![CDATA[Ubuntu 16.04 설치후 환경 설정하기]]></title><description><![CDATA[<p>Google Compute Engine에 Ubuntu 16.04를 설치하고 필요한 환경 설정하는 방법입니다. <br>
여기서는 두가지 정도 환경설정을 할려고 합니다.</p>

<h4 id="1locale">1. locale 설정</h4>

<p>Ubuntu 16.04 VM 인스턴스를 설치하고 처음 접속하면 아래와 같은 화면을 만나게 됩니다. <br>
아래에 나와 있는 것과 같이 <code>sudo apt-get install language-pack-ko</code> 또는 <code>sudo locale-gen ko_KR.UTF-8</code> 명령으로 locale을</p>]]></description><link>http://iamartin-gh.herokuapp.com/ubuntu16-04_environment/</link><guid isPermaLink="false">9aa69d57-74d2-4278-8545-271029dd8849</guid><category><![CDATA[ubuntu]]></category><category><![CDATA[16.04]]></category><category><![CDATA[locale]]></category><category><![CDATA[timezone]]></category><category><![CDATA[tzselect]]></category><category><![CDATA[ko_KR.UTF-8]]></category><category><![CDATA[locale-gen]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Fri, 04 Nov 2016 13:56:58 GMT</pubDate><content:encoded><![CDATA[<p>Google Compute Engine에 Ubuntu 16.04를 설치하고 필요한 환경 설정하는 방법입니다. <br>
여기서는 두가지 정도 환경설정을 할려고 합니다.</p>

<h4 id="1locale">1. locale 설정</h4>

<p>Ubuntu 16.04 VM 인스턴스를 설치하고 처음 접속하면 아래와 같은 화면을 만나게 됩니다. <br>
아래에 나와 있는 것과 같이 <code>sudo apt-get install language-pack-ko</code> 또는 <code>sudo locale-gen ko_KR.UTF-8</code> 명령으로 locale을 설정합니다.</p>

<p><img src="http://i.imgur.com/tJkJhPM.png" alt="Imgur"></p>

<p>여기에서는 <code>sudo locale-gen ko_KR.UTF-8</code> 명령을 사용했습니다.</p>

<script src="https://gist.github.com/iamartin-gh/674b203af6b1328f396ae496d9ba0eab.js"></script>

<p><img src="http://i.imgur.com/WqAiS9o.png" alt="Imgur"></p>

<p>로그오프 후 재접속하면 아래와 같이 locale이 <code>ko_KR.UTF-8</code>로 설정되었습니다.
<img src="http://i.imgur.com/YkNPEdY.png" alt="Imgur"></p>

<h4 id="2timezone">2. timezone 설정</h4>

<p>timezone 설정은 <code>tzselect</code> 명령으로 합니다.</p>

<script src="https://gist.github.com/iamartin-gh/794c73ac8cb52df63d4b53c1aade8303.js"></script>

<p><code>tzselect</code>명령 실행 후 .profile에 <code>TZ='Asia/Seoul'; export TZ</code> 내용을 추가합니다.
<img src="http://i.imgur.com/seeMSsf.png" alt="Imgur"></p>

<blockquote>
  <p>root 유저는 .bashrc에 추가합니다.</p>
</blockquote>

<p>설정 완료 후 로그오프 후 재접속 또는 <code>source .profile</code>로 설정을 반영합니다.
아래와 같이 timezone이 변경되었습니다.
<img src="http://i.imgur.com/BVCzbmc.png" alt="Imgur"></p>]]></content:encoded></item><item><title><![CDATA[kubeadm을 이용해서 아주 쉽게 Kubernetes 설치하기]]></title><description><![CDATA[<p>아주 쉽게 <code>kubeadm</code>을 이용해서 <code>ubuntu 16.04</code>에 kubernetes를 설치하는 방법입니다. <br>
CentOS 7에서의 설치방법은 원문을 참고하시기 바랍니다. <br>
<a href="http://kubernetes.io/docs/getting-started-guides/kubeadm/">http://kubernetes.io/docs/getting-started-guides/kubeadm/</a>  </p>

<p>kubernetes를 설치해보는 것은 kubernetes를 이해하는 데 좋은 출발점이 될 것으로 생각됩니다.</p>

<h4 id="googlecloudplatformvm">Google Cloud Platform에서 VM 인스턴스 생성</h4>

<p>여기서는 Google Compute Engine에 아래와 같이 3개의 VM을 생성합니다.</p>]]></description><link>http://iamartin-gh.herokuapp.com/kubernetes-install/</link><guid isPermaLink="false">7579e825-bfea-4381-a6a0-3028dadbf165</guid><category><![CDATA[docker]]></category><category><![CDATA[orchestration]]></category><category><![CDATA[install]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[kubeadm]]></category><category><![CDATA[설치]]></category><category><![CDATA[gcp]]></category><category><![CDATA[google cloud platform]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Thu, 03 Nov 2016 14:56:40 GMT</pubDate><content:encoded><![CDATA[<p>아주 쉽게 <code>kubeadm</code>을 이용해서 <code>ubuntu 16.04</code>에 kubernetes를 설치하는 방법입니다. <br>
CentOS 7에서의 설치방법은 원문을 참고하시기 바랍니다. <br>
<a href="http://kubernetes.io/docs/getting-started-guides/kubeadm/">http://kubernetes.io/docs/getting-started-guides/kubeadm/</a>  </p>

<p>kubernetes를 설치해보는 것은 kubernetes를 이해하는 데 좋은 출발점이 될 것으로 생각됩니다.</p>

<h4 id="googlecloudplatformvm">Google Cloud Platform에서 VM 인스턴스 생성</h4>

<p>여기서는 Google Compute Engine에 아래와 같이 3개의 VM을 생성합니다.</p>

<ul>
<li>kube-master-1</li>
<li>kube-node-1</li>
<li>kube-node-2   </li>
</ul>

<p><img src="http://i.imgur.com/Aouti3N.png" alt="Imgur"></p>

<p>이번에 새로 생긴 asia-northeast1(도쿄)을 이용했구요. asia-east1(대만)보다 확실히 빠른 응답속도를 느낄 수 있었습니다.</p>

<p><img src="http://i.imgur.com/FRv15zD.png" alt="Imgur"></p>

<p>이제 VM 인스턴스를 생성했으니 간단하게 환경설정을 합니다. 
Ubuntu 설치 후 환경설정은 아래 링크를 확인하시기 바랍니다. <br>
<a href="http://blog.iamartin.com/ubuntu16-04_environment/">ubuntu 16.04 환경설정</a>   </p>

<p>참고로 설치한 VM 인스턴스의 ssh 접속은 Ubuntu 리눅스에 Google Cloud SDK를 설치하고 <code>gcloud compute ssh</code>명령으로 접속했습니다.</p>

<p><img src="http://i.imgur.com/lSOkZcV.png" alt="Imgur"></p>

<h4 id="kubernetes">kubernetes 설치</h4>

<p>아래 명령을 이용해서 kubernetes를 설치합니다. <br>
설치에는 root권한이 필요하므로 <code>sudo su -</code>로 root권한을 얻은 후 설치를 진행하면 됩니다. <br>
설치방법은 master와 node 동일합니다. <br>
아래 명령을 kube-master-1, kube-node-1, kube-node-2등 모든 VM 인스턴스에서 실행합니다.</p>

<script src="https://gist.github.com/iamartin-gh/d1308df077837e99d369e8fe9eb1ace1.js"></script>

<h4 id="master">master 초기화</h4>

<p><code>kubeadm init</code>명령으로 master를 초기화합니다.
<img src="http://i.imgur.com/oIEgq7H.png" alt="Imgur"></p>

<p>마지막 부분의 <code>kubeadm join --token c07e38.feb309e6ea9ce270 10.146.0.2</code> 명령을 잘 기억해야 합니다. 이 명령을 통해 node가 master에 join할 수 있습니다. <br>
그러면  kube-node-1, kube-node-2에서 위의 명령으로 master에 join합니다.</p>

<p><img src="http://i.imgur.com/e8elwcp.png" alt="Imgur"></p>

<p><img src="http://i.imgur.com/81g6Dju.png" alt="Imgur"></p>

<p>master에서 <code>kubectl get nodes</code> 명령으로 정상적으로 node들이 join했는지 확인합니다.</p>

<p><img src="http://i.imgur.com/LdbF2GA.png" alt="Imgur"></p>

<h4 id="podnetworkaddon">pod network add-on 설치</h4>

<p>각각의 pod들이 통신하기 위한 pod network add-on을 설치합니다. <br>
종류가 여러가지 있으므로 마음에 드는 add-on을 설치하면 됩니다. <br>
<a href="http://kubernetes.io/docs/admin/addons/">http://kubernetes.io/docs/admin/addons/</a></p>

<p>여기서는 <a href="https://github.com/weaveworks/weave-kube">weave net</a>을 설치합니다.  </p>

<p>master에서 <code>kubectl create -f https://git.io/weave-kube</code> 명령을 실행합니다. <br>
설치과정은 정말 간단합니다.</p>

<p><img src="http://i.imgur.com/uKpn9b1.png" alt="Imgur"></p>

<p>pod network add-on이 정상적으로 설치되면 <code>kube-dns</code>의 상태가 <code>Running</code>으로 변경됩니다. <br>
아래 명령으로 확인합니다.</p>

<pre><code>kubectl get pods --all-namespaces  
</code></pre>

<p><img src="http://i.imgur.com/mzvnHrk.png" alt="Imgur"></p>

<p>각각의 VM에 <code>weave</code>라고하는 함께 네트워크도 생성이 되었습니다.</p>

<p><img src="http://i.imgur.com/lGDqOZ3.png" alt="Imgur">
<img src="http://i.imgur.com/zVSKMaF.png" alt="Imgur">
<img src="http://i.imgur.com/zVSKMaF.png" alt="Imgur"></p>

<h4 id="sampleapplication">sample application 설치</h4>

<p>설정이 완료된 kubernetes를 테스트하기 위하여 <del>간단한</del> sock-shop이라고 하는 쇼핑몰 샘플 앱을 실행해 봅니다.</p>

<script src="https://gist.github.com/iamartin-gh/8c3f49f9d8468b17e1f14b0bf4a8ead6.js"></script>

<p><img src="http://i.imgur.com/dPqzn4v.png" alt="Imgur"></p>

<p>pod들이 정상적으로 실행되었는지 확인합니다.  </p>

<pre><code>kubectl get pods -n sock-shop  
</code></pre>

<p><img src="http://i.imgur.com/2KwW93H.png" alt="Imgur"></p>

<p>sock-shop에 접속해 보기위해서 아래 명령을 실행하여 front-end service가 사용하고 있는 포트번호를 확인합니다.  </p>

<pre><code>kubectl describe svc front-end -n sock-shop  
</code></pre>

<p><img src="http://i.imgur.com/di9iQmQ.png" alt="Imgur"></p>

<p>master(kube-master-1)의 외부 IP와 위에서 확인한 포트를 조합하여 접속하면 됩니다.</p>

<p><img src="http://i.imgur.com/5bAkaCC.png" alt="Imgur"></p>

<pre><code>http://104.198.81.158:32609/  
</code></pre>

<p>하지만 <code>32609</code> 포트는 방화벽에서 오픈되어 있지않아 접속이 불가합니다. <br>
GCP 방화벽에서 해당 포트를 오픈합니다.</p>

<p><img src="http://i.imgur.com/tnTsTHv.png" alt="Imgur">
<img src="http://i.imgur.com/MpFiUn4.png" alt="Imgur">
<img src="http://i.imgur.com/LbPAYdr.png" alt="Imgur"></p>

<p>sock-shop으로 접속합니다.</p>

<p><img src="http://i.imgur.com/d8pwrvI.png" alt="Imgur"></p>

<blockquote>
  <p>kubernetes를 간단하게 테스트하려면 <a href="http://kubernetes.io/docs/getting-started-guides/minikube/">minikube</a>를 설치하면 됩니다. <br>
  하지만, minikube는 master 하나만을 생성하므로 명령어를 익히기에는 좋으나 진정한 의미의 docker container orchestration를 구현하기 위해서는 master와 node들로 구성된 multi node cluster를 위와 같은 방법으로 구현 테스트하는 것이 좋습니다.</p>
</blockquote>

<p>다음에는 <a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard">kubernetes dashboard</a> 설치와 <a href="https://www.weave.works/documentation/scope-latest-installing/#k8s">Weave Scope</a>서비스와 간단하게 연동해 보는 방법을 학습해 보려고 합니다. </p>]]></content:encoded></item><item><title><![CDATA[.vimrc 파일]]></title><description><![CDATA[<p>RubyOnRails 개발에 특화된 .vimrc 파일입니다.</p>

<script src="https://gist.github.com/iamartin-gh/9b4f5aa4e0337a551a6332a2562546fc.js"></script>

<p>실제 터미널에서 아래와 같이 사용할 수 있습니다.
<img src="http://i.imgur.com/YqoexBl.png" alt="Imgur"></p>]]></description><link>http://iamartin-gh.herokuapp.com/vimrc/</link><guid isPermaLink="false">f4c2b160-bce1-4725-9d6c-4257b0df971c</guid><category><![CDATA[vim]]></category><category><![CDATA[vi]]></category><category><![CDATA[rails]]></category><category><![CDATA[development]]></category><category><![CDATA[vimrc]]></category><category><![CDATA[vbundle]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Mon, 17 Oct 2016 15:35:30 GMT</pubDate><content:encoded><![CDATA[<p>RubyOnRails 개발에 특화된 .vimrc 파일입니다.</p>

<script src="https://gist.github.com/iamartin-gh/9b4f5aa4e0337a551a6332a2562546fc.js"></script>

<p>실제 터미널에서 아래와 같이 사용할 수 있습니다.
<img src="http://i.imgur.com/YqoexBl.png" alt="Imgur"></p>]]></content:encoded></item><item><title><![CDATA[Docker swarm 구성 따라하기 #1]]></title><description><![CDATA[<p>Docker 버전이 <code>1.12</code>로 올라가면서 swarm이 Docker Engine에 통합되었다. <br>
이제 Docker만 설치되어 있으면 바로 swarm을 통해 추가적인 설치 과정없이 container orchestration을 할 수 있다.</p>

<p>Kubernetes, Mesos 등 타 orchestration 툴에 비해 설치 과정이 단순하여 소규모 container orchestration을 하는데 최적의 툴이 아닌가 생각해 본다. <br>
<code>1.12</code>로 버전이 올라가면서 container</p>]]></description><link>http://iamartin-gh.herokuapp.com/docker-swarm-1/</link><guid isPermaLink="false">119abed0-3d03-4370-916e-8dafebddf3a7</guid><category><![CDATA[docker]]></category><category><![CDATA[swarm]]></category><category><![CDATA[docker init]]></category><category><![CDATA[docker node]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Fri, 12 Aug 2016 02:31:54 GMT</pubDate><content:encoded><![CDATA[<p>Docker 버전이 <code>1.12</code>로 올라가면서 swarm이 Docker Engine에 통합되었다. <br>
이제 Docker만 설치되어 있으면 바로 swarm을 통해 추가적인 설치 과정없이 container orchestration을 할 수 있다.</p>

<p>Kubernetes, Mesos 등 타 orchestration 툴에 비해 설치 과정이 단순하여 소규모 container orchestration을 하는데 최적의 툴이 아닌가 생각해 본다. <br>
<code>1.12</code>로 버전이 올라가면서 container orchestration에 꼭 필요한 기능인 Scaling, Multi-host networking, Load balancing, Service discovery 등 orchestration에 모두 구현되어서 간편하게 docker container를 배포하고 관리할 수 있게 되었다.   </p>

<p>무엇보다도 docker에서 직접 만드니 안정성 뿐만 아니라 다양한 기능 추가가 계속 진행되어 발전된 모습을 보게될 것 같다.</p>

<hr>

<p>먼저 Docker swarm을 구성하려면 아래의 요건이 필요하다.</p>

<ul>
<li><strong>네트워크에 연결된 3대의 머신</strong> : 머신은 PC에서 VMware나 Virtualbox를 ㅣ용하거나 AWS나 GCP 등Cloud에서 생성하여도 된다. 또한 Virtual Machine 뿐만 아니라 실제 물리서버에 설치하여도 가능하다. 해당 머신들로 각각의 node를 구성하게 된다.</li>
<li><a href="http://blog.iamartin.com/docker-install-script/">Docker Engine 1.12 설치</a></li>
<li>manager 머신에 사용할 ip 주소</li>
<li>swarm에서 사용할 포트 오픈
<ul><li>TCP port 2377 : cluster management 통신에 사용</li>
<li>TCP / UDP port 7946 : node간의 통신에 사용</li>
<li>TCP / UDP port 4789 : <code>overlay network</code> 트래픽에 사용</li></ul></li>
</ul>

<p>여기에서는 <mark><strong>Google Compute Engine(GCE)</strong></mark>에 Docker swarm을 설치하여 간단한 서비스를 실행하는 부분까지 진행해 본다.</p>

<h3 id="dockerswarm">Docker swarm 구성도</h3>

<h3 id="vm">VM 생성</h3>

<p>GCE에서 swarm 구성에 필요한 머신 3대를 생성한다. <br>
<a href="http://imgur.com/q8lyTDZ"><img src="http://i.imgur.com/q8lyTDZ.png" title="source: imgur.com"></a></p>

<p>아래와 같이 3대의 머신이 생성되었다.
<a href="http://imgur.com/HZqscaF"><img src="http://i.imgur.com/HZqscaF.png" title="source: imgur.com"></a></p>

<p>Google Cloud SDK 명령으로 확인 시 아래와 같이 모두 잘 생성되었다.</p>

<pre><code>gcloud compute instances list
</code></pre>

<p><a href="http://imgur.com/gsPOkC1"><img src="http://i.imgur.com/gsPOkC1.png" title="source: imgur.com"></a></p>

<h3 id="swarm">swarm 생성</h3>

<p><code>gcloud compute ssh</code> 명령으로 <code>manager1</code> 머신에 접속한다. </p>

<pre><code>gcloud compute ssh manager1
</code></pre>

<p><a href="http://imgur.com/dJKMcRZ"><img src="http://i.imgur.com/dJKMcRZ.png" title="source: imgur.com"></a></p>

<p>아래 명령으로 swarm을 초기화(생성) 한다.</p>

<pre><code>docker swarm init --advertise-addr &lt;MANAGER-IP&gt;
</code></pre>

<p><a href="http://imgur.com/9BlOZ1U"><img src="http://i.imgur.com/9BlOZ1U.png" title="source: imgur.com"></a></p>

<p><code>docker info</code>로 swarm mode가 생성되었는지 확인한다.</p>

<pre><code>docker info
</code></pre>

<p>아래와 같이 Swarm이 정상적으로 <code>Active</code> 된 것을 확인할 수 있으며 swarm 관련 여러 정보들을 확인할 수 있다ㅏ.
<a href="http://imgur.com/5Gxe0hb"><img src="http://i.imgur.com/5Gxe0hb.png" title="source: imgur.com"></a></p>

<p>worker를 설정하기 전에 worker 설정에 필요한 명령을 확인한다. <br>
manager1 머신에서 아래 명령을 실행한다</p>

<pre><code>docker swarm join-token worker
</code></pre>

<p><a href="http://imgur.com/W6ZuHOa"><img src="http://i.imgur.com/W6ZuHOa.png" title="source: imgur.com"></a></p>

<p>worker1에 접속한다.</p>

<pre><code>gcloud compute ssh worker1
</code></pre>

<p><a href="http://imgur.com/9rTV5r3"><img src="http://i.imgur.com/9rTV5r3.png" title="source: imgur.com"></a></p>

<p>해당 머신을 swarm의 worker로 등록한다.</p>

<pre><code>docker swarm join \  
    --token SWMTKN-1-62ecjm7y6664gi80edojyocv4m2jl6ehrowpjjx4jc41ph0x3b-d4oxeswwlevj22dtfjlo1j6qv \
    10.140.0.2:2377
</code></pre>

<p><a href="http://imgur.com/nHBcmvE"><img src="http://i.imgur.com/nHBcmvE.png" title="source: imgur.com"></a></p>

<p>worker2 머신도 worker1과 동일하게 설정한다.</p>

<p><code>docker info</code>로 Swarm이 정상적으로 <code>Active</code>되었는지 확인한다.</p>

<p><a href="http://imgur.com/F0EwMoI"><img src="http://i.imgur.com/F0EwMoI.png" title="source: imgur.com"></a></p>

<p>마지막으로 swarm node들이 잘 등록되었는지 확인한다.</p>

<pre><code>docker node ls
</code></pre>

<p><a href="http://imgur.com/jxYJVMj"><img src="http://i.imgur.com/jxYJVMj.png" title="source: imgur.com"></a></p>

<p><mark>swarm</mark>이 <code>Docker Engine</code>에 통합되면서 설치 과정이 무척 단순해졌다. 어떻게 보면 설치라고 하기보다는 swarm mode를 active 하기 위한 설정이라고 하는 편이 더 정확할 듯하다.  </p>

<p>설정하는 단계는 단순하지만 swarm이 가지고 있는 기능들만큼은 타 container orchestration 툴에 비해 이제는 부족하지 않다. <br>
또한 단순할수록 운영 상의 관점에서 봤을 때에도 장점이 많을 것으로 보여진다.</p>

<p>이제는 container orchestration의 설치 과정이나 설정 등이 단순화되므로 보다 더 Application을 어떻게 구성할 지에 대한 문제에 더 집중할 수 있게 되었다.</p>]]></content:encoded></item><item><title><![CDATA[Docker 초간편 설치]]></title><description><![CDATA[<p>이전 포스트에서 Docker를 설치하는 방법을 살펴봤다. <br>
<a href="http://blog.iamartin.com/2016/08/10/ubuntu-16-04-docker-install/">http://blog.iamartin.com/2016/08/10/ubuntu-16-04-docker-install/</a></p>

<p>하지만 한방에 설치하는 스크립트를 Docker에서 제공한다. <br>
그냥 실행만 하면 알아서 설치하니 꼭 매뉴얼로 설치하는 방법이 필요한 경우가 아니라면 이 방법이 Docker를 설치하는 좋은 방법이라고 생각된다.</p>

<pre><code>curl -sSL https://get.docker.com/ | sh  
or  
wget -qO- https:</code></pre>]]></description><link>http://iamartin-gh.herokuapp.com/docker-install-script/</link><guid isPermaLink="false">d8378f0e-58e0-48bc-9622-9f6149b4ee5f</guid><category><![CDATA[docker]]></category><category><![CDATA[install]]></category><category><![CDATA[get.docker.com]]></category><category><![CDATA[설치 스크립트]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Wed, 10 Aug 2016 23:57:47 GMT</pubDate><content:encoded><![CDATA[<p>이전 포스트에서 Docker를 설치하는 방법을 살펴봤다. <br>
<a href="http://blog.iamartin.com/2016/08/10/ubuntu-16-04-docker-install/">http://blog.iamartin.com/2016/08/10/ubuntu-16-04-docker-install/</a></p>

<p>하지만 한방에 설치하는 스크립트를 Docker에서 제공한다. <br>
그냥 실행만 하면 알아서 설치하니 꼭 매뉴얼로 설치하는 방법이 필요한 경우가 아니라면 이 방법이 Docker를 설치하는 좋은 방법이라고 생각된다.</p>

<pre><code>curl -sSL https://get.docker.com/ | sh  
or  
wget -qO- https://get.docker.com/ | sh  
</code></pre>

<p>웹브라우저에서 <a href="https://get.docker.com">get.docker.com</a>을 입력하면 스크립트 내용을 볼수 있다.</p>]]></content:encoded></item><item><title><![CDATA[Ubuntu 16.04에서 Docker 설치]]></title><description><![CDATA[<blockquote>
  <p>Ubuntu Xenial 16.04 (LTS)에서 docker 설치하는 방법입니다.</p>
</blockquote>

<h4 id="aptsourcesupdate">apt sources update</h4>

<ol>
<li><code>sudo</code> 또는 <code>root</code> 권한으로 로그인한다.  </li>
<li><p>패키지 정보 업데이트 및 CA certificates 설치, APT가 https로 동작할 수 있도록 설치한다.</p>

<pre><code>$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates          
</code></pre></li>
<li><p><code>GPG</code> key 추가</p>

<pre><code>$ $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:</code></pre></li></ol>]]></description><link>http://iamartin-gh.herokuapp.com/ubuntu-16-04-docker-install/</link><guid isPermaLink="false">ef41dec8-496c-4825-92da-54fba3ff1690</guid><category><![CDATA[ubuntu]]></category><category><![CDATA[16.04]]></category><category><![CDATA[docker]]></category><category><![CDATA[install]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Wed, 10 Aug 2016 09:01:39 GMT</pubDate><content:encoded><![CDATA[<blockquote>
  <p>Ubuntu Xenial 16.04 (LTS)에서 docker 설치하는 방법입니다.</p>
</blockquote>

<h4 id="aptsourcesupdate">apt sources update</h4>

<ol>
<li><code>sudo</code> 또는 <code>root</code> 권한으로 로그인한다.  </li>
<li><p>패키지 정보 업데이트 및 CA certificates 설치, APT가 https로 동작할 수 있도록 설치한다.</p>

<pre><code>$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates          
</code></pre></li>
<li><p><code>GPG</code> key 추가</p>

<pre><code>$ $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
</code></pre></li>
<li><p><code>/etc/apt/sources.list.d/docker.list</code>을 편집기로 열어서 (없다면 생성한다.) 모든 내용을 삭제한다.  </p></li>
<li><p>아래 내용을 추가한다.</p>

<pre><code>deb https://apt.dockerproject.org/repo ubuntu-xenial main
</code></pre></li>
<li><p>APT package index를 업데이트 한다.</p>

<pre><code>$ sudo apt-get update
</code></pre></li>
<li><p>오래된 저장소를 삭제한다.</p>

<pre><code>$ sudo apt-get purge lxc-docker
</code></pre></li>
<li><p>올바른 저장소에서 받은 지 검증한다.</p>

<pre><code>$ apt-cache policy docker-engine
</code></pre></li>
<li><p>linux-image-extra 패키지를 설치한다.</p>

<pre><code>$ sudo apt-get install linux-image-extra-$(uname -r)
</code></pre></li>
</ol>

<h4 id="">설치</h4>

<ol>
<li><code>sudo</code> 권한으로 로그인한다.  </li>
<li><p><code>APT</code> package index를 업데이트 한다.</p>

<pre><code>$ sudo apt-get update
</code></pre></li>
<li><p>Docker를 설치한다.</p>

<pre><code>$ sudo apt-get install docker-engine
</code></pre></li>
<li><p><code>docker</code> daemon을 시작한다.</p>

<pre><code>$ sudo service docker start
</code></pre></li>
<li><p><code>docker</code>가 정상적으로 설치되었는지 확인한다.</p>

<pre><code>$ sudo docker run hello-world
</code></pre></li>
</ol>

<h4 id="dockergroup">Docker group 생성</h4>

<ol>
<li><code>sudo</code> 권한으로 로그인한다.  </li>
<li><p><code>docker</code> group을 생성한다.</p>

<pre><code>$ sudo groupadd docker
</code></pre></li>
<li><p>사용자 계정을 <code>docker</code> group에 추가한다.</p>

<pre><code>$ sudo usermod -aG docker $USER
</code></pre></li>
<li>로그아웃 후 다시 로그인 한다.  </li>
<li><p><code>sudo</code> 권한없이 <code>docker</code> container가 기동되는지 확인한다.</p>

<pre><code>$ docker run hello-world
</code></pre></li>
</ol>

<h4 id="enableufwforwarding">Enable UFW forwarding</h4>

<ol>
<li><code>sudo</code> 권한으로 로그인한다  </li>
<li><p>UFW가 설치되어 있고 enable되어 있는지 확인한다.</p>

<pre><code>$ sudo ufw status
</code></pre></li>
<li><p><code>/etc/default/ufw</code>파일을 열어서 <code>DEFAULT_FORWARD_POLICY</code> 설정한다.</p>

<pre><code>DEFAULT_FORWARD_POLICY="ACCEPT"
</code></pre></li>
<li><p>새로운 설정을 반영하기 위해 <code>UFW</code>를 다시 로딩한다.</p>

<pre><code>$ sudo ufw reload
</code></pre></li>
<li><p>Docker port를 오픈한다.</p>

<pre><code>$ sudo ufw allow 2375/tcp
</code></pre></li>
</ol>

<h4 id="dnsserver">DNS server 설정</h4>

<ol>
<li><code>sudo</code> 권한으로 로그인한다  </li>
<li><p><code>/etc/default/docker</code> 파일을 열어서 아래 설정을 추가합니다.</p>

<pre><code>DOCKER_OPTS="--dns 8.8.8.8"
</code></pre></li>
<li><p>multiple DNS server 설정도 가능하므로 내부에서 사용하는 DNS Server가 있다면 아래와 같이 설정한다.</p>

<pre><code>--dns 8.8.8.8 --dns 192.168.1.1
</code></pre></li>
<li><p>Docker daemon을 재시작한다.</p>

<pre><code>$ sudo service docker restart
</code></pre></li>
</ol>

<h4 id="docker">부팅 시 Docker 자동으로 실행</h4>

<p>부팅할 때 Docker daemon이 자동으로 실행되게 하려면 아래 명령을 실행한다.</p>

<pre><code>$ sudo systemctl enable docker
</code></pre>

<h4 id="dockerupgrade">Docker Upgrade</h4>

<pre><code>$ sudo apt-get upgrade docker-engine
</code></pre>

<h4 id="">삭제</h4>

<p>Docker package 삭제</p>

<pre><code>$ sudo apt-get purge docker-engine
</code></pre>

<p>Docker package와 의존성있는 파일 모두 삭제</p>

<pre><code>$ sudo apt-get autoremove --purge docker-engine
</code></pre>

<p>위 명령으로는 image, container, volume들과 사용자 설정 파일들이 삭제되지 않는다. 삭제를 원한다면 <code>/var/lib/docker</code> 디렉토리를 삭제한다.</p>

<pre><code>$ rm -rf /var/lib/docker
</code></pre>]]></content:encoded></item><item><title><![CDATA[Proxy 내부에서 Atom Package 설치하는 방법]]></title><description><![CDATA[<p>Proxy를 사용하는 회사 내부에서 Atom Package를 설치하려면 아래와 같은 메시지가 발생합니다.</p>

<pre><code>UNABLE_TO_VERIFY_LEAF_SIGNATURE  
</code></pre>

<p><a href="http://imgur.com/wY3MJLy"><img src="http://i.imgur.com/wY3MJLy.png" title="source: imgur.com"></a></p>

<p>조치방법은 명령프롬프트를 열고 아래 명령을 입력합니다.</p>

<pre><code>apm config set strict-ssl false  
</code></pre>

<p><a href="http://imgur.com/URNsiXY"><img src="http://i.imgur.com/URNsiXY.png" title="source: imgur.com"></a></p>

<p>Atom을 다시 실행한 후 Package를 검색하면 정상적으로 검색됩니다. <br>
<a href="http://imgur.com/NBeDSmj"><img src="http://i.imgur.com/NBeDSmj.png" title="source: imgur.com"></a></p>

<p>Proxy를 사용하는 곳에서는 여러모로 불편한 점이 많군요.</p>]]></description><link>http://iamartin-gh.herokuapp.com/atom-proxy/</link><guid isPermaLink="false">e206e97d-1d0b-4ee4-87e2-4760e3f97007</guid><category><![CDATA[Atom]]></category><category><![CDATA[Proxy]]></category><category><![CDATA[Package 설치]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Mon, 04 Jul 2016 07:33:05 GMT</pubDate><content:encoded><![CDATA[<p>Proxy를 사용하는 회사 내부에서 Atom Package를 설치하려면 아래와 같은 메시지가 발생합니다.</p>

<pre><code>UNABLE_TO_VERIFY_LEAF_SIGNATURE  
</code></pre>

<p><a href="http://imgur.com/wY3MJLy"><img src="http://i.imgur.com/wY3MJLy.png" title="source: imgur.com"></a></p>

<p>조치방법은 명령프롬프트를 열고 아래 명령을 입력합니다.</p>

<pre><code>apm config set strict-ssl false  
</code></pre>

<p><a href="http://imgur.com/URNsiXY"><img src="http://i.imgur.com/URNsiXY.png" title="source: imgur.com"></a></p>

<p>Atom을 다시 실행한 후 Package를 검색하면 정상적으로 검색됩니다. <br>
<a href="http://imgur.com/NBeDSmj"><img src="http://i.imgur.com/NBeDSmj.png" title="source: imgur.com"></a></p>

<p>Proxy를 사용하는 곳에서는 여러모로 불편한 점이 많군요.</p>]]></content:encoded></item><item><title><![CDATA[VMware로 구축하는 mesos 테스트 환경]]></title><description><![CDATA[<h4 id="mesos">Mesos란</h4>

<p>Mesos는 2009년 버클리 대학에서 시작된 오픈소스 클러스터 매니저 프로젝트로, 2013년부터는 아파치 Top-level 프로젝트로 승격되었습니다. <br>
이미 Twitter와 Airbnb, Apple을 포함한 많은 기업이 실 서비스 환경에서 Mesos를 사용하고 있습니다. <br>
여기서는 docker container의 orchestration 용도로 사용할 예정입니다.   </p>

<h4 id="mesos">Mesos 아키텍처</h4>

<p>Mesos는 Master와 Master의 고가용성을 유지하기 위한 Zookeeper 클러스터, Framework, Scheduler, Slave로 구성되어 있습니다.</p>]]></description><link>http://iamartin-gh.herokuapp.com/vmware-mesos-install/</link><guid isPermaLink="false">ec95e235-2489-420a-96d4-ef87a2c4b0c5</guid><category><![CDATA[mesos]]></category><category><![CDATA[docker]]></category><category><![CDATA[zookeeper]]></category><category><![CDATA[marathon]]></category><category><![CDATA[orchestration]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Mon, 27 Jun 2016 05:43:49 GMT</pubDate><content:encoded><![CDATA[<h4 id="mesos">Mesos란</h4>

<p>Mesos는 2009년 버클리 대학에서 시작된 오픈소스 클러스터 매니저 프로젝트로, 2013년부터는 아파치 Top-level 프로젝트로 승격되었습니다. <br>
이미 Twitter와 Airbnb, Apple을 포함한 많은 기업이 실 서비스 환경에서 Mesos를 사용하고 있습니다. <br>
여기서는 docker container의 orchestration 용도로 사용할 예정입니다.   </p>

<h4 id="mesos">Mesos 아키텍처</h4>

<p>Mesos는 Master와 Master의 고가용성을 유지하기 위한 Zookeeper 클러스터, Framework, Scheduler, Slave로 구성되어 있습니다.</p>

<p><a href="http://imgur.com/wam1dgc"><img src="http://i.imgur.com/wam1dgc.png" title="source: imgur.com"></a></p>

<h4 id="mesos">Mesos 동작원리</h4>

<ol>
<li>Slave 1은 4 CPU와 4GB RAM을 사용할 수 있다고 Mesos Master에게 전달합니다. Mesos Master는 Framework 1이 tack를 실행하기 위해 리소스 오퍼를 요청한 사실을 Allocation module에 저장합니다.  </li>
<li>Mesos Master는 Slave 1의 리소스 오퍼를 Framework 1에 전달합니다.  </li>
<li>Framework의 Scheduler는 Slave 1에 2개(task1, task2)의 task를 실행하겠다는 정보를 Mesos Master에 응답으로 돌려줍니다. task1은 2 CPU, 1GB RAM, task2는 1 CPU, 2GB RAM을 사용합니다.  </li>
<li>최종으로 Mesos Master는 Slave 1에 2개의 task1, task2를 전달하고, Slave 1에서 실행된다. 이제 Slave 1에 남은 자원인 1 CPU와 1GB RAM이 리소스 오퍼로 Mesos Master에 전달됩니다.</li>
</ol>

<p><a href="http://imgur.com/y4rVl8a"><img src="http://i.imgur.com/y4rVl8a.jpg" title="source: imgur.com"></a></p>

<h4 id="">테스트 환경</h4>

<ul>
<li>VMWare Workstatin 12</li>
<li>Ubuntu 14.04</li>
</ul>

<p><a href="http://imgur.com/ax8jE4f"><img src="http://i.imgur.com/ax8jE4f.png" title="source: imgur.com"></a></p>

<h4 id="">공통 패키지 설치</h4>

<h6 id="java">JAVA설치</h6>

<p><code>Zookeeper cluster</code>를 구성하기 위해서는 JAVA가 필요합니다.</p>

<pre><code>sudo add-apt-repository ppa:webupd8team/java  
sudo apt-get update  
sudo apt-get install oracle-java8-installer  
sudo apt-get install oracle-java8-set-default  
</code></pre>

<h6 id="mesos">Mesos 저장소 추가</h6>

<pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF  
DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')  
CODENAME=$(lsb_release -cs)  
echo "deb http://repos.mesosphere.io/${DISTRO} ${CODENAME} main" | sudo tee /etc/apt/sources.list.d/mesosphere.list  
sudo apt-get -y update  
</code></pre>

<h4 id="mesosmaster">Mesos master 설치</h4>

<pre><code>sudo apt-get install mesosphere  
</code></pre>

<h4 id="mesosmaster">Mesos master 설정</h4>

<h6 id="zookeeper">Zookeeper 설정</h6>

<p>myid는 중복되지 않게 1~255 사이의 정수값을 할당합니다.  </p>

<pre><code>sudo nano /etc/zookeeper/conf/myid

1    # master1  
</code></pre>

<p>/etc/zookeeper/conf/zoo.cfg 에 <code>server.{myid}.{server ip}:2888:3888</code> 설정을 추가합니다.</p>

<pre><code>sudo vi /etc/zookeeper/conf/zoo.cfg

server.1=192.168.66.11:2888:3888  
server.2=192.168.66.12:2888:3888  
server.3=192.168.66.13:2888:3888  
</code></pre>

<h6 id="mesos">Mesos 설정</h6>

<p>‘zk://’ 형식의 Zookeeper URL을 설정합니다. <br>
Zookeeper URL은 <code>zk://</code>로 시작하고 <code>/mesos</code>으로 끝나며 그 사이에 마스터 서버 IP와 포트(2181)를 입력합니다. 각각의 마스터 서버들은 콤마(,)로 구분합니다. </p>

<pre><code>sudo vi /etc/mesos/zk

zk://192.168.66.11:2181,192.168.66.12:2181,192.168.66.13:2181/mesos  
</code></pre>

<p><code>quorum</code>을 설정합니다. <br>
<code>quorum</code>은 마스터 노드 수를 2로 나눈 값보다 큰 수를 넣는다. <br>
<mark><strong>quorum (마스터 노드 수 (3) / 2) + 1 = 2</strong></mark> </p>

<pre><code>sudo vi /etc/mesos-master/quorum  
2  
</code></pre>

<p>mesos master의 ip와 hostname을 설정합니다.  </p>

<pre><code>echo 192.168.66.11 | sudo tee /etc/mesos-master/ip

sudo cp /etc/mesos-master/ip /etc/mesos-master/hostname  
</code></pre>

<h6 id="marathon">Marathon 설정</h6>

<pre><code>sudo mkdir -p /etc/marathon/conf  
sudo cp /etc/mesos-master/hostname /etc/marathon/conf


sudo cp /etc/mesos/zk /etc/marathon/conf/master  
sudo cp /etc/marathon/conf/master /etc/marathon/conf/zk

sudo vi /etc/marathon/conf/zk  
zk://192.168.66.11:2181,192.168.66.12:2181,192.168.66.13:2181/marathon  
</code></pre>

<h6 id="">기타 설정</h6>

<p>Mesos slave 서비스를 비활성화합니다.  </p>

<pre><code>sudo stop mesos-slave  
echo manual | sudo tee /etc/init/mesos-slave.override  
</code></pre>

<h6 id="">서비스 재시작</h6>

<p>Zookeeper, Marathon 그리고 Mesos Master 서비스를 재시작합니다.  </p>

<pre><code>sudo restart zookeeper  
sudo restart mesos-master  
sudo restart marathon  
</code></pre>

<blockquote>
  <p>master2, master3 서버도 동일한 방법으로 설정합니다.</p>
</blockquote>

<h4 id="mesosslave">Mesos slave 설치</h4>

<pre><code>sudo apt-get install mesos  
</code></pre>

<h4 id="mesosslave">Mesos slave 설정</h4>

<h6 id="mesos">Mesos 설정</h6>

<p>‘zk://’ 형식의 Zookeeper URL을 설정합니다. <br>
Zookeeper URL은 <code>zk://</code>로 시작하고 <code>/mesos</code>으로 끝나며 그 사이에 마스터 서버 IP와 포트(2181)를 입력합니다. 각각의 마스터 서버들은 콤마(,)로 구분합니다. </p>

<pre><code>sudo vi /etc/mesos/zk

zk://192.168.66.11:2181,192.168.66.12:2181,192.168.66.13:2181/mesos  
</code></pre>

<p>Mesos slave의 ip와 hostname을 설정합니다.  </p>

<pre><code>echo 192.168.66.51 | sudo tee /etc/mesos-master/ip

sudo cp /etc/mesos-master/ip /etc/mesos-master/hostname  
</code></pre>

<h6 id="">기타 설정</h6>

<p>Zookeeper를 비활성화합니다.  </p>

<pre><code>sudo stop zookeeper  
echo manual | sudo tee /etc/init/zookeeper.override  
</code></pre>

<p>Mesos master를 비활성화합니다.  </p>

<pre><code>echo manual | sudo tee /etc/init/mesos-master.override  
sudo stop mesos-master  
</code></pre>

<h6 id="">서비스 재시작</h6>

<pre><code>sudo restart mesos-slave  
</code></pre>

<h4 id="">테스트</h4>

<p>Mesos master 에 접속합니다.  </p>

<pre><code>http://192.168.66.11:5050  
</code></pre>

<p><a href="http://imgur.com/ci0lRPm"><img src="http://i.imgur.com/ci0lRPm.png" title="source: imgur.com"></a></p>

<p>프레임워크과 스케쥴러를 확인합니다.
<a href="http://imgur.com/xNxbHtD"><img src="http://i.imgur.com/xNxbHtD.png" title="source: imgur.com"></a></p>

<p>slave 정보를 확인합니다. <br>
<a href="http://imgur.com/NN3GD88"><img src="http://i.imgur.com/NN3GD88.png" title="source: imgur.com"></a></p>

<p>Marathon <br>
<a href="http://imgur.com/JEgGuLv"><img src="http://i.imgur.com/JEgGuLv.png" title="source: imgur.com"></a></p>

<p>Chronos <br>
<a href="http://imgur.com/i57UGKN"><img src="http://i.imgur.com/i57UGKN.png" title="source: imgur.com"></a></p>

<h6 id="">참고</h6>

<ul>
<li><a href="http://mesos.apache.org/">http://mesos.apache.org/</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04</a></li>
<li><a href="http://www.hanbit.co.kr/store/books/look.php?p_code=E5920037963">처음 시작하는 Apache Mesos</a></li>
</ul>]]></content:encoded></item><item><title><![CDATA[babun을 아시나요?]]></title><description><![CDATA[<p><a href="http://imgur.com/uo1Gk9x"><img src="http://i.imgur.com/uo1Gk9x.png" title="source: imgur.com"></a></p>

<p>혹시 <a href="http://babun.github.io/">babun</a>을 아시나요? <br>
당신이 Windows를 사용하고 Linux 서버에 자주 ssh로 접속한다면 <code>babun</code>은 <code>putty</code>를 대신할 좋은 대안이 될 수 있습니다.</p>

<p>Windows에서 <code>babun</code>을 실행하고 ssh로 서버에 접속하고 vi로 간단히 소스를 편집하고 git으로 편집된 소스를 commit 하는 등 Linux를 사용하는 것과 거의 동일한 사용자 경험을 느낄 수 있습니다. <br>
그리고</p>]]></description><link>http://iamartin-gh.herokuapp.com/babun/</link><guid isPermaLink="false">d08f8d56-c663-42c4-969d-0b168e20e0b0</guid><category><![CDATA[Windows]]></category><category><![CDATA[ssh]]></category><category><![CDATA[shell]]></category><category><![CDATA[zsh]]></category><category><![CDATA[babun]]></category><dc:creator><![CDATA[행복한 유파]]></dc:creator><pubDate>Fri, 24 Jun 2016 08:43:17 GMT</pubDate><content:encoded><![CDATA[<p><a href="http://imgur.com/uo1Gk9x"><img src="http://i.imgur.com/uo1Gk9x.png" title="source: imgur.com"></a></p>

<p>혹시 <a href="http://babun.github.io/">babun</a>을 아시나요? <br>
당신이 Windows를 사용하고 Linux 서버에 자주 ssh로 접속한다면 <code>babun</code>은 <code>putty</code>를 대신할 좋은 대안이 될 수 있습니다.</p>

<p>Windows에서 <code>babun</code>을 실행하고 ssh로 서버에 접속하고 vi로 간단히 소스를 편집하고 git으로 편집된 소스를 commit 하는 등 Linux를 사용하는 것과 거의 동일한 사용자 경험을 느낄 수 있습니다. <br>
그리고 무엇보다도 zsh와 oh my zsh를 품고 있어서 <code>putty</code>보다 아름답습니다.</p>

<p>한번 사용해 보세요.</p>

<p><a href="http://imgur.com/dfgXipp"><img src="http://i.imgur.com/dfgXipp.png" title="source: imgur.com"></a></p>

<p><a href="http://imgur.com/EBrjiDJ"><img src="http://i.imgur.com/EBrjiDJ.png" title="source: imgur.com"></a></p>

<p><a href="http://imgur.com/p8rGQER"><img src="http://i.imgur.com/p8rGQER.png" title="source: imgur.com"></a></p>

<h5 id="">구성품</h5>

<ul>
<li>Cygwin</li>
<li>Package manager <code>pact</code></li>
<li>Shell <code>zsh</code> <code>bash</code></li>
<li>Console <code>xterm-256</code></li>
<li>Proxying <code>.babunrc</code></li>
<li>Developer tools <code>Python, Perl</code> <code>git</code> </li>
</ul>]]></content:encoded></item></channel></rss>