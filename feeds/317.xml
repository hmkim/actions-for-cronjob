<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>문동선의 블로그</title>
<link>https://dsmoon.tistory.com/</link>
<description></description>
<language>ko</language>
<pubDate>Mon, 13 May 2019 11:36:39 +0900</pubDate>
<generator>TISTORY</generator>
<managingEditor>21세기 유목민</managingEditor>
<item>
<title>Deep learning 발표자료 2015</title>
<link>https://dsmoon.tistory.com/entry/Deep-learning-%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C-2015</link>
<description>&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/ArvGVmKb4Eg6fn&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/DongsunMoon/1-v2-60535763&quot; title=&quot;1, 빅데이터 시대의 인공지능 문동선 v2&quot; target=&quot;_blank&quot;&gt;1, 빅데이터 시대의 인공지능 문동선 v2&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;//www.slideshare.net/DongsunMoon&quot; target=&quot;_blank&quot;&gt;Dongsun Moon&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
<category>이론</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/131</guid>
<comments>https://dsmoon.tistory.com/entry/Deep-learning-%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C-2015#entry131comment</comments>
<pubDate>Wed, 30 Dec 2015 14:06:01 +0900</pubDate>
</item>
<item>
<title>TensorBoard: Graph Visualization</title>
<link>https://dsmoon.tistory.com/entry/TensorBoard-Graph-Visualization</link>
<description>&lt;p&gt;&amp;nbsp; TensorFlow computation graph는 강력하지만 조금 복잡해하다. 그래프 시각화는 이해와 디버깅에 도움이 될 것이다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; line-height: 1.625rem; overflow-x: auto; font-family: Roboto, Helvetica, sans-serif; font-size:12pt;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/graph_vis_animation.gif&quot; alt=&quot;Visualization of a TensorFlow graph&quot; title=&quot;Visualization of a TensorFlow graph&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 내가 만든 그래프를 보려면, 우선 로그 디렉토리를 지정하여 TensorBoard를 실행해야 한다. 자세한 방법은 다음 글을 참고 하면 된다.&amp;nbsp;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;a href=&quot;http://dsmoon.tistory.com/entry/TensorBoard-Visualizing-Learning&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;http://dsmoon.tistory.com/entry/TensorBoard-Visualizing-Learning&lt;/a&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;Name scoping and nodes&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;   &lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; TensorFlow 그래프들은 보통 수천개의 노드들을 갖기 때문에 한번에 쉽게 보기도 어렵고, 보통 그래프 툴로도 보기가 어렵다. 변수 이름들은 scope되고 시각화 작업이 이 정보를 이용해서 그래프의 노드들을&amp;nbsp;계층적으로 보여준다. 기본적으로는 최상위 계층이 보여진다. 아래와 같은 예제가 있다고 하자&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
import tensorflow as tf

with tf.name_scope('hidden') as scope:
    a = tf.constant(5, name = 'alpha')
    W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name = 'weights')
    b = tf.Variable(tf.zeros([1]), name = 'biases')
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 세노드는 scope로 인해 접두어가 붙어,&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;hidden/alpha,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;hidden/wtight,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;hidden/biases&lt;/span&gt;&amp;nbsp;네이밍 된다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 기본적으로 시각화에서 3개 노드는 hidden 레이블이 붙은 한 노드로 합쳐질 것이다. 추가적인 디테일들을 사라진다. 더블 클릭 하거나 오렌지색 &quot;+&quot; 부분을 클릭하여 노드를 확장할 수 있다. 그러면 alpha, weights, biases subnode들을 볼 수 있다.&lt;br /&gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;table width=&quot;100%;&quot; style=&quot;box-sizing: border-box; margin: 20px 0px 0px; padding: 0px; border: 1px solid rgb(236, 239, 241); border-collapse: collapse; color: rgb(0, 0, 0); font-family: Roboto, Helvetica, sans-serif;&quot;&gt;&lt;tbody style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/pool1_collapsed.png&quot; alt=&quot;Unexpanded name scope&quot; title=&quot;Unexpanded name scope&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/pool1_expanded.png&quot; alt=&quot;Expanded name scope&quot; title=&quot;Expanded name scope&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px; height: 55px;&quot;&gt;&lt;p&gt;기본적인 모습. 더블 클릭하거나 ,&amp;nbsp;&quot;+&quot; 부분을 클릭하여 확장 가능.&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px; height: 55px;&quot;&gt;&lt;p&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;확장된 모습. 더블 클릭하거나 ,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&quot;-&quot; 부분을 클릭하여 원래 모습으로 작게 만들 수&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; &amp;nbsp;노드들을 이름 스코프로 묶는 것은 보기 좋은 그래프를 만들기 위해 필수적이다. 이름 스코프 작업에 공을 들일 수록 더 좋은 시각화가 가능해질 것이다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 위 그림은 시각화의 두가지 측면을 보여준다.&amp;nbsp;TensorFlow 그래프는 두가지 종류의 연결을 가진다. 바로, 데이터 의존성과 제어 의존성이다. 데이터 의존성은 두 op 사이의 텐서의 흐름을 보여주고, 실선 화살표로 보여진다. 반면 제어 의존성은 점선으로 표시된다. 위 우측 그림을 보면 모두 실선으로 데이터 흐름을 보여주는데, CheckNumerics와 control_dependency 사이만 점선으로 표시되어 있다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 레이아웃을 단순하게 하는 두번째 트릭이 있다. 대부분의 TensorFlow 그래프들은 연결이 매우 많은 노드를 몇개 갖는다. 예를 들어 많은 노드들이 초기화 스텝에서 제어 의존성을 갖는다. 초기화 노드들 사이의 모든 의존성들을 그리게 되면 매우 보기 힘들고 어수선한 그림이 될 것이다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 불필요하게 지저분해지는 것을 줄이기 위해, high-degree 노드들은 우측의 남는 공간에 따로 그려진다. 선을 그리는 대신, 연결을 나타내는 작은 아이콘들을 그린다. 이렇게 부수적인 노드들을 별도로 분리하는 작업이 중요한 데이터를 훼손하진 않는다. 이러한 노드들은 대부분 기록과 관련이 있기 때문이다.&lt;/p&gt;&lt;table width=&quot;100%;&quot; style=&quot;box-sizing: border-box; margin: 20px 0px 0px; padding: 0px; border: 1px solid rgb(236, 239, 241); border-collapse: collapse; color: rgb(0, 0, 0); font-family: Roboto, Helvetica, sans-serif;&quot;&gt;&lt;tbody style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/conv_1.png&quot; alt=&quot;conv_1 is part of the main graph&quot; title=&quot;conv_1 is part of the main graph&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/save.png&quot; alt=&quot;save is extracted as auxiliary node&quot; title=&quot;save is extracted as auxiliary node&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;p&gt;&amp;nbsp; conv_1 노드는&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;save 노드&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;에 연결되어 있다. 오른쪽에 작게 save 노드가 표시되어 있다.&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;code style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; position: relative; font-family: 'Source Code Pro', monospace; font-size: 16px;&quot;&gt;&lt;span style=&quot;font-family: Dotum, 돋움; font-size: 9pt;&quot;&gt;save 는 고차원 노드라서 오른쪽 여분 공간에 표시된다. conv_1과의 연결이 노드 왼쪽에 작게 표시된다. 간결성을 위해 처음 5개만 보여주고 12개는 축약되어 있음을 볼 수 있다.&lt;/span&gt;&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 우측의 여분 공간이라고 하면 이해가 안 될 수도 있을 것 같아, 전체적인 그림을 아래 첨부한다. 그림 전제에 서로 연결된 노드들의 그래프가 보여지고 우측 상단에 고차원 노드들이 몇개 표시된 것을 볼 수 있다. 저 노드들의 연결이 모두 표시 되었다면, 중요한 정보들이 부가적인 정보에 묻혀서 보기 힘들어지고, 일부 노드만 따로 선택해서 보는 지루하고 힘든 작업을 매번 해야 했을 것이다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto; text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:600px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/2239DE4E5674D12402&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:26:08.png&quot; height=&quot;347&quot; width=&quot;600&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 구조 단순화를 위한 마지막 트릭은 series collapsing이다. 노드의 이름이 끝에 숫자만 다르고 isomorphic한 것들은 단일 스택으로 보여준다. 매우 긴 시퀀스가 있다면 매우 많이 단순화를 해준다. 이 경우도 더블 클릭으로 확장해서 볼 수 있다.&lt;/p&gt;&lt;table width=&quot;100%;&quot; style=&quot;box-sizing: border-box; margin: 20px 0px 0px; padding: 0px; border: 1px solid rgb(236, 239, 241); border-collapse: collapse; color: rgb(0, 0, 0); font-family: Roboto, Helvetica, sans-serif;&quot;&gt;&lt;tbody style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/series.png&quot; alt=&quot;Sequence of nodes&quot; title=&quot;Sequence of nodes&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/series_expanded.png&quot; alt=&quot;Expanded sequence of nodes&quot; title=&quot;Expanded sequence of nodes&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;축약된 모습&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;p&gt;더블 클릭으로 확장된 모습&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp;마지막으로, 가독성을 위한 것이 하나 더 있는데, constant와 summary node들을 위한 특별한 아이콘들을 사용한다는 것이다. 아래 테이블을 참고하자.&lt;/p&gt;&lt;table style=&quot;box-sizing: border-box; margin: 20px 0px 0px; padding: 0px; border: 1px solid rgb(236, 239, 241); border-collapse: collapse; color: rgb(0, 0, 0); font-family: Roboto, Helvetica, sans-serif;&quot;&gt;&lt;thead style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;th style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;Symbol&lt;/th&gt;&lt;th style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;Meaning&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/namespace_node.png&quot; alt=&quot;Name scope&quot; title=&quot;Name scope&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;span style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;High-level&lt;/span&gt;&amp;nbsp;node representing a name scope. Double-click to expand a high-level node.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/horizontal_stack.png&quot; alt=&quot;Sequence of unconnected nodes&quot; title=&quot;Sequence of unconnected nodes&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;Sequence of numbered nodes that are not connected to each other.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/vertical_stack.png&quot; alt=&quot;Sequence of connected nodes&quot; title=&quot;Sequence of connected nodes&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;Sequence of numbered nodes that are connected to each other.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/op_node.png&quot; alt=&quot;Operation node&quot; title=&quot;Operation node&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;An individual operation node.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/constant.png&quot; alt=&quot;Constant node&quot; title=&quot;Constant node&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;A constant.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/summary.png&quot; alt=&quot;Summary node&quot; title=&quot;Summary node&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;A summary node.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/dataflow_edge.png&quot; alt=&quot;Data flow edge&quot; title=&quot;Data flow edge&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;Edge showing the data flow between operations.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/control_edge.png&quot; alt=&quot;Control dependency edge&quot; title=&quot;Control dependency edge&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;Edge showing the control dependency between operations.&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/reference_edge.png&quot; alt=&quot;Reference edge&quot; title=&quot;Reference edge&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241);&quot;&gt;A reference edge showing that the outgoing operation node can mutate the incoming tensor.&lt;br /&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;b style=&quot;font-size: 18pt; line-height: 1.5;&quot;&gt;Interaction&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 확대를 해볼수도 있고 패닝을 해볼 수도 있다.&amp;nbsp;이동하기 위해 클릭하고 드래그할 수 있고, 스크롤하여 확대도 할 수 있다. 더블클릭하거나 &quot;+&quot; 네임 스코프를 펼쳐볼 수 도 있다. 편의를 위해 우측 하단에 미니맵도 표시된다.&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 열려진 노드를 닫기 위해서 다시 더블 클릭하거나 &quot;-&quot; 버튼을 클릭한다. 클릭을 한 번만 하면 노드가 선택된다. 선택된 노드는 어두운 색으로 바뀌고 아래와 같은 정보 카드가 표시된다. 여기에는 노드에 대한 자세한 정보와 이에 연결된 노드들이 표시된다.&lt;/p&gt;&lt;table width=&quot;100%;&quot; style=&quot;box-sizing: border-box; margin: 20px 0px 0px; padding: 0px; border: 1px solid rgb(236, 239, 241); border-collapse: collapse; color: rgb(0, 0, 0); font-family: Roboto, Helvetica, sans-serif;&quot;&gt;&lt;tbody style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/infocard.png&quot; alt=&quot;Info card of a name scope&quot; title=&quot;Info card of a name scope&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/infocard_op.png&quot; alt=&quot;Info card of operation node&quot; title=&quot;Info card of operation node&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;p&gt;conv2 네임 스콥에 관한 자세한 정보 카드.&lt;/p&gt;&lt;p&gt;입력과 출력별로 보여짐.&lt;/p&gt;&lt;p&gt;네임 스콥에 관한 속성 정보는 없음.&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;p&gt;인포카드가 DecodeRaw 연산 노드에 대한 자세한 정보를 보여준다.&lt;/p&gt;&lt;p&gt;입력과 출력 정보와 속성 정보도 표시됨.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 노드를 선택하는 것은 고차원 노드들을 이해하는데 유용하다. 고차원 노드를 선택하면 이와 연결된 노드들도 선택된다. &lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;어떠한 노느들이 저장되고, 안되고 있는지 확인을 쉽게할 수 있다.&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 인포카드에서 노드 이름을 클릭하면 그것을 선택하는 것과 같다. 따라서 시점이 자동으로 해당 노드가 보이게 이동하게 된다.&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&amp;nbsp; 색인 위에 있는 컬러 메뉴를 통해 두가지 color scheme 중에 하나를 고를 수 있다. 기본 구조 뷰에서는 두 레벨 노드가 같은 구조를 가지고, 같은 색이 된다. 유일한 구조를 가진 노드는 회색이 된다.&amp;nbsp;다른 뷰에서는 어떤 디바이스가 어떤 노드를 실행했는지 보여준다. 네임 스콥은 실행된 비율에 따라 적절히 색칠된다.&lt;/p&gt;&lt;table width=&quot;100%;&quot; style=&quot;box-sizing: border-box; margin: 20px 0px 0px; padding: 0px; border: 1px solid rgb(236, 239, 241); border-collapse: collapse; color: rgb(0, 0, 0); font-family: Roboto, Helvetica, sans-serif;&quot;&gt;&lt;tbody style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/colorby_structure.png&quot; alt=&quot;Color by structure&quot; title=&quot;Color by structure&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/colorby_device.png&quot; alt=&quot;Color by device&quot; title=&quot;Color by device&quot; style=&quot;box-sizing: border-box; margin: 0px; padding: 0px; border: none; outline: 0px; max-width: 100%;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;box-sizing: border-box; margin: 0px; padding: 0px;&quot;&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px; height: 55px;&quot;&gt;&lt;p&gt;Structure view&lt;/p&gt;&lt;p&gt;고유한 구조를 가진 것들은 회색.&lt;/p&gt;&lt;p&gt;conv1,2 는 같은 구조를 가지고 있어서 빨간 색.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/td&gt;&lt;td style=&quot;box-sizing: border-box; margin: 0px; padding: 10px; border: 1px solid rgb(236, 239, 241); width: 331.2px; height: 55px;&quot;&gt;&lt;p&gt;Device view&lt;br /&gt;보라색은 GPU, 녹색은 CPU에서 실행된 비율.&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p style=&quot;box-sizing: border-box; margin-top: 20px; margin-right: 0px; margin-bottom: 20px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin-top: 20px; margin-right: 0px; margin-bottom: 20px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;span style=&quot;font-size: 24px; line-height: 28.8px;&quot;&gt;&lt;b&gt;Reference&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin-top: 20px; margin-right: 0px; margin-bottom: 20px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html&lt;/a&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;box-sizing: border-box; margin: 20px 0px; padding: 0px; overflow-x: auto;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/130</guid>
<comments>https://dsmoon.tistory.com/entry/TensorBoard-Graph-Visualization#entry130comment</comments>
<pubDate>Sat, 19 Dec 2015 14:40:58 +0900</pubDate>
</item>
<item>
<title>TensorBoard: Visualizing Learning</title>
<link>https://dsmoon.tistory.com/entry/TensorBoard-Visualizing-Learning</link>
<description>&lt;p&gt;&amp;nbsp; 텐서플로우에서 사용하는 거대한 딥뉴럴네트워크 계산들은 매우 복잡하고 혼란스러울 수도 있다. 이해와 디버깅, 최적화를 돕기 위해, &amp;nbsp;TensorBoard라는 시각화 툴이 있다. TensorBoard를 TensorFlow 그래프를 시각화하기 위해 사용할 수도 있고, 그래프 수행의&amp;nbsp;정량적인 메트릭들을 그려볼 수도 있다. TensorBoard를 세팅해서 띄우면 아래와 같은 모습이다.&lt;/p&gt;&lt;p&gt;
&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/mnist_tensorboard.png&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;데이터 직렬화하기&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; TensorBoard는 TensorFlow 이벤트 파일을 읽음으로써 동작한다. 여기에는 TensorFlow를 실행하면서 발생된 요약 데이터가 담겨있다. 이어서 요약 데이터에 대한,&amp;nbsp;TensorBoard의 일반적인 생명주기를 다룬다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 우선, 요약 데이터를 수집하기 원하는&amp;nbsp;TensorFlow 그래프를 만들고, 요약 연산을 할 노드를 결정한다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 예를 들어, MNIST를 위한 CNN을 학습한다고 하자. 시간에 따른 학습률과 목적함수의&amp;nbsp;변화를 기록하고 싶을 것이다.&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp;데이터를 수집하기 위해&amp;nbsp;&lt;b&gt;&lt;span style=&quot;color: rgb(152, 0, 0);&quot;&gt;scalar_summary&lt;/span&gt; &lt;/b&gt;op을 학습률 노드, 손실&amp;nbsp;노드에 각각 추가하라. 그리고 scalar_summary 각각에 &amp;nbsp;'learning rate'이나 'loss function' 같이 의미 있는 태그를 달아줘라.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 아마도 특정 레이어의 활성도, 그래디언트, 웨이트 등의&amp;nbsp;분포를 보고 싶을 수 있다. 그래디언트 출력이나 웨이트 변수 등에 각각&amp;nbsp;&lt;b&gt;&lt;span style=&quot;color: rgb(152, 0, 0);&quot;&gt;histogram_summary&lt;/span&gt;&lt;/b&gt; op을 붙여 이러하한 데이터를 수집할 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; TensorFlow안의 연산들은 당신이 그들이나 그들의 출력에 종속적인 op을 실행(run)할 때까지 아무것도 하지 않는다. 그리고 우리가 만든 요약 노드들은 부가적인 것일뿐, 당신이 실행중인 어떠한 op도 그것들에 종속적이지 않다. 따라서, 요약을 만들려면 요약 노드 모두를 실행해야 한다. 그것들을 일일이 손으로 관리한다는 것은 피곤한 일이다. 따라서 이들을 모든 요약 데이터를 생산하는&amp;nbsp;단일 op으로 묶기 위해&amp;nbsp;tf.merge_all_summaries를 사용하자.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 그러면 이제 합쳐진 요약 op을 실행할 수 있다. 이 op은 당신의 모든 요약 데이터에 대한,&amp;nbsp;직렬화된 Summary protobuf 객체를 정해진 step에&amp;nbsp;만들어 낼 것이다.&amp;nbsp;마지막으로, 요약 데이터를 디스크에 저장하기 위해, 요약 protobuf를 tr.train.SummaryWriter에 넘겨주어야 한다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; SummaryWriter 는 생성자에 logdir를 필요로 한다. 여기에서 logdir은 좀 중요한데, 모든 이벤트들을 기록할 디렉토리의 위치이기 때문이다.&amp;nbsp;게다가, SummaryWriter는 생성자에서 옵션으로&amp;nbsp;GraphDef를 받을 수도 있다. 만약 그렇다면,&amp;nbsp;TensorBoard 는 그래프도 시각화 해줄 것이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 자 이제 당신의 그래프를 수정하고 SummaryWriter를 실행해보자. 원한다면, 모든 step마다 합쳐진 요약 op을 실행할 수도 있고, 상당한 양의 훈련 데이터를 기록할 수도 있다.&amp;nbsp;당신이 원하는 것보다도 많은 것이 있을 수도 있다. 대신 n 스탭마다 실행하는 것을 고려해보자.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 아래 코드 예제는 MNIST 튜토리얼을 조금 수정한 것이다. 요약 op 을 추가했고, 매 10스텝마다 수행한다. 만약 이를 실행하고 &amp;nbsp;tensorboard --logdir=/tmp/mnist_logs 를 실행하면, 훈련동안의 웨이트와 정확도 같은 통계값들을 시각화해서 볼 수 있을 것이다. 아래 코드는 일부분이고 &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/mnist/mnist_with_summaries.py&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;풀버젼&lt;/a&gt;은 따로 참고하길 바란다.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;
# Create the model
x = tf.placeholder(&quot;float&quot;, [None, 784], name=&quot;x-input&quot;)
W = tf.Variable(tf.zeros([784,10]), name=&quot;weights&quot;)
b = tf.Variable(tf.zeros([10], name=&quot;bias&quot;))

# use a name scope to organize nodes in the graph visualizer
with tf.name_scope(&quot;Wx_b&quot;) as scope:
  y = tf.nn.softmax(tf.matmul(x,W) + b)

# Add summary ops to collect data
w_hist = tf.histogram_summary(&quot;weights&quot;, W)
b_hist = tf.histogram_summary(&quot;biases&quot;, b)
y_hist = tf.histogram_summary(&quot;y&quot;, y)

# Define loss and optimizer
y_ = tf.placeholder(&quot;float&quot;, [None,10], name=&quot;y-input&quot;)
# More name scopes will clean up the graph representation
with tf.name_scope(&quot;xent&quot;) as scope:
  cross_entropy = -tf.reduce_sum(y_*tf.log(y))
  ce_summ = tf.scalar_summary(&quot;cross entropy&quot;, cross_entropy)
with tf.name_scope(&quot;train&quot;) as scope:
  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)

with tf.name_scope(&quot;test&quot;) as scope:
  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))
  accuracy_summary = tf.scalar_summary(&quot;accuracy&quot;, accuracy)

# Merge all the summaries and write them out to /tmp/mnist_logs
merged = tf.merge_all_summaries()
writer = tf.train.SummaryWriter(&quot;/tmp/mnist_logs&quot;, sess.graph_def)
tf.initialize_all_variables().run()

# Train the model, and feed in test data and record summaries every 10 steps

for i in range(1000):
  if i % 10 == 0:  # Record summary data, and the accuracy
    feed = {x: mnist.test.images, y_: mnist.test.labels}
    result = sess.run([merged, accuracy], feed_dict=feed)
    summary_str = result[0]
    acc = result[1]
    writer.add_summary(summary_str, i)
    print(&quot;Accuracy at step %s: %s&quot; % (i, acc))
  else:
    batch_xs, batch_ys = mnist.train.next_batch(100)
    feed = {x: batch_xs, y_: batch_ys}
    sess.run(train_step, feed_dict=feed)

print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))

&lt;/code&gt;
&lt;/pre&gt;


&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 이제 TensorBoard를 이용해 데이터를 시각화할 준비가 다 되었다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;Launching TensorBoard&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; TensorBoard를 다음과 같이 실행해보자.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;
$ python tensorflow/tensorboard/tensorboard.py --logdir=path/to/log-directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp; &quot;logdir&quot; 는 SummaryWriter가 데이터를 직렬화하여 저장한 디렉토리를 가리킨다.&amp;nbsp;만약 &quot;logdir&quot; 디렉토리가 별도로 실행한 직렬화 파일을 가진&amp;nbsp;서브디렉토리를 가진다면 TensorBoard는 그 역시도 시각화해준다. TensorBoard를 실행하면, 웹브라우져로 &lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;localhost:6006&lt;/span&gt;&lt;/b&gt;에 접속하여 화면을 볼 수 있다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 만약 pip로 설치된 TensorFlow라면, tensorboard는 system path에 설치 되었을 것이고, 더 간단한 명령으로 실행 할 수도 있다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;
$ tensorboard --logdir=/path/to/log-directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;nbsp; TensorBoard를 보면, 우측상단에 네비게이션 탭이 보일 것이다. 각 탭은 시각화 할 수 있는직렬화된 데이터들의 집합을 나타낸다.&amp;nbsp;TensorBoard에서 보이는 로그에 탭과 관련된 데이터가 없다면, 해당 테이터를 직렬화하는 방법에 대한 메세지가 표시될 것이다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Reference&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.htmlhttps://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html&lt;/a&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/128</guid>
<comments>https://dsmoon.tistory.com/entry/TensorBoard-Visualizing-Learning#entry128comment</comments>
<pubDate>Thu, 17 Dec 2015 20:03:32 +0900</pubDate>
</item>
<item>
<title>[TensorFlow tutorial] Convolutional Neural Networks</title>
<link>https://dsmoon.tistory.com/entry/TensorFlow-tutorial-Convolutional-Neural-Networks</link>
<description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;입력 데이터 : CIFAR-10&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;&lt;span style=&quot;font-size: 9pt;&quot;&gt;&amp;nbsp; 아래와 같이 10개 분류의 benchmark이다. 32x32 RGB 이미지로 과거에는 연구용으로 많이 쓰였지만 요즘에 튜토리얼 소개용으로 많이 사용된다.&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/cifar_samples.png&quot; alt=&quot;CIFAR-10 Samples&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;Traning&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp; 텐서 플로우에서 제공하는 cifar10 튜토리얼을&amp;nbsp;100만 스텝으로 직접 돌려보니 48 시간이나 걸렸다. GTX-980로 가속을 했음에도 제법 오래 걸렸다. 테슬라에서 돌려도 아래와 같다고 하니 대략 Tesla K40과 비슷한 성능을 보여주었다.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;table class=&quot;txc-table&quot; width=&quot;564&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; border=&quot;0&quot; style=&quot;border:none;border-collapse:collapse;;font-family:돋움;font-size:12px&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;width: 188px; height: 24px; border: 1px solid rgb(204, 204, 204); background-color: rgb(250, 237, 125);&quot;&gt;&lt;p style=&quot;text-align: center;&quot;&gt;System&amp;nbsp;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 188px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-top-width: 1px; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: rgb(250, 237, 125);&quot;&gt;&lt;p style=&quot;text-align: center;&quot;&gt;Step time (sec/batch)&amp;nbsp;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 188px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-top-width: 1px; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: rgb(250, 237, 125);&quot;&gt;&lt;p style=&quot;text-align: center;&quot;&gt;&amp;nbsp;Accuarcy&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width:188;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;&quot;&gt;&lt;p&gt;&amp;nbsp;1 Tesla K20m&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width:188;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;&quot;&gt;&lt;p&gt;0.35~0.60&amp;nbsp;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width:188;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;&quot;&gt;&lt;p&gt;~86% at 60K&amp;nbsp;steps (5 hours)&amp;nbsp;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width:188;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;&quot; rowspan=&quot;1&quot;&gt;&amp;nbsp;1 Tesla K40m&lt;/td&gt;&lt;td style=&quot;width:188;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;&quot; rowspan=&quot;1&quot;&gt;0.25~0.35&amp;nbsp;&lt;/td&gt;&lt;td style=&quot;width:188;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;&quot; rowspan=&quot;1&quot;&gt;&lt;p&gt;~86% at 100K&amp;nbsp;steps (4 hours)&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:600px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/214B324F56718DA310&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-15 01:03:20.png&quot; height=&quot;395&quot; width=&quot;600&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:600px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/274C344F56718DA60F&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:12:16.png&quot; height=&quot;959&quot; width=&quot;600&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;Graph&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp; TensorBoard로 그래프를 확인하면 아래와 같은 모양이다. 안타깝지만 블로그에 업로드가 고해상도로 안된다. 왼쪽이 CNN이다.&amp;nbsp;&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:600px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/2373354C567190DA0A&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:26:08.png&quot; height=&quot;347&quot; width=&quot;600&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;Training 과정 분석&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left; clear: none; float: none;&quot;&gt;&amp;nbsp; 학습률은 다음과 같다. 처음엔 크게 하여 빠르게 수렴 시킨후 미세하게 튜닝한다.&amp;nbsp;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:481px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/222FD6505671961013&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:48:54.png&quot; height=&quot;327&quot; style=&quot;&quot; width=&quot;481&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 다음은 Cross Entropy의 변화이다. 처음에는 급격히 줄어들지만 30만 스텝이 지난 후에는 거의 변동이 없다. 위 로그에서 보면 알 수 있듯이 0.10 내외를 진동했다. 중간에 두번 정도 급격히 줄어드는 구간이 있다는 것은 눈여겨 볼만 하다. 이는 학습률의 수치 변화 시점과 일치하고 있다. Fine-tuning 되면서 점차 더 개선된 것이다. 처음에는 큰 학습률로 빠르게 최적 지점 가까이 학습하고 작은 학습률로 최적화 하는 것이 좋다. 학습률이 너무 작으면&amp;nbsp;시간이 많이 걸리고, 너무 크면 발산하거나 진동한다.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:481px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/242BE8365671935C31&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:37:33.png&quot; height=&quot;329&quot; style=&quot;&quot; width=&quot;481&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left; clear: none; float: none;&quot;&gt;&lt;span style=&quot;text-align: left; line-height: 1.5;&quot;&gt;&amp;nbsp; 처음의 급격한 최적화 지점의 변화는 conv1의&amp;nbsp;sparsity 변화의 영향을 받은 것으로 보인다.&amp;nbsp;추상화 되지 않은 feature의 sparsity가 떨어지면서 최적화 되고 더 표현력 좋은 representation을 찾았다고 볼 수 있다.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:481px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/2647CD47567194F40E&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:42:05.png&quot; height=&quot;328&quot; style=&quot;&quot; width=&quot;481&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left; clear: none; float: none;&quot;&gt;&lt;span style=&quot;text-align: left; line-height: 1.5;&quot;&gt;&amp;nbsp; 이는 마지막 레이어에도 다음과 같이 영향을 주었다. 200k후반대의 최적화 때는&amp;nbsp;conv1에 변화 없이 최적화가 이루어 졌는데, 이는 gradient vanish에 의한 것으로 생각할 수도 있지만, 이보다는 좋은 representation을 찾았기 때문이라고 보는 게 더 적절할 것이다. 학습 후반으로 갈수록 상위 레이어에서 추상화가 잘 된&amp;nbsp;representation을 찾아 최적화 하는 것이 더 유리해진다. 하위 레이어는 edge detector나 corner detector 정도의 기능만 하기 때문에 이러한 방향으로 최적화 되면 그 후에는 웨이트를 변경하여 시스템을 최적화 하는데 한계가 있기 때문이다.&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:476px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/211C4B415671979630&quot; filemime=&quot;image/jpeg&quot; filename=&quot;스크린샷, 2015-12-17 01:55:15.png&quot; height=&quot;328&quot; style=&quot;&quot; width=&quot;476&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left; clear: none; float: none;&quot;&gt;&lt;span style=&quot;text-align: left; line-height: 1.5;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;text-align: left; line-height: 1.5;&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;Reference&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html#convolutional-neural-networks&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html#convolutional-neural-networks&lt;/a&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/129</guid>
<comments>https://dsmoon.tistory.com/entry/TensorFlow-tutorial-Convolutional-Neural-Networks#entry129comment</comments>
<pubDate>Thu, 17 Dec 2015 01:27:33 +0900</pubDate>
</item>
<item>
<title>TensorFlow 101 - part2</title>
<link>https://dsmoon.tistory.com/entry/TensorFlow-101-part2</link>
<description>&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;font-size: 24px; line-height: 28.8px;&quot;&gt;&lt;b&gt;모델 훈련하기&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 일단 그래프가 만들어지면,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;fully_connected_feed.py&lt;/b&gt; 안의 코드에 의해&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;반복적으로 훈련되고 평가될 수 있다.&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;그래프&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; run_training() 함수의 제일 앞부분에는&amp;nbsp;파이썬 &lt;b&gt;with&lt;/b&gt; 명령어가 있다. 이는 모든 built op들이 기본 전역 tf.Graph 인스턴스와 연관이 있다는 것을 의미한다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;with tf.Graph().as_default():
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; tf.Graph는 그룹으로 실행되는 op들의 집합니다. 대부분 TensorFlow 는 하나의 그래프만들 사용할 것이다. 더 복잡하게 여러개 그래프를 사용하는 것도 가능은 하지만 여기서 다루진 않는다.&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;세션&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 일단 빌드 준비가 끝나고 모든 필수적인 op들도 생성되면, 그래프를 실행하기 위해&amp;nbsp;tf.Session을 생성한다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;sess = tf.Session()
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 위와 같이 하지 않고, 스코핑을 위해&amp;nbsp;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;아래와 같이&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;with 블록에 Session을 생성할 수도 있다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;with tf.Session() as sess:
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; Session에 아무런 파라미터도 넘기지 않았다는 것은, 이 코드가 기본 로컬세션에서 수행됨을 의미한다. 세션을 생성하자마자, 모든 tf.Variable 인스턴스들은 sess.run() 호출에 의해 초기화된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;init = tf.initialize_all_variables()
sess.run(init)
&lt;/code&gt;&lt;/pre&gt;


&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; &lt;b&gt;sess.run()&lt;/b&gt;&amp;nbsp;함수는 파라미터로 전달된 op들에 해당하는 그래프의 완전한(complete) 부분집합을 수행한다. 첫 호출에서는, init op 은 tf.group이다. 이는 변수들의 초기화 로직 만을 담고 있다. 그래프의 나머저 어떤 부분도 여기서 실행되지 않는다. 이는 아래의 훈련 루프에서 수행된다.&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Train Loop&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 세션으로 변수들이 초기화 된 후에, 훈련은 시작될 수 있다.&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;사용자의 코드는 스텝별&amp;nbsp;훈련을 제어한다. 의미 있는 훈련을 할 수 있는, 가장 간단한 형태의 루프는 아래와 같다.&lt;/span&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;for step in xrange(max_steps):
    sess.run(train_op)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 그러나 이 튜토리을은 조금 더 복잡해서, 이전에 생성된&amp;nbsp;placeholder들과 맞게 입력 데이터를 slice-up 하게 된다.&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;Feed the Graph&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 각 스텝에서 코드는&amp;nbsp;feed dictionary를 생성한다. 여기에는 각 스텝별로 학습할 예제들이 들어있다.&lt;/span&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; fill_feed_dict() 함수에는, 주어진&amp;nbsp;DataSet 이 다음 배치 사이즈 만큼의 이미지와 레이블들을 질의한다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 파이썬 딕셔너리 객체를 생성한다. 이때 키는 placeholder들이고, 값은 representative feed tensor들이다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;feed_dict = {
    images_placeholder: images_feed,
    labels_placeholder: labels_feed,
}
&lt;/code&gt;&lt;/pre&gt;


&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 이 딕셔너리 객체는 다음 학습 스텝의 입력 예제로서,&amp;nbsp;sess.run()함수의 feed_dict 파라미터에 전달된다.&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;Check the Status&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 코드는 실행호출에서 fetch할 두 값을 명세한다.:&amp;nbsp;[train_op, loss].&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;for step in xrange(FLAGS.max_steps):
    feed_dict = fill_feed_dict(data_sets.train,
                               images_placeholder,
                               labels_placeholder)
    _, loss_value = sess.run([train_op, loss],
                             feed_dict=feed_dict)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; Fetch할 값이 두개이기 때문에, sess.run()은 두 아이템의 tuple을 반환한다.&amp;nbsp;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;train_op은 출력 값이 없는&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;Operation이기 때문에, 반환된 튜플의 해당값이 None이고 버려진다. 그러나 loss tensor의 값은 훈련중 모델이 발산한다면 NaN일수 있기 때문에 이 값을 로깅하기 위해 캡쳐한다.&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;여기서는 훈련이 NaN없이 수행된다고 가정하고, 훈련 루프에서 100스텝마다 훈련 상태를 알려주는 간단한 텍스트만을 출력한다.&lt;/span&gt;&lt;/p&gt;


&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;if step % 100 == 0:
    print 'Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;Visualize the Status&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; TensorBoard를 위한 이벤트 파일을 출려하기 위해서는, 그래프 빌드 단계 부터&amp;nbsp;모든 요약들이(이번에는 하나지만) 단일 op으로 모여야 한다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;summary_op = tf.merge_all_summaries()
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 그리고 세션이 생성된 후에, tf.train.SummaryWriter는 이벤트 파일을 쓰기 위해 인스턴스화 될 수 있다. 이 파일에는 그래프 자체와 요약(summary)의 값들이 저장된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, graph_def=sess.graph_def)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 마지막으로, 이벤트 파일은 summary_op이 수행될 때마다 새로운 요약이 업데이트된다. 그리고 출력은 writer의 &amp;nbsp;add_summary() 함수로 전달된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;summary_str = sess.run(summary_op, feed_dict=feed_dict)
summary_writer.add_summary(summary_str, step)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 이벤트 파일들이 쓰여지면, TensorBoard는 요약값들을 출력하기 위해 수행될 수 있다.&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/mnist_tensorboard.png&quot; alt=&quot;MNIST TensorBoard&quot;&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;Save a Checkpoint&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 체크포인트 파일을 내보내려면&amp;nbsp;tf.train.Saver를 인스턴스화 해야 한다. 이 파일로 추후에 학습이나 평가를 위한 모델을 복구할 수 있다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;saver = tf.train.Saver()
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 훈련 루프에서, saver.save() 함수는 체크포인트 파일를 기록하기 위해 주기적으로 호출된다. 이 파일은 훈련 디렉토리에 기록되고, 훈련가능한 모든 변수들의 현재 값이 기록된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;saver.save(sess, FLAGS.train_dir, global_step=step)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 이후에 훈련은 saver.restore() 함수로 모델 파라미터를 리로드하여 resume될 수 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&amp;gt;saver.restore(sess, FLAGS.train_dir)
&lt;/code&gt;&lt;/pre&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;Evaluate the Model&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 모든 수천 스텝들에서, 코드는 훈련셋과 테스트셋에 대해 평가를 시도한다. do_eval()함수는 훈련, 검증, 테스트 셋에 대해 3번 호출 된다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;print 'Training Data Eval:'
do_eval(sess,
        eval_correct,
        images_placeholder,
        labels_placeholder,
        data_sets.train)
print 'Validation Data Eval:'
do_eval(sess,
        eval_correct,
        images_placeholder,
        labels_placeholder,
        data_sets.validation)
print 'Test Data Eval:'
do_eval(sess,
        eval_correct,
        images_placeholder,
        labels_placeholder,
        data_sets.test)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; Note that more complicated usage would usually sequester the data_sets.test to only be checked after significant amounts of hyperparameter tuning. For the sake of a simple little MNIST problem, however, we evaluate against all of the data.&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Build the Eval Graph&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 기본 Graph를 열기 전에, 테스트 데이터는 테스트 데이터셋을 사용하기 위해&amp;nbsp;파라미터 셋으로 &amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 10.2857px;&quot;&gt;get_data(train=False) 함수를 호출하여 fetch해야 한다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;test_all_images, test_all_labels = get_data(train=False)
&lt;/pre&gt;&lt;/code&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; 훈련 루프에 들어가기 전에, Eval op은 evaluation() 함수를 호출하여 빌드되어야 한다. 이 함수는 mnist.py에 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;eval_correct = mnist.evaluation(logits, labels_placeholder)
&lt;/pre&gt;&lt;/code&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; evaluation() 함수는 단지&amp;nbsp;tf.nn.in_top_k op을 생성한다. 이는 자동으로 각 모델의 출력을 평가한다. 이때 K most-likey prediction들 안에 있다면 true로 평가한다. 이 경우에는 K를 1로 설정하여 최고의 예측값이 맞았을 때만 참으로 평가한다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;eval_correct = tf.nn.in_top_k(logits, labels, 1)
&lt;/pre&gt;&lt;/code&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;출력 평가하기&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; feed_dic을 채우고, 주어진 데이터셋에 대해 모델을 평가하는 eval_correct op을 호출하는&amp;nbsp;sess.run()을 호출하는&amp;nbsp;루프를 생성할 수 있다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;for step in xrange(steps_per_epoch):
    feed_dict = fill_feed_dict(data_set,
                               images_placeholder,
                               labels_placeholder)
    true_count += sess.run(eval_correct, feed_dict=feed_dict)
&lt;/pre&gt;&lt;/code&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp; true_count 변수는 단지&amp;nbsp;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;in_top_k op이 참이라고 판단한&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;모든 예측들을 누적계수한다. 이를 전체 예제의 수로 나누어 정확도를 계산할 수 있다.&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;precision = float(true_count) / float(num_examples)
print '  Num examples: %d  Num correct: %d  Precision @ 1: %0.02f' % (
    num_examples, true_count, precision)
&lt;/pre&gt;&lt;/code&gt;

&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;Reference&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p style=&quot;line-height: 14.4px;&quot;&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html&lt;/a&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/127</guid>
<comments>https://dsmoon.tistory.com/entry/TensorFlow-101-part2#entry127comment</comments>
<pubDate>Mon, 14 Dec 2015 23:34:27 +0900</pubDate>
</item>
<item>
<title>TensorFlow 101 - part1</title>
<link>https://dsmoon.tistory.com/entry/TensorFlow-101</link>
<description>&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;TensorFlow Mechanics 101&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Code&lt;/b&gt;: &lt;a href=&quot;https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;tensorflow/g3doc/tutorials/mnist/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이 튜토리얼은 TensorFlow로 train하는 방법을 간단히 설명하기 위해 작성되었다.&amp;nbsp;MNIST로 숫자 손글씨 인식을 위한&amp;nbsp;simple feed-forward neural network를 훈련하고 평가하기를 해본다. 본 문서는 머신러닝에 대해서 사전 지식이 있는 사람을 대상으로 한다. 일반적인 머신러닝 교재로는 적절하지 않으며, TensorFlow 사용법에 대한 것임을 알고 읽기 바란다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;Tutorial Files&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;이 튜토리얼에서는 다음과 같은 파일을 사용한다&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;:&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;txc-table&quot; width=&quot;564&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; border=&quot;0&quot; style=&quot;border:none;border-collapse:collapse;;font-family:돋움;font-size:12px&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;width: 282px; height: 24px; border: 1px solid rgb(204, 204, 204); background-color: rgb(250, 236, 197);&quot;&gt;&lt;p&gt; File&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 282px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-top-width: 1px; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: rgb(250, 236, 197);&quot;&gt;&lt;p&gt;&amp;nbsp;Purpose&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width:282;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;a href=&quot;https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/mnist.py&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;mnist.py&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width:282;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;&quot;&gt;&lt;p&gt;The code to build a fully-connected MNIST model.&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width:282;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-left:1px solid #ccc;;&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;a href=&quot;http://fully_connected_feed.py&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;fully_connected_feed.py&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width:282;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;;&quot;&gt;&lt;p&gt;The main code to train the built MNIST model against the downloaded dataset using a feed dictionary.&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp;훈련을 시작하려면 fully_connected_feed.py 를 실행하면 된다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&lt;b&gt;$ python fully_connected_feed.py&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;데이터 준비하기&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; MNIST 는 머신러닝에서의 고전적인 문제이다. 그레이스케일 28x28 해상도의 숫자 손글씨를 입력으로 받아 0~9의 숫자중 무엇인지 결정하는 문제이다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/mnist_digits.png&quot; alt=&quot;MNIST Digits&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 더 자세한 정보는 Yann LeCun's MNIST page 나&amp;nbsp;Chris Olah's visualizations of MNIST에서 참고하길 바란다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;다운로드&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 함수&amp;nbsp;&lt;b&gt;run_training()&lt;/b&gt; 의 앞부분을 보면,&lt;b&gt; input_data.read_data_sets(&lt;/b&gt;) 함수는 적절한 데이터를 PC에 다운로드하고, 데이터를 풀고 읽어, DataSet 인스턴스의 dictionary를 반환할 것이다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;NOTE: fake_data flag 플래그는 단위테스트를 위한 것이고, reader에 의해 무시될 것이다.&lt;/p&gt;&lt;p&gt;&lt;table class=&quot;txc-table&quot; width=&quot;564&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; border=&quot;0&quot; style=&quot;border:none;border-collapse:collapse;;font-family:돋움;font-size:12px&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;width: 131px; height: 24px; border: 1px solid rgb(204, 204, 204); background-color: rgb(250, 244, 192);&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;Dataset&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 432px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-top-width: 1px; border-top-style: solid; border-top-color: rgb(204, 204, 204); background-color: rgb(250, 244, 192);&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;Purpose&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width: 131px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-left-width: 1px; border-left-style: solid; border-left-color: rgb(204, 204, 204);&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;data_sets.train&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 432px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204);&quot;&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp;55000 images and labels, for primary training.&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width: 131px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-left-width: 1px; border-left-style: solid; border-left-color: rgb(204, 204, 204);&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;data_sets.validation&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 432px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204);&quot;&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp;5000 images and labels, for iterative validation of training accuracy.&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;width: 131px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204); border-left-width: 1px; border-left-style: solid; border-left-color: rgb(204, 204, 204);&quot;&gt;&lt;p&gt;&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;data_sets.test&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td style=&quot;width: 432px; height: 24px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(204, 204, 204); border-right-width: 1px; border-right-style: solid; border-right-color: rgb(204, 204, 204);&quot;&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp;10000 images and labels, for final testing of trained accuracy.&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Inputs and Placeholders&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; &lt;b&gt;placeholder_inputs&lt;/b&gt;() 함수는 두개의&amp;nbsp;two tf.placeholder op들을 만들어낸다. 이때 입력으로 사용될 훈련예제들의배치 크기, 입력의 모양 등을 정의한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,&amp;nbsp;&lt;/b&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;IMAGE_PIXELS))&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 훈련 루프에서 이미지와 레이블들의 데이터셋들은 각 스텝을 위해, &amp;nbsp;placeholder에 맞게&amp;nbsp;배치 크기별로 나뉘어진다. 그리고 이어서 sess.run() 함수를 feed_dist 파라미터로 실행한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;font-size: 24px; line-height: 28.8px;&quot;&gt;&lt;b&gt;그래프 만들기&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 데이터를 위한 placehoder들을 만들고 나면, 그래프는 mnist.py 로 부터 만들어진다. 이때 3단계 패턴에 맞게 생성된다.&lt;/span&gt;&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;&lt;b&gt;inference()&lt;/b&gt; - 네트워크를 forward로 수행하여 예측을 하기 위한 정도만&amp;nbsp;그래프를 생성한다.&lt;/li&gt;&lt;li&gt;&lt;b&gt;loss()&lt;/b&gt; - 손실을 계산하기 위한 op을 그래프에 추가한다.&lt;/li&gt;&lt;li&gt;&lt;b&gt;training()&lt;/b&gt; - 손실 그래프에 그래디언트를 계산하고 적용하기 위한 op을 추가한다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://www.tensorflow.org/versions/master/images/mnist_subgraph.png&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Inference&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 출력으로 예측 텐서를 만들어 내기 위한 최소한의&amp;nbsp;그래프가&amp;nbsp;inference() 함수에서 생성된다. 이는 이미지 placeholder들을 입력으로 받아, ReLu 활성화 함수를 사용하는 한쌍의 fully connected layer들을 그 위에 쌓는다. 출력낼 10개의 노드들이 이서 생성된다. 각 레이어들은 유일한 tf.name_scope 아래 생성되는데, 이는 scope내에서&amp;nbsp;아이템들의 접두어처럼 동작한다.&lt;/p&gt;&lt;p&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;with tf.name_scope('hidden1') as scope:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 정의된 scope 내에서, 웨이트들과 바이어스들은 tf.Variable 인스턴스에 적절한 모양으로 생성된다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;weights = tf.Variable(&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;nbsp; &amp;nbsp; tf.truncated_normal([IMAGE_PIXELS, hidden1_units],&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;nbsp; &amp;nbsp; stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;nbsp; &amp;nbsp; name='weights')&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;biases = tf.Variable(tf.zeros([hidden1_units]),&amp;nbsp;&lt;/b&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;name='biases')&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이것들은 hidden1 scope 아래 생성되어,&amp;nbsp;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&quot;hidden1/weights&quot;과 같이&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp;고유한 이름이 주어진다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 각 변수는 생성 과정의 일부로서, 초기화 연산에 사용된다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 대부분 일반적인 경우에, 가중치들은&amp;nbsp;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;tf.truncated_normal&lt;/span&gt;&lt;span style=&quot;line-height: 14.4px;&quot;&gt;&amp;nbsp;에 의해 초기화 되고, 이때 2-D tensor의 shape이 주어진다. Shape은 아래 레이어와 위 레이어의 크기로 주어진다. 첫 레이어는 입력과 연결 되어 있으므로&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;[IMAGE_PIXELS, hidden1_units] 와 같이 설정 될 것이다. 앞서 이야기한 tf.truncated_normal initializer는 주어진 평균값과 표준 편차에 따라 값을 만들어낸다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 바이어스들은 tf.zeros()에 의해 모두 0으로 초기화 된다. 이때 바이어스의 크기는 다음 레이어의 크기로 설정한다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이 그래프에 3가지 주요 op들이 있다. 은닉층의&amp;nbsp;tf.matmul을 래핑하는&amp;nbsp;2개의 tf.nn.relu와 logit을 위한 추가적인 tf.matmul이다. 이들은 만들어지고 그들의&amp;nbsp;tf.Variable instance들은 input placeholder 나 output tensor에 연결된다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;logits = tf.matmul(hidden2, weights) + biases&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Loss&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; loss() 함수는 필요한 loss op들을 추가함으로써 그래프를 추가로 빌드한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 우선, labels_placeholder의 값들은&amp;nbsp;1-hot vector로 인코딩된다. 예들 들어 3이면 다음과 같이 변환된다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;batch_size = tf.size(labels)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;labels = tf.expand_dims(labels, 1)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;concated = tf.concat(1, [indices, labels])&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;onehot_labels = tf.sparse_to_dense(&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;nbsp; &amp;nbsp; concated, tf.pack([batch_size, NUM_CLASSES]), 1.0, 0.0)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; tf.nn.softmax_cross_entropy_with_logits op은&amp;nbsp;&lt;span style=&quot;line-height: 1.5;&quot;&gt;inference() function 함수의&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;출력 logit들과 1-hot label 들과&amp;nbsp;비교한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; onehot_labels,&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; name='xentropy')&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이어서 tf.reduce_mean 함수로 배치 차원에서 cross entropy값의 평균을 loss의&amp;nbsp;&amp;nbsp;구한다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Training&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; training() 함수는 gradient descent동안 loss를 최소화 하기 위한 연산들을 추가한다.&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;line-height: 1.5;&quot;&gt;&amp;nbsp; 우선, loss tensor를 loss() 함수에서 받아서 tf.scalar_summary에 넘겨준다. 이 함수는 SummaryWriter로 요약값들을 생성하여 이벤트 파일에 기록한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;tf.scalar_summary(loss.op.name, loss)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이어서 tf.train.GradientDescentOptimizer 를 인스턴스화 한다. 이는 주어진 학습률로 그래디언트를 적용한다.&lt;/p&gt;&lt;p&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 전체 훈련 step의 카운터에 대한 값 하나를 생성한다. 이 값으로 minimize() op은 웨이트를 업데이트하고, 카운터를 증가시킨다. 이는 관례적으로 train_op으로 알려져있고 전체 훈련 과정을 수행하기 위해 TensorFlow에서 해야 하는 일이다.&lt;/p&gt;&lt;p&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;global_step = tf.Variable(0, name='global_step', trainable=False)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;train_op = optimizer.minimize(loss, global_step=global_step)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 훈련 op의 output을 담고 있는 tensor가 반환된다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b style=&quot;line-height: 1.5;&quot;&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;Reference&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html&lt;/a&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/126</guid>
<comments>https://dsmoon.tistory.com/entry/TensorFlow-101#entry126comment</comments>
<pubDate>Fri, 11 Dec 2015 00:30:01 +0900</pubDate>
</item>
<item>
<title>TensorFlow Variables 사용법</title>
<link>https://dsmoon.tistory.com/entry/TensorFlow-Variables</link>
<description>&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;b&gt;&lt;span style=&quot;color: rgb(47, 157, 39);&quot;&gt;TensorFlow&lt;/span&gt;&lt;/b&gt;(텐서플로우)를 사용하기 앞서 가장 먼저&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt; Variable&lt;/b&gt;&lt;/span&gt;에 대해 알아보자. 모델을 훈련할 때 모델의 파라미터들을 저장할 변수들이 필요할 것이다. &lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;Variable&lt;/b&gt;&lt;/span&gt;은 tensor(텐서)를 메모리에 저장하는 변수이다. Variable들은 명시적으로 초기화되어야 하고, 학습뒤에 디스크에 저장하고 필요할 때 다시 불러올 수도 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이해를 돕기위해 작성된 문서인 만큼 더 정확하고 자세한 설명은 API 문서를 참고하자.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;변수 만들기&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Variable을 만들 때, Variable()에 &amp;nbsp;초기값을 넘겨주어야 한다. TensorFlow는 상수나 랜덤으로 초기화 하기위한 아래와 같은 함수들을 제공한다.&lt;/p&gt;&lt;ul style=&quot;list-style-type: square;&quot;&gt;&lt;li&gt;&lt;p style=&quot;line-height: 0.8;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.zeros(shape, dtype=tf.float32, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.zeros_like(tensor, dtype=None, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.ones(shape, dtype=tf.float32, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.ones_like(tensor, dtype=None, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.fill(dims, value, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.constant(value, dtype=None, shape=None, name=Const)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.linspace(start, stop, num, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.range(start, limit=None, delta=1, name=range)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: 'Courier New';&quot;&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;seed=None,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;name=None)&lt;/span&gt;&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: 'Courier New';&quot;&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;name=None)&lt;/span&gt;&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.random_shuffle(value, seed=None, name=None)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li style=&quot;line-height: 0.8;&quot;&gt;&lt;p style=&quot;line-height: 1;&quot;&gt;&lt;span style=&quot;color: rgb(0, 51, 153);&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-family: Dotum, 돋움; color: rgb(140, 140, 140);&quot;&gt;tf.set_random_seed(seed)&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;font color=&quot;#8c8c8c&quot; face=&quot;Dotum, 돋움&quot;&gt;&lt;span style=&quot;line-height: 12px;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/font&gt;&lt;/div&gt;&lt;p&gt;&amp;nbsp; 위의 함수(op)호출시에 tensor의 모양(shape)을 명시해야 함에 유의해야 한다. 이 모양대로 Variable이 생성된다. Variable은 고정된 모양으로 생성되지만, reshape 할수도 있다. 그리고 tf.Variable()는&amp;nbsp;tf.Variable 클래스를 반환한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create two variables.&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),&amp;nbsp;&lt;/span&gt;&lt;/b&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;name=&quot;weights&quot;)&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;biases = tf.Variable(tf.zeros([200]), name=&quot;biases&quot;)&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;초기화&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Variable&amp;nbsp;initializer 는 model의 다른 op들이 수행되기 전에 먼저 실행되어야 한다. Model을 사용하기 전에 한번에 모든 변수를 초기화 하면 편리하다. Checkpoint 파일로 부터 variable들을 읽어오는 방법도 있다. 변수들을 모두 초기화 할 때는&amp;nbsp;tf.initialize_all_variables()를 호출하자. 모델의 모든 연결을 마치고 세션을 런치할 때 호출해야된다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create two variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),&amp;nbsp;&lt;/b&gt;&lt;/span&gt;&lt;b style=&quot;color: rgb(204, 61, 61); line-height: 0.5;&quot;&gt;name=&quot;weights&quot;)&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;biases = tf.Variable(tf.zeros([200]), name=&quot;biases&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Add an op to initialize the variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;init_op = tf.initialize_all_variables()&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Later, when launching the model&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;with tf.Session() as sess:&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;&amp;nbsp; # Run the init operation.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; sess.run(init_op)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; ...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;&amp;nbsp; # Use the model&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; ...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;다른 변수로 초기화&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 다른 변수로 변수를 초기화 하고 싶을 때가 있다.&amp;nbsp;그런데 &lt;b&gt;tf.initialize_all_variables&lt;/b&gt;()는 모든 변수를 병렬로 초기화 하기 때문에 좀 주의해야 한다. 아래 코드와 같이, 변수의&amp;nbsp;&lt;b&gt;initialized_value&lt;/b&gt;() 함수를 이용해서 이 작업을 할 수 있다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create a variable with a random value.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),&amp;nbsp;&lt;/b&gt;&lt;/span&gt;&lt;b style=&quot;color: rgb(204, 61, 61); line-height: 0.5;&quot;&gt;name=&quot;weights&quot;)&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create another variable with the same value as 'weights'.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;w2 = tf.Variable(weights.initialized_value(), name=&quot;w2&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create another variable with twice the value of 'weights'&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;w_twice = tf.Variable(weights.initialized_value() * 2.0, name=&quot;w_twice&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Custom Initialization&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;편리한 함수인&amp;nbsp;tf.initialize_all_variables()는 모델의 모든 변수들을 초기화 하기 위한 연산을 추가한다.&amp;nbsp;초기화할 변수들의 목록을 명시적으로 넘겨줄 수도 있다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 18pt;&quot;&gt;Saving and Restoring&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 변수를 저장하고 다시 읽어올 때는&amp;nbsp;&lt;b&gt;tf.train.Saver&lt;/b&gt; 를 사용한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Checkpoint Files&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Variable들은 바이너리 파일로 저장하는데, variable name과 tensor 값 사이의 map을 담고있다. Saver 객체를 생성할 때, checkpoint file에서의 변수이름을 선택적으로 지정할 수 있다. 기본적으로는, 각 변수의&amp;nbsp;Variable.name property를 사용한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Saving Variables&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 모델의 모든 파일을 저장하기 위해 tf.train.Saver()로&amp;nbsp;Saver를&amp;nbsp;생성한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create some variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;v1 = tf.Variable(..., name=&quot;v1&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;v2 = tf.Variable(..., name=&quot;v2&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Add an op to initialize the variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;init_op = tf.initialize_all_variables()&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Add ops to save and restore all the variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;saver = tf.train.Saver()&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Later, launch the model, initialize the variables, do some work, save the&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# variables to disk.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;with tf.Session() as sess:&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; sess.run(init_op)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;&amp;nbsp; # Do some work with the model.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; ..&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;&amp;nbsp; # Save the variables to disk.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; save_path = saver.save(sess, &quot;/tmp/model.ckpt&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; print &quot;Model saved in file: &quot;, save_path&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Restoring Variables&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 같은 Saver object는 변수들을 restore하는데 사용된다. 변수들을 파일로부터 restore할 때는 변수들을 초기화 할 필요가 없음을 주의해야한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create some variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;v1 = tf.Variable(..., name=&quot;v1&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;v2 = tf.Variable(..., name=&quot;v2&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Add ops to save and restore all the variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;saver = tf.train.Saver()&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Later, launch the model, use the saver to restore variables from disk, and&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# do some work with the model.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;with tf.Session() as sess:&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;&amp;nbsp; # Restore variables from disk.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; saver.restore(sess, &quot;/tmp/model.ckpt&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; print &quot;Model restored.&quot;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;&amp;nbsp; # Do some work with the model&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&amp;nbsp; ...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Choosing which Variables to Save and Restore&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 만약 tf.train.Saver()에 아무 argument도 넘기지 않는다면, saver는 그래프 내의 모든 변수들을 다룰 것이다. 각각의 변수들은 생성될 때 선언된 이름들로 저장될 것이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 파일에 저장될 이름을 명시적으로 설정하는 것은 종종 편리할 때가 있다. 예를 들어 &quot;weights&quot;로 학습했지만, &quot;params&quot;로 restore할 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 일부 변수만 저장하고 복구하는 것을 원할 때도 있다. 예들 들어 5 레이어로 이미 학습을 했고, 한개 레이어를 추가로 학습하고 싶을 수 있다. 이때 5개 레이어의 모델을 읽어와 앞단의 5개 레이어의 파라미터들로 restore할 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이때 tf.train.Saver() constructor에 Python dictionary를 넘겨 주어 저장할 변수의 이름들을 설정할 수 있다. 이때 dictionary의 키는 사용할 이름이고, 값은 관리대상인 변수들이다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Notes:&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 모델의 변수들을 다양한 조합으로 저장하기 위해, 많은 saver 객체를 생성할 수 있다. 같은 변수라도 다른 saver 객체에 적용될 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 만약 세션의 시작에서 모델의 일부 변수들만 복구한다면, 다른 변수들을 위해 초기화를 수행해줘야 한다.&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Create some variables.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;v1 = tf.Variable(..., name=&quot;v1&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;v2 = tf.Variable(..., name=&quot;v2&quot;)&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Add ops to save and restore only 'v2' using the name &quot;my_v2&quot;&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;saver = tf.train.Saver({&quot;my_v2&quot;: v2})&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(140, 140, 140);&quot;&gt;# Use the saver object normally after that.&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;...&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;line-height: 0.5;&quot;&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Reference&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/how_tos/variables/index.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://www.tensorflow.org/versions/master/how_tos/variables/index.html&lt;/a&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/125</guid>
<comments>https://dsmoon.tistory.com/entry/TensorFlow-Variables#entry125comment</comments>
<pubDate>Mon, 07 Dec 2015 23:02:26 +0900</pubDate>
</item>
<item>
<title>TensorFlow 설치</title>
<link>https://dsmoon.tistory.com/entry/TensorFlow-install</link>
<description>&lt;p&gt;&amp;nbsp; &lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;TensorFlow &lt;/b&gt;&lt;/span&gt;(텐서플로우)를 설치해보면 구글이 신경을 많이 썼다는 것을 알 수 있다. 다른 ML 플랫폼들을 설치하다보면 홈페이지에 있는대로 잘 되지 않는 경우가 많다. 수많은 사람들이 사용하는 상용SW가 아니라 연구용으로, 오픈소스로 무료로 만들어진 것이다 보니 그럴수도 있겠다 생각하면서도, 시작도 하기 전에 설치에서 주저 앉는 경우가 많다. 그러나 &lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;TensorFlow&lt;/b&gt;&lt;/span&gt;는 매우 설치가 쉽다!!&lt;/p&gt;&lt;p&gt;&amp;nbsp; 우선 이런 저런 설정이 귀찮고 모르겠고 하면 &lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;Docker&lt;/span&gt;&lt;/b&gt;(도커)를 통해 설치하는 것을 고려해보자. &lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;TensorFlow&lt;/b&gt;&lt;/span&gt;는 감사하게도 Docker Container를 제공한다. 물론 이전에 Docker를 설치해놓아야 하지만 Docker만 있다면 한방에 설치가 가능하다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;실행프로그램만 설치할 경우&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;$ docker run -it b.gcr.io/tensorflow/tensorflow&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Source code도 같이 설치할 경우&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;$ docker run -it b.gcr.io/tensorflow/tensorflow-full&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;p&gt;&amp;nbsp; 소스코드를 참고할 일이 종종있을 수 있으니 full version 설치를 하면 좋겠다. 그런데 주의할 게 있는데 docker image는 cpu 모드만 가능하다는 것이다. Docker의 한계가 아니라 이미지에 CUDA가 설치 되어 있지 않기 때문인데... 아마도 license 문제이지 않을까 짐작해본다. 이미지 받고 여기에 CUDA toolkit을 설치해도 된다. 좀 귀찮지만...&lt;/p&gt;&lt;p&gt;&amp;nbsp; Docker가 익숙하지 않다면 &lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;pip&lt;/span&gt;&lt;/b&gt; 설치도 가능하다. 참고로 현재 python 2.7 에만 설치 가능하다. 3.0에도 설치할 수 있도록 작업중이라고 한다. 이는 기존에 ML lib 들이 2.7 기준으로 작성된 것들이 많아서 이를 사용하다보니 생긴 문제인듯 하다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 다음과 같이 pip를 설치해주자. Pip가 이미 설치되어 있다면 다음으로 넘어가자.&lt;/p&gt;&lt;p&gt;# Ubuntu/Linux 64-bit&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;$ sudo apt-get install python-pip python-dev&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;# Mac OS X&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;$ sudo easy_install pip&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 아래와 같이 명령어 하나로 바로 설치 가능하다. 맥북을 사용하는 많은 개발자들이 아쉬워하겠지만 맥에서는 GPU 가속을 하기 어렵다. 이는 Cuda를 사용하려면 nvidia의 그래픽 카드를 사용해야 하는 것과 연관이 있을 것같다. 맥은 대부분 intel iris&amp;nbsp;나 AMD radeon 그래픽을 사용한다. 그러나 CPU 모드도 수십~수백배 느릴뿐이므로 작은 예제로 개발하는데는 지장이 없다. 쫄지 말고 설치하고 개발하자. 개발된 코드로 큰 데이터를 학습하는 건 서버에서 하면 된다. 만약 서버가 없다면 PC에 GTX 그래픽 카드 하나 꼽아놓고 돌리면 된다.&lt;/p&gt;&lt;p&gt;# Ubuntu/Linux 64-bit, CPU only:&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;# Ubuntu/Linux 64-bit, GPU enabled:&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;# Mac OS X, CPU only:&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;$ sudo easy_install --upgrade six&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&lt;b&gt;$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 여기서 주의할게 있는데 맥의 경우엔 six의 버젼을 업그레이드 해줘야 한다는 것이다. 아니면 아래와 같은 에러를 만날수 있다.&amp;nbsp;TensorFlow 는 protobuf를 사용하는데, 이게&amp;nbsp;six-1.10.0를 사용한다. 그런데 애플의 기본 python에는&amp;nbsp;six-1.4.1가 설치되어 있기 때문이다. 구글 잘못이라기 보다, 기본 설치된 버젼이 낮아서 그런것이다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;ImportError: No module named copyreg&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 그리고 아래와 같은 에러가 발생하면 protobuf 자체를 업그레이드 하면 되니 쫄지 말자. 아래와 같이 pip로 쉽게 업그레이드 가능하다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(204, 61, 61);&quot;&gt;$ pip install --upgrade protobuf&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;table class=&quot;txc-table&quot; width=&quot;564&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; border=&quot;0&quot; style=&quot;border: none; border-collapse: collapse; font-size: 12px;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;width:564;height:24;border-bottom:1px solid #ccc;border-right:1px solid #ccc;border-top:1px solid #ccc;border-left:1px solid #ccc;;&quot;&gt;&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&lt;/p&gt;&lt;p&gt;Traceback (most recent call last):&lt;/p&gt;&lt;p&gt;&amp;nbsp; File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; File &quot;/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py&quot;, line 4, in &amp;lt;module&amp;gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; from tensorflow.python import *&lt;/p&gt;&lt;p&gt;&amp;nbsp; File &quot;/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py&quot;, line 13, in &amp;lt;module&amp;gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; from tensorflow.core.framework.graph_pb2 import *&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;&amp;nbsp; File &quot;/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py&quot;, line 22, in &amp;lt;module&amp;gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; &amp;nbsp; serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\&quot;d\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tb\x06proto3')&lt;/p&gt;&lt;p&gt;TypeError: __init__() got an unexpected keyword argument 'syntax'&amp;nbsp;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</description>
<category>TensorFlow</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/124</guid>
<comments>https://dsmoon.tistory.com/entry/TensorFlow-install#entry124comment</comments>
<pubDate>Mon, 07 Dec 2015 07:07:01 +0900</pubDate>
</item>
<item>
<title>[Pylearn2] Overview (2/2)</title>
<link>https://dsmoon.tistory.com/entry/Pylearn2-Overview-22</link>
<description>&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;The Monitor&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Monitor는 훈련을 모니터링하는 Model이다. Train 객체를 사용한다면,&amp;nbsp;자동으로 설정되고 유지될 것이다. 따라서 자세한 동작 원리는 몰라도 된다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; Monitor는 다양한 데이터셋의 몇몇 &quot;channel&quot;을 추적할 것이다. 채널은 분류기의 정확도 같은 것이 될 것이다.&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&quot;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;train_acc&lt;/b&gt;&lt;/span&gt;&quot;는&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;훈련셋에 대한&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;정확도를 추적하고,&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;&quot;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;valid_acc&lt;/b&gt;&lt;/span&gt;&quot;는 held-out validation set에 대한 정확도를 추적한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 채널을 추가하는 주요 방법은 모델들과 코스트 들과 같은 다양한 객체의&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5; color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;get_monitoring_channels&lt;/b&gt;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&lt;b&gt;&amp;nbsp;&lt;/b&gt;메소드를 구현하는 것이다.&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;훈련 알고리즘에 어떤 데이터 셋을 모니터링 할지 알려줘야한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 저장된 모델의 채널을 보는 것도&amp;nbsp;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;plot_monitor.py&lt;/b&gt;&lt;/span&gt; 스크립트를 이용해 가능하다.&amp;nbsp;알고리즘들은 모니터의 값들에 반응할 수 있기 때문에,&amp;nbsp;Monitor는 많은 훈련 알고리즘의 중요한 부분이다.&amp;nbsp;예를 들어,&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;많은 훈련 알고리즘이&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;overfitting을 막기 위해 준비된 validation set의&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;설정된 정확도에서 반복 훈련을 중지하는&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&quot;early stopping&quot;을 사용한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;TrainingAlgorithm&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; TrainingAlgorithm은 모델의 파라미터들을 갱신하기 위해&amp;nbsp;모델과 상호 연동하는 객체이다. TrainingAlgorithm을 Train object에 제공한다면, 모델의 train_all 메서드를 호출하기 보다, 각각의&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;epoch를 수행기 위해&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;TrainingAlgorithm를 호출할 것이다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Pylearn2 SGD (Stochastic Gradient Descent)와&amp;nbsp;BGD (Batch Gradient Descent)를 제공한다.&amp;nbsp;이 두 알고리즘은 훈련 데이터와 모델 파라미터에 대한 비용 함수를 최소화 하는데 사용된다. 예를 들어 logistic regression classifier를 훈련하면서, -log P(Y | X)를 최소화 할 수 있다.(Y: training labels, X:&amp;nbsp;training input features) 둘다 더 고도화된 알고리즘들을 수행하기 위해 설정할 수도 있다. 예를 들어 BGD는&amp;nbsp;nonlinear conjugate gradient descent를 수행하도록 설정할 수도 있다. SGD는&amp;nbsp;user-provided callback들도 지원한다. Pylearn2는 콜백이스템을 Polyak averaging을 구형하기 위해 사용한다.&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp; 이 두 알고리즘들은 학습을 종료할 때를 결정하기 위해&amp;nbsp;TerminationCriterion 객체를 사용한다.&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;TerminationCriterion가 모니터에서 채널을 통해 학습 알고리즘이 수렴하는지 보도록 하도록 구현하는게 일반적이다. 이것이 위에서 이야기한 early stoping을 구현하는 방법이다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;TrainingCallback&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Train 객체는 iteration이 끝날때 마다 수행하는 콜백을 지원한다. 훈련 과정을 더 customize 할 수 있는 것이다. 예를 들어&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;OneOverEpoch은 SGD 알고리즘이 고정된 learning rate 대신 1/t learning rate schedule을 사용하게 할 수 있다.&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;사용자 정의 콜백을 넘길 수 있어 제한 없이 변화를 줄 수 있다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Cost&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Cost는 objective function을 나타내는 객체이다. Pylearn2는&amp;nbsp;Cost를 SGD와 BGD algorithm을 제어하기 위해 사용한다.&amp;nbsp;Cost는 주어진 입력과 모델의 파라미터에 관한&amp;nbsp;목표 함수를 표현한다. 만약 gradient에 대한 custom 구현이 주어지지 않는다면, 기본 구현은 gradient를 제공하기 위해 단순히&amp;nbsp;Theano’s symbolic differentiation system을 사용할 것이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 그렇지만 다른 gradient들을 반환함으로써 더 강력한 기능을 얻을 수 있다. 예를 들어 RBM의 log likelihood는 다루기 어렵지만 그것의 gradient는 sample procedure를 통해 근사치를 얻을 수 있다. Objective fuction으로 None을 반환함으로써&amp;nbsp;SGD 객체를 통해 RBM을 구현할 수 있고, 샘플링 기반으로 gradient를 추정할 수 있다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Datasets&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Pylearn2 개발의 주요한 키포인트중 하나는 우리가 필요로 하는 기능을 직접 만드는 것이다. 라이브러리의&amp;nbsp;Dataset 부분이 그런 부분이다.&amp;nbsp;Pylearn2를 사용하는 대부분의 사람들이 RBM 같이 복잡한 모델을 MNIST나 CIFAR-10 같은 이미지 데이터셋을 이용하여&amp;nbsp;훈련하기 위해 그것을 사용한다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 데이터를 볼 수 있는 두가지 방법이 있다. Design matrix 나&amp;nbsp;topological view로서의 방법이 있다. Topological view 는 기본적으로 데이터의 근본적인 방법이다.&amp;nbsp;이미지 데이터의 배치는 4-tensor이다. 한 축(axis)은 각 이미지에 대한 인덱스이고, 한 축은&amp;nbsp;channels (red, green, blue)에 대한 인덱스이다. 그리고 남은 두 채널은 이미지의 행,열 좌표이다. Design matrix view에서는, 이미지는 (행*열*채널) 크기의 row vector로 평탄화(flatten)된다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 같은 데이터에 대해 다른 view가 필요할 수도 있다.&amp;nbsp;Dataset object는 언제라도 view를 제공할 수 있어야 한다. 조밀하게 연결된 RBM을 design matrix에 훈련하고 싶은 수도 있고, convolutional MLP를 topological view에 훈련하고 싶을 수도 있다. Topological view를 사용자에게 보여주고 싶을 수도 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; Dataset directory는 Dataset을 수정할 수 있는&amp;nbsp;Preprocessor 클래스를 포함한다.&amp;nbsp;이것이 View conversion 구현의 방법으로 고려되어선 안된다. View는 데이터의 뷰를 빠르게 제공할 준비가 항상 되어 있어야 하기 때문이다.&amp;nbsp;Preprocessor는 데이터 셋을 다양한 방법으로 수정할 수 있다. Example의 수나 저장된 방법을 바꿀수도 있다. 예를 들어&amp;nbsp;preprocess로 50,000개 32x32 이미지들로 구성된 CIFAR-10 dataset를 6x6의 200,000개 이미지 패치로 바꿀수도 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 많은 대부분의 전처리 연산은 Pylearn2의 클래스인 Block으로 손쉽게 표현될 수 있다. 이 클래스는 입력 예제의 배치를 사용하며, 그것들을 독립적으로 연산하고, 출력 예제의 배치를 반환한다.&amp;nbsp;같은 기능을 &lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;Block &lt;/span&gt;&lt;/b&gt;과&amp;nbsp;&lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;Preprocessor&lt;/span&gt;&lt;/b&gt;로 따로 구현하는 것을 피하기 위해,&amp;nbsp;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;BlockPreprocessor &lt;/b&gt;&lt;/span&gt;가 있어서 Block을 모든 &lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/span&gt;에 매핑할 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; Preprocessor들은 한번 수행하고 저장하길 바라는 비싼 연산을 가진 경우에 사용한다. 매 배치마다 수행하고 싶은 가벼운&amp;nbsp;preprocessing가 있다면, TransformerDataset을 통해 수행할 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; Dataset들은 상대적으로 추상적인 요소(entity)이다.&amp;nbsp;특히, 예제의 갯수가 유한하거나, 특정 인덱스로 점핑하는게 쉬울거라고 가정하면 안된다. 예를 들어, 사이즈가 고르지 않고 큰 이미지로 부터 가져온 작은 패치들의 데이터셋이 있다고 하자. 이미지의 고르지 않은 크기 때문에,&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;543,286,932번째 패치로 점프하고 싶다면 이에 앞서 각각의 크기를 알아야 한다.&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;&quot;pylearn2.utils.iteration&quot;는 예제들을 순회하는 인터페이스를 제공한다. 무한 스트림에서 랜덤 예제를 연속적으로 가져오는 것과 같은 순회 scheme을 제공한다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;&lt;b&gt;Spaces and LinearTransforms&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 우리의 많은 모델들은 공간에서 공간으로의 linear transform이란 관점에서 설명될 수 있다. 종종 exact type of linear transformation을 바꾸는 것은 유용한 모델 운영 방법 일수 있다. 예를 들어, RBM들을 image들오 scale하면, 우리는 모델 해석 내의&amp;nbsp;모든&amp;nbsp;matrix multiplie들을&amp;nbsp;convolution들로 변환할 수 있다.&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;abstract LinearTransform를 위해,&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;Pylearn2는 TheanoLinear library를 사용한다.&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;LinearTransform을 사용하여&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;Pylearn2 공간에서 다른 공간으로 transform하는 관점에서 당신의 모델을 본다면, 당신의 모델은 dense, convolutional, tiled convolutional, locally connected without weight sharing 등으로 쉽게 변이될 수 있다. 이는 적절한 공간의&amp;nbsp;LinearTransform을 사용함으로써 가능하다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Analyzing saved models&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;다양한 스크립트들로 저장된 모델들을 분석해볼 수 있다.:&lt;/p&gt;
&lt;ul style=&quot;list-style-type: disc;&quot;&gt;&lt;li&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;plot_monitor.py&lt;/span&gt;&lt;/b&gt;&amp;nbsp;모델의 모니터에 기록된 채널들을 그려준다.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;print_monitor.py &lt;/span&gt;&lt;/b&gt;모니터의 각 채널들의 마지막 값을 출력해준다.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;summarize_model.py&lt;/span&gt;&lt;/b&gt;&amp;nbsp;모델의 파라미터들의 통계를 출력해준다.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&lt;b&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;show_weights.py &lt;/span&gt;&lt;/b&gt;모델의 첫 레이어의 웨이트들의 시각화를 보여준다.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;reference&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://deeplearning.net/software/pylearn2/overview.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;http://deeplearning.net/software/pylearn2/overview.html&lt;/a&gt;&lt;/p&gt;</description>
<category>Pylearn2</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/123</guid>
<comments>https://dsmoon.tistory.com/entry/Pylearn2-Overview-22#entry123comment</comments>
<pubDate>Thu, 13 Aug 2015 23:31:00 +0900</pubDate>
</item>
<item>
<title>[Pylearn2] Overview (1/2)</title>
<link>https://dsmoon.tistory.com/entry/Pylearn2-Overview</link>
<description>&lt;p&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;&lt;b&gt;Overview&lt;/b&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이번 포스트에서는 Pylearn2 라이브러리에 관해 전반적인 overview에 대해 다룬다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 우선&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;Pylearn2 를 배우기 전에 Theano에 관애 잘 알고 있어야 한다. 아래 내용에 관한 사전 이해가 필요하다.&lt;/span&gt;&lt;/p&gt;&lt;ul style=&quot;list-style-type: disc;&quot;&gt;&lt;li&gt;Theano가&amp;nbsp;symbolic expressions을 표현하기 위해&amp;nbsp;Variables, Ops, Apply nodes를 어떻게 사용하는가&lt;/li&gt;&lt;li&gt;Theano function은 무엇인가&lt;/li&gt;&lt;li&gt;어떤 Theano shared variables 가 어떻게 Theano function들 호출 사이에 상태를 유지시켜주는가&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&amp;nbsp; 이것들을 이해해야&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;Pylearn2 의 세계로 넘어갈 수 있다.&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&amp;nbsp;다양한 클래스와 함수들이 구체적 설명없이 사용될 것이다.&amp;nbsp;이 페이지의 목적은 Pylearn2에서 어떠한 것들이 가능한지에 관한 이해를 돕는 것이다. 다양한 task를 수행하기 위해 필요한 object는 무엇인지, 또 그 object들은 어디에 있는지 등에 관한 것이다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;The Model class&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;pylearn2.models.model.py 에 정의된&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;모델은 좋은 시작점이다. Pylearn2의 대부분은 Model을 훈련하거나 훈련된 Model을 분석하기 위해&amp;nbsp;설계 되었다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Model은 무엇이든 될 수 있다. 데이터를 생성하는 확률모델(&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;probabilistic model)&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;일 수도 있다. 모델은 SVM같은, 입력을 출력으로 매핑하는 &lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;deterministic model일 수도 있다. Gaussian distribution 처럼 단순할 수도 있고 deep Boltzmann machine처럼 복잡할 수도 있다. 모델의 주요한 특징은&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;theano shared variables로 표현될 파라미터의 집합을 가진다는 것이다.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Training a Model while avoiding Pylearn2 entanglements&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;Pylearn2는&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;원하는 부분만 사용하는 라이브러리를 추구한다&lt;/span&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;. 이 경우 라이브러리 전체를 다 알아야 하는 것은 아니다. Pylearn2를 필요한 만큼만 최소한으로 배우고 싶다면 아래 두 메소드만 구현하면 된다.&lt;/span&gt;&lt;/p&gt;
&lt;ul style=&quot;list-style-type: square;&quot;&gt;&lt;li&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;train_all(dataset)&lt;/b&gt;&lt;/span&gt;: 이 함수에 Pylearn2 Dataset을 입력으로 넣어준다. 호출 되면 한 pass를 수행하고 파라미터들을 적절히 갱신한다.&amp;nbsp;예를 들어,&amp;nbsp;k-means는 데이터셋에 있는 모든 포인트를 centroid로 할당하고, centriod에 속한 모든 포인트의 평균으로 controid를 갱신한다.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-size: 9pt; line-height: 1.5;&quot;&gt;&lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;&lt;b&gt;continue_learning()&lt;/b&gt;&lt;/span&gt;: 이 함수는 학습을 계속 해야 하면 true를&amp;nbsp;반환한다. 예를 들어,&amp;nbsp;k-means algorithm에서는 centriod들이 특정 합보다 많이 움직였다면 true를 반환한다.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&amp;nbsp; Pylearn2 Dataset에 관해서는 뒤에 더 다루기로 하고, 우선은 데이터 예제 모음이라고만 알아두자.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 본인의 파이썬 스크립트에서 아래와 같이&amp;nbsp;훈련할 수 있는&amp;nbsp;Model 과 &amp;nbsp;Dataset이 있다고 하자:&lt;/p&gt;

&lt;pre class=&quot;prettyprint&quot;&gt;while model.continue_learning():
    model.train_all(dataset)
&lt;/pre&gt;

&lt;p&gt;&amp;nbsp; 훈련을 하기 위한 이 방법은 간단하고 쉽고, 매우 강력한 기능을 제공한다. 루프에서 모델에 대한 검사와 갱신을 매번 train_all() 이후에 수행 반복하는 고도화된 루프를 생각해 볼수 있다. 그러나 이는 Pylearn2가 목표로 하는 것은 아니다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 모델이 Gaussian visible units을 가진 RBM 이라고 가정하자. 이런&amp;nbsp;RBM 을 훈련하는 방법은 여러가지가 있다. Contrastive divergence, persistent contrastive divergence, score matching, denoising score matching, , noise contrastive estimation 등의 방법으로 훈련시킬수 있다. 이런 것들을 모두&amp;nbsp;“train_all” method 가 다 해주길 바란다면 잘못 생각한 것이다. Pylearn2에서는 학습 알고리즘을 다양한 경우에 대해 고려해봐야 한다. 만약 당신이 Pylearn2 생태계 전체를 다루며 작업하고자 하는 것이 아니라면 train_all 은 당신의 모델을 구현하난데 제법 괜찮은 방법일 것이다.&amp;nbsp;그러나 이상적인 Pylearn2 Model 의 기능은 부족할 수 있다.&amp;nbsp;다양한 방법으로 훈련하기 위해,&amp;nbsp;TrainingAlgorithms 와 Costs 을 사용하는 법에 대해 다룰 것이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 우선 Pylearn2의 control center라고 할 수 있는 Train object를 이용해서 Model을 훈련하는 법을 알아보자.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Training a Model with the Train object&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 앞에서 간단한 파이썬 while loop으로 Model을 훈련하는 법을 알아봤다. Pylearn2는 몇몇 강력한 추가 기능들을 사용하여 Model을 훈련하기 위한 편리한 클래스들을 제공한다. 바로 &lt;span style=&quot;color: rgb(34, 116, 28);&quot;&gt;pylearn2.train.Train&lt;/span&gt; 클래스이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 최소한으로,&amp;nbsp;Train 클래스는 생성자에서 dataset 과 model 을 입력으로 받는다. 이 클래스의&amp;nbsp;main_loop method를 호출 하는 것은 아래의 간단한 while loop과 같다.&lt;/p&gt;


&lt;pre class=&quot;prettyprint&quot;&gt;# This function...
def train(dataset, model):
    while model.continue_learning():
        model.train_all(dataset)

# does the same thing as this function...
def train(dataset, model):
    loop = Train(dataset, model)
    loop.main_loop()
&lt;/pre&gt;

&lt;p&gt;&amp;nbsp; Train class는&amp;nbsp;Pylearn2의 다른 강력한 기능들을 쓰기 쉽게 해준다. 예를 들어 save_path, save_freq 매개변수를 설정함으로써 Train 객체는 당신의 모델을 매 epoch 마다 저장할 수 있다. (&quot;epoch&quot;은 train_all() 호출 한번이다.)&amp;nbsp;저장은 기존 파일들을 overrwriter하지 않고 설정할 수 있다. 두번째 iteration부터는 저장이 필요할 수 있고 파일을 overwrite해야 할 수 있다. 이러한 경우에 쓰기가 성공하면 백업을 삭제할 것이고, 쓰기중에 죽었다면 백업으로 복구할 수 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; Pylearn2 Train 객체 사용의 편의성에 관란 예제가 몇개 있다. 이어서 더 강력한 콜백들과 훈련 알고리즘 플러그인 들을 어떻게 이용할 수 있는시 소개한다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;Configuring the Train object with a YAML file&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp; Pylearn2는 기계 학습과 직접적으로 관련 없는 문제들도 해야 한다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 이러한 문제들중 하나는 다양한 형식의 Dateset 객체를 모델과 연계하고, 모델이 디스크 등에 직렬화 되고도 관계를 유지 하냐는 것이다.&amp;nbsp;보통 Dataset들은 모든 모델과 전체를 복사하고 싶지 않을 만큼 충분히 크다. 그러나 우리는 Dataset을 충분히 전처리하여, 모든 전처리 결과를 저장하고 싶지 않고, 각 모델에 전처리된 dataset에 대한 hard link를 주고 싶다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 대신에, 우리는 모델에서 Dataset을 만드는 법을 기술ㄱ하기 위해&amp;nbsp;YAML을 사용하여 이 문제를 해결한다.&amp;nbsp;이것은 더 좋은 방법이 당신에게 있고 구현할 의지도 있다면,&amp;nbsp;이상적인 방법은 아닐 수도 있다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 우선 왜 Dataset을 Model과 연동하길 원하는 지 간단히 알아보자. 당신의 모델이&amp;nbsp;MNIST dataset에 관한 분류기라고 가정해보자.Model에 대한 훈련이 끝나고, 학습된 가중치들을 보고 싶을 것이다.&amp;nbsp;MNIST는&amp;nbsp;28 x 28 픽셀 크기의&amp;nbsp;greyscale 이미지들이지만&amp;nbsp;784개 원소를 가진 vector들이다. Pylearn2의 시각화 기능들이 가중치들을 이미지로 보여주기 위해, 모델들이&amp;nbsp;MNIST dataset에 훈련 되었다는 것을 알 필요가 있다.&amp;nbsp;이는 왜 저장된 모델에 대한 정확한 분석이 데이터셋에 대한 지식을 필요로 하는 지에 대한 하나의 예이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; 모든 모델을 Yaml로 잘 작성하는 것을 힘든 일이지만 우리의 모든 훈련 과정을 yaml로 기술할 수 있다. 파일의 일부분은 Model에 저장된 dataset을 기술하는 데 사용된다. 잘못 될 수 있는 유일한 경우는 모델의&amp;nbsp;직렬화, 역직렬화 사이에 변화가 생기는 것이다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp; 모든 것에 YAML을 사용하는 것은 라이브러리가 추구하는 이상적인 모습은 아니겠지만, 현재까지는 가장 간편한 방법이다.&lt;/p&gt;&lt;p&gt;&amp;nbsp; Yaml 만을 이용한 실험을 수행하기 위해서 제일 손위운 방법은 yaml을 아래와 같이 저장하는 것이다.&lt;/p&gt;

&lt;pre class=&quot;prettyprint&quot;&gt;!obj:pylearn2.train.Train {
        dataset : FILL IN DESCRIPTION OF DATASET HERE
        model : FILL IN DESCRIPTION OF MODEL HERE
        }
&lt;/pre&gt;

&lt;p&gt;&amp;nbsp; 아래 명령으로 실험을 수행할 수 있다.&lt;/p&gt;

&lt;pre class=&quot;prettyprint&quot;&gt;train.py my_train_job.yaml
&lt;/pre&gt;


&lt;p&gt;&amp;nbsp; 여기서 train.py 는 pylearn2/scripts에 저장된 script이다. Pylearn2 documentation에 있는 예제들은 PATH 환경 변수에 train.py의 path가 설정되어 있다고 가정한다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style=&quot;font-size: 14pt;&quot;&gt;reference&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://deeplearning.net/software/pylearn2/overview.html&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;http://deeplearning.net/software/pylearn2/overview.html&lt;/a&gt;&lt;/p&gt;</description>
<category>Pylearn2</category>
<author>21세기 유목민</author>
<guid>https://dsmoon.tistory.com/122</guid>
<comments>https://dsmoon.tistory.com/entry/Pylearn2-Overview#entry122comment</comments>
<pubDate>Tue, 11 Aug 2015 23:02:00 +0900</pubDate>
</item>
</channel>
</rss>