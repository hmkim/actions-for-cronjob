<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>angrypark</title>
    <description>Machine Learning &amp; Deep Learning Blog</description>
    <link>https://angrypark.github.io/</link>
    <atom:link href="https://angrypark.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 19 Mar 2019 08:10:52 +0000</pubDate>
    <lastBuildDate>Tue, 19 Mar 2019 08:10:52 +0000</lastBuildDate>
    <generator>Jekyll v3.7.4</generator>
    
      <item>
        <title>Multi Armed Bandit</title>
        <description>&lt;p&gt;최근 Recommendar System에 대해 공부하면서, Multi-armed bandit이라는 분야에 대해 공부할 필요가 있다고 생각하던 차에 &lt;a href=&quot;https://arxiv.org/abs/1510.00757&quot;&gt;A Survey of Online Experiment Design with the Stochastic Multi-Armed Bandit&lt;/a&gt;을 바탕으로 정리해보았습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;목차&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#기존-encoder-decoder-구조에서-생기는-문제&quot;&gt;기존 Encoder-Decoder 구조에서 생기는 문제&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#basic-idea&quot;&gt;Basic Idea&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#attention-score-functions&quot;&gt;Attention Score Functions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#what-do-we-attend-to&quot;&gt;What Do We Attend To?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#multi-headed-attention&quot;&gt;Multi-headed Attention&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#transformer&quot;&gt;Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;1-concept&quot;&gt;1. Concept&lt;/h1&gt;
&lt;p&gt;Multi-armed Bandit(이하 MAB)라는 단어가 나오게 된 배경은 겜블링입니다. 어떤 사람이 주어진 시간안에, 수익 분포가 다 다른 N개의 슬롯머신을 통해 최대의 수익을 얻는 방법은 무엇일까요? 만약 제한된 시간에 N개의 슬롯머신들을 당겨서 수익을 얻을 수 있는 기회가 주어진다면, 일단은 어느 시간 동안은 어느 슬롯 머신이 돈을 많이 얻을 수 있는 지 탐색하는 과정이 있어야 할꺼고(이를 Exploration이라고 합니다), 그 다음에는 자신이 판단하기에 괜찮은 슬롯 머신을 돌리면서 수익을 극대화하는 과정이 필요합니다(이를 Exploitation이라고 합니다).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-02/190205_concept.png&quot; alt=&quot;concept&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exploration을 많이 하면, 어떤 슬롯머신이 더 성공확률이 높은 것인 지를 더 잘 파악할 수 있지만, 그걸 찾기만 하다가 막상 수익을 많이 얻지 못한다는 단점이 있구요, exploitaion을 많이 하면 알려진 분포들 사이에서는 그나마 괜찮은 수익을 얻을 수 있겠지만, 더 좋은 슬롯머신을 찾아서 시도하지 못했다는 아쉬움이 생기겠죠. 이를 &lt;strong&gt;exploration-exploitation tradeoff&lt;/strong&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;MAB는 이런 exploration-exploitation tradeoff를 잘 조절해나가면서 빠른 판단과 좋은 결과를 내기 위한 실행을 결정합니다. 일단은 환경과 반응하면서 학습한다는 점과 decision making을 한다는 관점에서 강화학습의 한 종류라고 볼 수 있구요. 추천 시스템이나 주식 투자, 의료 실험 등에서 모두 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;기존 Supervised Learning과 가장 다른 점은, 실시간으로 이루어지는 exploration &amp;amp; exploitation와 변수에 자원(시간, 시도 횟수 등)을 넣었다는 점입니다. 기존 Supervised learning은 하나의 문제가 정해져 있고, 그 문제에 해당하는 데이터를 수집한 다음, 예측하고자 하는 값을 예측하는 decision boundary를 찾게 됩니다. 하지만 주식 투자나 추천 시스템에서는 예측하고자 하는 값이 자주 바뀌어서 데이터를 모으고 학습하고 이를 통해 예측하는 과정이 지나치게 오래 걸릴 때가 많습니다. 따라서 제한된 자원 내에서 최선의 수익을 얻기 위한 방법론 중 하나가 Mulit Armed Bandit입니다.&lt;/p&gt;

&lt;h1 id=&quot;2-considerations-of-mab-problems&quot;&gt;2. Considerations of MAB problems&lt;/h1&gt;
&lt;p&gt;MAB 실험 환경은 어떤 시도에 따른 결과를 즉각적으로 받을 수 있는 환경인 경우가 대부분입니다. 따라서 MAB 알고리즘에 대해 알기 전에 먼저 생각해야 할 것은, MAB 실험환경에서 어떻게 알고리즘을 평가하냐는 것입니다. 기존의 supervised learning이나 unsupervised learning은 확실한 loss function이 있고 이를 최소화하자는 목적이 있습니다. 그러나 MAB 실험 환경에서는 실제 온라인 환경으로 바로 평가하지 않는 한(사실 이것도 온전하다고는 볼 수 없죠.) 해당 MAB 알고리즘이 어떤 성능을 가지고 있는 지에 대한 평가가 어렵습니다. 이를 측정하기 위해 regret, variance and bounds of regret, stationary, feedback delay들을 통해 MAB 알고리즘을 평가합니다.&lt;/p&gt;

&lt;h2 id=&quot;1-regret&quot;&gt;1) Regret&lt;/h2&gt;
&lt;p&gt;Regret은 사전적 의미 그대로 이해하면 더 쉽습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“The remorse(losses) felt after the fact as a result of dissatisfaction with the agent’s (prior) choices.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이를 해석하면 사전에 기대했던 결과와 실제 결과의 차이라고 볼 수 있구요, 해당 bandits 중 가장 optimal한 결과와 내 결과의 차이로도 볼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{R}^E = \sum^{H}_{t=1}(\max_{i=1,2, ..., K}E[x_i,t ]) - \sum^{H}_{t=1} E[x_{S_t, t}]&lt;/script&gt;

&lt;p&gt;Regret에도 다양한 종류가 있지만, 위 수식은 선택된 arm에서의 기댓값과, 전체 arm에서의 가장 높은 기댓값과의 차이를 regret으로 정의내렸습니다. 즉, 이론적으로 사전에 각 arm별 분포를 정의내릴 수 있는데, 사전에 정의된 분포에 따른 기댓값의 최댓값과 MAB 알고리즘이 선택한 arm의 기댓값과의 차이를 구하는 것이죠. 이는 매우 직관적이고 쉽게 regret을 구할 수 있다는 장점이 있지만, 실제 서비스에 적용할 때 각 arm의 분포가 이론적으로 정의내린 분포와 다르면 결과가 매우 달라지기 쉽다는 단점이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-variance-and-bounds-of-regret&quot;&gt;2) Variance and Bounds of Regret&lt;/h2&gt;
&lt;p&gt;1)에서 언급한 regret은 결국 미리 정해놓은 분포(혹은 실제 분포)와 알고리즘이 예측한 분포가 얼마나 다른 지를 평가하는 지표입니다. 이를 supervised learning과 연결시켜 생각해보면, 위의 regret은 loss function의 일종이라고 볼 수 있습니다. 그런데 MAB 알고리즘에서도 supervised learning에서 나타나는 bias-variance tradeoff 문제가 발생할 수 있습니다. 평균이 낮은 regret도 중요하지만 variance가 낮은 regret도 중요하죠(기존 모델에서의 loss라고 생각하면 쉽습니다. Loss의 평균이 낮은 것도 중요하지만, variance가 낮아야 예측의 안정성을 보장합니다).&lt;/p&gt;

&lt;h2 id=&quot;3-stationary&quot;&gt;3) Stationary&lt;/h2&gt;

&lt;h2 id=&quot;4-feedback-delay&quot;&gt;4) Feedback Delay&lt;/h2&gt;
</description>
        <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/recommendar%20system/Multi-Armed-Bandit/</link>
        <guid isPermaLink="true">https://angrypark.github.io/recommendar%20system/Multi-Armed-Bandit/</guid>
        
        <category>Multi-armed bandit</category>
        
        
        <category>Recommendar System</category>
        
      </item>
    
      <item>
        <title>Attention in NLP</title>
        <description>&lt;p&gt;이 글에서는 attention이 무엇인지, 몇 개의 중요한 논문들을 중심으로 정리하고 NLP에서 어떻게 쓰이는 지를 정리해보았습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;목차&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#기존-encoder-decoder-구조에서-생기는-문제&quot;&gt;기존 Encoder-Decoder 구조에서 생기는 문제&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#basic-idea&quot;&gt;Basic Idea&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#attention-score-functions&quot;&gt;Attention Score Functions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#what-do-we-attend-to&quot;&gt;What Do We Attend To?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#multi-headed-attention&quot;&gt;Multi-headed Attention&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#transformer&quot;&gt;Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;기존-encoder-decoder-구조에서-생기는-문제&quot;&gt;기존 Encoder-Decoder 구조에서 생기는 문제&lt;/h1&gt;

&lt;p&gt;Encoder-Decoder 구조에서 가장 중요한 부분은 input sequence를 어떻게 vector화할 것이냐는 문제입니다. 특히 NLP에서는 input sequence이가 dynamic할 구조일 때가 많으므로, 이를 고정된 길이의 벡터로 만들면서 문제가 발생하는 경우가 많습니다. 즉, “안녕” 이라는 문장이나 “오늘 날씨는 좋던데 미세먼지는 심하니깐 나갈 때 마스크 꼭 쓰고 나가렴!” 이라는 문장이 담고 있는 정보의 양이 매우 다름에도 encoder-decoder구조에서는 같은 길이의 vector로 바꿔야 하죠. Attention은 그 단어에서 알 수 있는 것처럼, sequence data에서 상황에 따라 어느 부분에 특히 더 주목을 해야하는 지를 반영함으로써 정보 손실도 줄이고 더 직관적으로 문제를 해결하기 위해 처음 제안되었습니다.&lt;/p&gt;

&lt;h1 id=&quot;basic-idea-bahdanau-attention&quot;&gt;Basic Idea (Bahdanau Attention)&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;논문 : &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;가장 기본적인 아이디어는 encode할 때는 각각의 단어를 vector로 만들고, 각각을 attention weight에 따라 weighted sum을 한 다음, 이를 활용하여 다음 단어가 무엇일 지를 선택하는 것입니다.&lt;/p&gt;

&lt;p&gt;논문은 이 방식을 NMT에 사용하였는데요, bidirectional RNN을 encoder로 사용하고, &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;번째 단어에 대해 모든 단어에 대한 encoder output을 합쳐서 context vector로 만드는데, 이 때 단순 sum이 아닌 weight &lt;script type=&quot;math/tex&quot;&gt;\alpha_{ij}&lt;/script&gt;​를 곱해서 weighted sum을 한 것입니다(아래 첫번째 수식). 이 때 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;번째 단어에 대한 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;번째 단어의 attention weight는 아래 수식 처럼 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;번째 단어와 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;번째의 원래 encoder output끼리를 feedforward neural network(attention weight를 만드는 모델을 논문에서는 align 모델이라고 부릅니다)를 태워서 만듭니다(아래 두번째 수식).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e_{ij} = a(s_{i-1}, h_j)&lt;/script&gt;

&lt;p&gt;align 모델을 Multi-layer Perceptron으로 만든 이유는 비선형성을 반영하고자 한 것이라고 하구요, 결국 이 align 모델은 NMT에서 같은 의미를 가진 단어를 잘 정렬하고(그래서 align) 짝지어 주기 위해서 있는 겁니다. NMT에서의 cost function 자체를 loss로 backpropagation 했구요.&lt;/p&gt;

&lt;h1 id=&quot;attention-score-functions&quot;&gt;Attention Score Functions&lt;/h1&gt;

&lt;p&gt;위 논문 이후로 이 attention score &lt;script type=&quot;math/tex&quot;&gt;\alpha_{ij}&lt;/script&gt; 를 어떻게 만들 지에 대한 몇가지 변형들이 생겼는데요, 이를 정리해보겠습니다. 단어를 통일하기 위해 만들고자 하는 decoder state를 &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; (query vector), 여기에 쓰이는 모든 encoder states를 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; (key vector)라고 하겠습니다(이는 뒤에서 다룰 Attention is All You Need 논문에서 나온 정의입니다). 이 단어를 이용한다면 &lt;script type=&quot;math/tex&quot;&gt;\alpha_{ij}&lt;/script&gt; 는 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; 번째의 query vector를 만들기 위한 &lt;script type=&quot;math/tex&quot;&gt;i-j&lt;/script&gt; key vector들 사이의 attention score라고 할 수 있겠죠.&lt;/p&gt;

&lt;h2 id=&quot;1-multi-layer-perceptron-bahdanau-et-al-2015&quot;&gt;(1) Multi-layer Perceptron (Bahdanau et al. 2015)&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a(q, k) = w_2^T \tanh (W_1[q;k])&lt;/script&gt;

&lt;p&gt;위 논문의 MLP를 다시 적은 건데요, 이 방법은 나름 유연하고 큰 데이터에 활용하기 좋다는 장점이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-bilinear-luong-et-al-2015&quot;&gt;(2) Bilinear (Luong et al. 2015)&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a(q, k) = q^TWk&lt;/script&gt;

&lt;p&gt;같은 연도에 나온 Lunong Attention은 &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 사이에 weight matrix &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; 하나를 곱해서 만들어줍니다.&lt;/p&gt;

&lt;h2 id=&quot;3-dot-product-luong-et-al-2015&quot;&gt;(3) Dot Product (Luong et al. 2015)&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a(q, k) = q^Tk&lt;/script&gt;

&lt;p&gt;2와 유사하지만, 그냥 &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 를 dot product해서 이를 attention으로 쓰는 방법도 제안되었습니다. 이는 아예 학습시킬 파라미터가 없기 때문에 좋지만, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 의 길이를 같게 해야 한다는 단점이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-scaled-dot-product-vaswani-et-al-2017&quot;&gt;(4) Scaled Dot Product (Vaswani et al. 2017)&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a(q, k) = \frac{q^Tk}{\sqrt{\mid{k}\mid}}&lt;/script&gt;

&lt;p&gt;최근에 나온 논문 중에서 3을 개선 시킨 논문인데요. 기본적인 접근은 dot product 결과가 &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 의 차원에 비례하여 증가하므로, 이를 벡터의 크기로 나눠주는 겁니다.&lt;/p&gt;

&lt;h1 id=&quot;what-do-we-attend-to&quot;&gt;What Do We Attend To?&lt;/h1&gt;

&lt;p&gt;지금까지의 방법론들은 다 input sentence의 RNN output에다가 attention을 써서 이를 decoding에 활용했습니다. 이제 좀더 다양한 방식으로 attention을 맥이는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;1-input-sentence&quot;&gt;(1) Input Sentence&lt;/h2&gt;

&lt;p&gt;가장 기본적인 방법으로 그 전/ 그 후 input sentence들에다가 attention을 주는 방법입니다.&lt;/p&gt;

&lt;h3 id=&quot;--copying-mechanism-gu-et-al-2016&quot;&gt;- Copying Mechanism (Gu et al. 2016)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;논문 : &lt;a href=&quot;https://arxiv.org/pdf/1603.06393&quot;&gt;Incorporating Copying Mechanism in Sequence-to-Sequence Learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 방법은 output sequence에 input sequences의 단어들이 자주 중복될 때, 이를 잘 copy하기 위해 처음 제안되었습니다. 예를 들어 대화를 이끌어 나갈 때, 기존에 나왔던 단어들을 활용해서 대답해야 하는 경우가 많죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-01/190126_copynet.png&quot; alt=&quot;copynet&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;--lexicon-bias-arthur-et-al-2016&quot;&gt;- Lexicon Bias (Arthur et al. 2016)&lt;/h3&gt;

&lt;p&gt;위의 copying mechanism과 유사한데, 하나의 dictionary를 구축하고 있다가 이를 활용하여 generate하는 방식입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-previously-generated-things&quot;&gt;(2) Previously Generated Things&lt;/h2&gt;

&lt;p&gt;그 다음은 그 전 generated된 결과에 attention을 주는 방식입니다. Language Modeling에서 중요한 것은, 말하고 있는 문장들이 일관된 흐름으로 이어져야 한다는 점입니다. 이를 반영하기 위해 예전에 generated된 단어들에다가 attention을 맥여서 그걸로 계속 generate하는 방식으로 쓰일 수 있습니다(Merity et al. 2016). Translation에서도 유사하게 input과 그 전 input에 attention을 매깁니다(Vaswani et al. 2017).&lt;/p&gt;

&lt;p&gt;그런데 이 방법론의 문제점은, 한번 error가 생성되기 시작되면 그 error자체가 attention을 통해 계속 연결된다는 점입니다.&lt;/p&gt;

&lt;h2 id=&quot;3-hierachical-structures-yang-et-al-2016&quot;&gt;(3) Hierachical Structures (Yang et al. 2016)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;논문 : &lt;a href=&quot;https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf&quot;&gt;Hierachical Attention Networks for Document Classification&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 방법은 document level representation을 만들 때 쓰일 수 있습니다. 쉽게 생각하면, sentence representation을 만들 때 word들에 대해 attention을 맥이고, document representation을 만들 때 sentence들에 대해 attention을 맥이는 겁니다. 이는 document classification같이 Input sequence가 매우 긴 상황에서 사용해볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-multiple-sources&quot;&gt;(4) Multiple Sources&lt;/h2&gt;

&lt;p&gt;이제 문장들이 아니라 더 근본적인 출처를 다양화하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;--multiple-sentences-zoph-et-al-2015&quot;&gt;- Multiple Sentences (Zoph et al. 2015)&lt;/h3&gt;

&lt;p&gt;NMT에서 사용된 방법인데요, 하나의 target sentence를 생성해낼 때, 여러 개의 source 문장들을 동시에 사용해서 generate하는 방식입니다. 즉, 한국어를 만들 때 영어와 일본어 데이터를 동시에 써서 만드는 것이죠.&lt;/p&gt;

&lt;h3 id=&quot;--multiple-stratigies-huang-et-al-2016&quot;&gt;- Multiple Stratigies (Huang et al. 2016)&lt;/h3&gt;

&lt;p&gt;이 논문에서는 input image의 CNN output에다가 attention을 맥여서 sentence representation을 만들어냅니다.&lt;/p&gt;

&lt;h2 id=&quot;5-self-attention-cheng-et-al-2016&quot;&gt;(5) Self Attention (Cheng et al. 2016)&lt;/h2&gt;

&lt;p&gt;그 다음은 self attention입니다. 처음에 제안된건 2016년도 인데요, 하나의 문장에서 각각의 단어가 같은 문장의 다른 단어에 연관되어 있다는 관점에서 시작되었습니다. 즉 context sensitive한 encoding을 만들고자 한 것이죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-01/190126_self_attention.png&quot; alt=&quot;self_attention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 방법의 장점은 같은 문장끼리 attention을 구하기 때문에 길이가 항상 같다는 점입니다.&lt;/p&gt;

&lt;h1 id=&quot;multi-headed-attention&quot;&gt;Multi-headed Attention&lt;/h1&gt;

&lt;p&gt;기존의 방법들은 attention을 주고자 하는 구조에다가 attention을 각각 하나의 vector로 표현하고자 했던 것에 비해, multi-headed attention은 기본적으로 하나의 attention이 문장의 여러 부분에 영향을 줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-01/190126_multi_head_attention.png&quot; alt=&quot;multi_head_attention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Allamanis et al 2016에서는 copy하는 과정과 regular 과정에 각각을 attention을 주게 됩니다. ‘copy’하는 과정이 이전에 어느 부분을 가져다 쓸지에 주목한다면, ‘regular’ attention은 다음 결정을 만들기 위해 어느 부분에 집중할 것인지를 정하는 것이죠.&lt;/p&gt;

&lt;h1 id=&quot;transformer&quot;&gt;Transformer&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;논문 : &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마지막으로 transformer에 대해 알아보겠습니다. 많은 사람들이 self attention = transformer 라고 하는데요, 엄밀히 따지면 다릅니다. 다만 이 논문이 2016년도에 제안된 self attention을 많이 참고한 것은 사실입니다. 이 transformer는 크게 3가지 특징이 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Attention &lt;strong&gt;만을&lt;/strong&gt; 사용한 sequence to sequence 모델입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;WMT에서 압도적인 성능을 보여줬습니다.&lt;/li&gt;
  &lt;li&gt;단순 matrix 곱으로만 되어있기 때문에 빠릅니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;일단 구조를 그림으로 보면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-01/190126_transformer.png&quot; alt=&quot;transformer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;안에를 자세히 보면 이전에 언급했었던 attention 방식들이 많이 들어가 있습니다. 크게 4가지 attention trick이 있는데 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Self Attention : 각각의 layer는 하나의 단어를 같은 문장 내 다른 단어들과 연결합니다.&lt;/li&gt;
  &lt;li&gt;Multi-headed Attention :  8개의 attention head가 따로 학습됩니다.&lt;/li&gt;
  &lt;li&gt;Normalized Dot-product Attention : Dot product에서 나오는 bias를 제거해줍니다.&lt;/li&gt;
  &lt;li&gt;Positional Encodings : RNN이 없어도 문장 내에서의 위치를 잊어버리지 않도록 position을 같이 encoding 해줍니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;쉽게 설명하면 self attention을 multi-headed로 구현하고, 그 값에다가 feed forward network를 태워서 비선형성을 준 구조입니다. 그 밖에도 residual connection을 통해 gradient가 사라지는 것을 방지한 트릭도 있습니다.&lt;/p&gt;

&lt;p&gt;학습할 때의 트릭은 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Layer Normalization : Layer 각각의 값들이 정상적인 범위 안에 있도록 normalize해줍니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specialized Training Schedule : Adam optimizer의 default값을 개선시켜서 learning rate scheduling을 진행하였습니다. warm-start, dropping등이 여기서 쓰였죠&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Label Smoothing : traing 과정에 어느 정도의 uncertainty를 집어넣었습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;지금까지 다양한 attention 기법들이 어떻게 생겨났고 각각은 대략적으로 어떻게 쓰이는 지를 정리해보았습니다. 마지막 transformer의 경우, 워낙 중요한 논문이고 이 만을 정리한 좋은 블로그 포스트가 있으므로 더 알고 싶으시면 꼭 읽어 보시길 바랍니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;이 포스트는 CMU NLP 2018 강의 중 9강 Attention (&lt;a href=&quot;https://www.youtube.com/watch?v=ullLRKZ99qQ&amp;amp;list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE&amp;amp;index=21&quot;&gt;영상&lt;/a&gt;) 을 기반으로, 관련 논문들을 읽어보며 정리하였습니다. 각각의 reference는 글 본문에 있습니다.&lt;/p&gt;

</description>
        <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/Attention-in-NLP/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/Attention-in-NLP/</guid>
        
        <category>Attention</category>
        
        
        <category>Natural Language Processing</category>
        
      </item>
    
      <item>
        <title>Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots</title>
        <description>&lt;h2 id=&quot;retrieval-based-chatbots-vs-generation-based-chatbots&quot;&gt;Retrieval Based Chatbots vs Generation Based Chatbots&lt;/h2&gt;
&lt;p&gt;챗봇을 만드는 방법은 크게 2가지로 나눌 수 있습니다. 답변의 후보군을 정한 다음, 해당 query에 알맞은 적절한 reply를 후보군에서 찾아서 내보내는 방법(Retrieval Based)과, 아예 모델이 답변을 만들어내는 방법(Generation Based)입니다. 전자는 classification으로 접근할 수 있고, 나오는 답변을 직접 handle할 수 있다는 장점이 있지만, 주어진 답변의 coverage와 domain에 성능이 매우 달라질 수 있고, 답변이 제한적이라는 단점이 있습니다. 후자는 전자에 비해 답변의 variation이 훨씬 크다는 장점이 있지만, 애초에 문맥적으로 옳지 않거나, 나오는 답변을 handle할 수 없다는 단점이 있습니다. Generation Based의 단점이 워낙 심각하기 때문에(문법 자체가 틀린 경우가 많고, 부적절한 답이 나올 수 있음) 아직까지는 deep learning 방법을 쓰더라도 retrieval based로 접근하는 경우가 많습니다.&lt;/p&gt;

&lt;h2 id=&quot;기존-retrieval-based-chatbots의-문제&quot;&gt;기존 Retrieval Based Chatbots의 문제&lt;/h2&gt;
&lt;p&gt;기존에 알려진 문제(답변이 제한적이다, 답변 후보군에 따라 성능이 좌우된다) 이외에도 모델을 학습시키는 데에 있어서 발생하는 문제가 있습니다. 정상적인 대화를 어느정도 가지고 있을 때 이를 가지고 데이터를 만들 때, 보통 정상적인 대화의 각각의 pair를 1(positive), 나머지는 random negative sampling을 통해 0(negative)로 정의내린 다음 데이터를 만들지만, 이는 다음과 같은 문제를 가지고 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;--label-imbalance&quot;&gt;- Label Imbalance&lt;/h4&gt;
&lt;p&gt;이론상으로 random negative는 positive data에 주어진 pair 수가 n일 때 n(n-1)개만큼 만들 수 있지만 잘 학습하려면 지나치게 많은 negative를 넣으면 안되고, 결국 데이터의 수는 positive pair에 절대적으로 비례하는데 이는 비쌉니다.&lt;/p&gt;

&lt;h4 id=&quot;--weak-negative-vs-hard-negative&quot;&gt;- Weak negative vs Hard Negative&lt;/h4&gt;
&lt;p&gt;각각의 negative는 보통 학습할 때 batch에서 1~10개씩 뽑거나 전체 데이터에서 1~10개씩 뽑는데, 무작위로 뽑기 때문에 해당 negative가 모델이 학습하기 쉬운 negative일 확률도 있고, 반대로 random하게 뽑았는데 해당 query에 말이되는 reply(positive)이지만 negative로 잘못 학습하는 경우도 있습니다. 더 큰 문제는 이런 다양한 경우를 모두 같은 0으로 학습하기 때문에 제대로 학습하지 못하거나, 학습되더라도 뻔한 수준에 머무를 가능성이 있다는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;training-method&quot;&gt;Training Method&lt;/h2&gt;
&lt;p&gt;이 문제를 해결하기 위해서 이 논문에서는 기존에 1/0으로 학습하는 방식이 아닌 weak annotator 모델로 예측한 예측값을 사용하여 학습합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{N}\sum_{j=1}^{n}[r_{i, j}\log(\mathcal{M}(x_i, y_{i, j})) + (1-r_{i, j})\log{1-\mathcal{M}(x_i, y_{i, j})}]&lt;/script&gt;

&lt;p&gt;수식을 보면, 기존 방식은 positive pair의 matching score가 높을 수록 좋고, negative pair의 matching score가 낮을수록 좋게끔 선정한 것에 비해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;arg\min\sum_{i=1}^{N}\sum_{j=1}^{n}\max(0, \mathcal{M}(x_i, y_{i, j})-\mathcal{M}(x_i, y_{i, 1}) + s_{i, j}')&lt;/script&gt;

&lt;p&gt;여기서 제안한 방식은 negative일 경우 negative의 matching score(모델이 예측한 값)에서 positive의 matching score(모델이 예측한 값)을 빼고 거기에 weak annotator의 예측값을 더한게 작을수록 좋게끔 하는 것입니다. positive일 경우에는 &lt;script type=&quot;math/tex&quot;&gt;\max(0, s_{i, j}')&lt;/script&gt;로 상수가 됩니다. 이 수식에서 특징은&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;0보다 작을 경우 0으로 clipping&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;에 bias가 생기지 않도록 normalize (&lt;script type=&quot;math/tex&quot;&gt;s_{i, j}' = \max(0, \frac{s_{i, j}}{s_{i, 1}})&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;입니다. 기본적인 framework는 annotator에 어떤 모델을 사용해도 되지만 여기서는 대용량의 사람 사이의 대화로 pre-trained된 Seq2Seq(+Attention)을 사용하였습니다.&lt;/p&gt;

&lt;p&gt;위의 학습방법은 positive pair에 대한 matching score와 negative의 matching score의 차이를 최대한 크게 벌리는 데에는 기존과 같지만, &lt;script type=&quot;math/tex&quot;&gt;s_{i, j}'&lt;/script&gt;를 통해 각각의 sample 간의 차이를 준다는 장점이 있다고 생각합니다. 한 예로 semantic하게 유사하지도 않은 negative에게는 더 낮은 점수를 주고, semantic하게는 유사할 경우에는 어느 정도의 점수를 주고 학습을 할 수 있습니다. 논문에서는 이를&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“An advantage of out method is that it turns the hard zero-one labels in the existing learning paradigm to soft matching scores”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;라고 표현합니다.&lt;/p&gt;

&lt;h2 id=&quot;weak-annotator&quot;&gt;Weak Annotator&lt;/h2&gt;
&lt;p&gt;Weak Annotator은 Adversarial한 두 개가 있다는 점에서 GAN과 비슷하지만, discriminator를 통해 generator를 더 잘 학습시키는 GAN과 달리 이 방법은 generator(weak annotator)를 통해 discriminator(classifier)를 더 잘 학습시킵니다. 기존에도 &lt;a href=&quot;https://arxiv.org/pdf/1711.11383&quot;&gt;Dehghani et al., 2017&lt;/a&gt;와 같이 supervisor를 쓴 경우는 있었지만 이 모델은 unsupervised supervisor라는 점이 의미가 있는 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;updating-seq2seq-model&quot;&gt;Updating Seq2Seq model&lt;/h2&gt;
&lt;p&gt;가장 재밌게 읽었던 부분이 이 부분인데요, 보통의 Seq2Seq 모델에서 발생하는 “safe response” 문제(많이 나오는 답변에 편향되는 문제)를 해결하기 위해 20 배치마다 policy gradient를(!!) 통해 update를 했다고 하는데, 결과적으로는 성능이 안좋았다고 합니다. 그 이유로&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;PG로 Seq2Seq를 발전시키는 방법 자체가 어렵다&lt;/li&gt;
  &lt;li&gt;“safe response”를 해결하는 것이 weak annotator의 성능을 높이는데 도움을 준다고 판단할 수 없다
를 들었습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;STC Dataset(Short Text Conversation, 1-turn)과 Douban Conversation Corpus(multi-turn)에 적용한 결과는 다음과 같습니다. 이 때 &lt;code class=&quot;highlighter-rouge&quot;&gt;num_negative_samples&lt;/code&gt;는 20으로 했다고 합니다. (soft한 score로 인해 더 많은 negative를 넣을 수 있는 것도 장점인 것 같습니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-09-27/STC_result.png&quot; alt=&quot;STC_result&quot; width=&quot;350&quot; /&gt; &lt;img src=&quot;/images/2018-09-27/DCC_result.png&quot; alt=&quot;DCC_result&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/chatbots/paper%20review/Learning-Matching-Models-with-Weak-Supervision/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/chatbots/paper%20review/Learning-Matching-Models-with-Weak-Supervision/</guid>
        
        <category>weak supervision</category>
        
        <category>ACL2018</category>
        
        <category>retrieval chatbots</category>
        
        <category>Seq2Seq</category>
        
        <category>matching model</category>
        
        
        <category>Natural Language Processing</category>
        
        <category>Chatbots</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>Improving Deep Neural Networks - Hyperparameter Tuning</title>
        <description>&lt;h2 id=&quot;들어가며&quot;&gt;들어가며&lt;/h2&gt;

&lt;p&gt;최근 머신러닝을 활용한 프로젝트들이나 kaggle competition들은 기존 statistical machine learning이나 boosting methods에서 벗어나 딥러닝을 활용한 프로젝트들이 주를 이루고 있습니다. 저또한 이제 막 딥러닝을 활용한 다양한 프로젝트를 시도해보고 있는데, 세상엔 좋은 모델들이 많지만 모델을 내가 활용하고자 하는 특정 데이터에 잘 튜닝하는 것이 많이 어려웠습니다. 이 글에서는 cousera의 &lt;a href=&quot;https://www.coursera.org/learn/deep-neural-network?authMode=login&amp;amp;errorCode=invalidCredential&quot;&gt;Improving Deep Neural Networks : Hyperparameter Tuning, Regularization and Optimization&lt;/a&gt; 강의를 기반으로 어떻게 모델을 잘 최적화하는 지에 대한 방법들을 소개합니다. 추가적으로 자료를 찾아보면서 더 많은 내용을 담으려고 했습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;주의&quot;&gt;주의&lt;/h4&gt;
  &lt;p&gt;이 글에서는 특정 모델에 대한 이론적 내용이나 상황에 따라 어떤 모델을 선택해야 할 지에 대해서는 다루지 않습니다(상황, 과제에 따라 달라지기 때문에). 어떤 neural network모델을 정했다고 가정하고 이를 tuning할 수 있는 방법들을 정리한 것이라고 보면 됩니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;목차&quot;&gt;목차&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-checking&quot;&gt;Gradient Checking&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#initialization&quot;&gt;Initialization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#optimization&quot;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fine-tuning-techniques&quot;&gt;Fine Tuning Techniques&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dropout-and-activation-function&quot;&gt;Dropout and Activation Function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-initialization&quot;&gt;1. Initialization&lt;/h2&gt;
&lt;p&gt;Neural network를 학습시키려면 먼저 weights와 bias의 초기값들을 잘 선정해주어야 합니다. 초기값을 잘 정해놓기만 해도 더 효율적으로 학습할 수 있을 뿐만 아니라 실제로 더 좋은 성능을 보여준다고 합니다. 아직까지도 어떤 방법이 제일 좋은 방법인지는 증명되지 않았다고 합니다. 다만 실험적으로 몇몇 좋은 초기화 방법들은 존재합니다. Initialization 방법들에는 다음과 같은 방법들이 있습니다.
    - Zeros / Random initialization
    - Xavier initialization
    - He initialization
    - LSUV initialization (All you need is good init)&lt;/p&gt;

&lt;h3 id=&quot;1-zeros-initialization&quot;&gt;(1) Zeros initialization&lt;/h3&gt;
&lt;p&gt;가장 먼저 생각해볼 수 있는 초기화 방법들입니다. Weights를 0으로 초기화 할 경우 그냥 logistic regression이 되어버립니다. 유사한 문제로 symmetry problem이 있는데, 각 레이어의 각각의 뉴런이 같은 것을 배워서 똑같이 최적화되는 문제를 의미합니다. Bias를 0으로 초기화하는 것은 일반적인 방법 중 하나이므로 괜찮습니다. LSTM의 경우 bias를 1로 두는 방법도 있다고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;2-random-initialization&quot;&gt;(2) Random initialization&lt;/h3&gt;
&lt;p&gt;Standard normal distribution에서 random한 값을 뽑아 그 값으로 weight를 정하는 것입니다. 그럴 경우 symmetry problem은 해결하지만 다른 2가지의 문제가 생길 수 있습니다. 바로 vanishing gradients problem과 exploding gradients problem입니다. 딥러닝을 최적화하는데에 있어서 가장 일반적으로 만나게 되는 문제들이기도 한데요, 각각의 문제들의 정의와 다른 관점에서의 해결 방법들은 다음을 참고하시기 바랍니다.
    - &lt;a href=&quot;https://www.wikiwand.com/en/Vanishing_gradient_problem&quot;&gt;Vanishing Gradient Problem&lt;/a&gt;
    - &lt;a href=&quot;https://machinelearningmastery.com/exploding-gradients-in-neural-networks/&quot;&gt;Exploding Gradient Problem&lt;/a&gt;
Random initialization의 또다른 문제는 바로 random성을 결정할 난수 값의 따라 성능이 크게 달라진다는 점입니다. 아래 그림처럼 같은 네트워크에 같은 활성화함수(ReLU) 같은 데이터라도 어떤 시드 값(random seed)으로 random initialization하느냐에 따라 성능과 수렴 시간이 달라집니다.
&lt;img src=&quot;../assets/2018-05-23/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-xavier-initialization&quot;&gt;(3) Xavier initialization&lt;/h3&gt;
&lt;p&gt;Xavier initialization은 random initialization에서 발생하는 두가지 문제(vanishing &amp;amp; exploding gradient problem)을 어느정도 해결하였으며, 현재 가장 일반적으로 자주 쓰이는 초기화 방법입니다. 2010년에 나온 논문인 &lt;a href=&quot;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&quot;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;에서 처음 소개되었으며, 저자인 Xavier glorot의 이름을 따서 Xavier initialization이라고 불립니다. 그 전에 성능 향상을 위한 다양한 초기화 방법들이 제안되었지만, 대부분 너무 계산량이 많고 구현하기 어려웠습니다. Xavier initialization은 다음과 같이 코드 몇줄이면 구현 가능합니다!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Xavier initialization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fan_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fan_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fan_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;간단히 살펴보면, 입력값(&lt;code class=&quot;highlighter-rouge&quot;&gt;fan_in&lt;/code&gt;)과 출력값(&lt;code class=&quot;highlighter-rouge&quot;&gt;fan_out&lt;/code&gt;) 사이의 난수를 정한다음에 입력값의 제곱근으로 나눠준 값으로 초기화합니다.&lt;/p&gt;

&lt;h3 id=&quot;4-he-initialization&quot;&gt;(4) He initialization&lt;/h3&gt;
&lt;p&gt;He initialization은 Xavier와 거의 유사한데, &lt;code class=&quot;highlighter-rouge&quot;&gt;fan_in&lt;/code&gt;을 넣는 대신 &lt;code class=&quot;highlighter-rouge&quot;&gt;fan_in/2&lt;/code&gt;를 넣어줍니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# He initialization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fan_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fan_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fan_in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;5-lsuv-initialization&quot;&gt;(5) LSUV initialization&lt;/h3&gt;
&lt;p&gt;2015년에 나온 논문인 &lt;a href=&quot;https://arxiv.org/pdf/1511.06422.pdf&quot;&gt;All you need is good init&lt;/a&gt;에서 처음 소개된 방법으로, 다음의 2가지 순서를 통해 weight를 초기화합니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;각각의 convolution이나 inner-product layer를 orthonomal 한 행렬로 초기화합니다.&lt;/li&gt;
  &lt;li&gt;첫 레이어부터 마지막 레이어까지 차례대로 돌면서 각각의 layer의 출력값의 분산이 1이 되도록 정규화해줍니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;서로 다른 activation function(maxout, ReLU-family, tanh)에 대해 기존 초기화 방법들보다 더 좋은 성능을 보여주었다고 합니다. 성능 확인은 GoogLeNet, FitNets, Residual nets와 MNIST, CIFAR-10/100 데이터셋에서 최고 성능을 내는 네트워크들에게 이루어졌습니다. Pytorch code는 나와있는데(&lt;a href=&quot;https://github.com/ducha-aiki/LSUV-pytorch&quot;&gt;github&lt;/a&gt;) 아직 tensorflow code는 없습니다. (왜일까요? 구현하기 어려워보이지는 않는데..)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;찾아보니깐 누군가가 구현한 tensorflow코드는 있네요. 다만 따로 library형태는 아니라 성능 보장은 보장할 수 없습니다.&lt;/p&gt;

  &lt;p&gt;https://github.com/PAN001/All-CNN/blob/8b3f20c3f62ef631c7d84482c48e9811da7e094b/strided_all_CNN_tf_LSUV.py&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;2-before-learning-sanity-checks-tipstricks&quot;&gt;2. Before learning: sanity checks Tips/Tricks&lt;/h2&gt;
&lt;p&gt;일단 모델이 제대로 돌아갈 것인지를 판단해야 합니다. 다음 3가지 방법으로 이를 판단할 수 있습니다.&lt;/p&gt;
&lt;h3 id=&quot;1-look-for-correct-loss-at-chance-performance&quot;&gt;(1) Look for correct loss at chance performance&lt;/h3&gt;
&lt;p&gt;쉽게 말해서 baseline이 무엇인지를 파악하는 것입니다. 예를 들어 결과값의 class가 10개인 문제에서는 맞을 확률이 찍어도 0.1이고, 이 때 활성화함수로 softmax layer를 쓰고 loss가 negative log probability라면 baseline은 $-ln(0.1)=2.302$가 됩니다. 초기 loss가 이보다도 낮다면 초기화에 문제가 있다고 볼 수 있습니다.&lt;/p&gt;
&lt;h3 id=&quot;2-increasing-regularization-should-increase-the-loss&quot;&gt;(2) Increasing regularization should increase the loss&lt;/h3&gt;
&lt;p&gt;정규화를 더 세게 할수록 loss는 증가해야 합니다. 그렇지 않다면 애초에 tuning하기 전에 다른 문제들이 있을 수 있습니다.&lt;/p&gt;
&lt;h3 id=&quot;3-overfit-a-tiny-subset-of-data&quot;&gt;(3) Overfit a tiny subset of data&lt;/h3&gt;
&lt;p&gt;작은 데이터(약 20개)를 매우 overfitting 시킨다면 결국 loss는 0이 되어야 합니다. 이 때 정규화나 dropout은 사용하지 않고 해봐야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;3-babysitting-the-learning-process&quot;&gt;3. Babysitting the learning process&lt;/h2&gt;
&lt;p&gt;모델을 train할 때 유심히 봐야할 몇가지 지표들이 있습니다. 여기서는 그 지표들에 대해 소개하고, 정상적으로 train된다면 어떻게 되어야 할 지에 대해 소개합니다.&lt;/p&gt;
&lt;h3 id=&quot;1-loss-function&quot;&gt;(1) Loss function&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-05-23-improving-deep-neural-networks/3.jpeg&quot; alt=&quot;drawing&quot; width=&quot;300&quot; /&gt; &lt;img src=&quot;/images/2018-05-23-improving-deep-neural-networks/4.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 loss입니다. Train할 수록 떨어지는 것은 당연한데, 떨어지는 모양에 따라 &lt;strong&gt;무엇이 문제인지&lt;/strong&gt; 알 수 있습니다. 위의 왼쪽 그림에서 빨간색 선처럼 loss가 줄어들어야 정상인데, 너무 갑자기 overfitting되어 올라가면 learning rate가 너무 큰 것이고, 너무 천천히 내려가면 learning rate가 작은 것입니다. learning rate를 잘못 선정하면 최종 loss 자체가 줄어들 수 있으니 잘 모니터하고 선정해야 합니다. 또한 보통 loss는 위의 오른쪽 그림처럼 진동(wiggle)하기 마련인데, 진동하는 정도는 바로 &lt;strong&gt;batch size&lt;/strong&gt;와 관련이 있습니다. Batch size가 작을수록 진동하는 정도는 더 커집니다.&lt;/p&gt;

&lt;h3 id=&quot;2-trainval-accuracy&quot;&gt;(2) Train/Val accuracy&lt;/h3&gt;
&lt;p&gt;두번째로 중요하게 봐야할 지표는 train / validation accuracy입니다. 아래 그림처럼 train accuracy는 validation accuracy에 비해 높은 것은 당연하지만, 지나치게 높으면 train set에 overfitting되고 있다는 증거이고, 지나치게 낮으면 underfitting되있다는 것입니다. 만약 파란색 선처럼 너무 overfitting되었다면 regularization을 증가하거나(L2 weight 증가, dropout rate 증가) 더 많은 데이터를 확보해야 합니다. 만약 초록색 선처럼 매우 유사하게 증가한다면 모델의 파라미터 수를 증가해줄 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-05-23-improving-deep-neural-networks/accuracies.jpeg&quot; alt=&quot;accuracy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-ratio-of-weightsupdates&quot;&gt;(3) Ratio of weights:updates&lt;/h3&gt;
&lt;p&gt;마지막으로 gradient의 &lt;strong&gt;변화량&lt;/strong&gt;입니다. 각각의 파라미터 셋에 대해 평가하는데, 만약 이 변화량이 $1e-3$ 근처에 있다면 정상이고, 그보다 낮다면 learning rate가 너무 낮다는 증거입니다.&lt;/p&gt;

&lt;h3 id=&quot;4-activationgradient-distributions-per-layer&quot;&gt;(4) Activation/Gradient distributions per layer&lt;/h3&gt;
&lt;p&gt;잘못된 초기화는 전체 학습과정을 상당히 느리게 할 수 있습니다. 이 문제는 쉽게 진단할 수 있는데요, 전체 layer에 대해서 activation/gradient histogram을 그려보면 됩니다. 그 값이 -1에서 1 사이에 분포되어 있으면 정상이고 이상한 값이나 다 0으로 되어 있으면 문제가 있는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;5-first-layer-visualizations&quot;&gt;(5) First layer visualizations&lt;/h3&gt;
&lt;p&gt;특히 &lt;strong&gt;이미지 처리&lt;/strong&gt; 관련 분야에서는 첫번째 layer가 어떤 결과를 내는지 시각화해서 보면 대략적으로 잘 작동되고 있는지 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-05-23-improving-deep-neural-networks/weights.jpeg&quot; width=&quot;250&quot; /&gt; &lt;img src=&quot;/images/2018-05-23-improving-deep-neural-networks/cnnweights.jpg&quot; width=&quot;280&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 그림을 보면 딱봐도 noise가 많아보이므로, learning rate가 잘못 선정되었거나 regularization이 너무 적을 수 있습니다. 반면 오른쪽 그림은 다양한 feature를 잡아내는 것으로 볼 수 있어 train이 잘 되고 있다고 보면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;4-parameter-updates&quot;&gt;4. Parameter updates&lt;/h2&gt;
&lt;p&gt;계산된 loss와 gradient를 기반으로 optimization method들을 어떻게 시도하고 선정할 지에 대한 부분입니다. 다만 이 부분은 지금도 연구가 활발하게 진행되고 있는 분야이기 때문에, 어떤 방법이 정답이라기 보다는 제일 일반적이고 기본적인 관점과 접근을 소개합니다.&lt;/p&gt;

&lt;h3 id=&quot;1-sgd-with-bells-and-whistles&quot;&gt;(1) SGD with bells and whistles&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Vanilla update : &lt;script type=&quot;math/tex&quot;&gt;x += lr * dx&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Momentum update : &lt;script type=&quot;math/tex&quot;&gt;v = mu * v -lr * dx, x += v&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Nesterov momentum :&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-annealing-the-learning-rate&quot;&gt;(2) Annealing the learning rate&lt;/h3&gt;
&lt;p&gt;learning rate는 시간이 갈수록 줄여야 합니다. 쉽게 생각해서 learning rate가 크면 parameter vector가 극심하게 왔다갔다하기 때문에 깊은 자리에 들어가기 힘듭니다. 따라서 점차 learning rate를 줄여나가야 하는데, 그 일반적인 방법에는 다음이 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Step decay : 몇번의 epoch마다 일정한 값을 줄입니다.&lt;/li&gt;
  &lt;li&gt;Exponential decay : 몇번의 epoch마다 일정한 비율로 줄입니다.&lt;/li&gt;
  &lt;li&gt;1/t decay : &lt;script type=&quot;math/tex&quot;&gt;\alpha = \alpha_0 / (1+kt)&lt;/script&gt;
https://www.jeremyjordan.me/nn-learning-rate/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-per-parameter-adaptive-learning-rates&quot;&gt;(3) Per-parameter adaptive learning rates&lt;/h3&gt;
&lt;p&gt;지금까지 논의된 접근법들은 모든 파라미터에 똑같은 학습 속도를 적용하였습니다. 이를 해결하고 데이터에 맞추어 자동으로 학습속도를 정하는 방법을 찾고자 많은 사람들이 노력하였습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Adagrad : 데이터에 맞춘 학습속도 조정방법&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;cache += dx**2
x += -lr * dx / (np.sqrt(cache)+eps)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;RMSprop&lt;/li&gt;
  &lt;li&gt;Adam&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-hyperparameter-optimization&quot;&gt;5. Hyperparameter optimization&lt;/h2&gt;
&lt;p&gt;이제 학습에서 가장 빈번하게 조절해야할 hyperparameter를 어떻게 수정해나가면서 찾을 것인지를 알아보겠습니다. Neural network에서 가장 중요한 hyperparameter는 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;initial learning rate&lt;/li&gt;
  &lt;li&gt;learning rate decay schedule&lt;/li&gt;
  &lt;li&gt;regularization strength(L2 penalty, dropout strength)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;initial-learning-rate&quot;&gt;Initial learning rate&lt;/h3&gt;
&lt;p&gt;가장 중요한 점은 &lt;strong&gt;log scale&lt;/strong&gt;로 늘리고 줄여야 한다는 점입니다. 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt;이나 &lt;code class=&quot;highlighter-rouge&quot;&gt;3e-4&lt;/code&gt;를 결정하여 쓰지만, 이는 어떤 optimizer가 제일 좋다라는 말만큼이나 말도 안되는 얘기고, 이상적인 값은 없습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-3/&quot;&gt;Learning Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;An overview gradient descent algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ruder.io/deep-learning-optimization-2017/&quot;&gt;Optimization for Deep Learning Highlights in 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/case%20study/improving-deep-neural-networks/</link>
        <guid isPermaLink="true">https://angrypark.github.io/case%20study/improving-deep-neural-networks/</guid>
        
        <category>model tuning</category>
        
        
        <category>Case Study</category>
        
      </item>
    
      <item>
        <title>A Diversity-Promoting Objective Function for Neural Conversation Models</title>
        <description>&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기존 SMT(Statistical Machine Translation) 모델 소개 : 여러 문제점을 가지고 있음&lt;/li&gt;
  &lt;li&gt;문제점들 중 scalability와 language independence를 해결한 것이 Seq2Seq&lt;/li&gt;
  &lt;li&gt;Seq2Seq도 답변이 너무 뻔함 (ex. “I don’t know.”, “I’m OK”)&lt;/li&gt;
  &lt;li&gt;그런데 K-best list를 뽑아보면, 상황에 적합한 문장이 있긴 있는데 뻔한 답변보다는 랭크가 낮아서 안나왔던 것&lt;/li&gt;
  &lt;li&gt;이를 MMI로 해결하겠다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Related Works&lt;/strong&gt;(SMT, Seq2Seq 등의 논문들 소개), &lt;strong&gt;3. Sequence-to-Sequence Models&lt;/strong&gt;(LSTM 설명)은 패스&lt;/p&gt;

&lt;h3 id=&quot;4-mmi-models&quot;&gt;4. MMI Models&lt;/h3&gt;
&lt;h4 id=&quot;--용어-정의&quot;&gt;- 용어 정의&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;$S$ : input message sequence&lt;/li&gt;
  &lt;li&gt;$N_S$ : number of words in $S$&lt;/li&gt;
  &lt;li&gt;$T$ : {$t_1, t_2, …, t_{N_T},EOS$}, response to source sequence&lt;/li&gt;
  &lt;li&gt;$t$ : a word token which is associated with a K dimensional distinct word embedding $e_t$&lt;/li&gt;
  &lt;li&gt;$V$ : vocabulary size&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;--기존-objective-function--source-s에-대한-t의-log-likelihood&quot;&gt;- 기존 Objective Function : source S에 대한 T의 log-likelihood&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{T} = argmax_T(\log_p(T|S))&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;--mmi--s와-t의-pairwise-mutual-information을-최대화하게끔-선택됨&quot;&gt;- MMI : S와 T의 pairwise mutual information을 최대화하게끔 선택됨&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\log{\frac{p(S,T)}{p(S) p(T)}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;이를 이용한 목적함수는
&lt;script type=&quot;math/tex&quot;&gt;\hat{T} = argmax_T(\log_p(T|S)-\log_p(T))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;여기에 분모에 대해 페널티를 추가하면,
&lt;script type=&quot;math/tex&quot;&gt;\hat{T} = argmax_T(\log_p(T|S)-\lambda \log_p(T))&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;베이즈 정리를 이용하여 바꾸면,
&lt;script type=&quot;math/tex&quot;&gt;\log{p(T)} = \log_p(T|S) + \log_p(S) - \log_p(S|T)&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;MMI-antiLM : $\log_p(T&lt;/td&gt;
          &lt;td&gt;S)-\lambda \log_p(T)$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;anti language model이라고 불립니다. high-frequency, generic responses뿐만 아니라 fluent해서 ungrammatical output이 나올 수 있는 것들도 줄입니다. 이론적으로는 $\lambda$가 1보다 작으면 비문이 생기는 것을 방지하지만, 실제로는 뒤의 항으로 인해 비문들이 선택된다고 합니다. 이를 해결하기 위해서 $p(T)$를 계산할 때 $g(i)$를 곱해줍니다. 여기서 $g(i)$는 일정한 threshold보다 높으면 1, 아니면 0을 선정하여 일정 길이가 넘어가는 index의 토큰들을 안곱해줍니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;MMI-bidi : $(1-\lambda)\log_p(T)+\lambda \log_p(S&lt;/td&gt;
          &lt;td&gt;T)$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;위의 공식을 바로 구현하기는 매우 어렵다고 합니다(두번째 항이 target generation을 끝내야 계산할 수 있기 때문이죠). 따라서 먼저 standard Seq2Seq로 계산한 뒤에, N-best list에서 두번째 항을 기준으로 재정렬을 하면 된다고 합니다!!!! 이건 이해하기 쉽군요.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

</description>
        <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/paper%20review/a-diversity-promoting-objective-function/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/paper%20review/a-diversity-promoting-objective-function/</guid>
        
        <category>machine translation</category>
        
        <category>Seq2Seq</category>
        
        
        <category>Natural Language Processing</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>Deep Learning Glossary</title>
        <description>&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Activation function&lt;/p&gt;

    &lt;p&gt;딥러닝의 가장 큰 장점인 &lt;strong&gt;복잡한 decision boundary&lt;/strong&gt;를 적용하기위해, 비선형적인 activation function을 layer에 추가합니다. 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tanh&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ReLU&lt;/code&gt;와 이들의 변형을 사용합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adadelta&lt;/p&gt;

    &lt;p&gt;Gradient descent을 기반으로 만든 알고리즘으로 파라미터마다 learning rate를 최적화합니다. Adagrad를 개선하기 위해 만들어졌으며, hyperparameter에 더 민감합니다. Adadelta는 rmsprop과 유사하며 vanilla SGD 대신 사용되기도 합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adagrad&lt;/p&gt;

    &lt;p&gt;Adagrad는 adaptive learning rate algorithm 방법들 중 하나로 시간에 따라 바뀌는 griadients의 squared 값을 추적하고, 동시에 learning rate를 최적화시킵니다. Vanilla SGD 대신 쓰이기도 하며, 특히 sparse한 데이터에 좋습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adam&lt;/p&gt;

    &lt;p&gt;Adam도 optimizing algorithm 중 하나로, rmsprop과 유사하지만 update를 첫번째와 두번째 순간의 gradient의 running average로 추정합니다. 또 bias correction term도 포함되어 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Affine layer&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Fully-connected layer&lt;/strong&gt;와 같은 말입니다. 그 전 레이어의 모든 뉴런이 현재 레이어의 모든 뉴런과 연결되어 있다는 뜻입니다. 보통 output의 final prediction전에 추가됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attention mechanism&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Human visual attention&lt;/strong&gt;에서 영감을 받았으며, 어떤 이미지에서 특정 부분에 더 집중하게 됩니다. Language processing과 image recognition 구조에 모두 적용됩니다. 결국 예측할 때, 어느 부분에 집중하여 예측할 지를 결정하는 것입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alexnet&lt;/p&gt;

    &lt;p&gt;CNN architecture의 일종으로 ILSVRC 2012에서 우승하였으먀 5개의 convolution layer와 max-pooling을 추가하였고 3개의 affine layer가 추가되어 finally 1000 way softmax가 추가되어있는 형태입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autoencoder&lt;/p&gt;

    &lt;p&gt;Autoencoder는 목표가 input을 그대로 예측하는 것입니다. &lt;strong&gt;Bottleneck&lt;/strong&gt; 구조를 띄고 있으며, 그를 위해서 일단 저차원으로 represent하고 이를 다시 복원합니다. 따라서 PCA와 같은 다른 차원축소 기법들과 유사하지만, 비선형적인 환경으로 인해 더 복잡한 mapping이 가능합니다. 변형 기법으로 Denoising Autoencoders, Variational Autoencoders, Sequence Autoencoders 등이 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average-pooling&lt;/p&gt;

    &lt;p&gt;Pooling 기법 중 하나로 CNN for image recognition에 쓰입니다. 정해진 크기의 window(filter)를 이동시키면서 그 평균을 구합니다. 뭐, input을 저차원으로 압축시킨다고 보면 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Backpropagation through time&lt;/p&gt;

    &lt;p&gt;역적판 기법 중 하나로 RNN에 쓰입니다. 표준의 역전파 방식을 RNN에 적용시켰다고 보면 되고, layer가 시간을 의미하기 때문에, 시간을 관통하는 역전파라고 불리는 것입니다. 몇백개의 input이라고 한다면, BPTT가 computational cost를 줄이기 위해 사용됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization&lt;/p&gt;

    &lt;p&gt;작은 batch마다의 layer input들을 normalize하는 방법입니다. 학습을 더 빠르게 하고, learning rate를 더 빠르게 최적화하며, regularizer 역할도 해줍니다. CNN에는 좋지만 RNN에는 안좋다고 합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bidirectional RNN&lt;/p&gt;

    &lt;p&gt;두 개의 RNN을 반대 방향을 연결한 네트워크 구조입니다. Foward RNN은 input sequence를 순방향으로 읽고, Backward RNN은 역방향으로 읽습니다. NLP에 자주 쓰이고, 그 단어 전후의 문맥을 파악하고자 할때 쓰입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Categorical cross-entropy loss&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Negative log likelihood&lt;/strong&gt;와 같은 말입니다. 분류 문제에서 유명하고, 두 개의 확률분포에서의 유사성을 측정합니다. $L=-\sum(y*\log(y_{prediction}))$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Channel&lt;/p&gt;

    &lt;p&gt;input data의 channel은 여러 개일 수 있습니다. 이건 뭐..&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep belief network&lt;/p&gt;

    &lt;p&gt;DBN은 probabilistic graphic model 중 하나로 data의 hierachical representation을 비지도학습으로 배웁니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep dream&lt;/p&gt;

    &lt;p&gt;CNN에서 포착된 knowledge를 시각화하는 기법으로, 기존의 이미지를 변형하거나 새로운 그림을 만들면서 이를 꿈같은 표현으로 생성합니다..??&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropout&lt;/p&gt;

    &lt;p&gt;regularization technique의 일종으로 overfitting을 방지합니다. 임의로 가지치기를 한다고 생각하시면 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Embedding&lt;/p&gt;

    &lt;p&gt;input을 vector로 표현하는 방법입니다. word2vec같이 explicit하게 배우거나, sentiment analysis 같이 지도 학습을 통해 배워지기도 합니다. pre-trained embedding에서 시작되어 그 과제에 맞게 fine-tuning되기도 합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exploding gradient problem&lt;/p&gt;

    &lt;p&gt;Vanishing Gradient Problem의 반대의 의미입니다. Gradient clipping을 통해 이를 조절합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fine-tuning&lt;/p&gt;

    &lt;p&gt;다른 과제 또는 환경에서 학습된 네트워크를 원하는 과제에 맞게 다시 최적화하는 방법입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient clipping&lt;/p&gt;

    &lt;p&gt;exploding gradients를 조절하기 위해 사용하는 방법으로, 특히 RNN에서 자주 사용됩니다. 대표적인 방법으로는 L2 norm이 threshold를 넘을 때 normalize하는 방법입니다.
  &lt;script type=&quot;math/tex&quot;&gt;Gradients_{new} = \frac{Gradients_{old} * threshold}{l2}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GloVe&lt;/p&gt;

    &lt;p&gt;embedding 기법 중 하나로, co-occurence matrix의 통계량을 이용한다는 특징이 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GoogleNet&lt;/p&gt;

    &lt;p&gt;ILSVRC 2014에서 우승한 CNN architecture입니다. Inception module을 사용하여 네트워크의 computing resource를 향상시켰습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GRU&lt;/p&gt;

    &lt;p&gt;Gated Recurrent Unit의 약자입니다. LSTM의 간단 버전이고 파라미터 수가 더 적습니다. LSTM과 마찬가지로 RNN에서 gating mechanism을 써서 long-range dependency를 조절하고 vanishing gradient problem을 해결합니다. GRU도 reset과 update gate가 있어서 old memory중 어떤 부분을 보존하거나 update할 지를 결정합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Highway Layer&lt;/p&gt;

    &lt;p&gt;gating mechanism을 전체 layer에 적용시킨 모델입니다. 아주 깊은 모델 구조에서 input의 어떤 부분을 그대로 넘기고, 어떤 부분을 변형해서 넘길지를 결정합니다. 
  &lt;script type=&quot;math/tex&quot;&gt;T\times h(x)+(1-T)\times h(x)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ICML&lt;/p&gt;

    &lt;p&gt;International Conference for Machine Learning&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ILSVRC&lt;/p&gt;

    &lt;p&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inception module&lt;/p&gt;

    &lt;p&gt;CNN architecure에서 더 효율적인 계산과 더 깊은 네트워크를 위해 쓰이는 방식으로 차원 축소와 stacked 1x1 convolutions를 사용합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM&lt;/p&gt;

    &lt;p&gt;vanishing gradient problem을 해결하기 위해서 발명되었으며 memory gating mechanism을 사용합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Max-pooling&lt;/p&gt;

    &lt;p&gt;CNN에서 사용되는 pooling 기법들 중 하나입니다. 이름만 봐도 뭔지 알겠죠?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Momentum&lt;/p&gt;

    &lt;p&gt;Gradient descent algorithm의 확장으로, parameter 갱신을 가속화하거나 감속합니다. Gradient descent를 업데이트할 때, momentum term을 넣으면 더 잘 수렴합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural machine translation&lt;/p&gt;

    &lt;p&gt;언어간 번역을 의미합니다. end-to-end bilingual corpora를 통해 학습되기도 하며, 기본적으로는 encoder와 decoder rnn을 사용합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural turing machine&lt;/p&gt;

    &lt;p&gt;simple algorithm을 neural network로 구현하는 겁니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Noise-contrastive estimation&lt;/p&gt;

    &lt;p&gt;large output vocabulary를 가진 분류 모델을 학습시킬 때 쓰일 수 있는 sampling loss입니다. 모든 가능한 경우의 수에 대해 softmax를 쓰기는 어렵습니다. NCE를 사용하면, 문제를 binary classificaion problem으로 제한할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Restricted boltzmann machine&lt;/p&gt;

    &lt;p&gt;probabilistic한 graphic model의 일종으로 stochastic artifical neural network이다. 비지도 학습으로 데이터를 vector로 바꿉니다. Contrasive divergence를 사용하여 더 효율적으로 학습합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recursive nerual network&lt;/p&gt;

    &lt;p&gt;RNN을 트리 구조에 적용한 모델입니다. 이미 잘 train된 rnn에 사용되곤 합니다. parsing tree in NLP&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ResNet&lt;/p&gt;

    &lt;p&gt;ILSVRC 2015에 우승한 CNN architecture입니다. layer를 건너뛰어서 연결하는 residual mapping을 이용하였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RMSProp&lt;/p&gt;

    &lt;p&gt;gradient-based optimization algorithm 기법 중 하나입니다. adagrad와 유사하지만, decay term을 추가함으로서 adagrad의 learning rate의 급격한 감소를 방지합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Seq2Seq&lt;/p&gt;

    &lt;p&gt;Sequence-to-sequence model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SGD&lt;/p&gt;

    &lt;p&gt;Stochastic Gradient Descent입니다. 보통 minibatch에서 사용하곤 합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Vanishing gradient problem&lt;/p&gt;

    &lt;p&gt;너무 깊은 neural network에서 발생하는 문제 중 하나입니다. 역전파 과정에서 작은 gradient가 곱해져서 없어지는 경향을 말합니다. 이를 해결하기위해 ReLU를 쓰거나, LSTM같은 architecture를 쓰거나 합니다. 반대의 경우엔 exploding gradient problem이 발생합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 13 Mar 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/blog/deep-learning-glossary/</link>
        <guid isPermaLink="true">https://angrypark.github.io/blog/deep-learning-glossary/</guid>
        
        <category>glossary</category>
        
        <category>wildml</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>FastText &amp; GloVe</title>
        <description>&lt;h1 id=&quot;enriching-word-vectors-with-subword-information&quot;&gt;Enriching Word Vectors with Subword Information&lt;/h1&gt;

&lt;p&gt;기본적으로 word단위로 끊어서 이를 embedding하는 Word2Vec과 달리, word를 character단위로 n-gram한 뒤, 이들의 &lt;strong&gt;subword&lt;/strong&gt;를 embedding했더니 더 나은 성능을 보여주었다는 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;기존 모델은 각각의 단어를 벡터로 embedding&lt;/li&gt;
  &lt;li&gt;이는 내부 문장 구조를 무시(큰 한계), rich한 언어에 더 큰 한계&lt;/li&gt;
  &lt;li&gt;그래서 우리는 characer n-grams을 embedding함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-model&quot;&gt;3. Model&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;General Model&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;skip-gram model&lt;/p&gt;

        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;skip-gram model은 주어진 단어 시퀸스 $w&lt;em&gt;1,…,w_T$에 대해 log-likelihood를 최대화하는 목적을 가지고 있습니다. $$\sum&lt;/em&gt;{t=1}^{T}\sum_{c \in C_t} \log p(w_c&lt;/td&gt;
              &lt;td&gt;w_t) $$ 여기서 $C_t$는 해당 단어 $w_t$근처에 있는 단어들의 집합입니다.&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Subword Model&lt;/p&gt;

    &lt;p&gt;skip-gram 모델은 단어 자체의 internal structure를 고려하지 않는다는 단점이 있습니다. 따라서, 이를 해결하기 위해 다른 scoring fuction을 제안합니다. &amp;gt; Notations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$G_w$ : 단어 $w$에 속하는 n-grams subwords&lt;/li&gt;
      &lt;li&gt;$z_g$ : 각각의 $g$($g \in G_w$)의 벡터 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(w,c)=\sum_{g \in G_w} z_g^T v_c&lt;/script&gt;

&lt;h3 id=&quot;4-experimental-setup&quot;&gt;4. Experimental Setup&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Baseline&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;c의 word2vec에 있는 skipgram과 cbow 모델로 비교&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;stochastic gradient descent on the negative log likelihood &amp;gt; &lt;strong&gt;Stochastic gradient descent?&lt;/strong&gt; &amp;gt; &amp;gt; Gradient descent를 할 때 전체 데이터에 대해 optimization을 하지 않고 mini-batch에 대해 계산하여 optimization을 진행. 정확도는 다소 떨어지겠지만 계산 속도가 많이 빠르기 때문에 이를 여러번 실행하면 더 효율적으로 최적화 가능&lt;/li&gt;
    &lt;/ul&gt;

    &lt;blockquote&gt;
      &lt;p&gt;&lt;strong&gt;Negative log-likelihood?&lt;/strong&gt;&lt;/p&gt;

      &lt;table&gt;
        &lt;tbody&gt;
          &lt;tr&gt;
            &lt;td&gt;보통은 Maximum log-likelihood를 사용. 입력값 $X$와 파라메티 $\theta$가 주어졌을 때 정답 $Y$가 나타날 확률인 likelihood $P(Y&lt;/td&gt;
            &lt;td&gt;X;\theta)$를 최대화했었음. 정보이론으로 접근하면 두 확률분포 $p$, $q$ 사이의 차이를 계산하는 데에는 cross entropy가 사용되는데, 이 때 cross entropy가 파라메터 $\theta$안에서의 negative log-likelihood의 기댓값입니다. 그냥 cross entropy로 계산할 수도 있지만($H(P,Q)=-\sum_x P(x)\log Q(x)$) 손실함수로 negative log-likelihood를 사용할 경우, 1)만들려는 모델에 대한 다양한 확률분포를 가정할 수 있게 되 유연하게 대응가능하고, 2)&lt;/td&gt;
          &lt;/tr&gt;
        &lt;/tbody&gt;
      &lt;/table&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;5-results&quot;&gt;5. Results&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Human similarity judgement&lt;/p&gt;

    &lt;p&gt;사람의 유사도 평가와, 실제 단어 embedding의 유사도의 correlation 비교 결과 짱&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Word analogy tasks&lt;/p&gt;

    &lt;p&gt;$A:B = C:D$이다. 같은 문제에서 $D$를 예측하는 문제를 풀어봄.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison with morphological representations&lt;/p&gt;

    &lt;p&gt;word similarity 문제에 적용함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Effect of size of the training data&lt;/p&gt;

    &lt;p&gt;약 20%의 데이터만 가지고 전체 데이터의 embedding을 잘 해결해주었다. 대박&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Effect of the size of n-grams&lt;/p&gt;

    &lt;p&gt;3-6이 좋다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;6-qualitative-analysis&quot;&gt;6. Qualitative Analysis&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Nearest neighbors : baseline보다 좋다&lt;/li&gt;
  &lt;li&gt;Character n-grams and morphemes :&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;7-conclusion&quot;&gt;7. Conclusion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;character 단위의 n-grams로 이루어진 subword를 활용해서 embedding&lt;/li&gt;
  &lt;li&gt;train 빠름, preprocessing 필요없음&lt;/li&gt;
  &lt;li&gt;baseline 보다 훨씬 좋은 성능을 보임&lt;/li&gt;
  &lt;li&gt;open-source화함 —&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;global-vectors-for-word-representations-glove&quot;&gt;Global Vectors for Word Representations (GloVe)&lt;/h1&gt;
&lt;p&gt;word-word co-occurrence matrix에서 nonzero인 요소들만 학습하여 좋은 성능을 보인 word embedding 방법입니다. word-context의 sparse한 matrix에서 SVD를 통해 차원축소를 해서 좋은 결과를 보인 LSA보다 좋은 효과를 보여준다고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;1-introduction-1&quot;&gt;1. Introduction&lt;/h3&gt;

&lt;p&gt;기존에 word vector와 관련된 논문들은 크게 1) global matrix factorization, 2) local context window methods 정도로 나눌 수 있습니다. 1의 대표적인 방법인 LSA는 효율적으로 통계적 정보를 반영하지만, word analogy task($A:B=C:?$)같은 문제는 잘 풀지 못합니다. 2는 analogy 같은 문제들은 잘풀지만, 극소적으로 학습하기 때문에 corpus의 통계량을 반영하지 못합니다. 여기서는, specific weighted least squares 모델을 제안하여 전체 데이터셋에서의 word-word co-occurence matrix를 통해 효율적으로 통계량을 활용합니다. 이 모델은 word analogy에서 75%의 정확도를 보이고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-the-glove-model&quot;&gt;3. The GloVe Model&lt;/h3&gt;

&lt;p&gt;단어의 출연도에 대한 통계량은 모든 word representation 관련 비지도 학습에서 사용됩니다. 물론 어떤 원리로 얼마나 의미를 반영하는지는 아직 숙제가 남아있습니다. 이 숙제를 해결하기 위해서 이 논문에서 GloVe를 제안한 것인데, 쉽게 표현하면 global corpus statistics(co-occurence matrix)를 활용한 word representation 방식입니다.&lt;/p&gt;

&lt;p&gt;간단한 예시를 통해 어떻게 co-occurence matrix에서 의미를 도출할 수 있는지 살펴보겠습니다. Co-occurence matrix를 계산하면 어떤 단어들이 밀접하게 관련이 있고 어떤 단어들이 관련이 없는지 더 직관적으로 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h3&gt;

&lt;p&gt;요즘 빈도수 기반 word representation이 나을지 prediction-based word representation이 나을지 논쟁이 있어왔다. 이 모델은 prediction-based model에 counts-based information(co-occurence matrix)를 넣어 좋은 결과를 보여주었다.&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/embedding/paper%20review/fasttext-glove/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/embedding/paper%20review/fasttext-glove/</guid>
        
        <category>Word Embedding</category>
        
        <category>FastText</category>
        
        <category>GloVe</category>
        
        
        <category>Natural Language Processing</category>
        
        <category>Embedding</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>A Persona-Based Neural Conversation Model</title>
        <description>&lt;h1 id=&quot;a-persona-based-neural-conversation-model&quot;&gt;A Persona-Based Neural Conversation Model&lt;/h1&gt;

&lt;p&gt;대화 생성 모델에서의 응답자의 정보를 동일하게 유지하기 위한 방법을 소개합니다. 이 방법은 Seq2Seq 모델을 기반으로 만들어졌습니다. Speaker Model과 Speaker-Addressee Model 이 있는데요, Speaker Model은 인물의 개성을 추출하여 이를 벡터화 하는 모델이구요, Speaker-Addressee Model은 두명의 발화자의 상호작용을 통해 추가적인 정보를 추출해냅니다.&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;보통 data-driven system은 likelihood가 가장 높은 순서로 response를 정합니다. 그러나 그냥 &lt;em&gt;비슷한 결과&lt;/em&gt; 만을 도출하기 때문에 일관성이 부족합니다. 집의 주소를 약간의 화법만 바꿔서 물어봐도 집 주소가 바뀝니다. 이를 해결하기 위해서 PERSONA를 정의하고 이를 추출하기 위해 노력합니다. PERSONA는 내재된 사실, 내재된 유저의 정보, 언어 형식, 상호작용 스타일 등이 될 수 있습니다. PERSONA는 또한 가변적이어야 합니다. 왜냐하면 상호작용이나 대화상대가 바뀜에 따라 persona도 바뀌어야 하기 때문입니다.&lt;/p&gt;

&lt;p&gt;이를 두 개의 모델로 해결했는데요. 먼저 Speaker model은 speaker를 벡터로 표현한 다음 이를 SEQ2SEQ Target에 통합합니다. 비슷한 형태로, Speaker-Addressee model은 두 대화자의 interaction pattern을 Seq2Seq model에 결합합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;실제로 어떻게 넣었는지는 전혀 감이 안오지만, 일단 느낌만 알고 pass&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;2-related-works&quot;&gt;2. Related Works&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SMT 모델 소개&lt;/li&gt;
  &lt;li&gt;Seq2Seq 모델 소개&lt;/li&gt;
  &lt;li&gt;Standard dialog model 소개&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-seq2seq-models--pass&quot;&gt;3. Seq2Seq Models : Pass&lt;/h3&gt;

&lt;h3 id=&quot;4-personalized-response-generation&quot;&gt;4. Personalized Response Generation&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Notation
    &lt;ul&gt;
      &lt;li&gt;$M$ : input word sequence {$m_1, m_2, …, m_I$}&lt;/li&gt;
      &lt;li&gt;$R$ : word sequence in response to $M$ {$r_1,r_2,…,r_J,EOS$}&lt;/li&gt;
      &lt;li&gt;$J$ : length of the response&lt;/li&gt;
      &lt;li&gt;$r_t$ : $K$차원의 word embedding $e_t$에서 나온 단어 토큰&lt;/li&gt;
      &lt;li&gt;$V$ : vocabulary size&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Speaker Model&lt;/p&gt;

    &lt;p&gt;Speaker의 특수정보(dialect, register, age, gender, personal information)를 벡터로 변환하는 모델입니다. 이러한 정보들은 인위적으로 조절되지 않았습니다. 대신, 유저들을 특정 정보에 따라 clustering해줍니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Speaker-Addressee Model&lt;/p&gt;

    &lt;p&gt;발화 형식이나 대화내용 등의 개성은 발화자 뿐만 아니라 “누구와 대화하느냐”에 따라서도 달라지게 됩니다. 이 모델은 발화자 i가 발화자 j와 대화할 때 어떻게 대화할지를 예측합니다. 이는 엄청 큰 크기의 매트릭스를 만들어야 한다는 단점이 있지만, 유저의 정보를 임베딩하는 과정에서 비록 유저 $i$와 유저 $j$가 만난 적이 없더라도, $V_{i’j’]$를 계산하여 예측합니다. 여기서 $i’$, $j’$은 가장 비슷한 유저들을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Decoding and Reranking
 Decoding을 위해서, beam size $B$=200인 decoder로 만들어낸 N-best list를 활용합니다. 그 다음에 이전에 소개했던 MMI를 활용하여 최고의 답변을 찾아냅니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
</description>
        <pubDate>Thu, 08 Mar 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/chatbots/paper%20review/A-Persona-Based-Neural-Conversation-Model/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/chatbots/paper%20review/A-Persona-Based-Neural-Conversation-Model/</guid>
        
        <category>persona</category>
        
        <category>chatbots</category>
        
        <category>Seq2Seq</category>
        
        
        <category>Natural Language Processing</category>
        
        <category>Chatbots</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>Sequence to Sequence(Seq2Seq) Paper Review</title>
        <description>&lt;p&gt;Sequence에서 다른 Sequence로 연결하는 일반적인 end-to-end 접근 방법을 소개합니다. (여기서 sequence는 연관된 연속 데이터를 의미) LSTM을 활용하여 input sequence를 정해진 벡터로 mapping하고, 다른 LSTM을 활용하여 그 벡터를 target seqeunce(여기선 예로 다른 언어)로 mapping합니다. train하는 과정에서 source data의 단어의 순서를 바꾸는 과정이 LSTM의 성능을 엄청나게 올렸습니다.&lt;/p&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;딥러닝은 기존의 머신러닝이 풀기 힘들었던 여러 복잡한 문제들(음성 인식)에 뛰어난 성능을 보여줍니다. 큰 네트워크에 미리 학습된 파라미터들이 있다면 나중에 쉽게 응용 가능할 것입니다.
하지만 DNN은 input과 output이 정해진 차원의 vector여야 합니다. 그걸 구현하기 위해서 여기서는 하나의 LSTM이 input sequence를 timestamp와 함께 읽으면서 large fixed-dimensional vector로 표현하였고, 두번째 LSTM은 vector에서 output sequence를 추출합니다.&lt;/p&gt;

&lt;h3 id=&quot;2-the-model&quot;&gt;2. The Model&lt;/h3&gt;
&lt;p&gt;sequence를 학습하기 위한 가장 간단한 방법은 input sequence를 하나의 RNN을 활용해서 고정된 크기의 벡터로 변환하는 것입니다. 하지만 long term dependencies 문제로 인해 RNN이 제대로 학습되지 않을 수 있습니다. 그러나 LSTM은 이를 해결합니다.&lt;/p&gt;

&lt;p&gt;LSTM의 목표는 input sequence의 크기와 ouput sequence의 크기가 다르더라도 output sequence의 conditional probability를 잘 추정하는 것입니다. 먼저 input sequence($x_1$,…, $x_T$)를 LSTM의 마지막 hidden state를 활용해 정해진 크기의 벡터로 변환합니다. 그 다음 $y_1$,….,$y_{T’}$를 LSTM-LM으로 추정합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;LSTM-LM?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Language Model로 이전 단어가 들어왔을 때 다음에 해당 단어가 나올 확률을 계산하여 가장 나올 확률이 높은 단어를 추가하는 모델입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_1,...,y_{T'}|x_1,...,x_T) = \prod_{t=1}^{T'}p(y_t|v,y_1,...,y_{t-1})&lt;/script&gt;

&lt;p&gt;그러나!!!! 여기서 소개하는 모델은 바로 위의 모델과 다음에서 다릅니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;2개의 LSTM : input sequence를 위한 LSTM, output sequence를 위한 LSTM&lt;/li&gt;
  &lt;li&gt;deep LSTM : shallow LSTM 대신 4 레이어의 깊은 LSTM 사용&lt;/li&gt;
  &lt;li&gt;input sequence의 순서를 역순으로 함&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;3-experiments&quot;&gt;3. Experiments&lt;/h3&gt;
&lt;p&gt;그 다음은 성능 비교였는데, 뭐 대부분의 논문들이 그러하듯이 좋았다라는 말만 반복합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;긴 문장에서도 잘된다&lt;/li&gt;
  &lt;li&gt;BLEU Score가 높다&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5-결론--좋다&quot;&gt;5. 결론 : 좋다!&lt;/h3&gt;
&lt;hr /&gt;
</description>
        <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/paper%20review/seq2seq/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/paper%20review/seq2seq/</guid>
        
        <category>Seq2Seq</category>
        
        
        <category>Natural Language Processing</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>Deep Learning for Chatbots</title>
        <description>&lt;h1 id=&quot;deep-learning-for-chatbots-part-1---introduction&quot;&gt;Deep Learning for Chatbots, Part 1 - Introduction&lt;/h1&gt;

&lt;h3 id=&quot;a-taxonomy-of-models&quot;&gt;A taxonomy of models&lt;/h3&gt;

&lt;h4 id=&quot;retrieval-based-vs-generative-models&quot;&gt;Retrieval-based vs Generative Models&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Retrieval-based models&lt;/strong&gt;는 heuristic한 방법으로 미리 정해진 답변들을 mapping해줍니다. 그 과정에서 rule-based 모델이나 machine learning classifiers의 앙상블 모델을 사용하기도 합니다. 이 방법은 새로운 텍스트를 만들지 않고, 단지 이미 정해진 set에서 하나 고르게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generative Models&lt;/strong&gt;은 미리 정해진 답변에 의존하지 않습니다. 이들은 아예 새로운 답변들을 만들어 냅니다. 기본적으로 Machine translation techniques에 기반하여 만들어지지만, 하나의 언어를 다른 언어로 번역하는 것이 아니라, 하나의 input을 원하는 output으로 translate합니다.&lt;/p&gt;

&lt;p&gt;두가지 방법 모두 장점과 단점들이 있습니다. Retrieval-based models는 데이터셋이 정해져 있기 때문에, 문법적인 오류가 나타나지 않습니다. 하지만 예상치못한 케이스나 미리 정해져 있는 질문과 조금만 달라도 답을 내는 것이 불가능하죠. 같은 이유로 상황에 대한 정보(이름)같은 것들을 전혀 이해하지 못합니다. 반면에 Generative models는 더 똑똑합니다. 잘만 하면 마치 사람이 답변하는 듯한 답변을 만들어낼 수 있습니다. 그러나 train하기 힘들고, 문법적인 오류들을 내기 쉬우며, 엄청나게 큰 데이터셋을 필요로 합니다.&lt;/p&gt;

&lt;p&gt;딥러닝 기법들은 두가지 상황 모두에서 사용될 수 있지만, 여기서는 generative한 방향으로 소개합니다. Seq2Seq같은 모델은 텍스트를 생성하는 데에 나름 최적화되어 있지만, 제대로된 결과를 생성해내기까지는 시간이 좀더 걸릴 듯 합니다. 지금 기업들에서는 보통 retrieval-based models를 사용하고 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;long-vs-short-conversations&quot;&gt;Long vs Short Conversations&lt;/h4&gt;
&lt;p&gt;긴 대화일수록 이를 자동화하기는 더 어려워집니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;쉬움 : Short-Text Conversations&lt;/li&gt;
  &lt;li&gt;어려움 : Long Conversations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;open-domain-vs-closed-domain&quot;&gt;Open Domain vs Closed Domain&lt;/h3&gt;
&lt;p&gt;Open domain이 더 힘듭니다. 잘 정의된 목표도 없고, 대화가 어느 방향으로 흐를지도 잘 모릅니다. 무한한 가능성의 주제와, 그 주제가 주어지면 해당 주제에 대한 어느정도의 지식 또한 있어야 제대로된 답을 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Closed domain은 가능한 input이 정해져 있고, output도 제한적입니다. 기술적 고객 지원이나 구매 관련 문의 등이 좋은 예죠.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;common-challenges&quot;&gt;Common Challenges&lt;/h3&gt;
&lt;p&gt;다음은 대화가능한 챗봇을 만들 때 겪게 되는 명확한 vs 애매한 과제들입니다.&lt;/p&gt;

&lt;h4 id=&quot;incorporating-context---맥락-다루기&quot;&gt;Incorporating Context - 맥락 다루기?&lt;/h4&gt;
&lt;p&gt;센스 있는 답변을 내기 위해서는 linguistic context와 physical context를 모두 다룰 수 있어야 합니다. 여기서 linguistic context는 긴 대화에서 무엇을 말했는지, 어떤 정보가 교환되었는지 등을 의미합니다. 가장 일반적인 방법은 대화를 벡터로 임베딩하는 것입니다.&lt;/p&gt;

&lt;h4 id=&quot;coherent-personality-성향의-일관성&quot;&gt;Coherent Personality 성향의 일관성&lt;/h4&gt;
&lt;p&gt;문법적으로 다르지만 같은 의미의 질문에 대해 동일한 답변을 해야한다.
== 문법적으로 비슷하지만 완전 다른 의미의 질문에 대해 다른 답변을 해야한다.&lt;/p&gt;

&lt;p&gt;-&amp;gt; 이번에 고치려고 하는 문제와 많이 비슷합니다.&lt;/p&gt;

&lt;h4 id=&quot;evaluation-of-models&quot;&gt;Evaluation of Models&lt;/h4&gt;
&lt;p&gt;대화가능한 챗봇을 평가하는 가장 이상적인 방법은 그들이 해결해야한 문제를 얼마나 잘 해결했는지 입니다. 하지만 이를 평가해서 라벨을 얻기란 매우 비쌉니다(인건비). BLEU같은 일반적인 행렬구조가 있기도 하지만 센스있는 답변을 평가하기엔 부족합니다. 사실, 그 어떤 매트릭스 구조도 제대로 평가하지 못한다고 증명한 사례가 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;intention-and-diversity&quot;&gt;Intention and Diversity&lt;/h4&gt;
&lt;p&gt;Generative model의 가장 평범한 문제는 ‘몰라요’, ‘글쎄요’같은 일반적인 답변을 내기 쉽다는 것입니다. 어떤 연구는 이 문제를 다양한 목적함수를 합침으로서 좀더 다채롭게 하려고 시도했습니다. 그러나 사람은 보통 입력에 매우 한정된 답변을 하고 그를 위해서 특정한 의도를 가집니다. Generative model은 사람처럼 특정 의도를 가지지 못하기 때문에 이런 다양성을 가지기 힘듭니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;그래서-실제로-어떻게-작동하나요&quot;&gt;그래서 실제로 어떻게 작동하나요?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Retrieval-based open domain system : 불가능&lt;/li&gt;
  &lt;li&gt;Generative open-domain system $\approx$ Artificial General Intelligence
-&amp;gt; 둘다 매우 힘듬&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 제한된 상황에서의 문제 해결들만 남게 되었습니다. 문장이 길어질수록, 문맥적 의미가 중요해질수록 더 문제는 어려워집니다.&lt;/p&gt;

&lt;p&gt;앤드류 응님에 따르면,&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;딥러닝의 진가는 많은 양의 데이터를 얻을 수 있는 제한된 영역에서 나타납니다. 딥러닝이 절대 못하는 것은 “의미있는 대화 나누기”입니다. 의미 있는 대화를 만들기 위해 cherry-picking할 수는 있지만, 결국 다시 혼자 해보면 망합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;많은 기업들은 그들의 대화를 저장하고 외주를 맡기면 그들의 대답을 “자동화”할 수 있을 거라 기대합니다. 하지만 이는 그 대답이 충분히 “좁은 영역”일 때만 가능합니다. 하지만 그렇더라도 사람 노동자를 지원하기에는 충분합니다.&lt;/p&gt;

&lt;p&gt;생산 시스템에서의 문법오류는 매우 큰 손실을 끼칠 수 있습니다. 그 것이 아직까지도 retrieved-based model들이 주류를 이루는 이유 중 하나입니다. 만약 기업들이 엄청나게 큰 데이터를 갖게 된다면 generative model도 구현가능해지겠지만, 심각한 오류가 나지 않도록 하는 다른 기술지원들이 병행되어야 합니다.&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/natural%20language%20processing/chatbots/deep-learning-for-chatbots/</link>
        <guid isPermaLink="true">https://angrypark.github.io/natural%20language%20processing/chatbots/deep-learning-for-chatbots/</guid>
        
        <category>wildml</category>
        
        
        <category>Natural Language Processing</category>
        
        <category>Chatbots</category>
        
      </item>
    
      <item>
        <title>Moving Beyond Linearity</title>
        <description>&lt;p&gt;이번 시간에는 ISL(Introduction to Statistical Learning) 7장을 리뷰해보았습니다. 7장에서는 Moving Beyond Linearity, 선형성을 확장하고자 시도했던 여러 모델들을 배우는데요. 선형모델들의 단순 확장인 step function, polynomial regression 뿐만 아니라 splone, local regression, generalized additive model들도 다룹니다.&lt;/p&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#71-polynomial-regression&quot;&gt;7.1. Polynomial Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#72-step-function&quot;&gt;7.2. Step Function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#73-base-function&quot;&gt;7.3. Basis Function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#74-regression-splines&quot;&gt;7.4. Regression Splines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#75-smoothing-splines&quot;&gt;7.5. Smoothing Splines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#76-local-regression&quot;&gt;7.6. Local Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#77-generalized-additive-models&quot;&gt;7.7. Generalized Additive Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;71-polynomial-regression&quot;&gt;7.1. Polynomial Regression&lt;/h2&gt;
&lt;p&gt;역사적으로 선형회귀를 비선형적 모델로 확장시킨 가장 대표적인 회귀는 다음과 같은 선형 모델을&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i = \beta_0 + \beta_1 x_i+\epsilon_i&lt;/script&gt;

&lt;p&gt;우리가 알고 있는 다항식 형식으로 대체하는 것입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i = \beta_0 + \beta_1 x_i+\beta_2 x_i^2+\beta_3 x_i^3+...+\beta_d x_i^d+\epsilon_i&lt;/script&gt;

&lt;p&gt;단순선형회귀와 마찬가지로 최소제곱법을 활용하여 쉽게 추정할 수도 있지만, d(차원)이 커질수록 지나치게 flexible해져서 이상한 결과를 낳을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-02-14/1.PNG&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 점선으로 된 곡선은 표준오차 곡선들입니다. 그렇다면 특정 값 &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;에 대해 어떻게 분산을 구해서 신뢰구간을 그린 것일까요? 특정 값 &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;에서의 적합의 분산 &lt;script type=&quot;math/tex&quot;&gt;Var \hat f(x_0)&lt;/script&gt;은 &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat \beta_j&lt;/script&gt; 각각에 대한 분산 추정치&lt;/strong&gt; 들과 &lt;strong&gt;계수 추정치&lt;/strong&gt; 들 사이의 공분산을 가지고 구할 수 있다고 합니다. 오른쪽 그림은 polynomial regression을 설명변수로 logistic regression을 한 결과입니다. 그 결과 age에 따라서 신뢰구간의 길이가 상당히 많이 차이가 나는 것을 확인하실 수 있습니다. 그 이유는 표본은 크지만 고소득자의 비율(3000명 중 79명)이 매우 작기 때문입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;72-step-function&quot;&gt;7.2. Step Function&lt;/h2&gt;
&lt;p&gt;Step function은 비선형성을 보이는 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;의 범위를 여러 개의 bin으로 분할합니다. 즉, 연속적인 변수를 &lt;strong&gt;ordered categorical variable&lt;/strong&gt; 로 바꾸는 것입니다. 순서가 있는 범주형 변수로 말이죠. 즉, bin의 갯수가 &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;개라면 이를 활용하여 &lt;script type=&quot;math/tex&quot;&gt;K+1&lt;/script&gt;개의 새로운 변수를 만듭니다. 그 결과 다음과 같은 회귀식이 생성됩니다.
&lt;script type=&quot;math/tex&quot;&gt;y_i=\beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i)+...+\beta_KC_K(x_i)+\epsilon_i&lt;/script&gt;
물론 이렇게 하면 주어진 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;에 대해 위의 식에서 기껏해야 1개가 0이 아닌 값이 나옵니다. 그렇다면, 설명변수의 bin들을 나누는 그 기준은 어떻게 설정해야 할까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-02-14/2.png&quot; alt=&quot;2&quot; /&gt;
위에서 왼쪽 그림은 그 bin에 따라 age가 증가함을 보여주지 못합니다. 근데 또 epidemiology 분야에서 자주 사용된다고 하네요. 예를 들어 단순 나이를 연속형을 넣지 않고, 5년 단위로 끊어서 그룹화하는 형식이 있다고 합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;예상되는 단점 : 연속적이지 않아 그 경계를 반영 못함
장점 : segmentation하는 효과가 있을듯&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;73-basis-function&quot;&gt;7.3. Basis Function&lt;/h2&gt;
&lt;p&gt;결국 다항식 모델은 어떤 &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;에 대하여 이를 변수 변환한 특별한 케이스라고 볼 수 있습니다. 이러한 형식으로 다양한 base function들을 활용하여 변환의 family를 가질 수 있습니다.
&lt;script type=&quot;math/tex&quot;&gt;y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+...+\beta_Kb_1(x_K)+\epsilon_i&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;이는 지금까지 소개했던 모델들에 적용할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;polynomial regression : &lt;script type=&quot;math/tex&quot;&gt;b_j(x_i)=x_i^j&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;step function : &lt;script type=&quot;math/tex&quot;&gt;b_j(x_i)=I(c_j\leq x_i \leq c_{j+1})&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다른 방법들도 있습니다. Wavelets 또는 푸리에 급수 등을 예로 들 수 있는데요, 이제는 이런 base function으로 자주 사용되는 regression splines에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;질문 : base function 모아보기 + 언제 어떤 걸 쓸지&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;74-regression-splines&quot;&gt;7.4. Regression Splines&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/2018-02-14/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;741-piecewise-polynomial-regression&quot;&gt;7.4.1. Piecewise Polynomial Regression&lt;/h3&gt;
&lt;p&gt;위에서 소개했던 polynomial regression을 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;의 범위를 구분하여 각 범위에 다 다른 저차원 다항식을 적합하는 형식입니다. 그 결과 다음과 같은 형태를 가지게 됩니다.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
y_i = \beta_{01}+\beta_{11}x_i+\beta_{21}x_i^2+\beta_{31}x_i^3 (x_i&lt;c) %]]&gt;&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;y_i = \beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x_i^3 (x_i\geq c)&lt;/script&gt;
위의 그림에서 왼쪽 위와 같은 형식입니다.&lt;/p&gt;

&lt;h3 id=&quot;742-constraints-and-splines&quot;&gt;7.4.2. Constraints and Splines&lt;/h3&gt;
&lt;p&gt;Piecewise polynomial regression은 적합곡선이 너무 유연하다는 단점이 있습니다. 이를 해결하기 위해 2가지의 제한조건을 추가하였는데 그는 다음과 같습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;범위의 경계에서 smooth해야 합니다.&lt;/li&gt;
  &lt;li&gt;범위의 경계에서 연결되어야 합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;질문 : 그래서 뭐가 좋은거야..?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;75-smoothing-splines&quot;&gt;7.5. Smoothing Splines&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;#74-regression-splines&quot;&gt;7.4절&lt;/a&gt;에서 regression splines를 다룰 때, 매듭(knot)을 지정하고 base function을 도출한 다음 최소제곱을 사용하여 spline 계수를 추정하였습니다. 이 때&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;76-local-regression&quot;&gt;7.6. Local Regression&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Local regression&lt;/strong&gt; 은 유연한 비선형함수들을 적합하는 다른 기법으로, 목표점 &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;에서 그 주변의 train data들만 사용한다는 특징이 있습니다. 이는 시간과 같이 국소적으로 변하는 변수(책에서는 varying coefficient model)들에 적용하면 유용합니다. 이런 국소적 방법과 유사한 것이 앞에서 배웠던 k-Nearest-Neighbors 입니다. 고차원이면 안좋대요~&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;77-generalized-additive-models&quot;&gt;7.7. Generalized Additive Models&lt;/h2&gt;
&lt;p&gt;앞서 배웠던 모든 모델들은 모두 하나의 설명변수 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;를 유연하게 바꿔서 &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;를 예측하는 모델들이었습니다. 단순히 변수 변환이라고 생각할 수 있죠. 이 장에서는 여러 개의 설명변수들 &lt;script type=&quot;math/tex&quot;&gt;X_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;X_2&lt;/script&gt;, …, &lt;script type=&quot;math/tex&quot;&gt;X_p&lt;/script&gt;를 기반으로 &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;를 예측하는 문제를 소개합니다. 결국 단순선형회귀 &lt;script type=&quot;math/tex&quot;&gt;\rightarrow&lt;/script&gt; 다중선형회귀와 비슷한 전개라고 생각하시면 됩니다. &lt;strong&gt;Generalized Additive Models&lt;/strong&gt; (이하 GAM)은 additivity를 유지하면서 각 변수의 비선형함수를 허용하여 선형 모델들을 확장합니다.&lt;/p&gt;

&lt;h3 id=&quot;771-gams-in-regression&quot;&gt;7.7.1. GAMs in Regression&lt;/h3&gt;
&lt;p&gt;식으로 보면 금방 이해되실 겁니다. 아래의 다중선형모델을&lt;/p&gt;
&lt;h3 id=&quot;772-gams-in-classification&quot;&gt;7.7.2. GAMs in Classification&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;GAMs의 장점과 단점&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;GAMs는 &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt; 각각에 비선형 함수 &lt;script type=&quot;math/tex&quot;&gt;f_j&lt;/script&gt;를 적합할 수 있어 비선형 관계를 자동으로 모델링할 수 있습니다. 각 변수에 대해 다 할 필요가 없다는 거죠.&lt;/li&gt;
    &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;를 더 정확하게 예측할 가능성이 있다고 합니다.&lt;/li&gt;
    &lt;li&gt;이 모델은 additivity를 만족하기 때문에 &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;에 대한 &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt; 각각의 영향&lt;/strong&gt; 을 다른 변수들은 모두 고정하고서 개별적으로 확인할 수 있습니다. 즉, 추론에 적용하기 매우 좋습니다.&lt;/li&gt;
    &lt;li&gt;변수 &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt;에 대한 함수 &lt;script type=&quot;math/tex&quot;&gt;f_j&lt;/script&gt;의 smoothness를 자유도로 요약할 수 있습니다.&lt;/li&gt;
    &lt;li&gt;한계는 모델의 additivity입니다. 많은 변수들이 있을 경우 interaction effect를 놓칠 수 있습니다. 하지만, 수동으로 interaction effect를 후에 넣음으로서 이를 완화시킬 수 있습니다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
</description>
        <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/book%20review/isl/ISL-Ch07/</link>
        <guid isPermaLink="true">https://angrypark.github.io/book%20review/isl/ISL-Ch07/</guid>
        
        <category>ISL</category>
        
        <category>statistical machine learning</category>
        
        
        <category>Book Review</category>
        
        <category>ISL</category>
        
      </item>
    
      <item>
        <title>Case Studies of Classical CNN Networks</title>
        <description>&lt;p&gt;저희는 Andrew Ng님의 Cousera 강의 중 &lt;a href=&quot;https://www.coursera.org/learn/convolutional-neural-networks/home/welcome&quot;&gt;Convolutional Nerual Networks&lt;/a&gt;를 수강하고 이를 블로그 형태로 저장하고 있습니다. 오늘은 Week 2의 내용을 정리하였는데요, week 2에서는 기존 잘 알려진 대표적인 CNN Network 들 중 LeNet-5, AlexNet, VGG, GoogLeNet, ResNet 등을 살펴본 뒤에, ResNet의 중요한 원리인 residual networks와 1x1 convolutions, 그리고 inception networks에 대해 알아보고 있습니다. 이게 무슨 소리냐 하시는 분들은 쉽게 생각하면, CNN은 그 구조에 따라 무한히 많은 형태를 가질 수 있는데, 여러 연구자들이 특정 task에 잘 작동하는 네트워크를 찾아내기 시작했습니다. 흔히 아는 &lt;a href=&quot;www.image-net.org&quot;&gt;ImageNet Challenge&lt;/a&gt;등에서 말이죠. 여기서 우수한 성적을 받았던 네트워크들이 바로 지금 배울 네트워크 들입니다.&lt;/p&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#classic-networks&quot;&gt;Classic Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#network-in-network-1x1-convolutions&quot;&gt;Network in Network, 1x1 convolutions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#inception-network-googlenet&quot;&gt;Inception Network, GoogLeNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#residual-networks-resnet&quot;&gt;Residual Networks, ResNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;classic-networks&quot;&gt;Classic Networks&lt;/h2&gt;

&lt;h4 id=&quot;lenet-5&quot;&gt;LeNet-5&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/1.PNG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;거의 CNN의 창조와 같이 나온 네트워크라고 볼 수 있습니다. input size는 32x32x1이고 &lt;strong&gt;average pooling&lt;/strong&gt; 과 마지막에 2개의 &lt;strong&gt;FC&lt;/strong&gt;, 그리고 activation fuction으로 softmax function을 사용합니다. 기존에 있었던 단순 machine learning algorithm이나 FCNN보다 훨씬 좋은 성능을 보이게 됩니다. 학습해야할 총 파라미터의 수는 60k개입니다.&lt;/p&gt;

&lt;h4 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/2.PNG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ImageNet 영상 데이터를 기반으로 2012년 &lt;strong&gt;ILSVRC&lt;/strong&gt; (ImageNet Large Scale Visual Recognition Challenge)를 우승한 CNN Architecture입니다. 전체적으로 5개의 convolutional layers와 3개의 fully-connected layers로 구성되어 있으며 마지막은 마찬가지로 softmax 함수를 사용합니다. 227x227x3의 input size를 지원하고 1000개의 output이 가능합니다.(32x32x1을 지원했던 LeNet에 비해 더 고화질, 그리고 &lt;em&gt;컬러&lt;/em&gt; 를 지원합니다.) 중간중간에는 average pooling을 사용했던 LeNet-5와 달리 max-pooling을 사용합니다. 파라미터 수는 60m개로 훨씬 더 많아졌기 때문에, 학습을 위해 2개의 GPU를 사용하는 구조라고 합니다.&lt;/p&gt;

&lt;p&gt;성능향상을 위해 AlexNet에서는 ReLU, Overlapped pooling, response normalization, dropout, GPU를 사용하였습니다. - ReLU : 그 전까지는 대부분 sigmoid 함수를 활성화 함수로 사용하였지만, &lt;a href=&quot;http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf&quot;&gt;Deep Sparse Rectifier Neural Network&lt;/a&gt;에 따르면 신경망의 크기가 커질수록 학습속도에 치명적인 영향을 준다고 합니다. 이 때 ReLU를 사용함으로서, sigmoid나 tanh 함수에 비해 학습속도가 6배 정도 빨라지는 효과를 볼 수 있었다고 합니다. 추가로 vanishing gradient 문제를 해결하기도 했지만, 이 부분은 끝도 없으니깐 담에 지중이나 병규가 정리해줄껍니다. - Multiple GPU : 아예 네트워크 자체를 그 때 당시 나왔던 GPU의 사양(GTX 580, 3GB)에 맞추어 짰습니다. - Local Response Normalization : 강의에서는 지금 사용안하니깐 알 필요 없다고는 했지만, 간단히 살펴보면 genera - dropout : training 과정에서 random하게 특정 유닛들을 제외하여 overfitting을 개선하는 regularization 기법입니다.&lt;/p&gt;

&lt;h4 id=&quot;vgg-16&quot;&gt;VGG-16&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/3.PNG&quot; alt=&quot;img&quot; /&gt;
VGG는 2014년 ILSVRC에서 GoogLeNet과 함께 높은 성능을 보여준 네트워크입니다. 제일 다른 것은 convolution filter와 s가 그 때 그 때 달랐던 이전 구조들과 달리, 3x3 filter와 s=1인 동일한 구조를 반복하며 쌓는다는 것입니다. VGGNet은 AlexNet과 마찬가지로 224x224의 이미지를 입력으로 받습니다. 단, 엄청 깊습니다. 약 138m개의 파라미터 수를 최적화해야 합니다. 이전에 사용하던 LRN은 성능 향상에 비해 너무 컴퓨팅 시간이 오래걸려서 사용 안했다고 합니다. 그렇다면, 이전의 다양한 filter를 사용한 것과 달리 3x3 filter를 반복하여 쌓으면 어떤 장점이 있을까요? 1. 여러 개의 ReLU non-linear를 사용할 수 있기 때문에, 더 discriminative(flexible)하다고 할 수 있습니다. 2. 학습해야 할 weight의 수가 많이 줄어듭니다. 하지만 VGG는 워낙 깊어서 결국 파라미터 수는 훨씬 증가하긴 합니다. 7x7 filter를 3x3 filter 3개로 대체할 수 있는데, 이때 parameter 수는 &lt;script type=&quot;math/tex&quot;&gt;7^2&lt;/script&gt;(7x7 filter), &lt;script type=&quot;math/tex&quot;&gt;3^3&lt;/script&gt;(triple 3x3 filter)로 줄어들게 됩니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;network-in-network-1x1-convolutions&quot;&gt;Network in Network, 1x1 convolutions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/4.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN의 가장 큰 특징은 &lt;strong&gt;filter&lt;/strong&gt; 라고 볼 수 있는데, 일반적으로 convolution layer와 pooling layer를 거치면서 진행됩니다. 하지만 이 때마저도 단순한 filter가 아닌 작은 network를 사용하자는 컨셉이 바로 Network In Network입니다. 2014년에 ILSVRC에서 이 논문을 발표하면서 자세히 설명한 것이 바로 1x1 convolutions인데요, 이게 저는 개인적으로 이해가 잘 안되서 여기저기 찾아봤는데, GoogLeNet에서는 차원을 축소하기 위해 다음과 같이 사용했다라는 정도까지밖에 이해가 안되더라구요. 물론 차원이 축소되면 연산량도 줄어들고 망은 더 깊어지는 장점이 있지만요.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;inception-network-googlenet&quot;&gt;Inception Network, GoogLeNet&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/6.PNG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/7.PNG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 NIN(Network in Network)과 1x1 convolutions를 활용하여 만든 것이 바로 GoogLeNet입니다. 2014년 ILSVRC에서 VGGNet을 근소한 차이로 이기고 우승하였고, 둘다 망이 엄청 깊어졌지만 GoogLeNet은 Inception networks를 활용하며 이 문제를 해결합니다. 즉, 앞서 설명했었던 1x1 convolution을 적절히 사용하여 차원을 줄임으로서 망이 깊어졌을 때 연산량이 늘어나는 문제를 해결하였습니다. 그림 6 Inception Network는 영화 인셉션에서 유래된 말이 &lt;em&gt;맞습니다&lt;/em&gt; . 다양한 feature를 추출하기 위해 여러 개의 convolution을 병렬적으로 활용했습니다. Convolution kernel과 feature map, 그리고 1x1 convolution을 활용하여 하나의 모듈을 만듬으로서 NIN 구조를 가질 수 있게 된 것입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;residual-networks-resnet&quot;&gt;Residual Networks, ResNet&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/5.PNG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet은 residual block을 활용한 residual network입니다. 2015년 ILSVRC에서 우승하였고, “망이 깊어지는 것”에 대한 두려움으로 시작했다고 합니다. &lt;script type=&quot;math/tex&quot;&gt;H(x)&lt;/script&gt; 가 아닌 &lt;script type=&quot;math/tex&quot;&gt;H(x)-x&lt;/script&gt;를 학습시킴으로서 한칸씩 뛰며 지름길을 만드는 형식인거죠. 관점을 잠깐 바꾼 것이지만 입력의 작은 움직임을 쉽게 검출할 수 있게 되면서 나머지(residual)을 학습한다는 관점에서 residual block이 됩니다. 이렇게 하면 깊은 망도 쉽게 최적화가 가능하고, 늘어난 깊이로 인해 정확도를 개선할 수 있습니다. 맞아요 엄청어려워요..&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://nmhkahn.github.io/Casestudy-CNN&quot;&gt;NamHyuk Ahn blog&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
        <link>https://angrypark.github.io/computer%20vision/Week-2/</link>
        <guid isPermaLink="true">https://angrypark.github.io/computer%20vision/Week-2/</guid>
        
        <category>CNN</category>
        
        <category>case study</category>
        
        
        <category>Computer Vision</category>
        
      </item>
    
      <item>
        <title>Linear Regression 이론 정리</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/2018-01-20/islr.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;스터디를-시작하며&quot;&gt;스터디를 시작하며&lt;/h2&gt;

&lt;p&gt;YBIGTA에서는 교육세션과 프로젝트를 통해 다양한 머신러닝 및 딥러닝 기법들을 배우고 이들을 활용합니다. 그러나 워낙 다양한 모델들을 압축적으로 배우다 보니, 통계적으로 차근차근 살펴보는 것이 좀 부족합니다. 이에 사이언스 팀에서는 이번 겨울방학동안, 그동안 배웠던 대표적인 모델들에 대해 통계적으로 더 심도있게 공부해보고자 ISL Study를 시작하였습니다.&lt;/p&gt;

&lt;p&gt;저희가 스터디에서 다루는 내용은 모두 &lt;em&gt;Introduction to Statistical Learning&lt;/em&gt; (이하 ISL)을 기반으로 만들어졌습니다. 이 책은 전공자들의 입문서인 &lt;em&gt;Elements of Statistical Learning&lt;/em&gt; 이 엄청난 인기를 끌게 되면서 이를 더 쉽게 풀어쓴 책입니다. 이를 블로그 형식으로 요약하면서, 모르는 내용은 서로 물어보고 같이 답을 찾아나가는 형식으로 구성하였습니다.&lt;/p&gt;

&lt;p&gt;이 책의 구성을 살펴보면 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2과 : Statistical Learning에 대한 개괄과 기본 개념들을 설명합니다. KNN도 간략하게 설명합니다.&lt;/li&gt;
  &lt;li&gt;3,4과 : Classical Linear Model에 대해 다룹니다. 구체적으로는 3과에서 Linear Regression을, 4과에서는 Logistic Regression을 다룰 것입니다. (logistic의 경우 generalized linear model에 속합니다. 쉽게 말해 로짓(log odds)과 linear 관계가 있습니다.)&lt;/li&gt;
  &lt;li&gt;5과 : 모델의 성능을 측정하는 방법, cross-validation이나 bootstrap등에 대해 배웁니다. YBIGTA 교육세션에서 배웠던 것과 비슷하다.&lt;/li&gt;
  &lt;li&gt;6과 : Linear Method에 대해 좀더 배우게 됩니다. model selection, ridge, lasso등에 대해 다룹니다. (드디어!)&lt;/li&gt;
  &lt;li&gt;7과 : non-linear method에 대해 다룹니다. 난항이 예상됩니다&lt;/li&gt;
  &lt;li&gt;8과 : 인기만점 tree-based model에 대해 다룹니다. bagging, boosting, RF등을 다룹니다.&lt;/li&gt;
  &lt;li&gt;9과: 들어는 봤으나 설명하진 못하는, SVM(support vector machine)에 대해 다룹니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3장-linear-regression&quot;&gt;3장. Linear Regression&lt;/h2&gt;

&lt;p&gt;이 장에서는 Supervised Learning의 가장 기본적인 방법인 Linear Regression을 다룹니다. 많은 머신러닝 기법들이 이를 기반으로 만들어졌으므로, 한번 더 살펴보고 가는 것도 의미있을 것 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#31-simple-linear-regression&quot;&gt;3.1. Simple Linear Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#33-회귀모델에서-다른-고려할-사항&quot;&gt;3.3. 회귀모델에서 다른 고려할 사항&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#35-선형회귀-vs-k-nn&quot;&gt;3.5. 선형회귀 vs K-NN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;31-simple-linear-regression&quot;&gt;3.1. Simple Linear Regression&lt;/h2&gt;

&lt;p&gt;단순선형회귀는 &lt;strong&gt;하나의 설명변수 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;&lt;/strong&gt; 에 기초하여 &lt;strong&gt;양적 반응변수 &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;&lt;/strong&gt; 를 예측합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y \approx \beta_0+\beta_1X&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;질문 : 선형성의 가정을 판단하는 방법?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다들 아시다시피, 선형성을 가진다고 가정한다면 &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;을 추정하는 과정에서 &lt;script type=&quot;math/tex&quot;&gt;MSE&lt;/script&gt;를 최소화하는 방향으로 &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;을 구하게 되고, 수식을 정리하면 결론은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat \beta_1 = \frac{\sum_{i=1}^{n}(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^{n}(x_i-\bar x)^2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat \beta_0 = \bar y-\hat \beta_1\bar x&lt;/script&gt;

&lt;p&gt;이 때, &lt;script type=&quot;math/tex&quot;&gt;\bar x&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\bar y&lt;/script&gt;는 sample data의 평균입니다.&lt;/p&gt;

&lt;h3 id=&quot;계수-추정값의-정확도-평가&quot;&gt;계수 추정값의 정확도 평가&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;의 실제 상관관계는 어떤 알려지지 않은 함수 &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;에 대해 &lt;script type=&quot;math/tex&quot;&gt;Y=f(X)+\epsilon&lt;/script&gt;의 형태를 가지며, 이때 &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;은 평균이 0인 랜덤오차항입니다. 실제 응용에서는 관측 자료를 사용하여 최소제곱선을 계산할 수 있지만, 모회귀선은 관측되지 않습니다. 보통 우리는 주어진 데이터로 모집단의 parameter를 추정하게 되는데요, sample이 작을 경우, &lt;script type=&quot;math/tex&quot;&gt;\hat \mu&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;를 과소추정할 수도, 과대추정할 수도 있지만, 이는 단순히 sample의 크기가 커지면 해결됩니다. - 중요한 가정. 각 관측치에 대한 오차 &lt;script type=&quot;math/tex&quot;&gt;\epsilon_i&lt;/script&gt;가 공통의 분산 &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;과 uncorrelated되었다는 가정이 필요합니다. 이를 어떻게 확인할까?&lt;/p&gt;

&lt;p&gt;보통 계수 추정값은 &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt;든 &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;이든 &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;-test를 계산하여 &lt;script type=&quot;math/tex&quot;&gt;\beta=0&lt;/script&gt;이라는 가설을 기각할지 채택할 지를 결정합니다. 이는 통계학입문 내용이므로 패스~&lt;/p&gt;

&lt;h3 id=&quot;모델의-정확도-평가&quot;&gt;모델의 정확도 평가&lt;/h3&gt;

&lt;p&gt;가정에 따라 계수 추정값의 정확도를 평가하여, &lt;strong&gt;모든 추청값이 유의미하다&lt;/strong&gt; 라는 결론이 나왔다면, 이를 토대로 모델이 데이터에 얼마나 적합한지를 수량화할 수 있습니다. 보통 &lt;strong&gt;RSE&lt;/strong&gt; 와 &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;&lt;/strong&gt; 를 활용하여 평가합니다. 먼저 RSE는 데이터에 대한 모데의 적합성결여를 나타냅니다. 하지만, 단위에 따라 그 크기를 객관적으로 평가하기 힘들다는 단점이 있죠. 이를 해결한 것이 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;인데, 그 식은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R^2=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;가 클수록, 모델이 데이터를 잘 설명해준다고 볼 수 있습니다. 하지만 상황에 따라 좋은 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;가 무엇인지는 달라집니다. 어떤 문제에서는 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;가 0.9가 넘어도 문제가 있을 수도 있고, 어떤 상황에서는 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;가 0.1 미만이어도 좋은 분석이 될 수 있습니다. - 상황에 따라 좋은 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;가 무엇인지를 선정하는 기준이 있을까?&lt;/p&gt;

&lt;h2 id=&quot;32-multiple-linear-regression&quot;&gt;3.2. Multiple Linear Regression&lt;/h2&gt;

&lt;p&gt;여러 개의 설명변수를 통해 반응변수를 설명하는 다중선형회귀를 이제 살펴보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;모델이-성립하는가&quot;&gt;모델이 성립하는가?&lt;/h3&gt;

&lt;p&gt;3.1에서와 유사한 방식으로 &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;-statistic을 확인하여, 일단 모든 변수들이 다 무의미한지를 확인합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}&lt;/script&gt;

&lt;p&gt;해당 반응변수가 설명변수들과 모두 전혀! 관계가 없다면, &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;-statistic은 1에 가까운 값이 나오게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;모델이-성립한다면-내가-선택할-변수들은&quot;&gt;모델이 성립한다면, 내가 선택할 변수들은?&lt;/h3&gt;

&lt;p&gt;모든 설명변수가 반응변수와 상관성이 있을수도 있지만, 중요한 변수들만을 찾아내는 것 또한 중요합니다. 보통 변수를 선택하는 과정은 다음의 3가지 고전적인(classic) 기법들이 있습니다. - Foward Selection - Backward Selection - Stepwise Selection&lt;/p&gt;

&lt;h3 id=&quot;모델의-적합성&quot;&gt;모델의 적합성&lt;/h3&gt;

&lt;p&gt;모델이 성립하는지 확인하고, 어떤 변수들을 선택할지 결정했다면, 이 모델이 얼마나 적합한지를 확인하면 됩니다. 이는 앞과 마찬가지로 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;RSE&lt;/script&gt;로 판단합니다.&lt;/p&gt;

&lt;h2 id=&quot;33-회귀모델에서-다른-고려할-사항&quot;&gt;3.3. 회귀모델에서 다른 고려할 사항&lt;/h2&gt;
&lt;h3 id=&quot;quantitive-vs-qualitative&quot;&gt;Quantitive vs Qualitative&lt;/h3&gt;

&lt;p&gt;양적 변수가 아니라, 질적 변수일 경우, 우리는 보통 dummy화를 통해 이를 해결했습니다. 이 책에서 확인한 결과, 자동적으로 변수를 하나씩 제거한 효과가 나긴 하더라구요. 실제 분석 과정에서도 이렇게 하면 될 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;2가지-가정&quot;&gt;2가지 가정&lt;/h3&gt;

&lt;p&gt;선형회귀의 가장 기본적인 가정 2가지는 바로 1. 설명변수와 반응변수는 additive하다.(설명변수들끼리 독립이다.) 2. 설명변수와 반응변수는 linear하다.&lt;/p&gt;

&lt;p&gt;입니다. 물론, 이 가정들을 다 만족하면 좋겠지만, 이를 확인하기 위해서 다음과 같은 방식으로 분석을 진행합니다. 만약, &lt;script type=&quot;math/tex&quot;&gt;X_1&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;X_2&lt;/script&gt;가 상호작용이 존재한다면(이를 시너지 효과라고도 부른답니다.) 단순히 상호작용 항(&lt;script type=&quot;math/tex&quot;&gt;X_1X_2&lt;/script&gt;)을 넣는 것만으로도 additive 가정을 좀더 충족시킬 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_1X_2+\epsilon&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y=\beta_0+(\beta_1+\beta_3X_2)X_1+\beta_2X_2+\epsilon&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;= \beta_0+\tilde \beta_1X_1+\beta_2X_2+\epsilon&lt;/script&gt; 계층적 원리에 의하면, 모델에 상호작용을 포함한다면, 그 계수와 연관된 변수들의 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;값이 유의하지 않더라도 모델에 포함시켜야 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;상호작용 vs 상관관계&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;비선형-상관관계&quot;&gt;비선형 상관관계&lt;/h3&gt;

&lt;p&gt;비선형 상관관계를 해결하는 방법들은 여러가지 있지만, 이 장에서 소개하는 가장 간단한 방법은 단순히 다항식을 다 선형회귀 때리는 것입니다 ㅋㅋ&lt;/p&gt;

&lt;h3 id=&quot;데이터의-비선형성&quot;&gt;데이터의 비선형성&lt;/h3&gt;

&lt;p&gt;잔차 그래프를 통해 비선형성을 식별합니다. 만약 잔차 그래프가 비선형 상관성이 있다는 것을 나타낸다면 &lt;strong&gt;설명변수&lt;/strong&gt; 를 비선형적으로 변환하여 회귀모델에 적용하는 것이 간단한 접근법이라고 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;오차항의-상관성이-없다라는-가정&quot;&gt;오차항의 상관성이 없다라는 가정&lt;/h3&gt;

&lt;p&gt;선형회귀에서의 중요한 가정은 오차항들이 서로 상관되어 있지 않다는 것입니다. 잔차에서 강한 패턴이 드러난다면 데이터가 비선형적이라는 것을 나타냅니다. 오차항들의 패턴은 주로 시계열에서 일어난다고 하는데요, 이를 줄이기 위한 실험계획이 필요하다고 하는데, 어떻게 줄이는 지는 잘 안나와있네용 ㅠ&lt;/p&gt;

&lt;h3 id=&quot;오차항의-분산이-상수라는-가정&quot;&gt;오차항의 분산이 상수라는 가정&lt;/h3&gt;

&lt;p&gt;오차항들끼리 분산이 일정한 지는, 잔차 그래프의 형태로 확인할 수 있습니다. 잠시 뒤에 그래프로 보실 텐데, 잔차 그래프가 오른쪽처럼 크게 바뀌지 않으면 상수 분산을 가진다고 볼 수 있지만, 왼쪽처럼 패턴이 보인다면 &lt;strong&gt;이분산성이 있다&lt;/strong&gt; 라고 볼 수 있습니다. 이를 해결하려면 뒤에서도 나오지만 반응변수의 형태를 바꿔주면 될 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;outlier-vs-leverage&quot;&gt;Outlier vs Leverage&lt;/h3&gt;

&lt;h3 id=&quot;collinearity&quot;&gt;Collinearity&lt;/h3&gt;

&lt;p&gt;두 개 이상의 설명변수들이 서로 밀접한 상관이 있는 경우를 말합니다. Collinearity를 검출하는 가장 간단한 방법은 설명변수들의 상관행렬을 살펴보는 것입니다. 하지만 이로 완벽하게 볼 수 없으므로, VIF(variance inflation factor)를 계산하여 확인한다고 합니다. Collinearity를 발견하였을 경우, 이를 해결하는 방법에는 크게 2가지 방법들이 있습니다. 회귀에서 문제가 있는 변수들 중 하나를 해결하거나 문제가 있는 변수들을 합쳐서 하나로 만드는 방법이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;35-선형회귀-vs-k-nn&quot;&gt;3.5. 선형회귀 vs K-NN&lt;/h2&gt;

&lt;p&gt;다음의 그래프에 따르면, KNN은 상관관계가 선형적일 때는 선형회귀보다 약간 나쁘고 만약 비선형인 관계에서는 훨씬 좋다고 합니다. 즉, 관계가 선형적인지를 먼저 판단하고 만약 선형이라면 선형회귀를, 비선형이면 KNN을 하면 됩니다. 차원에 따라서 또 얘기는 달라지는데요, 설명변수의 수가 2개 이하면 KNN이 훨씬 좋고, 4개 이상이면 선형회귀가 더 좋다고 합니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;질문들-모음&quot;&gt;질문들 모음&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q1. RSE, MSE, &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;의 차이는?&lt;/strong&gt;
답변 :&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q2. 상황에 따른 적절한 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;를 구하기 위한 가이드라인이 존재하는가?&lt;/strong&gt;
답변 :&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q3. VIF와 correlation의 차이는?&lt;/strong&gt;
답변 :&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q4. 다중공산성의 정의와 해결방법에 대한 말끔한 정리가 필요합니다.&lt;/strong&gt;
답변 :&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Q5. 고차원에서의 Leverage 판단방법?&lt;/strong&gt;
답변 :&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 17 Jan 2018 18:47:00 +0000</pubDate>
        <link>https://angrypark.github.io/book%20review/isl/ISL-study/</link>
        <guid isPermaLink="true">https://angrypark.github.io/book%20review/isl/ISL-study/</guid>
        
        <category>ISL</category>
        
        <category>regression</category>
        
        <category>statistical machine learning</category>
        
        
        <category>Book Review</category>
        
        <category>ISL</category>
        
      </item>
    
      <item>
        <title>DiscoGAN Paper Review</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/2017-09-09-DiscoGAN-paper-reading/background.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 시간에는 GAN의 응용 논문 중 우리나라 SK T-Brain에서 발표되서 인정받고 있는 Learning to Discover Cross-Domain Relations with Generative Adversarial Networks(이하 Disco-GAN)을 소개하겠습니다. GAN팀에서는 다음과 같은 순서를 GAN을 공부하고 있으며, 클릭하면 내용을 확인하실 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Study 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kangbk0120.github.io/articles/2017-07/first-gan&quot;&gt;GAN 논문 이해하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://angrypark.github.io/generative%20models/GAN-tutorial-1/&quot;&gt;GAN Code 분석하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kangbk0120.github.io/articles/2017-08/conditional-gan&quot;&gt;Conditional-GAN 논문 이해하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kangbk0120.github.io/articles/2017-08/condgan-imple&quot;&gt;Conditional-GAN으로 내 핸드폰 손글씨 만들기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://angrypark.github.io/generative%20models/paper%20review/DCGAN-paper-reading/&quot;&gt;DCGAN 논문 이해하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kangbk0120.github.io/articles/2017-08/dcgan-pytorch&quot;&gt;DCGAN pytorch로 구현하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kangbk0120.github.io/articles/2017-08/info-gan&quot;&gt;InfoGAN 논문 이해하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/YBIGTA/Deep_learning/blob/master/GAN/2017-09-02-InfoGAN-implementation.ipynb&quot;&gt;InfoGAN Code 분석하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;discogan-논문-이해하기&quot;&gt;&lt;a href=&quot;https://angrypark.github.io/generative%20models/DiscoGAN-paper-reading/&quot;&gt;DiscoGAN 논문 이해하기&lt;/a&gt;&lt;/h2&gt;
    &lt;p&gt;&lt;strong&gt;목차&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#0-abstract&quot;&gt;0. Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-introduction&quot;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-model&quot;&gt;2. Model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-formulation&quot;&gt;2.1. Formulation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-notation-and-architecture&quot;&gt;2.2. Notation and Architecture&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-gan-with-a-reconstruction-loss&quot;&gt;2.3. GAN with a Reconstruction Loss&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#24-our-proposed-model-discovery-gan&quot;&gt;2.4. Our Proposed Model : Discovery GAN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;blockquote&gt;
  &lt;p&gt;논문 pdf :
&lt;a href=&quot;https://arxiv.org/pdf/1703.05192.pdf&quot;&gt;Learning to Discover Cross-Domain Relations
with Generative Adversarial Networks, 2016&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;0-abstract&quot;&gt;0. Abstract&lt;/h2&gt;
&lt;p&gt;지금까지는 GAN을 통해 사람이 구별하기 어려운 가짜 이미지를 만들어내는 데 목표를 두었고 어느정도 성공하였습니다. 그러나 우리가 원하는 목적의 가짜 이미지를 만들기 위해서는 그 이상의 목표가 해결되어야 합니다. 예를 들면, 소 그림을 넣으면 비슷한 느낌의 말을 만들어준다던가, 여러분의 사진을 넣으면 같은 사진인데 성별만 바뀌는 이미지를 만들어준다던가 말이죠. 이를 하나의 도메인에서 mode는 유지한 채로 다른 도메인으로 바꿔주는 목표로 생각할 수 있는데, 오늘 소개할 논문에서는 이를 멋지게 구현해주었습니다. 이번 글에서는 문제를 어떻게 정의내리고, 어떤 구조와 컨셉을 통해 이를 해결하였는지 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;사람들은 두개의 다른 도메인이 주어졌을 때 그 관계를 쉽게 찾아냅니다. 예를 들어, 영어로 구성된 문장을 프랑스어로 번역하여 주어진다면, 그 두 문장의 관계를 사람들은 쉽게 찾아낼 수 있습니다 (의미는 같다, 언어는 다르다). 또한, 우리는 우리가 입고 있는 정장과 비슷한 스타일을 가지고 있는 바지와 신발을 쉽게 찾아낼 수 있습니다. 같은 스타일을 가지되 도메인만 정장에서 바지나 신발로 옮겨주는 것이죠.&lt;/p&gt;

&lt;p&gt;그럼, 과연 기계도 서로 다른 두 개의 도메인 이미지의 관계를 찾아낼 수 있을까요? 이 문제는 &lt;em&gt;“한 이미지를 다른 조건이 달려 있는 이미지로 재생성할 수 있을까?”&lt;/em&gt; 라는 문제로 재정의됩니다. 다시 말하면, 같은 이미지인데 한 도메인에서 다른 도메인으로 mapping해주는 함수를 찾을 수 있는가의 문제인 것이죠. 사실 이 문제는 최근 엄청난 관심을 받고 있는 GAN에서 이미 어느 정도 해결되었습니다. 그러나 GAN의 한계는 사람이나 다른 알고리즘이 직접 명시적으로 짝지은 데이터를 통해서만 문제를 해결할 수 있다는 것입니다. (예, 국방무늬를 갖고 있는 옷을 바꾸면 국방무늬를 가진 신발이 돼!)&lt;/p&gt;

&lt;p&gt;명시적으로 라벨링된 데이터는 쉽게 구해지지 않으며, 많은 노동력과 시간을 필요로 합니다. 더군다나, 짝 중 하나의 도메인에서라도 그 사진이 없는 경우 문제가 생기고, 쉽게 짝짓기 힘들 정도로 훌륭한 선택지가 다수 발생하기도 하죠. 따라서, 이 논문에서는 우리는 2대의 다른 도메인에서 그 어떠한 explicitly pairing 없이 관계를 발견하는 것을 목표로 합니다(관계를 ‘발견’한다고 해서 DiscoGAN입니다.)&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해서, 저자는 &lt;strong&gt;Discover cross-domain relations with GANs&lt;/strong&gt; 을 새롭게 제안하였습니다. 이전에 다른 모델들과 달리, 우리는 그 어떤 라벨도 없는 두 개의 도메인 데이터 셋을 pre-training없이 train합니다.(이하 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;,&lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; 도메인이라 명시할께요). Generator는 도메인 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;의 한 이미지를 input으로 해서 도메인 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;으로 바꿔줍니다. DiscoGAN의 핵심은 두개의 서로 다른 GAN이 짝지어져 있다는 것인데, 각각은 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;로, &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;로 바꿔주는 역할을 해줍니다. 이 때의 핵심적인 전제는 하나의 도메인에 있는 모든 이미지를 다른 도메인의 이미지로 표현할 수 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2017-09-09-DiscoGAN-paper-reading/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결론부터 이야기 하자면 DiscoGAN은 Toy domain 과 real world image dataset에서 다 cross-domain relations를 알아내는 데 적합하다는 것을 확인할 수 있었습니다. 단순한 2차원 도메인에서 얼굴이미지 도메인으로 갔을 때에도 DiscoGAN 모델은 mode collapse problem에 좀더 robust하다는 것도 확인할 수 있었죠. 또한 얼굴, 차, 의자, 모서리, 사진 사이의 쌍방향 mapping에도 좋은 이미지 전환 성능을 보여주었습니다. 전환된 이미지는 머리 색, 성별, orientation 같은 특정한 부분만 바뀔수도 있었습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;2-model&quot;&gt;2. Model&lt;/h2&gt;
&lt;p&gt;우리는 DiscoGAN이 어떤 문제들을 해결할 수 있는지 알아보았습니다. 이제 이 모델이 어떻게 이 문제를 해결하는지 좀 더 자세히 분석해보죠.&lt;/p&gt;

&lt;h3 id=&quot;21-formulation&quot;&gt;2.1. Formulation&lt;/h3&gt;
&lt;p&gt;관계라는 것은 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;로 정의내려질 수 있습니다. 즉 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;라는 것은 도메인 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;에 있는 성분들을 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;로 바꿔주는 것을 의미합니다. 완전 비지도 학습에서는, &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;G_{BA}&lt;/script&gt;는 모두 처음에 정의내릴 수 없습니다. 따라서, 일단 모든 관계는 1대1 대응으로 만들어주고 시작합니다. 그러면 자연스럽게, 각각의 대응은 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;가 되며, &lt;script type=&quot;math/tex&quot;&gt;G_{BA}&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;의 역반응이 됩니다.&lt;/p&gt;

&lt;p&gt;함수 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;의 범위는 도메인 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;에 있는 모든 &lt;script type=&quot;math/tex&quot;&gt;x_A&lt;/script&gt;가 도메인 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;에 있는 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}(x_A)&lt;/script&gt;로 연결된 것입니다.&lt;/p&gt;

&lt;p&gt;자 이를 목적함수로 표현해봅시다. 이상적으로는, 보시는 것처럼 &lt;script type=&quot;math/tex&quot;&gt;G_{BA} \circ G_{AB}(x_A) = x_A&lt;/script&gt;이면 됩니다. 하지만 이런 제한식은 너무 엄격해서 이를 만족시키기 어렵습니다(사실 불가능하죠. generate해서 원래 사진 그대로 나온다는게 ㅎㅎ).  따라서 여기서는 &lt;script type=&quot;math/tex&quot;&gt;d(G_{BA} \circ G_{AB}(x_A), x_A)&lt;/script&gt;를 최소화하려고 합니다. 비슷하게, &lt;script type=&quot;math/tex&quot;&gt;d(G_{AB} \circ G_{BA}(x_B), x_B)&lt;/script&gt;도 최소화해야합니다. 이를 Discriminator와 generative adversarial loss가 들어간 loss로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;22-notation-and-architecture&quot;&gt;2.2. Notation and Architecture&lt;/h3&gt;
&lt;p&gt;각각의 Generator와 Discriminator의 input, output 형태는 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{AB} :  \mathbb{R}_{A}^{64\times64\times3} \rightarrow \mathbb{R}_{B}^{64\times64\times3}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{B} :  \mathbb{R}_{A}^{64\times64\times3} \rightarrow [0,1]&lt;/script&gt;

&lt;p&gt;입니다.&lt;/p&gt;

&lt;h3 id=&quot;23-gan-with-a-reconstruction-loss&quot;&gt;2.3. GAN with a Reconstruction Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/2017-09-09-DiscoGAN-paper-reading/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;처음에는 기존 GAN을 약간 변형한 구조를 생각했었다고 합니다(그림 2-a). 기존 GAN은 input이 gaussian noise였던 것 기억하시나요? 여기서는 일단 input을 도메인 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;의 image로 해줍니다. 이를 기반으로 generator가 fake image를 만들어내고, 이를 Discriminator는 도메인 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;의 이미지와 함께 넣어서 무엇이 진짜인지를 구분하게 합니다. 즉, Generator의 입장에서는 비록 input은 도메인 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;였지만, Discriminator를 속이기 위해서는 도메인 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;와 유사한 이미지를 만들어야 한다는 것이죠. 이렇게만 잘 학습이된다면, Generator는 앞서 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;의 역할을 충실히 할 수 있게 됩니다. 도메인 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;를, 도메인 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;로 바꿔주는 역할을 해주는 것이죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2017-09-09-DiscoGAN-paper-reading/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 이는 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;에서 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;로 가는 mapping만 배우게 됩니다. 동시에 &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;에서 다시 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;로 가는 mapping도 학습하기 위해서 그림 2-b에서처럼 두번째 generator를 추가하게 됩니다. 또한 reconstruction loss도 추가하는데요, 이러한 과정들을 통해, 각각의 generator는 input 도메인에서 output 도메인으로 mapping하는 것은 물론 그 관계까지 discover하게 됩니다. 이 때 각각의 함수를 정의하고 loss function을 정의하면 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{AB} = G_{AB}(x_A)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{ABA} = G_{BA}(x_{AB}) = G_{BA} \circ G_{AB}(x_A)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{CONST_A} = d(G_{BA}\circ G_{AB}(x_A), x_A)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{GAN_A} = - \mathbb{E}_{x_A \sim P_A}[logD_B(G_{AB}(x_A))]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{G_{AB}} = L_{GAN_{B}} + L_{CONST_{A}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{D_B} = - \mathbb{E}_{x_B \sim P_B}[logD_B(D_{B}(x_B))] - \mathbb{E}_{x_A \sim P_A}[log(1 - D_B(G_{AB}(x_A))]&lt;/script&gt;

&lt;h3 id=&quot;24-our-proposed-model--discovery-gan&quot;&gt;2.4. Our Proposed Model : Discovery GAN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/2017-09-09-DiscoGAN-paper-reading/4.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적으로 이 논문에서 구현한 모델은 앞서 언급했던 그림 2-b의 모델 2개를 서로 다른 방향으로 이어주는 것입니다(그림 2-c). 각각의 모델은 하나의 도메인에서 다른 도메인으로 학습하며, 각각은 reconstruction을 통해 그 관계도 학습하게 됩니다.(&lt;script type=&quot;math/tex&quot;&gt;G_{ABA}&lt;/script&gt;의 BA와 &lt;script type=&quot;math/tex&quot;&gt;G_{BAB}&lt;/script&gt;의 BA는 다릅니다.) 이 때 &lt;script type=&quot;math/tex&quot;&gt;G_{AB}&lt;/script&gt;의 두 개의 Generator와 &lt;script type=&quot;math/tex&quot;&gt;G_{BA}&lt;/script&gt;의 두 개의 Generator는 서로 파라미터를 공유합니다. 그리고 &lt;script type=&quot;math/tex&quot;&gt;x_{BA}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;x_{AB}&lt;/script&gt;는 각각 &lt;script type=&quot;math/tex&quot;&gt;L_{D_A}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;L_{D_B}&lt;/script&gt;로 들어가게 됩니다. 이전 모델과 중요한 차이는 두 도메인의 input 이미지가 다 reconstruct되었으며 그에 따라 두 개의 reconstruction loss(&lt;script type=&quot;math/tex&quot;&gt;L_{CONST_A}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;L_{CONST_B}&lt;/script&gt;)가 생성된다는 것입니다.&lt;/p&gt;

&lt;p&gt;이처럼 두 개의 모델을 짝지어줌으로서 전체 Generator의 loss는 다음과 같이 정의합니다.&lt;/p&gt;

&lt;p&gt;\begin{matrix}
L_G &amp;amp;=&amp;amp; L_{G_{AB}} + L_{G_{AB}} &lt;br /&gt;
    &amp;amp;=&amp;amp; L_{GAN_B} + L_{CONST_A} + L_{GAN_A} + L_{CONST_B}
\end{matrix}&lt;/p&gt;

&lt;p&gt;비슷하게 전체 Discriminator의 loss는 다음과 같이 정의합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_D = L_{D_A} + L_{D_B}&lt;/script&gt;

&lt;p&gt;여기까지 DiscoGAN의 성능, 발전 과정, 해결할 수 있는 문제들, 그리고 각각의 구조와 loss function을 알아보았습니다. 다음에는 코드로, 몇몇 실험에 대해 어떻게 문제를 정의 내리고 해결했는지 알아보겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.05192.pdf&quot;&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks, 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dogfoottech.tistory.com/170&quot;&gt;sweetzLab 기술블로그 - DiscoGan (Learning to Discover Cross-Domain Relations with Generative Adversarial Network)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sat, 09 Sep 2017 20:30:00 +0000</pubDate>
        <link>https://angrypark.github.io/generative%20models/paper%20review/DiscoGAN-paper-reading/</link>
        <guid isPermaLink="true">https://angrypark.github.io/generative%20models/paper%20review/DiscoGAN-paper-reading/</guid>
        
        <category>GAN</category>
        
        <category>DiscoGAN</category>
        
        
        <category>Generative Models</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>DCGAN 논문 이해하기</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/background.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;요약&quot;&gt;요약&lt;/h2&gt;

&lt;p&gt;지금까지 가장 기본적인 GAN의 이론적 내용과 그 코드의 작동 방법에 대해 살펴보았다면, 이제는 GAN을 활용한 다양한 논문들로 그 이론을 확장하고자 한다. 셀 수도 없을 만큼 다양한 GAN 응용 논문들이 있지만, 가장 기본적인 응용 GAN 논문으로 시작한다. 바로 2016년 발표된 &lt;strong&gt;Deep Convolutional Generative Adversarial Nets (DCGAN)&lt;/strong&gt; 이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;논문 pdf :
&lt;a href=&quot;https://arxiv.org/abs/1511.06434&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - Alec Radford el ec, 2016&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;목차&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#gan-review&quot;&gt;GAN Review&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#기존-gan의-한계&quot;&gt;기존 GAN의 한계&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dcgan의-목표&quot;&gt;DCGAN의 목표&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#architecture-guidelines&quot;&gt;Architecture Guidelines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#generator-model&quot;&gt;Generator Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#visualization&quot;&gt;Visualization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;gan-review&quot;&gt;GAN Review&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/gan-workflow.jpg&quot; alt=&quot;gan-workflow&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://angrypark.github.io/First-GAN/&quot;&gt;[GAN] First GAN&lt;/a&gt;
&lt;a href=&quot;https://angrypark.github.io/GAN-tutorial-1/&quot;&gt;[GAN] 1D Gaussian Distribution Generation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;기존-gan의-한계&quot;&gt;기존 GAN의 한계&lt;/h2&gt;
&lt;p&gt;기존 GAN의 한계는 다음과 같다.&lt;/p&gt;

&lt;h4 id=&quot;1-gan은-결과가-불안정하다&quot;&gt;1. GAN은 결과가 불안정하다&lt;/h4&gt;
&lt;p&gt;기존 GAN만 가지고는 좋은 성능이 잘 안나왔다.&lt;/p&gt;

&lt;h4 id=&quot;2-black-box-method&quot;&gt;2. Black-box method&lt;/h4&gt;
&lt;p&gt;Neural Network 자체의 한계라고 볼 수 있는데, 결정 변수나 주요 변수를 알 수 있는 다수의 머신러닝 기법들과 달리 Neural Network은 처음부터 끝까지 어떤 형태로 그러한 결과가 나오게 되었는지 그 과정을 알 수 없다.&lt;/p&gt;

&lt;h4 id=&quot;3-generative-model-평가&quot;&gt;3. Generative Model 평가&lt;/h4&gt;
&lt;p&gt;GAN은 결과물 자체가 &lt;strong&gt;새롭게 만들어진 Sample&lt;/strong&gt; 이다. 이를 기존 sample과 비교하여 얼마나 비슷한 지 확인할 수 있는 정량적 척도가 없고, 사람이 판단하더라도 이는 주관적 기준이기 때문에 얼마나 정확한지, 혹은 뛰어난지 판단하기 힘들다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;dcgan의-목표&quot;&gt;DCGAN의 목표&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;1. Generator가 단순 기억으로 generate하지 않는다는 것을 보여줘야 한다.&lt;/strong&gt;
&lt;strong&gt;2. z의 미세한 변동에 따른 generate결과가 연속적으로 부드럽게 이루어져야 한다(이를 walking in the latent space라고 한다).&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;architecture-guidelines&quot;&gt;Architecture Guidelines&lt;/h2&gt;
&lt;p&gt;GAN과 DCGAN의 전체적인 구조는 거의 유사하다. 다만 각각의 Discriminator와 Generator의 세부적인 구조가 달라진다. 논문에서는 이 구조를 개발한 방법을 다음과 같이 소개한다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;After extensive model exploration we identified a family of architectures that resulted stable training across a range of datasets and allowed for training higher resolution and deeper generative models.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;쉽게 말하면 엄청난 노가다(?) 끝에 안정적이고 더 성능이 향상된 결과를 찾게 되었다는 말이다.&lt;/p&gt;

&lt;h4 id=&quot;기존-gan-architecture&quot;&gt;기존 GAN Architecture&lt;/h4&gt;
&lt;p&gt;기존 GAN은 자세히 살펴보면 다음과 같은 아주 간단하게 fully-connected로 연결되어 있다. &lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/gan-architecture.png&quot; alt=&quot;gan-architecture&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cnn-architecture&quot;&gt;CNN Architecture&lt;/h4&gt;
&lt;p&gt;CNN은 이러한 fully-connected 구조 대신에 convolution, pooling, padding을 활용하여 레이어를 구성한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/cnn-architecture.png&quot; alt=&quot;cnn-architecture&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;dcgan-architecture&quot;&gt;DCGAN Architecture&lt;/h4&gt;
&lt;p&gt;DCGAN은 결국, 기존 GAN에 존재했던 fully-connected구조의 대부분을 CNN 구조로 대체한 것인데, 앞서 언급했던 것처럼 엄청난 시도들 끝에 다음과 같이 구조를 결정하게 되었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/architecture-guidelines.png&quot; alt=&quot;architecture-guidelines&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Discriminator에서는 모든 pooling layers를 &lt;strong&gt;strided convolutions&lt;/strong&gt; 로 바꾸고, Generator에서는 pooling layers를 &lt;strong&gt;fractional-strided convolutions&lt;/strong&gt; 으로 바꾼다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generator와 Discriminator에 batch-normalization을 사용한다. 논문에서는 이를 통해 deep generators의 초기 실패를 막는다고 하였다. 그러나 모든 layer에 다 적용하면 sample oscillation과 model instability의 문제가 발생하여 Generator output layer와 Discriminator input layer에는 적용하지 않았다고 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fully-connected hidden layers를 삭제한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generator에서 모든 활성화 함수를 Relu를 쓰되, 마지막 결과에서만 Tanh를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Discriminator에서는 모든 활성화 함수를 LeakyRelu를 쓴다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;- Strided convolutions?&lt;/strong&gt;
&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/padding_strides.gif&quot; alt=&quot;padding-strides&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;- Fractionally-strided convolutions?&lt;/strong&gt;
&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/padding_strides_transposed.gif&quot; alt=&quot;padding-strides-transposed&quot; /&gt;
논문을 읽으며 가장 이해가 안되었던 부분인데, 기존의 convolutions는 필터를 거치며 크기가 작아진 반면에, fractionally-strided convolutions은 input에 padding을 하고 convolution을 하면서 오히려 크기가 더 커지는 특징이 있다. 쉽게 transposed convolution이라고도 불리고, deconvolution이라고도 불리는데, deconvolution는 잘못된 단어라고 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;- Batch-normalization?&lt;/strong&gt;
Batch Normalization은 2015년 arXiv에 발표된 후 ICML 2015에 publish 된 이 논문 (&lt;a href=&quot;http://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariance Shift&lt;/a&gt;) 에 설명되어 있는 기법으로, 발표된 후 최근에는 거의 모든 인공신경망에 쓰이고 있는 기법이다. 기본적으로 Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나이며, 지금까지는 이 문제를 Activation 함수의 변화 (ReLU 등), Careful Initialization, small learning rate 등으로 해결하였지만, 이 논문에서는 이러한 간접적인 방법보다 training 하는 과정 자체를 전체적으로 안정화하여 학습 속도를 가속시킬 수 있는 근본적인 방법을 제안하였다.
더 자세한 내용은 다음 포스트를 참고하길 바란다.
&lt;a href=&quot;https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/&quot;&gt;Batch Normalization 설명 및 구현&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;generator-model&quot;&gt;Generator Model&lt;/h2&gt;
&lt;p&gt;위에서 설명된 Generator의 구조를 시각화하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/generator-model.png&quot; alt=&quot;generator-model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;100 dimensional uniform distribution(Z)이 들어오면 이들이 4개의 fractionally-strided convolution layer을 거치며 크기를 키워서 더 높은 차원의 64x64 pixel 이미지가 된다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;visualization&quot;&gt;Visualization&lt;/h2&gt;
&lt;h4 id=&quot;generated-bedrooms&quot;&gt;Generated bedrooms&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/visualization-1.png&quot; alt=&quot;Visualization&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;walking-in-the-latent-space&quot;&gt;Walking in the latent space&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/visualization-2.png&quot; alt=&quot;Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 DCGAN의 목표들 중 하나인 walking in the latent space를 직접 구현한 그림이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;visualize-filters-no-longer-black-box&quot;&gt;Visualize filters (no longer black-box)&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/visualization-3.png&quot; alt=&quot;Visualization&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;applying-arithmetic-in-the-input-space&quot;&gt;Applying arithmetic in the input space&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017-08-03-DCGAN-paper-reading/visualization-4.png&quot; alt=&quot;Visualization&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06434&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - Alec Radford el ec, 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html&quot;&gt;초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN) (1)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-2.html&quot;&gt;초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN) (2)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=soJ-wDOSCf4&amp;amp;t=890s&quot;&gt;김태훈: 지적 대화를 위한 깊고 넓은 딥러닝 (Feat. TensorFlow) - PyCon APAC 2016&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/&quot;&gt;Batch Normalization 설명 및 구현&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Thu, 03 Aug 2017 20:30:00 +0000</pubDate>
        <link>https://angrypark.github.io/generative%20models/paper%20review/DCGAN-paper-reading/</link>
        <guid isPermaLink="true">https://angrypark.github.io/generative%20models/paper%20review/DCGAN-paper-reading/</guid>
        
        <category>DCGAN</category>
        
        <category>GAN</category>
        
        <category>PyTorch</category>
        
        
        <category>Generative Models</category>
        
        <category>Paper Review</category>
        
      </item>
    
      <item>
        <title>Movie Recommendation</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/2017-08-01-Movie-Recommendation/background.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;요약&quot;&gt;요약&lt;/h2&gt;

&lt;p&gt;2016년 여름방학, 연세대학교 빅데이터 학회 YBIGTA에 처음 들어와서 시작했던 프로젝트이다. 지금 보면 정말정말 부족한 부분이 많지만, 그래도 꽤 의미있었다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;발표-자료&quot;&gt;발표 자료&lt;/h2&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/FMPhIIaOiibzpP&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/SungnamPark2/kaggle-movie-recommendation&quot; title=&quot;[Kaggle] Movie recommendation&quot; target=&quot;_blank&quot;&gt;[Kaggle] Movie recommendation&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://www.slideshare.net/SungnamPark2&quot;&gt;Sungnam Park&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 01 Aug 2017 14:01:00 +0000</pubDate>
        <link>https://angrypark.github.io/project/kaggle/Movie-Recommendation/</link>
        <guid isPermaLink="true">https://angrypark.github.io/project/kaggle/Movie-Recommendation/</guid>
        
        <category>kaggle</category>
        
        <category>recommendation</category>
        
        
        <category>Project</category>
        
        <category>Kaggle</category>
        
      </item>
    
      <item>
        <title>Markdown에 plotly 그래프 넣기</title>
        <description>&lt;h2 id=&quot;요약&quot;&gt;요약&lt;/h2&gt;

&lt;p&gt;이번 포스트에서는 블로그나 기타 markdown을 작성할 때 쉽게 interactive한 graph를 넣는 방법을 소개한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#plotly&quot;&gt;Plotly?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#예시&quot;&gt;예시&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#결과&quot;&gt;결과&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;plotly&quot;&gt;Plotly?&lt;/h2&gt;
&lt;p&gt;plotly는 간단한 막대그래프에서부터 3차원의 복잡한 그래프까지 누구나 쉽게 예쁜 차트를 만들 수 있는 시각화 오픈 소스이다. 데이터 분석에서 가장 많이 쓰이는 언어인 R과 Python 뿐만 아니라 Javascript까지 완벽하게 호환되며 프로그래밍 언어를 모르더라도 데이터만 가져와서 그래프를 만들 수 있다. 특히 기본적으로 그래프를 만든 뒤에, 쉽게 색깔이나 점 모양, 크기 등을 세부적으로 조절할 수 있어서 매력적이고, interactive한 그래프이기 때문에 그래프를 돌려본다거나, 확대한다거나 같은 일들도 쉽게 할 수 있다. 기본적으로는 ipython notebook을 사용할 때 이용하지만, 이를 markdown에 embed할 수 있다면 결과물을 만들 때 훨씬 유용할 것이라 생각한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;예시&quot;&gt;예시&lt;/h2&gt;
&lt;h4 id=&quot;데이터-전처리를-위한-모듈-불러오기&quot;&gt;데이터 전처리를 위한 모듈 불러오기&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.plotly&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;plotly-로그인&quot;&gt;plotly 로그인&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'your_username'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'your_api_key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;데이터-만들고-시각화&quot;&gt;데이터 만들고 시각화&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://raw.githubusercontent.com/plotly/datasets/master/2011_us_ag_exports.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(242,240,247)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(218,218,235)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(188,189,220)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;\
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(158,154,200)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(117,107,177)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(84,39,143)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'state'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&amp;lt;br&amp;gt;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
    &lt;span class=&quot;s&quot;&gt;'Beef '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'beef'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' Dairy '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dairy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&amp;lt;br&amp;gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
    &lt;span class=&quot;s&quot;&gt;'Fruits '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'total fruits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' Veggies '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'total veggies'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&amp;lt;br&amp;gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
    &lt;span class=&quot;s&quot;&gt;'Wheat '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'wheat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' Corn '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'corn'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'choropleth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;colorscale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;autocolorscale&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;locations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'total exports'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;locationmode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'USA-states'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(255,255,255)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;colorbar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Millions USD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;layout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'2011 US Agriculture Exports by State&amp;lt;br&amp;gt;(Hover for breakdown)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;geo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'usa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;projection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'albers usa'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;showlakes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;lakecolor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rgb(255, 255, 255)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layout&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'d3-cloropleth-map'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;
&lt;p&gt;기본적으로 png나 jpg로 export 가능하기 때문에 markdown에 그 결과를 사진으로 보여주는 것은 어렵지 않다.&lt;/p&gt;
&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;![&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;결과&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;/your_directory/your_graph.jpg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/images/2017-07-27-Embed-Plotly/graph.png&quot; alt=&quot;결과&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러나 이는 단순 스크린샷일 뿐, 세부적으로 각 항목이 수치가 얼마나 되는지도 알 수 없고 그래프가 복잡해지거나 3차원이 될 때 보기가 힘들다.&lt;/p&gt;

&lt;p&gt;따라서 이를 해결하기 위해 다음과 같은 과정을 거치면 interactive한 그래프를 넣을 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#데이터-만들고-시각화&quot;&gt;위의 코드&lt;/a&gt;를 실행하였을 때 그래프 우측 하단에 있는 &lt;strong&gt;edit chart&lt;/strong&gt; 클릭&lt;/li&gt;
  &lt;li&gt;여기서 세부적인 색깔 조정, 크기 조정 등을 할 수 있음. 좌측 하단에 있는 &lt;strong&gt;share&lt;/strong&gt; 클릭
&lt;img src=&quot;/images/2017-07-27-Embed-Plotly/share.png&quot; alt=&quot;share&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;거기서 &lt;strong&gt;embed&lt;/strong&gt; 를 클릭하면 본인 plotly 사이트를 통해 연동할 수 있는 iframe 링크 생성 가능!
&lt;img src=&quot;/images/2017-07-27-Embed-Plotly/embed.png&quot; alt=&quot;embed&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
</description>
        <pubDate>Thu, 27 Jul 2017 14:01:00 +0000</pubDate>
        <link>https://angrypark.github.io/blog/Embed-Plotly/</link>
        <guid isPermaLink="true">https://angrypark.github.io/blog/Embed-Plotly/</guid>
        
        <category>markdown</category>
        
        <category>plotly</category>
        
        <category>visualization</category>
        
        
        <category>Blog</category>
        
      </item>
    
      <item>
        <title>TripAdvisor Comments Analysis</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/2017-07-26-TripAdvisor-comment-analysis/background.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;요약&quot;&gt;요약&lt;/h2&gt;

&lt;p&gt;2017년 1학기 연세대학교 고객관계경영 수업(손소영 교수님)을 들으며 진행하였던 프로젝트를 소개한다. 해외 사이트에서 댓글을 긁어오는 것부터 이를 통해 댓글의 감정을 분석하는 과정까지 정리해보았다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;목차&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#프로젝트-목표&quot;&gt;프로젝트 목표&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#comments-crawling&quot;&gt;Comments Crawling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preprocessing&quot;&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#발표-자료&quot;&gt;발표 자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;프로젝트-목표&quot;&gt;프로젝트 목표&lt;/h2&gt;

&lt;h4 id=&quot;tripadvisor&quot;&gt;TripAdvisor&lt;/h4&gt;
&lt;p&gt;TripAdvisor는 전세계에서 가장 많은 사람들이 사용하는 여행 정보 사이트 중 하나이다. 숙박 예약에서부터 각 주요 관광지에 대한 수많은 관광객들의 평가 정보까지 아주 자세히 나와있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-07-26-TripAdvisor-comment-analysis/what-is-tripadvisor.gif&quot; alt=&quot;what-is-tripadvisor&quot; /&gt;&lt;/p&gt;

&lt;p&gt;수업에서 진행한 프로젝트는 한국 관광지에 방문한 외국인 관광객들의 댓글들을 불러온 후, 그 중 무슬림 관광객들에 대한 한국 관광지의 선호도와 댓글을 분석하여 무슬림이 좋아하는 관광지를 추천했지만 이번 포스트에서는 그중에서도 내가 맡았던 &lt;strong&gt;댓글의 감정 분석&lt;/strong&gt; 을 소개한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;comments-crawling&quot;&gt;Comments Crawling&lt;/h2&gt;

&lt;h4 id=&quot;필요한-라이브러리-불러오기&quot;&gt;필요한 라이브러리 불러오기&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trange&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;crawling할-장소들-불러오기&quot;&gt;Crawling할 장소들 불러오기&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;info_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 1. 고궁&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Gyeongbokgung_Palace'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d324888-Reviews-Gyeongbokgung_Palace-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;378&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Changdeokgung_Palace'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d320359-Reviews-Changdeokgung_Palace-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;183&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Changgyeonggung Palace'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1554603-Reviews-Changgyeonggung_Palace-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Deoksugung'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d324887-Reviews-Deoksugung-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;33&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 2. 박물관(기념관)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'The War Memorial of Korea'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d554537-Reviews-The_War_Memorial_of_Korea-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;225&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'National Museum of Korea'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d325043-Reviews-National_Museum_of_Korea-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;94&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Seoul Arts Center'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1862210-Reviews-Seoul_Arts_Center-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 3. 인사동&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Insadong'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d592506-Reviews-Insadong-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;267&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 4. 남산타워&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N_Seoul_Tower'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1169465-Reviews-N_Seoul_Tower-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;362&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 5. 명동&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Myeongdong_Shopping_Street'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d553546-Reviews-Myeongdong_Shopping_Street-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;360&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 6. 남대문시장&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Namdaemun Market'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d324907-Reviews-Namdaemun_Market-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;81&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 7. 코엑스&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Starfield COEX Mall'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d554303-Reviews-Starfield_COEX_Mall-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 8. 동대문시장&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Dongdaemun_Design_Plaza'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d6671988-Reviews-Dongdaemun_Design_Plaza-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 9. 이태원&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Itaewon'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d2571660-Reviews-Itaewon-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 10. 잠실(롯데월드)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Lotte World'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d324891-Reviews-Lotte_World-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 11. 여의도(63빌딩)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'63 City'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d554551-Reviews-63_City-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Yeouido Hangang Park'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d4798715-Reviews-Yeouido_Hangang_Park-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 12. 한강/유람선&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hangang Park'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1519813-Reviews-Hangang_Park-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;41&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 13. 청계천/광화문광장&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Gwanghwamun Gate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d590748-Reviews-Gwanghwamun_Gate-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Gwanghwamun Square'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d6847900-Reviews-Gwanghwamun_Square-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 14. 신촌/홍대주변&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hongik_University_Street'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1958940-Reviews-Hongik_University_Street-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;76&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Ewha Womans University'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1862191-Reviews-Ewha_Womans_University-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 15. DMC/월드컵 경기장&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Seoul World Cup Stadium'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d561808-Reviews-Seoul_World_Cup_Stadium-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 16. 한옥마을(남산)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Namsangol Hanok Village'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1551271-Reviews-Namsangol_Hanok_Village-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;33&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 17. 북촌/삼청동&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Bukchon Hanok Village'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1379963-Reviews-Bukchon_Hanok_Village-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;164&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 18. 청담동 / 압구정동&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Apgujeong Rodeo Street'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1847008-Reviews-Apgujeong_Rodeo_Street-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 19. 가로수길&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Garosu-gil'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1604009-Reviews-Garosu_gil-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 20. 강남역&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ?&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Bongeunsa_Temple'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d592486-Reviews-Bongeunsa_Temple-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Gwangjang Market'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://www.tripadvisor.com/Attraction_Review-g294197-d1552278-Reviews-Gwangjang_Market-Seoul.html'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;    
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;자동으로-crawling하게-함수-정의&quot;&gt;자동으로 Crawling하게 함수 정의!&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pagenum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s Start...&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;userid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 평가자 아이디&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;home&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 평가자 출신지&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rating&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 평점&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pagenum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reviews-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reviews-{}-'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'or&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'html.parser'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'div'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'review_collection'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'html.parser'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'div'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'div'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rating reviewItemInline'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;rating&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&quot;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'of'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'span'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'noQuotes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'partial_entry'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'div'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'username mo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;userid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'''user_name_name_click')&quot;&amp;gt;'''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rating&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'region'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'home'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rating'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'comment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'userid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'userid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'region'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'home'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rating'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'comment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;이를-csv로-저장&quot;&gt;이를 csv로 저장&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'review.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;이 과정에서는 영어 단어를 불러올 때 과거형 / 과거분사형 / 복수형 등의 단어를 기본형으로 바꾸고, 쉼표와 마침표, 대문자를 제거하는 과정이다.&lt;/p&gt;

&lt;h4 id=&quot;필요한-라이브러리-불러오기-1&quot;&gt;필요한 라이브러리 불러오기&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_option&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Display.max_columns&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cufflinks&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.plotly&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plotly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_credentials_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'your_username'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'your_api_key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;대문자-품사-처리&quot;&gt;대문자, 품사 처리&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc_en&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;TripAdvisor_reviews.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'comment'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fname&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class