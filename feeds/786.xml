<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Jongdae Lim on Medium]]></title>
        <description><![CDATA[Stories by Jongdae Lim on Medium]]></description>
        <link>https://medium.com/@jongdae.lim?source=rss-ec24e1bbe5d8------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*Lw7J_jr9hJY5GQwa0bIBlQ@2x.jpeg</url>
            <title>Stories by Jongdae Lim on Medium</title>
            <link>https://medium.com/@jongdae.lim?source=rss-ec24e1bbe5d8------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 13 May 2019 21:12:18 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/@jongdae.lim" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 8]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/d9507cf20352</guid>
            <category><![CDATA[black-box-attack]]></category>
            <category><![CDATA[머신러닝]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Thu, 09 Nov 2017 05:55:30 GMT</pubDate>
            <atom:updated>2017-11-10T13:39:39.524Z</atom:updated>
            <content:encoded><![CDATA[<h3>신경망을 의도적으로 속이는 방법</h3><p>해킹의 미래에 관한 이야기</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p>프로그래머들이 컴퓨터 프로그램을 작성하기만 하면, 컴퓨터 해커들은 그 프로그램을 악용할 방법을 알아냈습니다. 악의적인 해커들은 프로그램의 작은 버그를 이용해 시스템에 침입하여 데이터를 훔쳐냈는데, 이는 일반적으로 막대한 피해를 입혔습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*o0DlWJnn2hwTbqn9zZXpYg.gif" /><figcaption>100% 진짜 해커들™*</figcaption></figure><blockquote>* 역자주: 위 해커 영상은 사실 NICS(미국 범죄 TV 시리즈)의 한 장면으로, 두 명이 함께 타이핑을 하고 말도 안되는 화면과 코드들이 보이는 장면입니다. 저자는 해커 이미지에 대한 이 과장된 장면을 “100% 진짜”라는 단어로 살짝 비꼬며 재미있게 표현한 것입니다.</blockquote><p>그렇다면, 딥러닝(deep learning) 알고리즘으로 동작하는 시스템도 인간의 간섭으로부터 안전해야 합니다. 그렇죠? 해커는 테라 바이트 단위의 데이터로 훈련 된 신경망을 어떻게 뚫을 수 있을까요?</p><p>가장 진보된 딥신경망(deep neural networks) 조차 쉽게 속일 수 있음이 밝혀졌습니다. 몇 가지 트릭을 사용하면, 신경망이 당신이 원하는 결과를 예측하게 할 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*3Hh4MrtjzPSXS6kC07kp_Q.png" /><figcaption>이 고양이 사진을 토스터기로 인식하도록 변형했습니다.</figcaption></figure><p>따라서 당신은 딥신경망으로 동작하는 새로운 시스템의 서비스를 시작하기 전에, 이를 파괴하는 방법과 공격자로부터 자신을 보호하기 위해 무엇을 해야 하는지 정확하게 알아야 합니다</p><h3>경비원으로서의 신경망</h3><p>우리가 Ebay와 같은 경매 웹사이트를 운영한다고 상상해 보겠습니다. 우리의 웹사이트에서는 사람들이 금지 된 품목(살아있는 동물과 같은 것)을 판매하지 못하도록 막고 싶습니다.</p><p>수백만 명의 사용자가 있는 경우에 이런 규칙을 적용하는 것은 어렵습니다. 수백 명을 고용해서 모든 경매 목록을 수작업으로 검토 할 수는 있지만, 비용이 많이 들 것입니다. 대신에, 우리는 딥러닝을 활용해 금지된 품목에 대한 경매 사진을 자동으로 확인하고 규칙을 위반한 것들에 플래그(flag)가 달리게 할 수 있습니다.</p><p>이것은 전형적인 이미지 분류 문제입니다. 이를 제작하기 위해, 딥 컨볼루션 신경망(<a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">deep convolutional neural network</a>)을 훈련해서 허용된 품목과 금지된 품목을 구분하도록 훈련시킨 후, 우리의 사이트에 있는 모든 사진을 이 신경망에서 실행할 것입니다.</p><p>먼저, 과거의 경매 목록으로부터 수천 개의 이미지 데이터 세트를 만들어야 합니다. 우리는 허용된 품목과 금지된 품목을 구분하도록 신경망을 훈련시켜야 하기 때문에 두 종류의 이미지가 모두 필요합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/761/1*yh_ky_2xzjFyPdEK-JFOnw.png" /></figure><p>신경망을 훈련시키기 위해, 우리는 표준 역전파(<em>back-propagation)</em> 알고리즘을 사용할 것입니다. 이는 훈련 사진을 전달하고, 이 사진에 대한 예상 결과를 전달한 다음에 신경망의 각 층으로 되돌아가 좀더 나아지도록 이들의 가중치를 약간씩 조정해 그 그림에 대한 정확한 결과가 나오게 하는 알고리즘입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*eTWVMWIjy7Rw6IC4DYQcrg.png" /></figure><p>이 모델이 허용 가능한 정확도로 올바른 결과를 신뢰할 수준으로 만들어 낼 때까지, 수천 장의 사진으로 수천 번을 반복할 것입니다.</p><p>최종 결과는 신뢰할 수준으로 이미지를 분류하는 신경망입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*4_LMnAkaosT5FDG1A7-FTw.png" /></figure><p><em>참고: 컨볼루션 신경망이 이미지에서 어떻게 객체를 인식하는지 좀더 자세히 알고 싶다면, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part3</em></a><em>를 확인하세요.</em></p><h4><strong>하지만, 그렇게 신뢰할 수는 없습니다…</strong></h4><p>컨볼루션 신경망은 이미지를 분류 할 때 전체 이미지를 고려하는 강력한 모델입니다. 컨볼루션 신경망은 이미지상에 나타나는 위치에 관계없이 복잡한 모양과 패턴을 인식 할 수 있습니다. 다양한 이미지 인식에 있어, 이 신경망은 인간의 능력과 같거나 심지어 더 나을 수 있습니다. https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa</p><p>이러한 환상적인 모델을 사용할 때, 이미지의 몇몇 픽셀을 어둡게 또는 밝게 변경한다고 해서 최종 예측에 큰 영향을 미치지 않을 것입니다. 그렇죠? 물론, 최종 확율값이 약간 변경 될 수 있겠지만, 해당 이미지를 “금지됨”에서 “허용됨”으로 바꿔서는 안됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*z8oxIkPCe9nifKoQtPHy0A.png" /><figcaption>우리의 기대: 입력 사진에 작은 변화를 주면 최종 예측에도 단지 작은 변화만이 생겨야 한다.</figcaption></figure><p>하지만, 신경망의 아주 흥미로운 특징(<a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>)이라고 불리는 2013년의 유명한 논문에서, 이것이 항상 사실이 아니라는 것이 밝혀졌습니다. 당신이 <em>정확히 어떤 픽셀을 변경하고 정확히 얼마만큼 변경해야하는지</em> 안다면 사진의 모습을 그렇게 많이 변경하지 않고도 의도적으로 신경망이 주어진 사진에 대해 잘못된 출력을 예측하게 할 수 있습니다.</p><p>이는 분명히 금지된 품목의 사진을 의도적으로 조작해 신경망을 완전히 속일 수 있다는 것을 의미합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*QynisLlk0BHd4hO9S2fBWg.png" /></figure><p>왜 이렇게 되는 걸까요? 기계 학습의 분류기(classifier)는 우리가 구별고자 하는 것들 사이의 구분선을 찾아 냄으로써 동작합니다. 다음은 녹색 좌표(허용)과 적색 좌표(금지)를 구분 짓는 간단한 2 차원 분류 기준선을 그래프로 나타낸 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*5225tODu1ieDhirpy8qfqw.png" /></figure><p>현재, 이 분류기는 100% 정확도로 동작합니다. 적색 좌표과 녹색 좌표을 완벽하게 구분하는 선을 찾아 냈습니다.</p><p>그런데, 만약 우리가 적색 좌표 중 하나를 녹색 좌표으로 잘못 분류하게 하게 조작하려면 어떻게 해야 할까요? 적색 좌표을 녹색의 영역으로 옮기기 위한 최소 이동량은 얼마인가요?</p><p>경계 바로 옆에있는 적색 좌표의 Y 값에 약간의 값만 더하면 녹색 영역으로 손쉽게 밀어 넣을 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*N291TkA96ogCF-_iLCmHxw.png" /></figure><p>따라서, 분류기를 속이기 위해서는 그 좌표 이 선을 넘기 위해 조금씩 움직일 방향을 알아야합니다. 그리고 우리의 사악함이 드러나지 않기 위해, 이상적으로 좌표을 가능한 조금씩 옮겨서 정직한 실수처럼 보이게 해야 합니다.</p><p>딥신경망을 사용한 이미지 분류에서, 우리가 분류하는 각각의 “좌표”는 바로 전체 이미지를 구성하는 수천개의 픽셀입니다. 이는 좌표를 구분선(decision line)을 넘게하기 위해 조작해야할 가능한 값이 수천 개나 된다는 뜻입니다. 또한, 사람이 눈치채지 못할 정도로 이미지의 픽셀을 조정해야만, 이미지가 조작된 것처럼 보이지 않으면서 분류기를 속일 수 있습니다.</p><p>다시 말해서, 어떤 물체의 실제 사진을 찍은 후 픽셀을 아주 조금만 변경해서 신경망이 이 사진을 전혀 다른 것으로 판단하도록 완전히 속일 것입니다. 그리고, 신경망이 대신 인식하게 될 객체 또한 정확히 조정할 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*7t8kb9PnDrEYu07v_X_ERg.png" /><figcaption>고양이를 토스트로 인식했습니다. 이미지 검색 결과는 <a href="https://transcranial.github.io/keras-js/#/inception-v3">Keras.js web-based demo</a>에서 진행했습니다.</figcaption></figure><h4><strong>신경망을 속이는 방법</strong></h4><p>앞서 신경망을 훈련시켜 사진을 분류하는 기본적인 과정에 대해 이야기했습니다:</p><p>1. 훈련 사진을 제공(feed)합니다.</p><p>2. 신경망의 예측을 확인하고 정답에서 얼마나 떨어져 있는지 확인합니다.</p><p>3. 최종 예측이 정답에 조금씩 가까워 지도록 역전파(back-propagation)를 사용해 신경망의 각 계층의 가중치를 조정합니다.</p><p>4. 수천 개의 훈련 사진으로 1 ~ 3 단계를 수천 번 반복합니다.</p><p>그런데, 신경망 계층들의 가중치를 조정하는 대신에 우리가 원하는 답을 얻을 때까지 입력 이미지 자체를 수정하는것은 어떨까요?</p><p>이제 이미 훈련 된 신경망을 이용해 다시 “훈련”을 시작하겠습니다. 이번에는 역전파를 신경망 계층에 사용하는 대신 <strong><em>입력 이미지(input image)</em></strong>를 조정하는데 쓸 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*QbXo5d7ruyoPCar5DYhabw.png" /></figure><p>이를 위한 새로운 알고리즘을 만들었습니다:</p><p>1. 우리가 해킹하고 싶은 사진을 제공(feed)합니다.</p><p>2. 신경망의 예측을 확인해서, 이 사진에 대해 <strong><em>우리가 원하는 답변</em></strong>과 얼마나 차이가 나는지 확인합니다.</p><p>3. 역전파(back-propagation)를 사용해서 사진을 조정해 최종 예측이 우리가 얻고 싶은 답변에 조금 가까워지도록 만듭니다.</p><p>4. 신경망이 원하는 답을 내 놓을 때까지 <strong><em>동일한 사진</em></strong>으로 1 ~ 3 단계를 수천 번 반복합니다.</p><p>결국에 우리는 신경망 자체의 내부를 전혀 바꾸지 않고도 이미지만으로 신경망을 속일 수 있게 될 것입니다.</p><p>유일한 문제는 아무런 제한없이 어떤 픽셀이라도 조정할 수 있기 때문에, 이미지에 대한 이러한 변경들이 당신이 알 수 있을 정도로 충분히 극단적이라는 것입니다. 이 변화들은 변색된 점이나 물결같은 영역으로 보여집니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/299/1*b4XHWHTzSy505T4kI1dhnw.png" /><figcaption>얼마나 많은 픽셀을 조정할 수 있는지에 대한 제약없이 해킹된 이미지. 고양이 주면에 녹색으로 변색된 점들과 흰색 벽에 물결 패턴을 볼 수 있습니다.</figcaption></figure><p>이러한 명백한 왜곡을 방지하기 위해서, 우리의 알고리즘에 간단한 제약 조건을 추가 할 수 있습니다. 예를 들어 해킹된 이미지의 어떤 픽셀도 원래 이미지로부터 아주 적은 양 이상으로 변경될 수 없다 라고 정할 수 있습니다 — 0.01 %와 같은 기준 같은 것입니다. 이렇게 하면 우리의 알고리즘은 원래의 이미지와 너무 다르게 보지 않으면서도 신경망을 여전히 속일 수 있도록 이미지를 조정할 수 있습니다.</p><p>다음은 제약을 추가한 다음에 만들어진 이미지 입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/299/1*obcaDy5iSCtT_UhGibiZjQ.png" /><figcaption>하나의 필섹이 얼마나 변경될 수 있는지에 대한 제약을 두고 만들어낸 해킹된 이미지</figcaption></figure><p>우리에겐 똑같은 이미지 처럼 보이지만, 여전히 신경망을 속일 수 있습니다!</p><h3>코딩을 해봅시다</h3><p>코딩을 하기 위해서는, 먼저 사전에 훈련된 신경망이 필요합니다. 처음부터 신경망을 훈련하는 대신 Google에서 만든 것을 사용하겠습니다.</p><p>인기있는 딥러닝 프레임워크인 <a href="https://keras.io/">Keras</a>에는 여러가지 사전에 훈련된 신경망(<a href="https://keras.io/applications/">pre-trained neural networks</a>)이 함께 제공됩니다. 우리는 1000가지 종류의 객체(<a href="https://gist.github.com/ageitgey/4e1342c10a71981d0b491e1b8227328b">1000 different kinds of objects</a>)를 인식하기 위해 사전 훈련 된 Google의 Inception v3 딥신경망 복사본을 사용할 것입니다.</p><p>신경망을 사용하여 사진에 무엇이 있는지 인식하기 위한 Keras의 기본 코드입니다. 실행하기 전에 <a href="https://www.python.org/downloads/">Python 3</a>과 <a href="https://keras.io/#installation">Keras</a>를 설치해야 합니다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9982246f5ef7288fd668eb87ad3d36b1/href">https://medium.com/media/9982246f5ef7288fd668eb87ad3d36b1/href</a></iframe><p>실행해 보면, 아마도 우리 이미지를 페르시아 고양이로 인식할 것입니다 :</p><pre>$ python3 predict.py</pre><pre>This is a Persian_cat with 85.7% confidence!</pre><p>자 이제 신경망을 속일 때 까지 이미지를 조정해 이 고양이를 토스터라고 생각하도록 조작해 보겠습니다.</p><p>Keras에는 신경망 계층을 훈련하는 대신 입력 이미지를 훈련시키는 기능이 탑재되어 있지 않기 때문에, 조금은 어렵지만 수동으로 훈련할 단계를 코딩해야만 합니다.</p><p>그 코드는 다음과 같습니다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4bf0de1653b62811398c501f121f7139/href">https://medium.com/media/4bf0de1653b62811398c501f121f7139/href</a></iframe><p>이를 실행하면, 결국에는 신경망을 속일 이미지를 뱉어낼 것입니다.</p><pre>$ python3 generated_hacked_image.py</pre><pre>Model&#39;s predicted likelihood that the image is a toaster: 0.00072%</pre><pre>[ .... a few thousand lines of training .... ]</pre><pre>Model&#39;s predicted likelihood that the image is a toaster: 99.4212%</pre><p><em>참고 : GPU가 없는 경우에는 실행하는데 몇 시간이 걸릴 수 있습니다. Keras 및 </em><a href="https://developer.nvidia.com/cuda-downloads"><em>CUDA</em></a><em>를 통해 GPU를 올바르게 구성했다면, 실행에는 수분 이상 소요되지 않습니다.</em></p><p>이제 기존 모델을 다시 실행해서 막 만들어낸 해킹된 이미지를 테스트 해 보겠습니다:</p><pre>$ python3 predict.py</pre><pre>This is a toaster with 98.09% confidence!</pre><p>해냈습니다. 고양이를 토스터기로 생각하도록 속였습니다.</p><h3><strong>해킹된 이미지로 무엇을 할 수 있을까요?</strong></h3><p>이와 같이 해킹된 이미지를 만드는 것을 “적대적인 사례 만들기”라고 합니다. 우리는 의도적으로 데이터 조각을 조작해서 기계 학습 모델이 분류를 잘못 하도록 만들었습니다. 이것은 신기한 기술이긴 하지만, 왜 이것이 현실 세계에서 중요할까요?</p><p>연구에 따르면 이러한 해킹 된 이미지에는 몇몇 놀라운 특징들이 있습니다.</p><p>1. <a href="https://www.youtube.com/watch?v=zQ_uMenoBCk">해킹된 이미지를 종이에 인쇄해도</a> 여전히 신경망을 속일 수 있습니다! 따라서 이렇게 해킹된 이미지를 사용해서 파일을 직접 업로드하는 시스템 뿐만 아니라 실제 카메라나 스캐너도 속일 수 있습니다.</p><p>2. 특정 신경망을 속일 수 있는 이미지는 완전히 다르게 설계된 신경망이라도 만약 유사한 데이터로 훈련되었다면 이를 속일 수 있습니다.</p><p>또한, 이러한 해킹된 이미지로 다양한 것들을 할 수 있습니다!</p><p>하지만, 이러한 이미지를 만드는 방법에는 여전히 큰 한계가 있습니다 — 우리의 공격은 신경망에 직접 접근해야 합니다. 우리는 사실 신경망을 속이기위한 “훈련”을 하는 것이기 때문에, 이 신경망의 복사본이 필요합니다. 그러나 현실에서는 어떤 회사도 그들의 훈련된 신경망 코드의 다운로드를 허락하지 않습니다. 이렇게 되면 공격을 할 수가 없습니다. 그렇죠?</p><p>틀렸습니다! 연구자들은 최근 신경망이 어떻게 동작하는지 조사함으로써, <a href="https://arxiv.org/abs/1602.02697">우리의 대체 신경망이 다른 신경망을 미러링하도록 훈련</a>시킬 수 있음을 보여주었습니다. 그리고 이 대체 신경망을 사용해 해킹된 이미지를 만들어 원래 신경망을 속일 수 있습니다! 이를 <em>블랙박스 공격(black-box attack)</em>이라고 합니다.</p><p>블랙박스 공격은 무한하게 응용할 수 있습니다. 그럴듯한 몇 가지 예가 있습니다.</p><ul><li>자가 운전 차량을 속여 녹색 신호를 정지 신호로 판단하게 속일 수 있습니다 — 이는 자동차 충돌 사고를 일으킬 수 있습니다!</li><li>컨텐츠 필터링 시스템을 속여 공격적이고 불법적인 컨텐츠를 올릴 수 있습니다</li><li>ATM 수표 스캐너를 속여서 수표에 쓰여진 금액이 실제보다 훨씬 더 큰 금액이라고 판단하게 할 수 있습니다. (실제로 잡혀도 부인할 수준으로 말이죠!)</li></ul><p>그리고 이러한 공격 방법은 단지 이미지에만 국한되지 않습니다. 동일한 접근을 통해 다른 유형의 데이터에서 동작하는 분류기도 속일 수 있습니다. 예를 들어, 컴퓨터 바이러스 스캐너를 속여 바이러스를 안전한 코드로 인식하게 할 수 있습니다!</p><h3>이러한 공격으로부터 우리 스스로를 어떻게 보호 할 수 있을까요?</h3><p>이제 우리는 신경망(및 다른 모든 기계 학습 모델들 역시)을 속일 수 있다는 것을 알게되었는데, 우리는 어떻게 이를 방어할 수 있을까요?</p><p>간단한 대답은 아직은 아무도 <em>완전히</em> 확신할 순 없다는 것입니다. 이러한 종류의 공격을 방어하는 것은 계속 진행중인 연구 분야입니다. 최신의 성과물을 알아보는 가장 좋은 방법은 이 분야에서 가장 영향력있는 연구원인 Ian Goodfellow와 Nicolas Papernot이 관리하는 <a href="http://www.cleverhans.io/">cleverhans blog</a>를 읽어 보는 것입니다.</p><p>다음은 지금까지 알아낸 것들입니다.</p><ul><li>간단히 해킹된 이미지를 많이 만들어 훈련 데이터 세트에 포함 시키면, 신경망이 이러한 공격에 좀더 저항성을 갖게 됩니다. 이를 적대적 훈련(Adversarial Training)이라고 하고, 아마도 현 시점에 채택할 수 있는 가장 합리적인 방어법일 것입니다.</li><li>원래 모델을 모방한 두번째 모델을 훈련시키는 <a href="https://arxiv.org/abs/1511.04508">Defensive Distillation</a>이라고 하는 꽤 효과적인 접근방법도 있습니다. 그러나 이 접근법은 새롭고 다소 복잡하기 때문에, 특별한 필요성이 없는 한 아직은 고려않을 것입니다.</li><li>이 외에 지금까지 시도한 다른 모든 아이디어는 이러한 공격을 방어하는 데 도움이되지 못했습니다.</li></ul><p>아직 결론적인 답변이 없기 때문에, 신경망을 사용할 때 당신의 비지니스에 손해를 끼칠지도 모를 이러한 종류의 공격으로 인한 위험을 최소한 알고 있다는 시나리오를 가치있게 생각해 봐야 합니다.</p><p>예를 들어, 제한된 자원에 대한 접근 권한을 부여하는 유일한 방어선으로 단 하나의 기계 학습 모델을 사용하면서 이 모델이 절대 속지 않을 것이라고 가정한다면, 이는 아마도 좋지 못한 생각일 것입니다. 그러나 기계 학습을 여전히 인간의 검증이 있는 프로세스의 한 단계로 사용한다면 그것은 아마도 괜찮을 것입니다.</p><p>다시 말해서, 당신의 아키텍처에서 기계 학습 모델도 잠재적으로 우회할 수 있는 다른 구성 요소와 마찬가지로 생각해야 합니다. 사용자가 의도적으로 속일 때 어떤 일이 발생하는지 가정한 다음 이러한 시나리오를 완화 할 수 있는 방법을 생각해야 합니다.</p><h3>더 배우기</h3><p>적대적인 사례(Adversarial Examples)와 이를 방어하는 방법에 대해 더 자세히 배우고 싶은가요?</p><ul><li>이 연구분야는 몇 년 정도 밖에 되지 않았기 때문에, 몇 가지 주요 논문을 읽으면 쉽게 이해할 수 있습니다 : <a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>, <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a>, <a href="https://arxiv.org/abs/1602.02697">Practical Black-Box Attacks against Machine Learning</a> 그리고 <a href="https://arxiv.org/abs/1607.02533">Adversarial examples in the physical world</a>.</li><li>Ian Goodfellow와 Nicolas Papernot의 <a href="http://www.cleverhans.io/">cleverhans blog</a>를 보고 최신 연구를 확인하세요.</li><li>연구자들이 이러한 공격을 방어 할 수 있는 새로운 방법을 찾아 경쟁 중인 Kaggle 경쟁 대회(<a href="https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack">on-going Kaggle competition</a>)을 확인하십시오</li></ul><p>이 글이 마음에 들었다면, 제 <a href="http://eepurl.com/b9fg2T"><strong>Machine Learning is Fun! 이메일 리스트에 가입</strong></a>하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.</p><p>Twitter의 <a href="https://twitter.com/ageitgey">@ageitgey</a>인 저를 팔로우하거나, <a href="mailto:ageitgey@gmail.com">직접 이메일</a>을 보내거나 또는 <a href="https://www.linkedin.com/in/ageitgey">linkedin에서 저를 찾을 수도 있습니다</a>. 기계 학습으로 제가 당신이나 당신의 팀을 도울 수 있다면 언제든 연락 주십시오.</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230737"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d9507cf20352" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[자바(Java)는 어디로 가고 있을까?]]></title>
            <link>https://medium.com/@jongdae.lim/%EC%9E%90%EB%B0%94-java-%EB%8A%94-%EC%96%B4%EB%94%94%EB%A1%9C-%EA%B0%80%EA%B3%A0-%EC%9E%88%EC%9D%84%EA%B9%8C-37dc915f00f1?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/37dc915f00f1</guid>
            <category><![CDATA[java-programming]]></category>
            <category><![CDATA[자바]]></category>
            <category><![CDATA[java-history]]></category>
            <category><![CDATA[java]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Thu, 14 Sep 2017 05:59:40 GMT</pubDate>
            <atom:updated>2017-09-25T08:06:26.482Z</atom:updated>
            <content:encoded><![CDATA[<p>이 글은 2015년, Java 20주년을 기념해 마이크로소프트웨어(마소)에서 특집기사 요청이 와 쓰게된 원고입니다.</p><p><em>* 이 글은 자바(Java)의 역사를 한 엔지니어가 자바 기술을 만나 그 변화와 혁신을 경험한 내용으로 꾸민 허구의 이야기 입니다. 본문에 등장하는 한국내 인물과 사건은 어쩌면(?) 사실과 다를 수도 있음을 밝혀둡니다.</em></p><blockquote><strong>“근데 자바가 왜 좋아?”</strong></blockquote><p>1998년 어느 겨울밤, 그날도 어김없이 부평 연구소에 함께 남아 있던 입사 동기인 재성이가 내게 물었다 – 현재는 호주에서 머신러닝 기반의 솔루션 개발을 총괄하고 있는 전재성은 프로그래머로서의 나의 인생에 가장 큰 영감을 준 사람 중 한 명이다. 그와 난 부평 연구소을 떠나 자바 기술 기반의 벤처를 거쳐 BEA 연구소도 함께 했다.</p><p>그 때 난 새롭게 출시된 자바 2(자바 1.2 부터 자바 2라 부른다)의 새로운 GUI 라이브러리인 스윙(Swing) 데모를 화면에 띄우고는 행복해하던 중이었다…</p><p>스윙은 자바가 세상에 나왔을 때 만큼이나 충격적이고 멋졌다. 나는 검은 화면에 하얀 글자뿐이었던 C보다, 촌스럽고 딱딱하게 보이던 파워빌더의 화면보다 스윙이 깔끔하고 멋지게 보였다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0sGjAtWBBZLs8owuWKEWZg.png" /><figcaption>Java SwingSet</figcaption></figure><p>그 당시 사람들은 자바가 느리다고 했지만 나는 자바의 이런 변화가 좋았다. 현재 발전된 스윙은 느리지 않으며 유연하다. 스윙은 현재 이클립스(Eclipse)와 함께 가장 많은 개발자가 사용하는 통합개발환경중(IDE, Integrated Development Environment) 중 하나인 넷빈즈(NetBeans)에서 사용되고 있다. 자바의 표준 스윙 컴포넌트로 복잡한 UI 툴킷(Toolkit)을 구현했다. 최근 넷빈즈를 사용하는 개발자들에게서 이 복잡한 통합개발환경(IDE)이 느리다는 평을 들 수 없을 정도로 스윙의 화면은 이제 빠르다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/925/1*LV_7f0NIjGCbtopkTltGmA.png" /><figcaption>넷빈즈(NetBeans) IDE</figcaption></figure><p>자바의 슬로건은 <strong>“한 번 프로그래밍 한 것은 어떤 플랫폼에서도 실행된다(Write Once, Run Anywhere)”</strong>이다.</p><p>한번 프로그래밍한 코드가 자바 가상 머신(JVM)이 설치된 어떠한 플랫폼에서도 바로 실행된다는 개념은 태생적으로 느리다는 공격을 받았지만 분명히 혁신적이었다. 자바가 프로그래밍 언어로써 근간으로 삼는 객체지향(Object Oriented)이라는 개념은 개발자들에게 좀더 유연하고 유지보수가 쉬운 코드를 만들어 낼 수 있다는 점에서 크게 부각됐다. 그렇게 우리는 당연하게도 자바가 만들어 갈 변화와 혁신에 동참하고 있었다.</p><h3><strong>“이제 현실을 보는 방법이 완전히 바뀌었습니다. 왜냐하면 움직였으니까요.”</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*dX595YywKM9VY4K-08T6zg.jpeg" /><figcaption><em>James Gosling, the Father of Java said </em><strong>“Their view of reality had completely changed because it MOVED.”</strong></figcaption></figure><p>1995년 초, 미국 서부 몬테레이에서 열린 ‘테크놀로지, 엔터테인먼트와 디자인 컨퍼런스(Technology, Entertainment and Design Conference)’에서 제임스 고슬링(James Gosling)은 영화 ‘블레이드 러너(Blade Runner)의 이름을 딴 ’웹러너(WebRunner)’라는 브라우저에서 애플릿(Applet)으로 만들어진 3D 분자 모형을 시연했다. 고슬링이 분자 모형을 브라우저 상에서 이리저리 돌리자, 그냥 뻔한 텍스트 기반의 모자이크(Mosaic) 브라우저의 클론 쯤으로 생각하고 집중하지 않던 사람들은 모두 “와아아!”하고 탄성을 내뱉으며 다음 데모인 시각화된 정열 알고리즘(Sorting Argorithm) 화면에 집중할 수 밖에 없었다. 그도 그럴 것이 그 때의 웹브라우저는 단순히 텍스트를 화면에 보여주는 것이 전부던 시절이었다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/450/1*DhztSVRgq_wVLWmYbCju9A.jpeg" /><figcaption>웹러너(WebRunner) 브라우저와 자바 마스코트 듀크(Duke)</figcaption></figure><p>이렇게 단 두 개의 데모가 세상을 바꿔 놓았다.</p><p>자바는 이렇듯 화려하게 세상에 등장했다. 자바는 단순한 텍스트 기반 정보만 제공하던 기존 인터넷에 애플릿이라는 동적 콘텐츠로 브라우저에 생명을 불어 넣었다. 누군가는 느리다고 했지만, 이제 웹은 단순한 정보 전달을 넘어 세상과 소통하고 모든 것을 네트워크에 연결할 수 있게 되었다.</p><h3><strong>“새로운 물결을 만들자”</strong></h3><p>하지만, 자바는 단순히 브라우저에 동적 콘텐츠를 담을 수 있는 애플릿으로 출발한 것은 아니었다. 자바는 1991년 패트릭 노튼(Patrick Naughton), 마이크 세리든(Mike Sheridan), 그리고 제임스 고슬링(James Gosling)에 의해 시작된 비밀스러운 ‘더 그린 프로젝트(The Green Project)’라는 썬마이크로시스템즈의 작은 내부 프로젝트 팀에서 만들어진 프로그래밍 언어이다. 그렇다고 이 프로젝트 팀이 처음부터 새로운 프로그래밍 언어를 만들려고 했던 것은 아니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*UxFuF_yjWuz73UBv7yaK5A.jpeg" /><figcaption>제임스 고슬링과 더 그린 팀 구성원(The Green Team Members)</figcaption></figure><p>처음 13명으로 구성된 그린 팀의 목표는 컴퓨팅에서의 ‘새로운 물결’을 만들어내자는 것이었다. 이 프로젝트 팀은 첫번째 주제로 디지털로 작동되는 디바이스와 컴퓨터가 합쳐질 것이라는 결론을 내렸다. 1991년에 말이다. 그리고 그들이 18개월 동안 문을 걸어 잠그고 만들어낸 것이 바로 ‘*7(StarSeven)’이라는 터치스크린으로 동작하는 반응형 홈 엔터테인먼트 컨트롤러 였다. 이 디바이스는 그 당시에는 상상할 수 없었던 기능을 탑재하고 있었다. 손으로 들고 다닐 수 있는 크기로 네트워크에 연결돼 있었고, 컬러 화면에는 현재는 자바의 마스코트로 유명해진 듀크(Duke)가 손을 흔들고 있었다. 모든 기능이 터치스크린으로 동작했고 심지어 사무실의 전화를 당겨 받을 수 있었으며, 네트워크에 연결된 다양한 가정용 홈 엔터테인먼트 디바이스와 전자제품을 제어할 수 있었다. 사물인터넷(IoT, Internet of Things)의 역사를 이야기 할 때, 최초로 네트워크에 연결된 가전 제품이 2000년 발표된 LG전자의 냉장고라는 것을 상기해 볼 때 정말이지 대단한 발상의 전환이었다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*EZCqw1KqxZJW3sr3ASJAZQ.jpeg" /><figcaption>*7(StarSeven, 스타세븐)</figcaption></figure><p>처음에는 C/C++로 프로그램을 제작하려 했지만, 심지어 애니메이션을 보여주면서 이렇게 다양한 디바이스를 네트워크로 연결하기에는 너무 어렵고 복잡했다. 이러한 새로운 요구사항에 맞춰 제임스 고슬링은 이기종 디바이스를 손쉽게 네트워크에 연결하기 위한 프로그래밍 언어를 만들었는데, 이것이 바로 자바다. 제임스 고슬링은 사무실 창밖에 서있던 나무를 보고 처음에는 이 언어를 ‘<strong>오크(Oak)</strong>’라 불렀다.</p><p>현재 우리는 세상의 모든 것들이 인터넷에 연결된 IoT 세상에 살고 있다. 자바는 태생적으로 이러한 환경에 매우 적합한 기술이다. 1992년 제임스 고슬링이 자바를 만들 때 그는 이미 이러한 세상을 꿈꾼 것은 아닐까?</p><p>이를 증명이라도 하듯, 고슬링은 2010년 오라클을 떠나 지금은 해양정보수집 로봇을 개발하는 벤처기업 ‘리퀴드로보틱스’의 최고 소프트웨어 아키텍트(CSA)로 일하고 있다. IoT, 빅데이터 그리고 클라우드가 바로 고슬링이 꿈꿨던 세상임에 틀림 없을 것이다.<br>​</p><h3><strong>“함께 공부하고 지식을 나누려고 합니다”</strong></h3><p>1999년, 나는 자바 기술 기반의 “OO/OR DB 구축기술 내재화”라는 연구과제에 지금은 한 통신사에서 클라우드를 담당하고 있는 장현춘 선배와 함께 참여하고 있었다. 후에 썬마이크로시스템즈의 자바 팀에서 함께 일하게 된 장현춘 선배가 던졌던 선문답은 언제나 나를 잠못들게 했고 새로운 기술과 개념에 접근하는 계기를 만들어줬다.</p><p>그 날도, 그는 내게 “이 자바 코드는 thread-safe 한가?”라는, 그 당시 나의 지식으로는 매우 어려운 질문을 던졌다. 우리의 이야기를 재미있게 듣고 있던 또 다른 회사 선배가 그날 저녁 우리에게 “함께 공부하고 지식을 나누려고 합니다”라는 제목의 메일을 보내왔는데, 그 선배는 후에 ‘제니퍼’라는 국산 APM으로 유명해진 이원영 선배였다. 그는 연구소 내에서도 단연 뛰어난 엔지니어였는데, 메일의 내용은 회사에서 자바를 시작한 사람들이 함께 커뮤니티를 만들어 지식을 공유하고 함께 아키텍처를 고민하자는 이야기였다. 객체지향 언어인 자바에서 관계형데이터베이스(RDBMS)에 접근하기 위한 DB Wrapper 개념을 설명해준 그의 제안에 나는 망설임 없이 가장 먼저 자바 서비스넷(http://www.javaservice.net/)의 회원이 됐다. 후에 이 커뮤니티를 시작으로 국내에는 자바누리, 자바스터디, JCO 등 많은 정보 공유의 장이 만들어졌다. 커뮤니티를 통한 이러한 지식의 공유는 오픈 소스 기반 자바의 특징이자 가장 큰 장점으로, 자바가 빠르게 발전하고 혁신할 수 있는 근본적인 힘의 원천이 됐다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f_iV5LVJ9oHVU3OmqGNoMA.png" /><figcaption><em>2000년도의 Java Service Net</em></figcaption></figure><p>세계 커뮤니티의 활동, 특히 자바 커뮤니티 프로세스(JCP)를 통해 극적인 발전을 경험한 자바는 현재 해마다 10억건의 다운로드가 일어나며, 기업용 데스크탑의 97%에서 사용 중이다. 또한, 자바는 개발자가 가장 선호하는 프로그래밍 언어이자 기술로써 9백만 개발자를 보유하고 있고, 자바 카드와 스마트 TV, 게이트웨이 등과 같은 디바이스는 70억개나 자바를 실행하고 있다.</p><h3><strong>“The Network is the Computer”</strong></h3><p>처음 자바는 멋지고 쉬운 데스크톱PC용 프로그래밍 언어로 빠르게 세상에 퍼져 나갔다. C/C++과 유사한 모습이라 친근했으며, 개발자의 실수를 자주 만들던 복잡한 포인터 조작이 없고, 메모리 관리도 대신한다는 개념은 개발자를 매료시켰다. 또한, 스윙으로 멋진 GUI 프로그램을 만들 수 있었다. 네트워크 프로그래밍은 너무나도 쉬었다. 1990년대 말에서 2000년대 초에 유행했던 네트워크 기반 게임들은 대부분 자바로 만들어졌다. 느리다고 비판받았지만, 웹브라우저에 플러그인된 애플릿으로 다른 언어로로는 개발하기 어려웠던 다중사용자 환경의 게임을 손쉽게 개발할 수 있었기 때문이다.</p><p>조금은 가볍게, 또는 친근하게 시작한 자바 기술은 1998년 자바2 마이크로 에디션(J2ME)이 발표되면서, 세 개의 플랫폼으로 나뉘어졌다. 소형 디바이스와 임베디드 환경을 위한 J2ME(현재는 Java ME라 불린다), 데스크톱PC 환경에 최적화한 자바2 스탠다드 에디션(J2SE), 그리고 기업용 애플리케이션을 위한 자바2 엔터프라이즈 에디션(J2EE)이 그것이다. 이 분리는 당시 디바이스의 컴퓨팅 한계와 분산환경, 고가용성, 트랜잭션 보증 및 신뢰성, 확장성과 같은 기업용 애플리케이션의 요구사항을 수용하기 위해서는 어쩔 수 없는 선택이었다. 이로 인해 자바가 가진 최대의 장점 중 하나인 이식성 즉, WORA(Write Once, Run Anywhere)라는 최초의 개념이 플랫폼 단위로 분리되는 결과를 낳았다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/602/1*og2IfpMLn_Uqz89vN_zdHQ.jpeg" /><figcaption>2000년대 초 썬에서 사용했던 발표 자료</figcaption></figure><p>하지만, 이러한 분리는 결과적으로는 자바의 엄청난 성공을 가져왔다. 안드로이드(Android)가 등장하기 전까지 ME는 당시 많이 사용됐던 플립형 휴대폰 산업에서 대단한 입지를 구축했다. 우리가 기억하는 대부분의 피처폰(feature phone) 화면, 메뉴와 동적아이콘, 서비스가 대부분 자바 ME 위에서 구축됐다. 유일하게 ME만 라이선스 비용을 지불하는 자바의 플랫폼으로 썬마이크로시스템즈에게 큰 수익을 가져다줬다. 세계에 막대한 양의 휴대폰을 공급하는 굴지의 전자회사를 다수 보유한 한국은 썬마이크로시스템즈에 매우 중요한 나라였는데, 2005년 4월 6일 한국에 자바 리서치센터(Java Research Center)가 설립되고 4년간 5천만 달러 이상이 투자됐다. 한국 자바 리서치센터 개소 당일엔 본사 회장인 스캇 맥닐리(Scott McNealy)가 방한하는 중요한 이정표도 남겼다.</p><p>안드로이드의 등장은 자바로 스마트폰용 모바일 애플리케이션을 개발할 수 있다는 점에서 자바 진영에게 매우 긍정적인 효과를 가져왔으나, 반대로 자바 ME에는 커다란 상처를 남겼다. 최근 오라클과 구글의 소송 결과는 안드로이드가 자바의 권리를 침범했음을 보여주고 있다. 하지만, 어쩌면 안드로이드의 성공은 썬의 ME 전략에 실패가 있었음을 반증하기도 한다. 하지만, 최근 긍정적인 소식도 있다. 오라클의 자바 담당 수석 부사장인 카메론 퍼디(Cameron Purdy)가 자바 9에서는 모든 플랫폼이 모듈화를 통해 다시 통합된다고 발표한 것이다. 처음 자바가 지향했던 플랫폼 독립(Platform Independence)라는 이상적인 개념이 모듈화를 통해 다시 주목받고 있으며, 클라우드와 IoT 시대에 자바가 디바이스에서 클라우드까지 광범위하게 사용될 것임을 확실하게 보여주고 있다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*TCellS-_UJQEgJs9vK-v5Q.jpeg" /><figcaption><em>썬의 슬로건, “The Network is the Computer”</em></figcaption></figure><p>자바가 가는 길을 따라 온 나는 2003년 여름, 자연스럽게 자바가 태어난 곳 ‘썬마이크로시스템즈’의 자바 팀에 합류했다. 처음 회사에 출근해 인상깊었던 것이 바로 ‘스윙의 그 보라색’이 선명한 회사 로고 아래의 <strong>“The Network is the Computer”</strong>라는 썬의 테마이자 슬로건이었다. 어쩌면 “네트워크가 바로 컴퓨터”라는 이 단순한 문장을 이해하는 것이 자바가 태어나 어디로 가고 있는지 확실히 알 수 있는 것은 아닐까?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IScDg2LziBsaNFsaVpDI1A.jpeg" /><figcaption>최근까지도 한국 오라클 사무실에는 썬의 슬로건 남아 있었다. 굿바이 썬~</figcaption></figure><h3><strong>“나는 J2EE에 집중할까 해”</strong></h3><p>썬에 입사하기 전인 2000년 초, 나는 벤처에서 웹사이트에 접속한 고객의 구매의사(Buying Intention)를 실시간으로 파악하고 오퍼링(Offering)을 하는 고성능 CRM 엔진을 자바로 개발하고 있었다. 어느날 오랜만에 장현춘 선배에게 전화가 왔고 이런 저런 자바 이야기를 나눴다. 그는 J2EE(지금은 Java EE라 부른다)가 얼마나 중요하고 주목해야 하는지 설명했다. 그는 내게 “나는 J2EE에 집중할까 해”라고 말하며 나도 곧 그 길을 따라올 것이라고 웃었다. 그의 예견처럼 몇 년이 지나지 않아 나는 서비스지향아키텍처(SOA) 담당자로 썬마이크로시스템즈의 자바 팀에 합류했으니, 그의 안목은 예나 지금이나 내게 많은 영향을 주고 있다.</p><p>1999년 12월 공식적으로 J2EE 1.2라는 기업용 버전의 자바 플랫폼이 발표됐다. J2EE는 빠르게 기업용 애플리케이션 시장에 변화를 줬다. 다른 의미에서 J2EE의 등장은 BEA의 웹로직(WebLogic)과 같은 웹애플리케이션서버(WAS, Web Application Server)라는 미들웨어(Middleware)의 등장을 의미했다. WAS 또는 앱 서버(App Server)로 불리우는 이 미들웨어는 기업용 애플리케이션이 제공해야할 분산환경(Distributed Environment), 신뢰성(Reliability), 고가용성(High Availability), 확장성(Scalability) 등을 지원하며 빠르게 턱시도(Tuxedo)와 같은 TP-Monitor의 자리를 대체해 나갔다. 국내 거의 모든 기업이 앞다퉈 자바 프로젝트를 수행했고 빠르게 자사 애플리케이션을 웹 기반으로 변경하던 시기였다.</p><p>단순히 프로그래밍 언어로 알려진 자바는 이렇듯 기업용 애플리케이션을 위한 플랫폼으로 빠르게 진화했고, 그 결과는 매우 성공적이었다. WAS에 이어 엔터프라이즈 아키텍처를 지원하기 위한 다양한 미들웨어가 등장했는데, 대표적인 것으로는 SOA의 근간이 되는 ESB( Enterprise Service Bus)와 BPM(Business Process Management)이 있고, 비동기 메시지 전송을 위한 MOM (Message Oriented Middleware)에 해당되는 JMS(Java Messaging Service)가 있다. 이 외에도 다양한 디바이스가 연결되는 환경인 RFID(Radio-Frequency Identification)와 USN(Ubiquitous Sensor Network)을 지원하기 위해 RFID 미들웨어 제품군이 등장했고, 최근에는 IoT와 빅데이터 환경에서 발생하는 실시간 이벤트를 초고속으로 처리하고 분석하는 CEP(Complex Event Processing)와 방대한 양의 빅데이터를 분산 파일 시스템상에서 병렬적으로 처리하기 위한 맵리듀스(MapReduce) 같은 빅데이터 관련 기술도 자바로 제작되고 있다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fhrTuk1jqDFzpTslwsHc3A.png" /><figcaption>Middleware의 종류와 특징</figcaption></figure><p>2006년 어느날, 나는 썬의 RFID 미들웨어에 한참 빠져 있었다. 이 제품은 지금은 시들해졌지만 그 당시 “플러그 앤드 워크(Plug &amp; Work)”라는 콘셉트로 유명했던 지니(Jini) 기술로 만들어 졌는데, OSGi(Open Service Gateway initiative)와 마찬가지로 모듈화가 가능했고 가벼웠으며 RFID 리더와 같은 이기종 디바이스를 연결하는데 최적의 기술이었다. 지니의 최초 데모가 네트워크 프린터가 설정 없이 연결되고 인쇄버튼을 누르자 이 프린터를 네트워크 상에서 찾아 듀크를 인쇄하는 것이 었으니, 이미 그 시절 자바 기술은 지금의 IoT 세상을 향해 나아가고 있었음에 틀림없었다.</p><h3><strong>“EJB is DEAD!”</strong></h3><p>썬의 기업용 플랫폼인 자바 EE의 발전은 웹의 발전과 함께 그리고 MS의 닷넷(.Net)과 경쟁을 통해 기업용 애플리케이션을 웹 기반으로 전환하는 계기를 마련했다. Java EE는 이에 따라 기업이 요구하는 다양한 기능을 추가하며 발전하게 됐는데, 문제는 이러한 다양한 기능의 추가가 결과적으로 WAS를 매우 무겁고(Heavy -weight) 어려운 플랫폼으로 만드는 역설이 일어났다. 대표적인 예로 분산환경 아래 자바 애플리케이션과 RDBMS간 트랜잭션, 그리고 영속성(Persistency)을 지원하기 위한 EJB의 복잡성 문제다. EJB(Enterprise JavaBeans)의 스펙은 방대했으며 복잡한 콜백(call-back) 체계는 개발자가 이해하기에 매우 난해했다.<br>​<br>썬의 자바 팀에는 장현춘 선배가 먼저 합류해 있었다. 그 때부터 함께 일하게된, 현재는 NHN에 Lab장으로 근무하고 있는 이성진 선배도 빼놓을 수 없는 자바 전문가이다. 그는 매우 중요한 여러 기업용 자바 프로젝트의 아키텍처를 담당했다. 엔지니어라면 누구나 그렇겠지만, 이성진 선배와 나는 상업용 제품을 개발하고 싶은 공통된 열망이 있었고 후에 우리는 BEA 연구소에 SOA 제품군(당시에는 아쿠아로직 ‘AquaLogic’ 이라 불렀다)을 개발하는 엔지니어로 함께 참여했다. BEA 연구소에서는 현재 Google에서 클라우드를 담당하고 있는 정명훈이란 뛰어난 자바 엔지니어를 만났다. 그는 왜 BEA가 한국에서 뛰어난 결과를 가져왔는지 증명해준 친구로 나와는 지금도 다양한 주제로 이야기를 나누는 친구가 됐다.<br>​<br>세 명은 자주 모여 여러 자바 이야기를 나누곤 했는데, 어느날, 장현춘 선배가 충격적인 기사를 읽었다며 그 이야기를 들려 주었다. 무시무시한 그 기사의 제목은 <strong>“EJB는 죽었다”</strong>였다. 이제 더이상 EJB가 사용되지 않으며 그 이유는 복잡성과 너무 무거운 특성 때문이라고 했다. 우리 모두 그 사실에 동의할 수 밖에 없었지만, 사실 EJB는 죽지 않았다. 역설적으로 이러한 EJB의 문제는 J2EE 컨테이너 기반이 아닌 POJO(Plain Old Java Object) 기반 경량 프레임워크와 경량 컨테이너(Light-weight Container) 기술의 발전을 가져왔다. 스트러츠(Struts)와 하이버네이트(Hybernate)를 거쳐 최근의 스프링(Spring), 마이바티스(MyBatis)와 같은 POJO 기반 프레임워크의 발전과 함께 OSGi와 RESTful 컨테이너를 결합하는 형태의 경량 컨테이너 기술의 발전을 가져왔다. 이러한 변화는 결국 EJB 3.0도 다시 POJO 기반의 프레임워크로 변화되는 결과를 가져왔다.</p><h3><strong>“이거 봐, 5%나 느리네!”</strong></h3><p>2004년 어느날, 한 증권사에서 iSAM 파일 읽기/쓰기에 대한 자바와 C 프로그램의 품질성능평가시험(BMT) 결과를 발표하고 있었다. 결과적으로 자바의 NIO(Non-blocking IO)가 C 기반의 기존 프로그램보다 5% 정도 느린 성능을 나타냈지만, 그 결과는 시스템 운영에 문제가 없는 수준이었다. 하지만, 자바로 전환을 반대하는 한 고객은 이 결과를 “5%나 느린 기술”이라고 깎아내렸다. 이에 대한 대답으로 나는 어셈블리보다 느린 C 언어를 고수하는 이유를 물었다. 머뭇거리던 고객에게 나는 5% 부족한 성능을 채우고도 남을 자바의 장점들, 예를 들어 개발 생산성, 유지 보수성, 이식성, 확장성, 안정성 등 자바가 제공하는 가치를 설명하고 이해를 구했다. 그 이후로도 많은 저항에 부딪혔지만, 결과적으로 이 고객사는 자바를 선택하고 정보계를 자바 기반으로 전환했다.</p><p>I/O와 함께 지적받는 유명한 자바의 문제는 GC(Garbage Collection) 매커니즘으로 인한 성능 저하와 메모리 누수(Memory Leaks)다. 자바는 기본적으로 메모리 상에 생성된 객체(인스턴스, Instance)에 대한 메모리 해제(Deallocation)를 JVM에게 위임하는 모델이다. 이는 개발자의 오류로 야기될 수 있는 메모리 관리의 문제를 근본적으로 해결할 뿐만 아니라, 코드를 단순화해 개발 생산성과 유지 보수성을 높일 수 있는 장점이 있다. 하지만, 마크 앤드 스윕(Mark and Sweep)의 개념을 근간으로 하는 자바 초기 GC의 매커니즘은 메모리 해제 작업 중에 애플리케이션의 모든 쓰레드의 동작이 멈추는 현상, 소위 말해 ‘Stop the World’의 문제를 내포하고 있었다. 결과적으로 ‘Stop the World’란 애플리케이션이 응답없이 지연(Latency)이 발생한다는 것을 의미 한다.<br>​<br>초기의 이 문제는 비교적 짧은 시간의 Full GC로 인해 큰 문제로 부각되지는 않았다. 하지만, 자바가 기업용 애플리케이션으로 사용되면서 매우 빈번한 호출을 감당해야 했고, 보다 많은 물리적 메모리를 사용하게 됨으로써 Full GC로 인한 “Stop the World” 시간 지연이 미션 크리티컬(Mission Critical)한 기업용 애플리케이션에서 큰 문제를 야기했다. 불특정 다수의 고객에게 서비스를 하고 있던 WAS가 무려 몇 초 동안 동작을 멈추는 상황을 상상해본다면 이 문제가 얼마나 큰 이슈였는지 알 수 있다.<br>​<br>지금은 작고하신 썬의 자바 팀 수장이셨던 당시 황성훈 이사께서 2004년 겨울 어느날, 다급하게 나를 불렀다. 광양의 한 제철소에 GC 관련 이슈로 지원을 해야 한다는 요청이었다. 다양한 센서로부터 실시간으로 들어오는 센싱 데이터(Sensing Data)를 처리하고 있던 자바 애플리케이션은 썬의 WAS위에서 동작하고 있었는데 GC로 인한 “Stop the World”로 인해 장애가 발생하는 상황이었다. 이러한 애플리케이션을 지금은 IoT 또는 M2M으로 부르지만, 그 당시 기술로는 이를 해결하기 매우 힘든 상황이었다. 다행히도 때마침 출시된 멀티쓰레드 GC 방식의 ‘Parallel GC’ 그리고 “Stop the World” 시간을 최소화 할 수 있는 ‘CMS(Concurrent Mark and Sweep)’을 통해 이러한 문제를 해결할 수 있었다. 재미있는 것은 그 당시 WAS 인스턴스에 할당된 메모리가 겨우 256MB라는 걸 생각해보면 참으로 IT의 발전 속도가 얼마나 빠른지 다시 한번 느낄 수 있다.<br>​<br>현재의 자바 성능은 어떤가? 아직도 자바는 느리다는 편견을 가지고 있는 독자가 있다면 다음 이야기에 귀를 기울이길 바란다. 오라클의 많은 미들웨어 제품 중에서 CEP(Complex Event Processing) 제품으로 오라클 스트림 익스플로러(Oracle Stream Explorer, 현재는 Oracle Stream Analytics라 부른다)가 있다. 이 제품은 2007년 발표된 BEA의 웹로직 이벤트 서버(WebLogic Event Server)에 그 뿌리를 두고 있는데, 재미있게도 BEA 웹로직의 기반이 아닌 OSGi 기반의 경량 컨테이너 제품이다. 이 제품이 OSGi를 채택한 이유는 간단하다. 매우 빠른 속도로 이벤트를 처리하고 자바가 처음 그랬듯 다양한 이기종 디바이스를 유연하게 연결하기 위해서다. 그 속도는 어떨까? C보다 느리다는 자바는 1초에 몇 개의 이벤트를 처리할 수 있을까? 놀랍게도 오라클 스트림 익스플로러는 오라클 엑사로직(Exalogic)의 1개의 노드(Node, 24 cores)에서 <strong>초당 100만 건의 이벤트를 처리</strong>하고 최신 오라클 SPARC T5 서버에서 <strong>초당 400만 건의 이벤트를 처리</strong>한다. 이 값을 역산하면 밀리초(millisecond) 보다 작은 마이크로초(microsecond) 단위로 데이터를 처리한다는 것을 의미한다. 그래서 이 제품이 IoT와 빅데이터의 실시간 이벤트 처리 엔진으로 사용되는 것이다. 이 벤치마크 테스트의 절차와 그 결과 데이터는 오라클 홈페이지에 공개돼 있다.</p><p>이렇듯 자바는 그 성능을 향상하기 위해 다양한 방법으로 노력을 기울여 왔다. 자바 7부터 공식적으로 사용가능하게 된 G1 GC는 기존의 GC 개념을 획기적으로 발전시켜 놓았다. G1 GC는 멀티 프로세서 환경에서 대용량의 메모리가 사용되는 현대 애플리케이션을 목표로 새롭게 개발된 GC 메커니즘으로 간단히 말해 메모리 영역을 작은 단위(Region)로 나눠 CMS를 수행한다. 즉, 작은 영역별로 나뉘어진 메모리에 병렬로 GC를 수행해 지연이 최소화된 최적의 중단 없는 고성능 자바 실행 환경을 제공한다.</p><h3><strong>“자바 is DEAD?”</strong></h3><p>자바의 운명이 언제나 순탄했던 것만은 아니었다. 썬마이크로시스템즈는 운명을 다하고 2010년 1월 27일 공식적으로 오라클에 인수됐다. 오라클의 썬 인수 후 몇 년간 자바는 모회사의 운명처럼 큰 난관에 직면했는데, 그 것은 바로 유명한 제로데이 취약성(Zero-day vulnerability) 문제였다. 2013년에는 미국 국토안보부(Department of Homeland Security, DHS)가 정말 불가피한 상황이 아니라면 자바를 사용하지 말라고 권고하는 사태에 까지 이르렀다. 이 문제에 관해 그 당시 자바의 아버지 제임스 고슬링은 다음과 같이 말했다.<br>​<br><em>“내 판단으로는 분노와 위안이 교차하고 있었다. 상당 부분은 오라클의 자바 인수와 관련이 있었다. 자바와 같이 인터넷에서 주로 사용되는 제품은 문제를 그 즉시 해결할 의지와 능력이 있어야 한다”</em><br>​<br>이로 인해 그 당시 많은 사람들이 자바의 생명력이 다했다고 주장했다. 구글에서 “Java is dead”를 검색해보면 22,000,000건 이상의 결과를 볼 수 있으니 그 당시 얼마나 큰 이슈로 부각되었는지를 알 수 있다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*p9UJRpRif-SPiAZ0NjFsbA.jpeg" /><figcaption>구글 트렌드의 “Java is dead” 검색결과. 아무도 의심하지 않던 자바의 운명을 2009년 말 오라클 인수 때부터 많은 사람이 걱정하기 시작했다. 출처: 구글 트렌드(trends.google.com)</figcaption></figure><p>하지만 자바는 죽지 않았다. 이 글을 쓰고 있는 순간에도 당연히 살아 있다. 자바 6 시절의 시련을 이겨낸 후 변화하며 새로운 혁신을 만들어 내고 있다. 오라클을 떠난 제임스 고슬링도 이 점을 인정했다. 최근 그는 <strong>“아주 만족한다. 오라클이 기대 이상으로 잘하고 있다. 사실 자바가 생명력을 크게 잃을 것이라고 생각했었다. 그런데 오라클이 자바를 잘 관리했다”</strong>고 말했다.<br>​<br>보안 취약성으로 야기된 문제를 해결하기 위해 오라클은 많은 노력을 기울였다. 그 결과 2014 폰투오운(Pwn2Own) 해커 콘테스트에서 유일하게 해킹할 수 없었던 표적이 자바였다. 2015 콘테스트에서는 자바가 아예 표적에서 제외됐다. 해킹을 성공시킬 수 있는 해커가 없기 때문이다.</p><h3><strong>“Java is ALIVE!”</strong></h3><p>오라클은 2014년 자바 8을 출시하면서 자바의 릴리즈 사이클(Release Cycle)을 다시 2년으로 되돌릴 것이라고 공식 발표했다. 8의 가장 큰 변화 중에는 함수형 언어로써 람다(Lambda)를 지원하고, 콜렉션(Collection) 처리를 멀티 프로세서 기반 하에서 빠르게 처리할 수 있도록 도와주는 스트림(Stream)의 지원이 있다. 그리고 최근 오라클은 자바 인수 후 가장 큰 혁신인 자바 9를 발표했다. 자바 9의 기능 중에 가장 중요한 것 중에 하나가 바로 자바의 내부 API 들을 모듈화해서 관리하는 프로젝트 직소(Project Jigsaw)다. 퍼즐과 같은 모듈화를 통해 자바는 다시 한번 플랫폼의 통합을 가져올 것으로 기대되고 있다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/525/1*tgdzBYGpvT1shOKmsS68jg.png" /><figcaption>Java Development Kit (JDK) 9 is set for release on 21 September, 2017</figcaption></figure><p>앞서 살펴본 것 처럼, 자바는 태어날 때 부터 다양한 디바이스를 네트워크에 연결할 수 있고 통신할 수 있으며, 분산환경에 적합한 솔루션이었다. 이렇듯 세상의 모든 것들이 인터넷에 연결되는 IoT에 가장 적합한 것이 바로 자바 기술이다. IoT를 통해 다양해진 채널로부터 방대한 양의 데이터가 발생되는데 이를 분석해야 하는 곳에도 바로 자바가 있다. 바로 빅데이터 그리고 패스트데이터(Big Data and Fast Data) 분석 영역이다. IT 인프라스트럭처를 구매해 유지하던 시대는 저물고 필요한 컴퓨팅 자원이나 플랫폼 그리고 소프트웨어를 사용한 만큼 임대하는 클라우드(Cloud) 시대에 자바는 반드시 필요한 기술이 됐다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/440/1*iHoYes3o-yej7YR-HXreyw.jpeg" /><figcaption>Java, 1# Development Platform is Now in the Cloud</figcaption></figure><p>2015년 6월, 오라클은 클라우드 벤더로 새롭게 태어났음을 선언했다. 그 중심에 오라클의 DBMS, 웹로직, SOA 제품군, IoT, 빅데이터를 아우르는 자바 기술 기반 PaaS인 ‘오라클 클라우드 플랫폼’이 있다. 자바는 이렇듯 혁신을 통해 세상에 건재하다.</p><p>1998년 그날 밤, 나는 그의 물음에 이렇게 답했다.​</p><blockquote><strong>“그냥…. 이 스윙의 보라색이 좋아. 다르잖아”</strong></blockquote><p>​<br>우리 모두는 알고 있다. 자바는 계속해서 놀라움을 주는 기술로 발전할 것임을… 처음 세상에 등장한 후 20년을 혁신한 것처럼!<br>​<br><strong>“Happy Birthday, Java!”</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/470/1*CFKAXaz2JK0OTqAW5dT23A.jpeg" /><figcaption>20 years of Java Innovation!</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=37dc915f00f1" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 7]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/2435b4a55ccd</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[dcgan]]></category>
            <category><![CDATA[기계학습]]></category>
            <category><![CDATA[머신러닝]]></category>
            <category><![CDATA[생성모델]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Tue, 05 Sep 2017 05:43:41 GMT</pubDate>
            <atom:updated>2017-11-09T13:25:11.443Z</atom:updated>
            <content:encoded><![CDATA[<p>생성적 적대 신경망(Generative Adversarial Networks)을 활용해 8비트 픽셀 아트(Pixel Art) 만들기</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p>생성 모델(Generative models)을 활용하면, 컴퓨터가 스스로 사진, 영화 또는 음악과 같은 데이터를 만들 수 있습니다.</p><p>1년보다 조금 더 전에, <a href="https://twitter.com/alecrad">Alec Radford</a>는 (<a href="https://twitter.com/goodfellow_ian">Ian Goodfellow</a>의 작업을 기반으로) 기계 학습을 이용해 생성 모델을 만드는 방법에 대한 사람들의 이전 생각을 변화시킨 논문을 발표했습니다. 이 새로운 시스템을 딥 컨볼루션 생성적 적대 신경망(Deep Convolutional Generative Adversarial Networks 또는 간단히 DCGAN)라고합니다.</p><p>DCGAN은 서로 경쟁하는 두 개의 딥 신경망을 영리하게 조합해서 진짜 사진같은 현실적 그림을 상상해 낼 수 있습니다. 다음의 모든 침실 그림들은 DCGAN이 상상해 낸 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*n63ADMch0NU4RxLDt04rHA.png" /><figcaption>Alec Radford의 <a href="https://arxiv.org/abs/1511.06434">DCGAN 논문</a> 에서 가져온 사진</figcaption></figure><p>AI 연구자들은 생성 모델에 관심이 많습니다. 왜냐하면, 이 생성 모델이 세상으로부터 원시 데이터(raw data)를 소비하고 자동적으로 이를 이해하는 인공 지능 시스템의 제작으로 나아갈 징검다리 역할을 할 것으로 보이기 때문입니다.</p><p>하지만, 지금은 생성 모델을 사용해 약간 덜 똑똑한 것을 해보겠습니다 — 8비트 비디오 게임용 배경 그림을 만들어 보겠습니다!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*oL9SLILkis_iWvVbbi56FQ.png" /><figcaption>이 게임 레벨에서 보이는 모든 그림은 컴퓨터가 생성한 것입니다.</figcaption></figure><h3><strong>생성 모델의 목표</strong></h3><p>그렇다면 왜 AI 연구자들은 조금은 어색한 침실 사진을 생성하기 위해 이런 복잡한 시스템을 구축하는 걸까요?</p><p>그 이유는 당신이 어떤 것을 확실히 이해하고 있어야만, 어떤 그림을 생성할 수 있다는 생각 때문입니다.</p><p>다음 그림을 보시죠:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*wxZbdCEdGyGed5aiHRY2Lw.jpeg" /><figcaption>개, 정확히는 나의 개.</figcaption></figure><p>당신은 이것이 네 개의 다리와 꼬리가 있는 털이 난 것 즉, 개 사진이라는 것을 바로 알 수 있습니다. 그러나 컴퓨터에게 사진은 각 픽셀의 색상을 나타내는 숫자 그리드일 뿐입니다. 컴퓨터는 그림이 어떤 개념을 나타낸다는 것에 대한 이해가 없습니다.</p><p>그러면, 이제 수천 장의 개 사진을 컴퓨터에 보여주었고, 컴퓨터가 이제 다양한 개 품종과 다른 사진 각도로 자신만의 개 사진을 생성할 수 있게 되었다고 상상해 보십시오. 어쩌면 우리는 “비글(beagle)의 옆모습”과 같은 특정 유형의 사진을 요청할 수도 있을 것입니다.</p><p>컴퓨터가 이렇게 할 수 있고 생성된 사진들이 정확한 다리의 개수, 꼬리, 귀가 있다면, 명시적으로 말해주지 않아도 컴퓨터가 어떤 부분들로 “개”가 구성되는 지 알고 있다는 것을 증명해 줄 것입니다. 따라서 어떤 의미에서 좋은 생성 모델은 최소한 유아 수준의 기본적인 이해를 한다는 증거입니다.</p><p>그래서 연구자들은 생성 모델을 구축하는 것에 대해 매우 열광하고 있습니다. 이 것은 어떤 개념의 의미를 명시적으로 가르치지 않고, 그 개념을 이해하도록 컴퓨터를 훈련시키는 방법으로 보입니다. 사람이 정말 힘들게 미리 라벨을 붙여둔 훈련 데이터로 부터만 학습할 수 있는 현재의 시스템에서 한 걸음 크게 진보하는 것입니다.</p><p>그런데, 이 모든 연구 결과로 개 그림을 생성하는 프로그램이 만들어 진다면, 부수적으로 컴퓨터가 만든 첫 번째 Dog-A-Day(하루에 개 사진 한장) 달력이 발간되는데 몇 년이나 걸릴까요?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*2yxJ7tH-r5VqpYONTul1RA.jpeg" /><figcaption>네, 로봇이 모든 사람들의 일을 대신할 것입니다. 결국은.</figcaption></figure><p>개를 이해하는 프로그램을 제작할 수 있다면, 다른 것을 이해하는 프로그램은 안될 이유가 없겠지요? 사람들이 악수하는 사진을 무한대로 생성 할 수있는 프로그램은 어떻습니까? 나는 누군가 분명히 돈을 지불할 것이라 확신합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/605/1*KvDZxiFws9ynJs0doaEn0Q.jpeg" /><figcaption>제 말은.. 확실히 AI 스타트업 아이디어치곤 끔찍하네요. 하지만 제가 들은 더 나쁜 스타트업 아이디어들도 있었습니다. 그래서…. 어쩌면?</figcaption></figure><p>좋습니다. 아마도 쓸데 없는 사진들을 생성하는 프로그램은 그다지 흥미롭지 않을 것입니다. 하지만 겨우 일년 사이에 생성 모델이 발전한 것을 감안해 보면, 우리가 5 년에서 10 년 사이에 어디까지 와 있을지 누가 알 수 있겠습니까. 누군가 영화 전체를 생성하는 시스템을 발명한다면 어떻게 될까요? 아니면 음악? 아니면 비디오 게임?</p><p>눈을 가늘게 뜨고 20–30년 후를 내다본다면, 엔터테인먼트의 100%가 컴퓨터에 의해 만들어 지는 세상를 이미 상상할 수 있습니다:</p><p>비디오 게임 산업은 <a href="https://www.technologyreview.com/s/601258/artificial-intelligence-can-now-design-realistic-video-and-game-imagery/">AI을 사용한 기본 컨텐츠(raw content) 생성을 진지하게 실험</a>하기 시작한 엔터테인먼트의 첫 번째 분야입니다. 컴퓨터 게임 엔지니어들과 기계 학습 엔지니어들은 명백하게 벤 다이어그램 상 중복이 있는 것은 사실이지만, 최근의 <a href="https://en.wikipedia.org/wiki/AAA_%28video_game_industry%29">AAA 비디오 게임</a> 예산이 3억 달러(대략 3천억원)가 넘는 다는 것을 감안해 볼 때 비디오 게임의 개발 자동화에 투자하는 것은 충분히 비용 대비 효과가 있습니다.</p><blockquote>*역자주: “AAA 비디오 게임”(triple A 라 읽음)이란 대규모 예산으로 개발하고 대대적 홍보를 하는 최근의 게임을 말하며, 일반적으로 수익을 얻기 위해서는 대량의 판매가 이루어져야하는 위험성이 있다.</blockquote><p>아직은 기계 학습 기반의 생성 모델이 초기 단계에 있고 실제 사용 범위도 아직은 매우 좁습니다. 하지만 놀아 볼만한 충분한 재미가 있습니다. 그럼 우리가 할 수 있는 것을 하나 살펴보겠습니다.</p><h3><strong>DCGAN의 동작 방식</strong></h3><p>DCGAN을 구축하기 위해서는, 두 개의 딥 신경망(deep neural networks)를 생성해야 합니다. 그런 다음 서로 싸우며 끝없이 상대방을 능가하도록 만듭니다. 그 과정에서 이 둘은 모두 강해지게 됩니다.</p><p>첫번째 딥 신경망을 위조 지폐를 감별할 수 있도록 훈련된 신입 경찰관이라고 가정해 보겠습니다. 그의 업무는 어떤 사진을 보고 그 사진에 진짜 돈이 있는지 알려주는 것입니다.</p><p>우리가 그림 속에서 객체를 찾고 있기 때문에, 이 작업은 표준 컨볼루션 신경망(<a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.ib3p63rx3">Convolutional Neural Network</a>)를 사용할 수 있습니다. 만약 ConvNets(컨볼루션 신경망)이 익숙하지 않다면, <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa">이전 글을 읽어보시기 바랍니다</a>. 그러나, 기본적인 아이디어는 신경망이 이미지를 가져와 이미지에서 매우 복잡한 형상을 인식하는 몇가지 단계를 통해 이를 처리 한 다음 단일 값을 출력한다는 것입니다. 이 경우에는, 특정 이미지에 진짜 돈의 사진이 있다 또는 없다.</p><p>이 첫 번째 신경망을 <strong>선택기(Discriminator)</strong>라 부릅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*OP7BD-KgnmFPvUyqicIaFw.png" /><figcaption>선택기 신경망(The Discriminator Network)</figcaption></figure><p>이제 두 번째 신경망은 가짜 돈을 만드는 법을 막 배운 신입 위조범이라고 가정해 보겠습니다. 이 두 번째 신경망을 위해, 일반적인 ConvNet의 단계들을 뒤집어 모든 것이 반대로 돌아가도록 하겠습니다. 따라서, 사진을 가져와 값을 출력하는 대신 값 목록을 가져와 사진을 출력합니다.</p><p>이 두 번째 신경망을 <strong>생성기(Generator)</strong>라 부릅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*62gEj921YoXt2XSjklj3yQ.png" /><figcaption>생성기 신경망(The Generator Network)</figcaption></figure><p>이제 우리는 가짜 돈을 찾는 경찰관(<strong>선택기, Discriminator</strong>)과 가짜 돈을 인쇄하는 위조범(<strong>생성기, Generator</strong>)가 있습니다. 이들이 싸우게 해보겠습니다!</p><p>첫 번째 라운드에서 <strong>생성기</strong>는 거의 돈 같지 않은 안타까운 위조 지폐를 만들어 낼 것입니다. 왜냐하면 돈이 어떻게 생겼는지 전혀 모르기 때문입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ynH4JL1D_v8KIe67HrEBmw.png" /><figcaption>생성기가 첫 번째 (형편 없는) 위조 지폐를 만들었습니다</figcaption></figure><p>그런데, 현재 <strong>식별기</strong>도 돈을 인식하는 일에 똑같이 형편 없기 때문에 별 차이가 없습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*FUQ6C3oiErZn5gmJcOwgaQ.png" /><figcaption>식별기는 이 돈이 진짜라고 생각합니다!</figcaption></figure><p>이 시점에 우리가 개입해서 식별기에게 이 달러 지폐가 실제로는 가짜라고 말해줍니다. 그런 다음 진짜 달러 지폐를 보여주고 이 것이 가짜와 어떻게 다르게 보이는지 묻습니다. 식별기는 가짜 지폐와 진짜 진폐를 구분하는데 도움이 되는 새로운 세부 정보를 찾아낼 것입니다.</p><p>예를 들어, 식별기는 진짜 돈에는 사람 그림이 있고 가짜 돈에는 없다는 것을 알아챌 것입니다. 이 지식을 활용해서, 식별기는 가짜와 진짜를 구분하는 법을 배우게 됩니다. 이제 일하는 것이 조금 나아졌습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*TJRLWUSjw1N17gHMygQsPA.png" /><figcaption>식별기 수준이 올라갔습니다! 이제 아주 별로인 가짜돈을 구분할 수 있습니다.</figcaption></figure><p>이제 2 라운드를 시작합니다. 우리는 생성기에게 그 지폐 이미지가 가짜로 갑자기 거부되어 이제 게임의 수준을 높여야 한다고 알려줍니다. 또한 식별기가 이제 얼굴을 찾고 있어서, 식별기를 혼란시키는 가장 좋은 방법은 지폐에 얼굴을 넣는 것이라고도 알려줍니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*2BwOYGF_lLGvZCtKcJ7uDQ.png" /><figcaption>생성기가 아주 조금 나아진 위조 달러를 만들었습니다.</figcaption></figure><p>이제 가짜 지폐가 다시 유효한 것으로 받아 들여지고 있습니다! 이제 식별기는 진짜 달러를 다시 보고 가짜를 구분해 낼 새로운 방법을 찾아야 합니다.</p><p>생성기와 식별기가 벌이는 이 시소 게임은 이 두 신경망이 모두 전문가가 될 때까지 수천 번 계속됩니다. 결국에 생성기는 거의 완벽한 위조 지폐를 만들어내고 식별기는 사소한 실수도 찾아내는 전문 형사가 되었습니다.</p><p>두 신경망이 충분히 훈련되어 인간이 만들어진 가짜 이미지에 깊은 인상을 받는 시점이 되면, 우리는 원하는 어떤 목적으로든 이 가짜 이미지를 사용할 수 있을 것입니다.</p><h3><strong>비디오 게임에 적용하기</strong></h3><p>이제 DCGAN이 어떻게 동작하는지 알게 되었으니, 이를 1980년대 스타일의 비디오 게임용 새로운 배경을 만드는데 사용할 수 있는지 알아 보겠습니다.</p><p>자 이제 실제 게임의 스크린샷을 기반으로 NES 즉, 닌텐도 엔터테인먼트 시스템(Nintendo Entertainment System) 용 가상의 비디오 게임의 스크린샷을 만들어낼 DCGAN을 제작해 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Daqh1Qpf63hRU4AxPIVRzg.png" /></figure><p>이 아이디어는 상상속 비디오 게임의 그럴듯한 스크린샷을 생성 할 수 있다면, 이 그림의 일부를 카피 앤 페이스트해서 우리의 복고 스타일 비디오 게임에 사용할 수 있다는 것입니다. 생성된 비디오 게임은 존재한 적이 없기 때문에, 심지어 도용 당하지도 않을 것입니다 (아마도.. 이 후에는 많을수도).</p><p>그 당시의 비디오 게임용 화면은 매우 간단했습니다. NES에는 아주 적은 양의 메모리만 있었기 때문에(이 게임들은 지금 이 글이 뜨는데 사용하는 메모리보다도 적은 메모리를 사용했었습니다), 프로그래머들은 게임용 그림을 메모리에 맞추기 위해 다양한 트릭을 사용해야 했습니다. 제한된 공간을 최대로 활용하기 위해서, 게임의 각 화면은 몇 개의 (일반적으로 16x16 픽셀) 반복되는 그래픽 타일로 구성되는 타일 기반 그래픽(tile-based graphics)을 사용했습니다.</p><p>예를 들어, ‘젤다의 전설(<a href="https://en.wikipedia.org/wiki/The_Legend_of_Zelda_%28video_game%29">The Legend of Zelda</a>)’의 시작 화면은 단지 8개의 고유한 타일로 만들어 졌습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*wQ2g9KcQDFWmt2iPCOlQPg.png" /></figure><p>다음은 ‘젤다의 전설” 전체 맵을 구성하는 타일들입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*mZ0OWcYMonCDlQU6sHXltw.png" /><figcaption>가끔은 주변 색을 바꿔 다른 영역처럼 보이게도 만들었습니다. 간단하죠.</figcaption></figure><p>우리의 목표는 우리만의 게임을 위한 비슷한 타일를 만드는 것입니다. 그렇기 때문에 우리가 만들어낸 게임 스크린샷이 완벽하게 사실적인 것처럼 보이는지 신경쓸 필요가 없습니다. 그 대신에 우리는 단지 우리의 게임에서 돌, 물, 다리 등과 같은 16 x 16 타일로 사용할 수 있는 모양과 패턴을 찾을 것입니다. 이렇게 하면, 이 타일들을 사용해서 우리만의 8 비트 스타일 비디오 게임 레벨을 구축할 수 있습니다.</p><h4>데이터 수집하기</h4><p>우리의 시스템의 훈련시키기 위해서, 많은 데이터가 필요합니다. 다행히도 데이터를 가져올 수 있는 <a href="https://en.wikipedia.org/wiki/List_of_Nintendo_Entertainment_System_games">700개가 넘는 NES용 게임들</a>이 있습니다</p><p><a href="http://www.vgmuseum.com/nes.htm">비디오 게임 박물관 웹 사이트</a>에서 모든 NES 게임들의 스크린샷을 다운로드하기 위해 <a href="https://www.gnu.org/software/wget/">wget</a>을 사용했습니다(그쪽 사이트를 스크랩해서 죄송해요!). 몇 분 정도 다운로드 하자, 수백개의 NES 게임 스크린샷이 10,000개 조금 넘게 생겼습니다</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Q_vlkb6RXB_bY2d9Sn7fTw.png" /><figcaption>데이터 세트를 구성하는 10,000개의 스크린샷 중 아주 일부</figcaption></figure><p>현재 DCGAN은 256 픽셀 제곱 정도의 아주 작은 이미지에서만 동작합니다. 그러나 NES의 전체 화면 해상도는 겨우 256 x 224 픽셀 이었기 때문에 전혀 문제가 되지 않습니다. 일을 간단하게 하기 위해, 각 NES 스크린샷을 224 픽셀 제곱 크기로 잘라냈습니다.</p><h4><strong>DCGAN 설정 하기</strong></h4><p>Github에 직접 사용해 볼 만한 DCGAN의 오픈 소스 구현 몇 가지가 있습니다. 저는 김태훈 님의 <a href="https://github.com/carpedm20/DCGAN-tensorflow">Tensorflow 구현</a>을 사용했습니다. DCGAN은 비지도 학습이기 때문에, 폴더에 데이터를 넣고 기본적인 파라미터를 조정하고, 교육을 시작한 후 결과가 나올때 까지 기다려야 합니다.</p><p>원본 훈련 데이터의 샘플들은 다음과 같습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*_SxfxzP5lvMY898FHXivrA.png" /></figure><p>이제 훈련이 시작되었습니다. 처음 생성기(Generator)로 나온 출력물은 완전히 노이즈입니다. 그러나 생성기가 일을 좀더 잘하게 되면서 부터 서서히 형태를 띄게 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*tTVoa_DMIQ_yrPsyUi0SGg.gif" /></figure><p>몇 번의 훈련 단계를 거치자, 이미지들은 전통적인 닌텐도 게임의 악몽 버전 같아 지기 시작했습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*4PPIXM8I6ffm-83Gyt5j9Q.gif" /></figure><p>교육이 계속되자, 우리가 찾고자 했던 벽돌과 블록을 보이기 시작합니다. 생명 막대(life bar)나 일부 텍스트와 같은 화면 구성요소도 볼 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*PXU9lfcADxGzhpn0Uf1uaQ.png" /></figure><p>그런데 복잡한 문제가 있습니다. 컴퓨터가 훈련 이미지로부터 그냥 그림을 베낀(regurgitating) 것이 아니라 새로운 그림을 창조하고 있는지 어떻게 알 수 있을까요? 위 이미지들 중 두 개의 이미지에서 Super Mario Bros. 3의 메뉴바(menu bar)와 진짜 Super Mario Bros의 헤더바(header bar) 그리고 벽돌을 명확하게 볼 수 있습니다.</p><p>훈련 데이터를 베끼는 것(Regurgitating)은 분명히 일어날 수 있는 일입니다. 대규모의 훈련 데이터 세트를 길지 않게 훈련시킴으로써, 이런 일이 발생할 확률을 줄일 수 있습니다. 그럼에도 골치아픈 문제이고 이에 대한 연구가 계속되고 있습니다.</p><p>저는 그저 미적인 것(aesthetics)을 좋아하기 때문에, 제가 보기에 진짜같이 보이는 그림이 만들어 질 때까지 모델을 조정했습니다. 그러나 유사한 그림이 훈련 데이터에 있는지 검색해서 이것이 존재하지 않는다는 것을 증명하는 것 이외에는 이 새로운 그림이 완전히 독창적인지 아닌지 <em>증명</em>할 길은 없습니다.</p><p>몇 시간의 훈련을 통해 생성 된 이미지에는 제가 보기에 괜찮은 16 x 16 타일들이 포함되어 있습니다. 기본적인 돌 블록, 벽돌 패턴, 물 패턴, 덤불과 몇가지 “으스스하게 보이는” 배경 분위기 타일들을 꽤 찾을 수 있었습니다.</p><p>그런 다음, NES에서 지원하는 64가지 색상만이 사용될 수 있도록 이미지를 전처리(pre-process)해야 합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*yVr4BD17ex1XO0YWrrtTGQ.png" /><figcaption>원래 닌텐도는 위와 같은 64색만을 표시 할 수 있었습니다. 정확히는 단지 54가지의 고유한한 색상이 있는데 그 이유는 그 중 일부가 중복되어 있기 때문입니다.</figcaption></figure><p>이제 <a href="http://www.mapeditor.org/">Tiled Map Editor</a>에서 이 64색 이미지를 열어 보겠습니다. 이를 이용해서 내가 원하는 미적 이미지와 일치하는 16 x 16 타일을 쉽게 잘라낼(grab) 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*1TwcsjKcTL2JDwUZQuClKw.png" /><figcaption>생성된 스크린샷에서 잘라낸 타일들</figcaption></figure><p>그런 다음 Tiled Map Editor에서 16x16 타일들을 NES 게임 중에 하나인 ‘<a href="https://en.wikipedia.org/wiki/Castlevania">Castlevania</a>’를 연상시키는 간단한 레벨 레이아웃으로 배열해 보겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*Fsneb0OWGsvRxl4r9PAxcg.png" /></figure><p>제 생각에는 꽤 괜찮아 보입니다! 이미지 편집기로 타일의 픽셀 하나도 건드리지 않았다는 것을 명심하십시오. 모든 타일들은 DCGAN 모델에서 나온 것 그대로 입니다.</p><p>그리고 ‘Castlevania’ 게임의 주인공과 적들을 넣어보면 이 레벨이 실제로 어떻게 보일지 알 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*D1-zRoKfkoH-t6L8OhPrXQ.png" /></figure><p>전체 모습을 완성하기 위해서, 메뉴 구성요소를 넣어 게임 내에서 이 레벨이 어떻게 보이는지 확인해 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*oL9SLILkis_iWvVbbi56FQ.png" /><figcaption>정말 으스스 하군요</figcaption></figure><p>제 생각에 이것은 제가 기억하는 NES 게임처럼 보입니다! 이 것이 지금까지 창조된 최고의 NES 그림이라고 주장하는 것은 아니지만, 확실히 최악은 아닙니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/256/1*LQwB9OuY_rOMEUYTxcXeSQ.png" /><figcaption>별로 였던 <a href="https://en.wikipedia.org/wiki/The_Cheetahmen">The Cheetahmen</a> 보다는 낫습니다.</figcaption></figure><h3><strong>이게 단가요?</strong></h3><p>저는 이 생성 모델에 열광하고 있습니다. 언젠가 컴퓨터가 끝없이 작품을 만들어 낸다는 이 아이디어는 제게 대단히 매력적인 것입니다. 하지만, 이에 대해 다른 사람들과 이야기 할 때면, 때로는 반응이 “이게 단가요?”입니다. 뭐 그럴 수 있습니다.</p><p>현재로써는 생성 모델에 대해 많은 과장이 실제로 있습니다. GAN은 훈련이 어렵기로 악명 높고 아주 작은 이미지를 생성한다는 제약이 있음에도 불구하고, 이미 AI의 미래라고 불리고 있습니다. 사실, 현재 가장 좋은 모델마저도 우표 크기 정도의 합성 개사진 만을 생성 할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/412/1*o0c-0FWxwEKipD68IAMe8Q.png" /><figcaption>끔찍한 동물이네요! <a href="https://arxiv.org/pdf/1701.00160.pdf">Ian Goodfellow’s GAN Tutorial paper</a>에서 가져온 사진</figcaption></figure><p>그러나 몇 년전만 해도 이와 비슷한 일 조차도 할 수 없었습니다. 그 때는 정말이지 다음과 같이 생성 된 그림에도 꽤나 흥분했었습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/346/1*eCBRTWGQUf2ru6DvEQsDYg.png" /><figcaption>자전거 맞습니다! 맹세코!</figcaption></figure><p>그리고 이 기술은 하루가 다르게 발전하고 있습니다. GAN을 사용하여 사람들의 얼굴을 노화시키는 <em>이번 주</em>에 발표된 <a href="https://arxiv.org/pdf/1702.01983v1.pdf">이런 논문</a>도 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*AYE2-Csa-Zzgq08VVcj7lQ.png" /><figcaption>“<a href="https://arxiv.org/abs/1702.01983v1">Face Aging With Conditional Generative Adversarial Networks</a>”로 부터 가져온 이미지</figcaption></figure><p>이런 속도로 계속해서 발전한다면, 생성 모델이 창작을 위한 주요 도구가 되는 날도 멀지 않았습니다. 그래서 지금이 바로 이 실험을 시작해야할 아주 좋은 때입니다.</p><h3>좀 더 배울수 있는 곳</h3><p>생성 모델과 DCGAN에 대해 좀더 깊이있게 알고 싶다면, 다음의 추천 자료를 확인하세요.</p><ul><li><a href="http://www.foldl.me/2015/conditional-gans-face-generation/">Conditional generative adversarial networks for face generation</a> by <a href="http://www.foldl.me/">Jon Gauthier</a></li><li><a href="https://openai.com/blog/generative-models/">Generative Models</a> overview from <a href="https://openai.com/about/">OpenAI</a></li><li><a href="https://bamos.github.io/2016/08/09/deep-completion/">Image Completion with Deep Learning in TensorFlow</a> by <a href="https://bamos.github.io/">Brandon Amos</a></li><li>See how <a href="https://twitter.com/dribnet">Tom White</a> uses generative models to make art in his <a href="http://cargocollective.com/dribnet/facegrid">Neural Facegrid project</a></li><li><a href="http://arxiv.org/abs/1406.2661">Ian Goodfellow’s original paper on GANs</a> and <a href="https://arxiv.org/pdf/1701.00160">his recent tutorial on them</a></li></ul><p>이 글이 마음에 들었다면, 제 <a href="http://eepurl.com/b9fg2T"><strong>Machine Learning is Fun! 이메일 리스트에 가입</strong></a>하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.</p><p>Twitter의 <a href="https://twitter.com/ageitgey">@ageitgey</a>인 저를 팔로우하거나, <a href="mailto:ageitgey@gmail.com">직접 이메일</a>을 보내거나 또는 <a href="https://www.linkedin.com/in/ageitgey">linkedin에서 저를 찾을 수도 있습니다</a>. 기계 학습으로 제가 당신이나 당신의 팀을 도울 수 있다면 언제든 연락 주십시오.</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2435b4a55ccd" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 6]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/eb0ed6b0ed1d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[인공지능]]></category>
            <category><![CDATA[음성인식]]></category>
            <category><![CDATA[speech-recognition]]></category>
            <category><![CDATA[머신러닝]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Fri, 26 May 2017 00:26:33 GMT</pubDate>
            <atom:updated>2017-11-09T13:24:32.887Z</atom:updated>
            <content:encoded><![CDATA[<p>딥러닝(Deep Learning)을 이용한 음성 인식(Speech Recognition)</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p>음성 인식은 이미 우리 생활속에 깊숙히 들어와 있습니다. 휴대 전화, 게임 콘솔 및 스마트 시계에 이미 음성인식이 포함되어 있습니다. 심지어 우리집을 자동화하고 있습니다. 50달러만 있으면 Amazon Echo Dot을 구입할 수 있습니다. 이 마술 상자로 피자를 주문하거나 날씨 정보를 확인하거나 심지어 쓰레기 봉투를 구입할 수도 있습니다. 그냥 큰 소리로 말하기만 하면 됩니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*2K8ai32v7jLndvFIzX4j7Q.jpeg" /><figcaption>Alexa, 라지 피자 하나 주문해줘!</figcaption></figure><p>이 Echo Dot가 이번 연말 휴가철에 얼마나 인기가 있었던지 아마존이 <a href="https://www.bloomberg.com/news/articles/2016-12-21/amazon-sells-out-of-echo-speakers-in-midst-of-holiday-rush">재고를 유지하기도 힘들어 보입니다.</a></p><p>음성 인식은 지난 수십년 간 우리 주위에 있어 왔습니다. 그런데, 갑자기 요즘 대세가 된 이유는 무엇일까요? 그 이유는 딥러닝(deep learning)이 마침내 세심하게 통제된 환경 밖에서도 음성 인식을 사용할 수 있을 정도로 정확하게 만들었기 때문입니다.</p><p><a href="https://en.wikipedia.org/wiki/Andrew_Ng">Andrew Ng</a>는 음성 인식이 95 %에서 99 %로 정확해져서 우리가 컴퓨터와 상호 작용하는 주된 방법이 될 것이라고 이미 오래전에 예측했습니다 . 여기서 중요한 것은 이 4 %의 정확도 격차가<em> 짜증나게 신뢰가 안가는</em> 것과 <em>믿을 수 없을 정도로 유용한 것</em>의 차이라는 것입니다. 딥러닝 덕분에 드디어 이 높은 수준에 도달했습니다.</p><p>딥러닝을 음성 인식에 어떻게 활용하는지 알아보겠습니다!</p><h3><strong>기계 학습이라고 만능은 아닙니다</strong></h3><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e">신경망 기계 번역이 어떻게 동작하는지</a> 알고 있다면, 당신은 어쩌면 단순하게 음성 녹음을 신경망에 공급(feed)하고 텍스트를 생성하도록 훈련시킬 수 있다고 생각할지도 모릅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*0u6Xg2tRoqPqIwjsEebv7A.png" /></figure><p>이렇게 바로 된다면 딥러닝을 활용한 음성 인식에 있어 이것이 성배가 되겠지만, 아직까지는 이런 수준에 이르지 못했습니다 (적어도 이글을 쓰고 있는 시점에는 아닙니다만, 몇 년 후에는 분명히 가능해질 것입니다).</p><p>문제는 말하는 속도가 다양하다는 것입니다. 어떤 사람은 “hello!”라고 말할 수도 있고 다른 사람은 “heeeelllllllllllllloooo!”라고 매우 느리게 말할 수도 있는데, 그렇게 되면 훨씬 더 많은 데이터의 훨씬 긴 음성 파일이 만들어 집니다. 이 두 개의 음성 파일은 “hello!”라는 동일한 텍스트로 인식되어야합니다. 다양한 길이의 오디오 파일을 고정 길이의 텍스트로 자동 정열하는 것은 사실 매우 어렵습니다.</p><p>이 문제를 해결하기 위해서, 우리는 심층 신경망(deep neural network)과 함께 특별한 기법과 추가 처리를 사용해야 합니다. 어떻게 동작하는지 살펴 보겠습니다!</p><h3><strong>소리를 비트로 바꾸기</strong></h3><p>음성 인식의 첫 번째 단계는 명확합니다 — 우리는 컴퓨터에 음파(sound waves)를 공급(feed)해야 합니다.</p><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa">Part 3</a>에서, 이미지를 숫자 배열로 만들어서 이미지 인식을 위한 신경망에 직접 공급하는 방법에 대해 알아봤습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/581/1*6UcQdP-1yFoUotb-QnDWsg.gif" /><figcaption>이미지는 각 픽셀의 강도(intensity)를 인코딩한 숫자 배열일 뿐입니다.</figcaption></figure><p>그러나 소리는 파동(waves)으로 전달됩니다. 어떻게 음파(sound waves)를 숫자로 바꿀 수 있을까요? 제가 “Hello”라고 말하는 다음의 음성 파일을 사용해 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*ci5URoFbHoeOxIDkeQ-dpw.png" /><figcaption>제가 말한 “Hello”의 파형(waveform)</figcaption></figure><p>음파는 1차원 입니다. 시간의 흐름속 매 순간마다 음파의 높이를 기준으로 한 단일 값을 가집니다. 음파의 아주 작은 부분을 확대해서 살펴 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*qlpO2JPeUqJbwQbndypefA.png" /></figure><p>이 음파를 숫자로 표현하기 위해서는, 파동의 높이를 등간격의 좌표값으로 저장하면 됩니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*PkvxKtomXS4sR4HzMVL8gA.gif" /><figcaption>음파 샘플링(표본 추출, sampling) 하기</figcaption></figure><p>이러한 과정을 <em>샘플링(sampling)</em>이라고합니다. 초당 수천번을 읽어들여서 그 시점에서의 음파 높이를 숫자로 저장합니다. 이러한 방법은 압축되지 않은 .wav 오디오 파일과 기본적으로 동일합니다.</p><p>“CD 품질”의 오디오는 44.1khz (초당 44,100번 읽기)로 샘플링됩니다. 그러나 음성 인식에서는 16khz (초당 16,000번 추출)의 샘플링 속도로도 인간의 음성 주파수 범위를 커버하기에 충분합니다.</p><p>우리의 “Hello” 음파를 초당 16,000번 샘플링해 보겠습니다. 다음은 처음 100개의 샘플입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*JQoSkUIadHmCBZ-Xky62iQ.png" /><figcaption>각 숫자는 1/16000 초 간격에 대한 음파의 진폭(amplitude)을 나타냅니다.</figcaption></figure><h4><strong>디지털 샘플링에 관해 알아야 할 것</strong></h4><p>샘플링은 데이터의 일부만을 추출하기 때문에 원래의 음파에 대한 대략적인 근사치를 만드는 것이라고 생각할 수 있습니다. 우리가 읽어드린 데이터는 원본과 차이가 있기 때문에 데이터 유실이 발생합니다. 그렇죠?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*Ol2SusBbyQJJS2pKQ2Sd5w.png" /><figcaption>디지털 샘플링으로 원래의 아날로그 음파를 완벽하게 재현할 수 있을까요? 그림에서 보는 것과 같은 차이는 어떻게 해결해야 할까요?</figcaption></figure><p>하지만, <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist 정리</a> 덕분에, 기록하고자하는 가장 높은 주파수의 최소 두 배 빠르게 샘플을 추출한다면, 간격이 생긴 샘플로부터 원래의 음파를 수학적으로 완벽하게 재구성해 사용할 수 있습니다.</p><p>이 내용을 언급하는 이유는, <a href="http://gizmodo.com/dont-buy-what-neil-young-is-selling-1678446860">거의 모든 사람들이 높은 샘플링 속도가 언제나 더 좋은 품질을 만들어 낸다는 잘못된 생각을 갖고 있기 때문</a>입니다. 그렇지 않습니다.</p><h3><strong>샘플링 된 음성 데이터 전처리(pre-processing) 하기</strong></h3><p>이제 각 숫자가 초당 1/16,000 간격에 대한 음파의 진폭을 나타내는 일련의 숫자들이 있습니다.</p><p>이 숫자들을 신경망에 바로 제공(feed)<em>할 수</em> 있습니다. 하지만, 이러한 샘플을 직접 처리해서 음성 패턴(speech patterns)을 인식하는 것은 어렵습니다. 대신에 몇 가지 오디오 데이터에 대한 사전 처리를 수행함으로써 문제를 좀더 쉽게 해결할 수 있습니다.</p><p>샘플링한 오디오를 20 밀리 초 길이로 잘라 그룹으로 만드는 것부터 시작하겠습니다. 다음은 오디오의 처음 20 밀리초 (즉, 최초 320 개의 샘플) 입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*uHel1XG79OVBtge5pHLP1w.png" /></figure><p>이 숫자들을 간단한 선 그래프로 그려보면, 이 20 밀리 초 동안의 원래 음파에 대한 대략적인 근사값을 확인할 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*nz2YqjuUE-28Sfjf_JGNuw.png" /></figure><p>이 레코딩은 단지 <em>1/50 초 길이</em>입니다. 그러나 이 짧은 레코딩 조차도 음성의 서로 다른 주파수가 복잡하게 뒤섞인 것입니다. 여기에는 낮은 음역, 중간 음역들이 있고 심지어 높은 음역도 몇 군데 나타납니다. 하지만 이러한 다양한 주파수가 혼합되어 복잡한 인간 언어의 음성을 구성합니다.</p><p>신경망이 이 데이터를 좀더 쉽게 처리할 수 있도록, 복잡한 음파를 구성 요소들로 분리하려고 합니다. 가장 낮은 음역, 그 다음 낮은 음역, 이런식으로 분리할 것입니다. 그런 다음 각 주파수 대역별(낮은 주파수에서 높은 주파수까지)로 분포된 에너지를 합산해서, 이 오디오 정보에 대한 일종의 <em>지문(fingerprint)</em>을 만듭니다.</p><p>누군가 피아노로 C 장조 화음을 연주하는 것을 녹음했다고 상상해보십시오. 이 소리는 도, 미, 솔 이 세가지 음들이 함께 섞여져 하나의 복잡한 소리로 만들어진 것입니다. 우리는 이 복잡한 소리를 개별 음으로 분해해서 도, 미, 솔이라는 것을 알아내고자 합니다. 우리가 하고자 하는 것이 바로 정확히 동일한 아이디어입니다.</p><p>푸리에 변환 (Fourier transform)이라고하는 수학 연산을 사용하면 이를 수행할 수 있습니다. 이 연산은 복잡한 음파를 이를 구성하는 단순한 음파로 분해합니다. 이러한 개별적인 음파로 분해된 후에는, 각각의 음파에 얼마나 많은 에너지가 포함되어 있는지를 합산합니다.</p><p>최종 결과는 저음(즉, 베이스 음)부터 고음까지 각각의 주파수 범위가 얼마나 중요한지에 대한 점수입니다. 아래의 각 숫자는 20 밀리초의 오디오 클립에서 각 50hz 대역별로 얼마나 많은 에너지가 있었는지 나타냅니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*qO2fBhvWBJ4-zi7VL_fJEw.png" /><figcaption>목록의 각 숫자는 50hz 주파수 대역별로 얼마나 많은 에너지가 있었는지 나타냅니다</figcaption></figure><p>이를 차트로 그리면 훨씬 더 쉽게 이해할 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*vHO-CJG7KsHFD7XsMX6__A.png" /></figure><p>오디오의 20 밀리초 조각마다 이 과정을 반복하면, 음향 스펙트럼(spectrogram)이 만들어 집니다.(왼쪽에서 오른쪽으로의 각 열은 하나의 20ms 조각입니다):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*4ZbwSOwewzurLo4aLOSntw.png" /><figcaption>“hello” 음성에 대한 전체 음향 스펙트럼(spectrogram)</figcaption></figure><p>음향 스펙트럼은 오디오 데이터에서 음정과 여러 음역대 패턴을 실제로 <em>볼</em> 수 있어서 아주 멋집니다. 신경망은 원래의 소리보다는 이런 종류의 데이터로부터 좀더 쉽게 패턴을 찾을 수 있습니다. 이 때문에 우리가 신경망에 실제로 제공하는 것이 바로 이러한 데이터 표현입니다.</p><h3><strong>짧은 소리로부터 특징 인식하기</strong></h3><p>이제 오디오를 처리하기 쉬운 포맷으로 만들었고, 딥신경망(deep neural network)에 이를 제공(feed)할 것입니다. 신경망에 입력되는 데이터는 20 밀리초 단위의 오디오 조각입니다. 각각의 작은 오디오 조각마다 현재 발음되는 소리에 해당하는 문자가 무엇인지 찾게 될 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*5sIjtQGhJ7WBpx-o0vYQcg.png" /></figure><p>우리는 순환 신경망(<a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3">recurrent neural network</a>) — 즉, 향후 예측에 영향을 미치는 메모리를 가지고 있는 신경망을 사용할 것입니다. 그 이유는 예측하는 각 글자가 이 후 예측할 다음 글자에 대한 가능성에 영향을 주어야하기 때문입니다. 예를 들어, 앞서 “HEL”이라고 말했다면, “Hello”라는 단어를 끝내기 위해 “LO”라고 말할 가능성이 높습니다. “XYZ”와 같이 발음하기 어려운 것을 뒤이어 말할 가능성은 훨씬 적습니다. 따라서 앞선 예측을 기억하면 신경망이 더 정확한 예측을 할 수 있습니다</p><p>신경망을 통해 전체 오디오 클립을 (한 번에 조각씩) 모두 수행하면, 각 오디오 조각 별로 해당 조각에서 들리는 가장 가능성이 높은 문자가 매핑되게 됩니다. 다음은 내가 “Hello”라고 말하는 음성의 매핑 결과입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*W4dWcHRpT94iC3LZHCstWQ.png" /></figure><p>우리의 신경망은 내가 말했을 가능성이 높은 것이 “HHHEE_LL_LLLOOO”라고 예측하고 있습니다. 그러나 또한 내가 “HHHUU_LL_LLLOOO”또는 “AAAUU_LL_LLLOOO”라고 말할 가능성도 있다고 생각합니다.</p><p>이 결과를 정제하기 위해 수행 할 몇 가지 단계가 있습니다. 먼저, 반복되는 모든 문자를 하나의 문자로 대체합니다:</p><ul><li>HHHEE_LL_LLLOOO becomes HE_L_LO</li><li>HHHUU_LL_LLLOOO becomes HU_L_LO</li><li>AAAUU_LL_LLLOOO becomes AU_L_LO</li></ul><p>그리고 모든 공백을 제거합니다:</p><ul><li>HE_L_LO becomes HELLO</li><li>HU_L_LO becomes HULLO</li><li>AU_L_LO becomes AULLO</li></ul><p>이제 “Hello”, “Hullo” 그리고 “Aullo”의 세가지 가능한 단어(필사, transcription)가 만들어졌습니다. 이 세가지 단어를 큰소리로 말하면 모두 “Hello”와 비슷하게 들립니다. 신경망이 한 번에 한 문자만을 예측하기 때문에, 이와 같이 소리나는데로 받아 적은 필사(transcription)를 만들어 냅니다. 예를 들어 “He would not go”라고 말하면, “He wud net go”와 같은 필사도 가능성이 있다고 할 것입니다.</p><p>이를 해결할 방법은 이러한 발음 기반의 예측과 글로 쓰여진 텍스트(서적, 뉴스 기사 등)의 대규모 데이터베이스를 기반으로 하는 가능성 점수를 결합하는 것입니다. 현실이 될 가능성이 가장 적은 필사들은 버리고 가장 현실적인 것으로 보이는 필사를 취합니다.</p><p>“Hello”, “Hullo” 그리고 “Aullo”와 같은 필사 중에서, 당연히 “Hello”가 (원래의 오디오 기반 교육 데이터 뿐만아니라) 텍스트 데이터베이스에 훨씬 더 자주 나타나므로 아마도 맞을 것입니다. 따라서 우리는 다른 것들 대신에 “Hello”를 최종 전사로 선택할 것입니다. 끝!</p><h4><strong>잠시만요!</strong></h4><p><em>“그런데 누군가가 정말로 ‘</em><a href="https://www.merriam-webster.com/dictionary/hullo"><em>Hullo</em></a><em>’라고 한다면? 당연히 이것이 맞는 것이고 ‘Hello’ 가 아마도 잘못된 필사이다”</em>라고 생각할 지도 모릅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*G2r5SsYDlVrUUy91sshG5w.jpeg" /><figcaption>“Hullo! Who dis?”</figcaption></figure><p>물론 누군가가 실제로 “Hello”대신에 “Hullo”라고 말한 것일 수 있습니다. 그러나 미국식 영어로 훈련된 이러한 음성 인식 시스템은 기본적으로 절대 “훌루 (Hullo)”를 필사로 제시하지 않습니다. “Hello”와 비교해서 이 단어는 강능성이 거의 없는 것이기 때문에 신경망은 당신이 아무리 ‘U” 소리를 강조하더라도 언제나 “Hello”라고 말한다고 생각할 것입니다.</p><p>한번 시도해 보세요! 당신의 휴대전화의 언어 설정이 American English로 세팅되어 있다면, 음성 인식이 “Hullo”를 인식하는지 시도해 보세요. 안됩니다! 계속해봐도 안됩니다! 언제나 “Hello”로 이해할 것입니다.</p><p>“Hullo”를 인식하지 못하는 것은 이해할 만한 상황이지만, 때로는 당신의 휴대전화가 제대로 말하는 말도 이해하지 못하는 짜증나는 경우들도 있습니다. 이때문에 음성 인식 모델은 이러한 특이 케이스를 고치기 위해서 항상 더 많은 데이터로 재훈련되고 있습니다.</p><h3><strong>나만의 음성 인식 시스템을 만들 수 있을까요?</strong></h3><p>기계 학습에서 가장 멋진 점 중 하나는 바로 가끔 아주 심플하다는 것입니다. 수많은 데이터를 가져와 기계 학습 알고리즘에 제공하면 마술처럼 당신의 게임용 랩톱의 비디오 카드에서 실행되는 세계적 수준의 AI 시스템을 만들 수 있습니다… 그렇죠?</p><p>어떤 경우에는 맞는 얘기입니다만, 음성인식에는 해당되지 않습니다. 음성을 인식하는 것은 어려운 문제입니다. 나쁜 품질의 마이크, 배경 잡음, 잔향(reverb)과 반향(echo), 다양한 말투(accent) 등과 같은 거의 무한한 문제를 극복해야 합니다. 이러한 모든 문제는 신경망이 처리 할 수 있도록 훈련 데이터에 포함되어 있어야만 합니다.</p><p>또 다른 예가 있습니다: 큰 방에서 이야기 할 때, 당신은 소음때문에 무의식적으로 소리를 높인다는 것을 알고 있습니까? 인간은 어떻게 하더라도 이해하는 데 아무런 문제가 없지만, 이 특수한 경우를 처리하기 위해 신경망은 훈련이 필요합니다. 따라서 소음 때문에 고함을 지르는 사람들의 훈련 데이터가 필요합니다.</p><p>Siri, Google Now! 또는 Alexa의 수준으로 동작하는 음성 인식 시스템을 구축하려면 수백명의 사람을 고용해서 녹음을 해야만 얻을 수 있는 데이터보다 훨씬 <em>많은</em> 훈련 데이터가 필요합니다. 그리고 사용자들은 저품질의 음성 인식 시스템을 이해해 주지 않기 때문에, 이 문제를 간과해서는 안됩니다. 아무도 80% 수준으로 동작하는 음성 인식 시스템을 원하지 않습니다.</p><p>Google이나 Amazon과 같은 회사들은 실생활에서 녹음된 수십억 시간 분량의 음성 오디오가 바로 <em>황금</em>과 같습니다. 이것이 바로 세계적인 수준의 음성 인식 시스템과 당신의 취미로 만든 시스템을 구분짓는 단 하나의 가장 큰 차이입니다. 모든 휴대폰에 무료로 설치된 <em>Google Now!</em> 와 <em>Siri</em> 또는 추가비용이 없이 $50에 팔고 있는 <em>Alexa</em>는 당신이 <strong>이를 가능한 한 많이 사용하도록</strong> 유도하고 있습니다. 이들 시스템 중 하나에 당신이 말하는 모든 것은 <strong>영원히 기록</strong>되고 향후 음성 인식 알고리즘의 향후 버전의 훈련 데이터로 사용됩니다. 정말 이렇게 돌아가고 있습니다!</p><p>못 믿으시겠다구요? <em>Google Now!</em>가 설치된 Android 휴대폰이 있다면, <a href="https://myactivity.google.com/udc/vaa">여기를 클릭해서 당신이 직접 말한 모든 멍청한 것들에 대한 실제 녹음 내용을 들어보십시오</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*YE0ymiQTkdqeY5I9p1ukZg.png" /><figcaption>Alexa 앱을 통해 Amazon에 저장된 동일한 것들을 확인할 수 있습니다. Apple은 안타깝게도 Siri 음성 데이터에 접근하는 것을 허용하지 않습니다.</figcaption></figure><p>따라서 당신이 스타업 아이디어를 고민하고 있다면, Google과 경쟁할 나만의 음성 인식 시스템을 개발하는 것은 절대 추천하지 않습니다. 대신에, 사람들이 몇 시간 동안 말하는것을 녹음해서 제공하도록 하는 방법을 찾아보십시오. 이 데이터가 당신의 제품이 될 수 있습니다.</p><h3><strong>좀 더 배울수 있는 곳</strong></h3><ul><li>여기서 (대략적으로) 설명된 가변 길이 오디오(variable-length audio)를 처리하기 위한 알고리즘을 Connectionist Temporal Classification 또는 CTC라고 합니다. 2006년도 <a href="http://www.cs.toronto.edu/~graves/icml_2006.pdf">원본 논문을 읽어 볼 수 있습니다</a>.</li><li>Baidu의 <a href="https://cs.stanford.edu/~acoates/">Adam Coates</a>는 Bay Area Deep Learning School에서 음성 인식을 위한 딥러닝에 대해 멋진 프레젠테이션을 했습니다. YouTube에서 <a href="https://youtu.be/9dXiAecyJrY?t=13874">이 비디오를 볼 수 있습니다</a> (그의 말은 3:51:00에 시작됩니다). 강추합니다.</li></ul><p>이 글이 마음에 들었다면, 제 <a href="http://eepurl.com/b9fg2T"><strong>Machine Learning is Fun! 이메일 리스트에 가입</strong></a>하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.</p><p>Twitter의 <a href="https://twitter.com/ageitgey">@ageitgey</a>인 저를 팔로우하거나, <a href="mailto:ageitgey@gmail.com">직접 이메일</a>을 보내거나 또는 <a href="https://www.linkedin.com/in/ageitgey">linkedin에서 저를 찾을 수도 있습니다</a>. 기계 학습으로 제가 당신이나 당신의 팀을 도울 수 있다면 언제든 연락 주십시오.</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb0ed6b0ed1d" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 5]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/83b7a44b797a</guid>
            <category><![CDATA[시퀀스투시퀀스]]></category>
            <category><![CDATA[기계]]></category>
            <category><![CDATA[머신러닝]]></category>
            <category><![CDATA[기계번역]]></category>
            <category><![CDATA[rnn]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Fri, 24 Feb 2017 09:00:50 GMT</pubDate>
            <atom:updated>2017-12-06T06:56:31.358Z</atom:updated>
            <content:encoded><![CDATA[<p>딥러닝(Deep Learning)과 시퀀스(Sequence)의 마법을 사용한 언어 번역(Language Translation)</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p>우리는 모두 마법처럼 100 가지 다른 언어를 즉시 번역 할 수 있는 웹 사이트 인 구글 번역(<a href="https://translate.google.com/">Google Translate</a>)을 알고 있고 사랑합니다. 심지어 휴대 전화나 스마트 워치에서도 사용할 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/360/1*F5SOQIjM7txwPXIfWxX1AA.gif" /></figure><p>구글 번역에 사용된 기술을 기계 번역(Machine Translation)이라고 합니다. 다른 방법으로는 절대 불가능했던 전세계 사람들의 의사 소통을 가능하게 함으로써 세상을 변화시켰습니다.</p><p>그런데, 사실 고등학생들이… 음… 지난 15 년간 스페인어 숙제를 하기위해 구글 번역의 <em>도움</em>을 받아 왔다는 것을 모두 알고 있습니다. 그렇다면 이건 오래된 뉴스가 아닌가요?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*RzhahL_L-TGsDZXIOAOSog.gif" /></figure><p>지난 2 년 동안, 딥러닝(deep learning)은 기계 번역에 대한 우리의 접근 방식을 완전히 새롭게 바꿔 놓았습니다. 언어 번역에 대해 거의 아는 것이 없는 딥러닝 연구자들이 세계에서 가장 뛰어는 언저 전문가가 제작한 언어 번역 시스템을 능가하는 상대적으로 간단한 기계 학습 솔루션을 앞다퉈 내놓고 있습니다.</p><p>이 획기적인 기술을 <strong>시퀀스-투-시퀀스 학습(sequence-to-sequence learning)</strong> 이라고 부릅니다. 이는 많은 종류의 문제들을 해결하는 데 사용되는 매우 강력한 기술입니다. 이제 이것이 번역에 어떻게 사용되는지 확인한 후에, AI 챗봇(Chat Bots)을 작성하거나 그림을 묘사하는 데에도 정확히 동일한 알고리즘이 어떻게 사용되는지 배울 예정입니다.</p><blockquote>*역자주: 시퀀스-투-시퀀스(sequence-to-sequence)에서 시퀀스란 연관된 연속의 데이터를 의미합니다. 이 글에서 주로 이야기하고 있는 문장(sentences)은 결국 문법 등의 규칙으로 연관된 일련의(sequential) 단어들 집합으로 볼 수 있고, 좋은 시퀀스의 예입니다. 따라서, 시퀀스-투-시퀀스 문제란 사진을 제공하면 어떤 사물인지 분류하는 것(clustering)과는 다르게 소스 시퀀스를 결과 시퀀스로 바꾸는 문제를 말합니다. 예를 들어, 사진을 신경망에 제공하면 어떤 사진인지(슬픈지 또는 기쁜지) 일련의 문장으로 묘사하는 것이 가능해진 것입니다. 일반적으로, 앞선 설명과 같이 시퀀스-투-시퀀스 모델은 입력 데이터 처리를 위한 인코더(encoder)와 출력 데이터 처리를 위한 디코더(decoder)로 구성됩니다.</blockquote><blockquote>참조, <a href="https://arxiv.org/abs/1406.1078">https://arxiv.org/abs/1406.1078</a>, <a href="https://arxiv.org/abs/1409.3215">https://arxiv.org/abs/1409.3215</a></blockquote><p>자 시작하시죠!</p><h3><strong>컴퓨터가 번역할 수 있게 만들기</strong></h3><p>그렇다면, 컴퓨터가 인간 언어를 번역할 수 있도록 하려면 어떻게 프로그래밍해야 하나요?</p><p>가장 심플한 접근 방법은 문장의 모든 단어를 대상 언어의 번역 된 단어로 바꾸는 것입니다. 다음은 단어 단위로 스페인어에서 영어로 번역하는 간단한 예제입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*K0sy2D3Z9DcUA7BrlSOuOQ.png" /><figcaption>단순히 각각의 스페인어 단어를 일치하는 영어 단어로 바꿉니다.</figcaption></figure><p>필요한 것은 각 단어의 번역을 찾기 위한 사전이므로 이는 구현하기 쉽습니다. 그러나 문법과 문맥을 무시하기 때문에 결과는 좋지 않습니다.</p><p>이제 다음에 해야할 일은 결과를 개선하기 위해 해당 언어에 맞는 규칙들을 추가하는 것입니다. 예를 들어, 일반적인 2 단어 구문을 하나의 그룹으로 번역하는 것입니다.그리고 영어와는 다르게 스페인어에서는 명사와 형용사의 순서가 반대이기 때문에 명사와 형용사의 순서를 뒤집어 번역하는 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*QXUD1MJg6qsPitBRbc2UCQ.png" /></figure><p>효과가 있네요! 문법의 모든 부분을 적용할 때까지 규칙을 계속 추가하면, 우리 프로그램은 모든 문장을 번역 할 수 있게 될 것입니다, 그렇겠죠?</p><p>이것이 바로 가장 초기의 기계 번역 시스템이 동작한 방법입니다. 언어 학자들은 복잡한 규칙을 찾아서 하나씩 차례로 프로그래밍했습니다. 세계에서 가장 현명한 언어 학자 중 일부는 냉전 기간 동안 <a href="https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment">러시아어 통신을 보다 쉽게 해석할 수 있는 번역 시스템을 만들기 위해</a> 수년간 노력했습니다.</p><p>유감스럽게도, 이것은 일기 예보와 같이 간단하고 평이하게 구조화 된 문서에서만 동작했습니다. 실생활 문서에서는 안정적으로 동작하지 않았습니다.</p><p>여기서의 문제는 인간의 언어가 고정된 규칙들을 따르지 않는다는 것입니다. 인간의 언어는 특수한 상황, 지역적 다양성들로 가득하며 규칙 위반이 아무렇지 않게 일어납니다. 우리가 영어로 이야기하는 방식은 앉아서 문법 규칙을 정의하는 사람들보다 <a href="https://en.wikipedia.org/wiki/History_of_English#Middle_English">수백년 전에 누군가를 침략했던 사람</a>에 따라 더 많은 영향을 받았습니다.</p><h3><strong>통계를 이용해 컴퓨터가 번역을 더 잘할 수 있도록 만들기</strong></h3><p>규칙 기반 시스템이 실패한 후에, 문법 규칙을 대신해서 확률 및 통계 기반의 모델을 사용하는 새로운 번역 접근 방법이 개발되었습니다.</p><p>통계 기반의 번역 시스템을 만들기 위해서는 동일한 텍스트가 최소한 두 가지 언어로 번역되어 있는 많은 훈련 데이터가 필요합니다. 이러한 이중 번역문(double-translated text)를 병렬 말뭉치(<em>parallel corpora 또는 병렬 코퍼스</em>)라고 부릅니다. 1800년대 과학자들이 <a href="https://en.wikipedia.org/wiki/Rosetta_Stone">로제타 스톤 (Rosetta Stone)</a>을 그리스어로부터 이집트 상형 문자를 이해하는 데 사용했던 동일한 방식으로, 컴퓨터는 병렬 말뭉치를 사용해 텍스트를 한 언어에서 다른 언어로 어떻게 변환해야 하는지를 추측할 수 있습니다.</p><p>다행하게도 이중 번역문(double-translated text)들은 희한한 곳에 이미 많이 있습니다. 예를 들어, 유럽 의회는 그들의 의사 진행 절차를 21개의 언어로 번역해 놨습니다. 그래서 연구자들은 종종 번역 시스템을 구축하는 데 <a href="http://www.statmt.org/europarl/">이 데이터</a>를 사용합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*RHR75ItbbAkDh_HhMLEi1g.png" /><figcaption>훈련 데이터는 보통은 흥미롭습니다! 하지만 이건 단지 수백만 줄의 건조한 정부 문서들입니다…</figcaption></figure><h4><strong>확률로 생각하기</strong></h4><p>통계기반 번역 시스템과의 근본적인 차이점은 하나의 정확한 번역을 생성하려고하지 않는다는 것입니다. 그 대신에, 수천 개의 가능한 번역을 생성 한 다음 각각의 번역이 정확한가에 대해 순위를 매깁니다. 이 시스템은 훈련 데이터와 얼마나 유사한지에 따라 “정확도”를 평가합니다. 그럼, 어떻게 동작하는지 알아보겠습니다:</p><h4><strong>Step 1: 원본 문장을 여러 조각들로 나누기</strong></h4><p>먼저, 문장을 쉽게 번역될 수 있는 단순한 조각들로 분해합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*CETbyzuaWQCJlk046FKXcw.png" /></figure><h4><strong>Step 2: 각각의 조각에 대해 가능한 모든 번역을 찾기</strong></h4><p>그런 다음, 훈련 데이터에 있는 동일한 단어 조각에 대해 사람이 이미 번역한 모든 방법을 찾아 이러한 조각들을 번역합니다.</p><p>그런데, 여기서 중요한 점은 단순히 번역 사전에서 이 조각들을 찾는게 아니라는 것입니다. 그 대신에, 우리는 실제 사람들이 실생활에서 이 동일한 단어 조각들을 어떻게 번역하는지를 확인합니다. 이를 통해 다양한 맥략으로 사용될 수 있는 모든 다른 번역 방식을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*z0Jdsw__E5TelRhfkCpgYQ.png" /><figcaption>가장 일반적인 문장이라 할지라도 다양한 번역이 가능합니다.</figcaption></figure><p>그런데, 가능한 번역 중 일부는 다른 것들보다 자주 사용될 수 있습니다. 이렇게 훈련 데이터에서 각각의 번역이 얼마나 자주 나타나는지에 따라 점수를 부여 할 수 있습니다.</p><p>예를 들어, “Quiero”는 “I try”라는 의미 보다는 “I want”를 의미하는 경우가 훨씬 더 일반적입니다. 따라서 우리는 Quiero가 우리 훈련 데이터에서 덜 빈번하게 사용되는 번역 보다 “I want”로 번역되는 빈도만큼 여기에 가중치를 줄 수 있습니다.</p><h4><strong>Step 3: 모든 가능한 번역문을 만들고 가장 좋은 것 찾기</strong></h4><p>다음으로, 이 조각들의 가능한 모든 조합을 통해 가능한 모든 번역문을 생성합니다.</p><p>Step 2에서 나열했던 조각들에 대한 번역으로부터 우리는 이 번역 조각들을 여러 가지 방법으로 조합해서 거의 2,500 가지의 다양한 문장을 생성 할 수 있습니다. 여기 몇 가지 예가 있습니다:</p><blockquote>I love | to leave | at | the seaside | more tidy.<br>I mean | to be on | to | the open space | most lovely.<br>I like | to be |on | per the seaside | more lovely.<br>I mean | to go | to | the open space | most tidy.</blockquote><p>그런데, 실제 시스템에는 더 많은 조각들의 조합이있을 것입니다. 왜냐하면, 단어의 또다른 순서와 문장을 조각내는 다양한 방법을 시도해야 하기 때문입니다:</p><blockquote>I try | to run | at | the prettiest | open space.<br>I want | to run | per | the more tidy | open space.<br>I mean | to forget | at | the tidiest | beach.<br>I try | to go | per | the more tidy | seaside.</blockquote><p>이제 이렇게 생성된 모든 문장을 확인하고 “가장 인간”다운 문장 하나를 찾아야 합니다.</p><p>이렇게 하기 위해서, 우리는 생성 된 각 문장을 영어로 쓰여진 책과 뉴스 기사의 수백만 개 실제 문장과 비교합니다. 비교할 영어 텍스트가 많을수록 좋습니다.</p><p>다음과 같은가능한 번역을 살펴 보겠습니다:</p><blockquote>I try | to leave | per | the most lovely | open space.</blockquote><p>아무도 이런식의 영어 문장은 쓰지 않았을 것처럼 보입니다. 따라서, 우리의 훈련 데이터 세트에 있는 어떤 문장과도 유사하지 않습니다. 우리는 이 가능한 번역에 낮은 확률 점수를 줄 것입니다.</p><p>그러면, 다음과 같은 다른 가능한 번역을 보겠습니다.</p><blockquote>I want | to go | to | the prettiest | beach.</blockquote><p>이 문장은 분명 우리의 훈련 데이터 세트에 있는 어떤 것과 유사 할 것입니다. 따라서, 높은 확률 점수를 얻을 것입니다.</p><p>모든 가능한 번역문을 확인해서, 실제 영어 문장과 가장 유사한 문장이면서 가장 가능성이 큰 조각 번역을 포함한 번역을 선택할 것입니다.</p><p>우리의 최종 번역은 “I want to go to the prettiest beach.”입니다. 나쁘지 않네요!</p><h4><strong>통계기반 기계 번역(Statistical Machine Translation)은 엄청난 이정표였습니다</strong></h4><p>통계기반 기계 번역 시스템은 충분한 훈련 데이터를 제공하면 규칙 기반 시스템보다 훨씬 뛰어납니다. <a href="https://en.wikipedia.org/wiki/Franz_Josef_Och">Franz Josef Och</a>는 이러한 아이디어를 개선해서 2000 년대 초 구글 번역(Google Translate)을 구축했습니다. 마침내 전 세계에 기계 번역(Machine Translation) 서비스가 제공되었습니다.</p><p>초기에는, 언어학자들이 설계한 규칙 기반 시스템보다 확률에 기반한 이 “바보 같은” 접근 방식이 더 효과적인 것에 대해 모든 사람들이 놀랐습니다. 이 때문에, 80 년대 연구원들 사이에서는 (다소 심한) 이런말이 있었습니다.</p><blockquote>“언어학자를 한명 해고할 때 마다, 정확도가 높아진다”</blockquote><blockquote>— <a href="https://en.wikipedia.org/wiki/Frederick_Jelinek">Frederick Jelinek</a></blockquote><h4><strong>통계기반 기계 번역의 한계</strong></h4><p>통계기반 기계 번역 시스템은 잘 동작하지만 구축하고 유지하기가 어렵습니다. 번역하고자 하는 새로운 언어 한쌍마다 새로운 여러 단계의 번역 경로(pipeline)을 전문가가 수정하고 조정해야합니다.</p><p>이렇게 언어별로 다른 경로(pipeline)을 구축하는 것은 힘들기 때문에 절충안이 필요합니다. Google에서 그루지야어(Georgian)를 텔루구어(Telegu)로 번역 요청을 하면, 그루지야어를 텔루구어로(Georgain-to-Telegu) 번역하는 것은 충분한 투자를 할정도로 빈번하지 않기 때문에, 내부적으로는 중간단계인 영어로 번역을 합니다. 그렇다 보니, 보다 일반적인 경우인 불어를 영어로(French-to-English) 번역하는 것을 요청했을 때보 다 덜 향상된 번역 경로을 사용해 번역을 수행하게 될 것입니다.</p><p>컴퓨터가 우리를 위해 이 모든 귀찮은 개발 작업을 하게 한다면 있다면 멋지지 않을까요?</p><h3><strong>컴퓨터가 번역을 더 잘하게 만들기 — 비싼 전문가들 없이</strong></h3><p>기계 번역의 성배는 훈련 데이터만 주면 알아서 번역하는 방법을 배우는 블랙박스 시스템(black box system)입니다. 통계기반 기계 번역을 사용하더라도 인간이 직접 여러 단계의 통계 모델을 만들고 조정해야 합니다.</p><blockquote>*역자주: 여기서 블랙박스 시스템(black box system)이란 작동원리는 감춰진 또는 알아서 동작하는 시스템을 의미</blockquote><p>2014 년, <a href="http://arxiv.org/abs/1406.1078">조경현(KyungHyun Cho) 팀이 돌파구를 만들었습니다</a>. 그들은 딥러닝(deep learning)을 적용해 이 블랙박스 시스템을 구축하는 방법을 찾아냈습니다. 그들의 딥러닝 모델은 병렬 말뭉치(<em>parallel corpora 또는 병렬 코퍼스</em>)를 사용해 사람의 아무런 개입이 없이도 두 언어 사이의 번역 방법을 학습합니다.</p><p>이는<em> 순환 신경망(recurrent neural networks) 인코딩(encodings)</em>이라는<em> </em>두 가지의 큰 아이디어를 통해 실현되었습니다. 이 두 가지 아이디어를 영리하게 결합하면, 자가 학습 번역 시스템(self-learning translation system)을 만들 수 있습니다.</p><h4><strong>순환 신경망(Recurrent Neural Networks)</strong></h4><p>우리는 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761">Part 2에서 순환 신경망에 대해 이미 다루었습니다만</a>, 빠르게 다시 확인해 보겠습니다.</p><p>일반적인 (순환하지 않는) 신경망은 숫자 목록을 받아 (이전 훈련을 기반으로) 결과를 계산하는 일반 기계 학습 알고리즘입니다. 신경망은 많은 문제를 해결하는 블랙 박스로 사용될 수 있습니다. 예를 들어 신경망을 사용해서 어떤 집의 속성을 기반으로 그 집의 대략적인 가치를 계산할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*gPVoxkVhh7B3O3yAmAPBsA.png" /></figure><p>그러나 대부분의 기계 학습 알고리즘과 마찬가지로 신경망도 상태를 저장하지 않습니다(<em>stateless</em>). 숫자 목록을 전달하면 신경망은 그 결과를 계산합니다. 동일한 숫자를 다시 전달하면 항상 동일한 결과과 값을 계산합니다. 과거의 계산값들에 대한 저장소(memory)가 없습니다. 즉, 2 + 2는 항상 4입니다.</p><p>순환 신경망 (또는 줄여서 RNN)은 신경망의 이전 상태가 다음 계산의 입력 값 중 하나가 되는 신경망이 약간 수정 된 버전입니다. 이는 이전 계산이 미래의 계산 결과에 영향을 미친다는 것을 의미합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*toCLZoYzEsCOVcqRBzk0tw.png" /><figcaption>인간은 순환신경망을 싫어합니다: 이 단 하나의 이상한 묘책이 기계를 더 똑똑하게 만들었습니다!</figcaption></figure><p>그런데 왜 이 일을 해야 하나요? 우리가 마지막으로 계산한 결과와 상관 없이 2 + 2는 항상 4여야 하지 않나요?</p><p>이 묘책을 통해 신경망은 일련의 데이터에서 패턴을 학습 할 수 있습니다. 예를 들어, 단어의 처음 몇 글자만 보고도 다음으로 나올 가장 가능성있는 글자를 예측할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*l9WuoYi1U1bKGNeB8pGYKA.gif" /><figcaption>순환망은 스마트폰 키보드 앱의 ‘자동 수정’을 구현할 수 있는 유일한 방법입니다.</figcaption></figure><p>RNN은 데이터에서 패턴을 학습하고자 할 때 유용합니다. 인간 언어는 단지 하나의 크고 복잡한 패턴 이기 때문에, 자연어 처리의 많은 영역에서 RNN이 점점 더 많이 사용되고 있습니다.</p><p>RNN에 대해 더 자세히 알고 싶다면, 이를 사용해서 가짜 Ernest Hemingway 책을 만들고 또 가짜 Super Mario Brothers 레벨을 만들어 봤던 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761">Part 2를 읽어보십시오</a>.</p><h4><strong>인코딩(Encodings)</strong></h4><p>추가로 검토해야할 개념는 <em>인코딩(Encodings)</em>입니다. <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c">Part 4</a>에서는 얼굴 인식 과정의 한 부분으로 인코딩에 대해 설명했습니다. 인코딩을 설명하기 위해, 약간 우회해서 서로 다른 두 사람을 컴퓨터가 어떻게 구별 할 수 있는지 알아보겠습니다.</p><p>컴퓨터로 두 얼굴을 구별하기위해서는 각각의 얼굴에서 서로 다른 측정 값을 수집해 얼굴을 비교합니다. 예를 들어, 각 귀의 크기 또는 눈 사이의 간격을 측정하고 두 사진에서 이러한 측정 값을 비교하면 동일한 사람인지 확인할 수 있습니다.</p><p>당신은 아마도 이미 황금시간대 방송되는 CSI 같은 수사물을 통해 이러한 아이디어에 익숙할 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*GoSlTxtPsJ2ep8fp3iSvmA.gif" /><figcaption>CSI에서 나온 장면인 이 단순한 gif를 너무 좋아해서 다시 사용했습니다 — 왜냐하면 사실 완전히 말도 안되긴 하지만, 어쨌든 이 아이디어를 잘 보여주기 때문입니다.</figcaption></figure><p>얼굴을 측정값 목록으로 바꾸는 아이디어가 바로 <em>인코딩(encoding)</em>의 예입니다. 우리는 원본 데이터(얼굴 사진)를 대표하는 측정값 목록(인코딩)으로 변환합니다.</p><p>그런데, <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c">Part 4</a>에서 봤듯이 우리가 직접 얼굴을 측정하기 위한 특별한 측정 목록을 신경쓸 필요는 없습니다. 대신에, 신경망이 얼굴 사진으로부터 측정값을 생성하게 하면 됩니다. 컴퓨터는 어떤 측정값이 비슷한 두 사람을 구분하기에 딱 맞는지를 찾아내는데 있어 우리 보다 훨씬 뛰어납니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*xwtGXHiWYcrmxjdDHm9JHw.png" /><figcaption>이러한 얼굴 특징 측정값들은 다른 사람들의 얼굴이 다른 결과값으로 나오도록 훈련 된 신경망에 의해 생성됩니다.</figcaption></figure><p>이것이 바로 <em>인코딩</em>입니다. 인코딩은 우리가 매우 복잡한 무엇(얼굴 사진)인가를 단순한 것(128 개의 숫자)로 표현할 수있게 해줍니다. 이제 두 개의 다른 얼굴을 비교하는 것이 전체 이미지를 비교하는 대신에 각각의 얼굴에 대한 128 개의 숫자들만 비교하면 되기 때문에 훨씬 쉬어졌습니다.</p><p>그런데 말입니다. 문장으로도 똑같은 작업을 할 수 있습니다! 가능한 모든 문장들을 일련의 고유한 숫자로 나타내는 인코딩을 만들수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*qwpHhpFOC-Frz9GXBHxaUA.png" /><figcaption>이 숫자 목록은 영어 문장인 “Machine Learning is Fun!”을 나타냅니다. 다른 문장은 다른 숫자의 집합으로 표현될 것입니다.</figcaption></figure><p>이러한 인코딩을 생성하기 위해서는 RNN에 문장을 제공하되 한 번에 한 단어 씩 제공합니다. 마지막 단어가 처리되면 최종 결과는 전체 문장을 나타내는 값들이 됩니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*T5nAIQ0DU6-UKeDXs-Qkbw.gif" /><figcaption>RNN에는 전달된 각 단어를 “저장(memory)”하고 있기 때문에, 계산된 최종 인코딩은 문장에 있는 모든 단어를 나타냅니다.</figcaption></figure><p>좋습니다. 이제 문장 전체를 고유 한 숫자들로 표현할 수 있는 방법이 생겼습니다. 인코딩의 각 숫자가 무엇을 의미하는지 알지 못하지만 실제로 중요하지도 않습니다. 각 문장이 고유 한 숫자 집합으로 식별될 수만 있다면, 그 숫자가 어떻게 생성되었는지 정확하게 알 필요는 없습니다.</p><h4><strong>이제 번역을 해 봅시다!</strong></h4><p>자 이제 우리는 RNN을 사용해서 문장을 고유 한 숫자들로 인코딩하는 방법을 알았습니다. 그래서 이것이 어떤 도움이 될까요? 정말 멋진 것들이 여기 있습니다!</p><p>두 개의 RNN을 서로 연결하면 어떤일이 일어날까요? 첫 번째 RNN은 문장을 나타내는 인코딩을 생성하게 할 수 있습니다. 그런 다음 두 번째 RNN이 해당 인코딩으로부터 원래 문장으로 다시 디코딩하기 위해 동일한 로직을 역으로 수행하게 할 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*8pdZRP8NmClOhgFZrbhXKQ.png" /></figure><p>물론 원래 문장을 인코딩하고 다시 디코딩하는 것이 그리 쓸모 있는 것은 아닙니다. 그러나 두 번째 RNN을 영어 대신 스페인어로 디코딩하도록 훈련시킬 수 있다면 어떨까요? 병렬 말뭉치(<em>parallel corpora )</em>를 사용해 이를 수행하도록 훈련시킬수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*GFPhspIf1OWGsOfWlB2u7Q.png" /></figure><p>이렇게 해서, 영어 단어로 된 시퀀스를 이에 해당하는 스페인어 단어의 시퀀스로 변환해주는 일반적인 방법이 만들어졌습니다!</p><p>이것은 정말 멋진 아이디어 입니다:</p><ul><li>이 접근 방법은 보유한 훈련 데이터의 양과 수행할 컴퓨터의 성능에만 제약이 있습니다. 기계 학습 연구자들이 겨우 <strong>2년</strong>전에 이것을 발명했지만, 벌써 개발하는데만 <strong>20년</strong>이 걸리는 통계기반 기계 번역 시스템과 동일한 성능을 보여주고 있습니다.</li><li>이 방법은 인간 언어에 관한 어떠한 규칙에도 의존하지 않습니다. 알고리즘이 이러한 규칙 자체를 파악합니다. 즉, 번역 경로(pipeline)의 각 단계를 조정해줄 전문가가 필요하지 않습니다. 컴퓨터가 대신 그 일을 합니다.</li><li><strong><em>이러한 접근법은 거의 모든 종류의 시퀀스-투-시퀀스(sequence-to-sequence) 문제에 적용할 수 있습니다!</em></strong> 사실 많은 흥미로운 문제들은 결국 시퀀스-투-시퀀스 문제입니다. 어떤 멋진 일들을 할 수 있는지 계속 읽어보세요!</li></ul><p>실제 데이터로 이러한 작업을 수행하는 데 필요한 몇 가지 사항만을 강조했다는 것을 아셔야 합니다. 예를 들어, 다양한 길이의 입력 및 출력 문장을 처리하기 위한 추가 작업이 있습니다 ( <a href="https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html#bucketing-and-padding">bucketing and padding</a> 참조). <a href="https://cs224d.stanford.edu/papers/addressing.pdf">드물게 쓰이는 단어를 올바르게 번역</a>하는 데에도 문제가 있을 수 있습니다.</p><h4><strong>나만의 시퀀스-투-시퀀스(sequence-to-sequence) 번역 시스템 구축</strong></h4><p>나만의 언어 번역 시스템을 만들고 싶다면, <a href="https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html">TensorFlow에 포함된 영어와 불어간 번역 을 위한 실제 데모를 참고하십시오.</a> 그렇다고 해서, 이 데모가 대충 테스트해보고 싶거나 예산이 부족한 사람들을 위한 것은 아닙니다. 이 기술은 여전히 새롭고 많은 리소스를 필요로 합니다. 고성능 비디오 카드가 있는 고속 컴퓨터를 사용하더라도 나만의 언어 번역 시스템을 훈련하는데 <a href="https://github.com/tensorflow/tensorflow/issues/600#issuecomment-226333266">대략 1개월</a> 정도의 연속적인 처리 시간이 소요될 수 있습니다.</p><p>또한 시퀀스-투-시퀀스<strong> </strong>언어 번역 기술이 급속도로 향상되고 있어 이를 따라잡기는 매우 어렵습니다. 최근의 많은 개선(예를 들어, <a href="http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf">attention mechanism</a> 또는 <a href="http://arxiv.org/abs/1607.00578">tracking context</a>)은 결과를 크게 향상시켜 주지만, 이러한 것들은 너무 새롭기 때문에 아직 위키피디아 페이지에 조차 없습니다. 시퀀스-투-시퀀스 학습으로 무엇인가 대단한 일을 하고 싶다면, 새로운 개발 방법이 나올때 마다 이를 공부해야만 합니다.</p><h3><strong>시퀀스-투-시퀀스(Sequence-to-Sequence) 모델의 황당한 힘</strong></h3><p>그렇다면, 시퀀스-투-시퀀스 모델로 또 어떤 것을 할 수 있나요?</p><p>약 1 년 전, <a href="https://arxiv.org/pdf/1506.05869.pdf">Google의 연구원들</a>은 시퀀스-투-시퀀스 모델을 사용해 AI 챗봇(chat bot) 을 만들 수 있음을 보여주었습니다. 아이디어는 너무 간단해서 동작하는 것이 놀라울 정도입니다.</p><p>먼저 Google 직원과 Google 기술 지원 팀 간의 채팅 로그를 수집했습니다. 그런 다음 직원의 질문이 입력 문장이고 기술 지원 팀의 응답이 해당 문장의 “번역”으로 해서 시퀀스-투-시퀀스 모델을 훈련시켰습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*ZkxQROt0QCJH3iVtzohumg.png" /></figure><p>사용자가 챗봇과 대화할 때, 각각의 사용자 메시지를 이 시스템으로 “번역”하고 챗봇의 응답을 얻습니다.</p><p>결과적으로 실제 기술 지원 질문에 (때때로) 대답 할 수 있는 준-인텔리전트 챗봇(a semi-intelligent bot)이 만들어졌습니다. 다음은 <a href="https://arxiv.org/pdf/1506.05869.pdf">이 논문</a>에서 따온 사용자와 챗봇 간의 샘플 대화의 일부입니다.</p><blockquote><strong>Human: </strong>connection refused or something like that<br><strong>Machine:</strong> may i know the version of network connect you connect?<br><strong>Human:</strong> i am not sure i know that<br><strong>Machine:</strong> is the network connect prompting for any user certificate?<br><strong>Human:</strong> yes<br><strong>Machine: </strong>are you at home or at work at the moment? office?<br><strong>Human:</strong> i am at home<br><strong>Machine:</strong> try this. goto &lt;URL redacted&gt; page and enter your name and password<br><strong>Human:</strong> done<br><strong>Machine:</strong> try logging in with &lt;NAME redacted&gt; and check if its working fine now<br><strong>Human:</strong> yes, now it works!<br><strong>Machine:</strong> great. anything else that i can help?</blockquote><p>이들은 또한 수백만 개의 영화 자막을 기반으로 한 챗봇을 만들어 봤습니다. 이 아이디어는 영화 등장인물들 간의 대화를 사용해 챗봇을 훈련시키고 인간처럼 이야기하게 하는 것이 었습니다. 입력 문장은 한 등장 인물이 말한 대화의 한줄이고 “번역”은 다음 등장 인물이 대답한 한 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*XZEJ9KLxi7gaCzJEef3ntg.png" /></figure><p>정말로 흥미로운 결과가 나왔습니다. 챗봇은 인간처럼 대화를 나눌 수 있었을 뿐만 아니라 아주 적긴 하지만 지능을 보여줬습니다.</p><blockquote><strong>Human: </strong>my name is david. what is my name?<br><strong>Machine:</strong> david.<br><strong>Human:</strong> my name is john. what is my name?<br><strong>Machine:</strong> john.<br><strong>Human:</strong> are you a leader or a follower?<br><strong>Machine:</strong> i’m a leader.<br><strong>Human:</strong> are you a follower or a leader?<br><strong>Machine:</strong> i’m a leader.</blockquote><p>이것은 단지 가능성의 시작일뿐입니다. 한 문장을 다른 문장으로 변환하는 것에만 국한되지 않습니다. 이미지를 문장으로 바꾸는 이미지-투-시퀀스(image-to-sequence)모델을 만드는 것도 가능합니다!</p><p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">Google의 또다른 팀</a>은 첫 번째 RNN을 (<a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa">Part 3</a>에서 배웠던) 컨볼루션 신경망(Convolutional Neural Network)로 바꿔서 이를 구현했습니다. 이렇게하면 입력을 문장 대신 사진으로 사용할 수 있습니다. 나머지는 기본적으로 같은 방식으로 동작합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*zJVeGg2AoTiaN3LI7QioVQ.png" /></figure><p>그러면 당연하게도, 우리는 그림을 글로 바꿀 수 있습니다(물론 아주 많은 훈련 데이터가 필요합니다)!</p><p><a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>는 <a href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf">이 아이디어를 확장해서</a> 이미지의 여러 영역을 개별적으로 처리함으로써 이미지를 매우 자세하게 설명 할 수있는 시스템을 구축했습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*-juJdIlL65YLx_x7OBg2LQ.png" /><figcaption><a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>의 <a href="http://cs.stanford.edu/people/karpathy/cvpr2015.pdf">논문</a>에서 가져온 이미지</figcaption></figure><p>이를 이용하면, 구체적인 검색어와 신기할 정도로 일치하는 이미지를 찾을 수있는 <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/">이미지 검색 엔진</a>을 만들 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/444/1*gzhEps_3xAD55J5afEo8vQ.gif" /><figcaption><a href="http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/">http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/</a>의 예제 화면</figcaption></figure><p>심지어 이와는 반대로 설명하는 문장 만으로 전체 그림을 생성하는 <a href="http://arxiv.org/pdf/1506.03500.pdf">문제를 해결하려는 연구원들</a>도 있습니다!</p><p>이러한 예제들로부터 당신은 어떤 가능성도 상상할 수 있습니다. 지금까지 음성 인식(speech recognition)에서 영상 처리(computer vision)에 이르기까지 모든 분야에서 시퀀스-투-시퀀스 애플리케이션이 연구되어 왔습니다. 분명히 내년에는 훨씬 더 많은 것이 만들어 질 것입니다.</p><p>시퀀스-투-시퀀스 모델과 번역에 대해 좀더 깊게 알고 싶다면, 추천하는 다음 자료들을 참고하십시오:</p><ul><li><a href="https://www.youtube.com/watch?v=qGlmW2n4s1w">Richard Socher’s CS224D Lecture — Fancy Recurrent Neural Networks for Machine Translation</a> (video)</li><li><a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf">Thang Luong’s CS224D Lecture — Neural Machine Transation</a> (PDF)</li><li><a href="https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html">TensorFlow’s description of Seq2Seq modeling</a></li><li><a href="http://www.deeplearningbook.org/contents/rnn.html">The Deep Learning Book’s chapter on Sequence to Sequence Learning</a>(PDF)</li></ul><p>이 글이 마음에 들었다면, 제 <a href="http://eepurl.com/b9fg2T"><strong>Machine Learning is Fun! 이메일 리스트에 가입</strong></a>하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.</p><p>Twitter의 <a href="https://twitter.com/ageitgey">@ageitgey</a>인 저를 팔로우하거나, <a href="mailto:ageitgey@gmail.com">직접 이메일</a>을 보내거나 또는 <a href="https://www.linkedin.com/in/ageitgey">linkedin에서 저를 찾을 수도 있습니다</a>. 기계 학습으로 제가 당신이나 당신의 팀을 도울 수 있다면 언제든 연락 주십시오.</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=83b7a44b797a" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 4]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/63ed781eee3c</guid>
            <category><![CDATA[얼굴인식]]></category>
            <category><![CDATA[딥러닝]]></category>
            <category><![CDATA[기계학습]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[머신러닝]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Wed, 21 Dec 2016 06:54:54 GMT</pubDate>
            <atom:updated>2017-11-09T13:23:45.275Z</atom:updated>
            <content:encoded><![CDATA[<p>딥러닝(Deep Learning)을 사용한 최신 얼굴 인식(Face Recognition)</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p>Facebook이 당신의 사진 속에서 친구들을 인식하는 신기한 기능을 이미 개발한 것을 알고 계셨나요? 이전 Facebook에서는 사진을 클릭하고 이름을 입력해야만 친구들에 대한 태그를 달 수 있었습니다. 이제 사진을 업로드하자마자 Facebook은 마술처럼 사람들에 대한 태그를 달아줍니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*1cqoJbR49tytC36yZsLl-g.gif" /><figcaption>Facebook에서 이전에는 태그를 추가해야만 했지만 이제는 사진에 사람들에 대한 태그를 자동으로 달아줍니다. 이것이 도움을 주는것인지 오싹한 일인지 확실하진 않네요!</figcaption></figure><p>이 기술을 얼굴 인식(face recognition)이라고 합니다. 페이스북의 알고리즘은 친구사진에 단 몇 번 만 태그를 달아도 바로 그 얼굴을 인식 할 수 있습니다. 이것은 꽤 놀라운 기술입니다 — 페이스북은 인간이 할 수 있는 만큼이나 좋은 98%의 정확도로 얼굴을 인식할 수 있습니다!</p><p>이제 최신의 얼굴 인식이 어떻게 동작하는지 알아보겠습니다! 그런데 단지 당신의 친구들을 인식하는 것은 너무 쉬울지도 모릅니다. 우리는 이 기술을 한계까지 밀어붙여서 <a href="https://en.wikipedia.org/wiki/Will_Ferrell">Will Ferrell</a>(유명 배우)와 <a href="https://en.wikipedia.org/wiki/Chad_Smith">Chad Smith</a>(유명한 록 뮤지션)을 구분하는 것과 같은 더욱 도전적인 문제를 해결해 볼 수도 있습니다</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*6EVz0iIrcjC1Sb0wnx_o8Q.jpeg" /><figcaption>이 중 한 명은 Will Farrell이고 다른 사람은 Chad Smith입니다. 맹세컨데 정말 다른 사람입니다!</figcaption></figure><blockquote>* 역자주: 여기서 <a href="https://en.wikipedia.org/wiki/Chad_Smith">Chad Smith</a>와 <a href="https://en.wikipedia.org/wiki/Will_Ferrell">Will Ferrell</a> 사진을 구분하는 것을 어려운 예로 드는 이유는, 이 두 사람이 닮은 꼴로 유명해서 도플갱어로 불리고 유명한 The Tonight Show라는 토크쇼에도 출연했었기 때문입니다. <a href="https://www.google.co.kr/search?q=will+ferrell+chad+smith+look+alike&amp;espv=2&amp;biw=1364&amp;bih=629&amp;tbm=isch&amp;tbo=u&amp;source=univ&amp;sa=X&amp;ved=0ahUKEwiFpYTp-f3QAhXCFpQKHSnwC9IQsAQIFw">구글에서 검색</a>해 보면 이 두사람을 비교하는 사진과 글을 많이 확인하실 수 있습니다.</blockquote><h3><strong>매우 복잡한 문제에 기계 학습을 사용하는 방법</strong></h3><p>지금까지 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a>, <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a>에서, 우리는 주택 가격 예측하기, 기존 데이터를 기반으로 새로운 데이터 생성하기 그리고 이미지에 특정 객체가 있는지 여부를 알려 주기와 같은 단 한 가지 단계(step)만 있는 독립된 문제를 해결하기 위해 기계학습을 사용해 봤습니다. 이러한 종류의 문제는 하나의 기계 학습 알고리즘을 선택하고 데이터를 제공(feed)해서 결과를 얻으면 해결할 수 있습니다.</p><p>그러나 얼굴 인식은 일련의 여러가지 관련된 문제들을 해결하는 과정입니다:</p><p>1. 먼저 사진을 보고 그 안에 있는 모든 얼굴을 찾아야 합니다.</p><p>2. 둘째, 각 얼굴에 초점을 맞추고 얼굴이 이상한 방향으로 틀어졌거나 또는 조명이 안좋은 상황이라도, 여전히 같은 사람이라는 것을 이해할 수 있어야 합니다.</p><p>3. 셋째, 눈이 얼마나 큰지, 얼굴은 얼마나 긴지 등과 같이 다른 사람들과 구분하는데 사용하는 얼굴의 고유한 특징을 찾아 낼 수 있어야 합니다.</p><p>4. 마지막으로, 그 얼굴의 고유한 특징을 기존에 알고 있는 모든 사람들과 비교해서 그 사람의 이름을 결정해야 합니다.</p><p>인간으로서, 당신의 두뇌는 이 모든 것을 자동적으로 또 즉시 수행하도록 연결되어 있습니다. 사실 인간은 일상의 모든 사물의 얼굴 또는 표면을 발견하고 인식하는 일을<em>너무나도 잘</em> 합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/580/1*zLTdIvaQXJglp_Nps8033A.jpeg" /></figure><p>컴퓨터는 이러한 높은 수준의 일반화를 (적어도 아직까지는…) 할 수 없기 때문에, 이 프로세스를 분리해서 각 단계별로 수행하는 방법을 가르쳐야 합니다.</p><p>우리는 얼굴 인식을 분리해서 각 단계별로 문제를 해결하고, 현 단계의 결과를 다음 단계로 넘겨주는 <em>경로(pipeline)</em>를 만들어야 합니다. 다시 말해서, 몇가지 기계 학습 알고리즘을 함께 연결할 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ykj2p7mPYYBDLhTyMe98Uw.gif" /><figcaption>얼굴을 인식하기 위한 기본 경로(pipeline)의 동작 방식</figcaption></figure><h3><strong>단계별 얼굴 인식</strong></h3><p>한 번에 한 단계씩 이 문제를 해결해 보겠습니다. 각 단계마다 다른 기계 학습 알고리즘에 대해 배우게 됩니다. 이 글이 책처럼 되지 않도록 하기 위해서 모든 알고리즘을 하나 하나 완전하게 설명하진 않을 것입니다. 하지만, 전체적인 중요 아이디어를 배우고 나서 <a href="https://cmusatyalab.github.io/openface/">OpenFace</a>와 <a href="http://dlib.net/">dlib</a>을 사용해 Python으로 자신 만의 얼굴인식 시스템을 만드는 방법을 배우게 될 것입니다.</p><h4><strong>Step 1: 모든 얼굴 찾기</strong></h4><p>우리의 경로(pipeline)에서 첫 번째 단계는 <em>얼굴 검출(face detection)</em>입니다. 당연하게도 우리가 얼굴을 구별하기 위해서는 먼저 사진 상에서 얼굴을 찾아야 합니다!</p><p>지난 10년 사이 아무 카메라나 사용해 봤다면, 얼굴 인식 기능이 들어있는걸 본적이 있을겁니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*WN6dCaX70miZBltLD69sbA.png" /></figure><p>얼굴 인식은 카메라의 중요한 기능입니다. 카메라가 얼굴을 자동으로 인식할 수 있다면, 사진을 찍기 전에 모든 얼굴에 확실하게 초점을 맞춰줄 수 있습니다. 그런데 우리는 이 기능을 다른 목적으로 사용할 것입니다 — 즉, 이 기능을 우리의 경로(pipeline)의 다음 단계로 넘길 이미지 영역을 찾는데 사용할 것입니다.</p><p>Paul Viola와 Michael Jones가 값싼 카메라에도 사용할 정도로 빠른 <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework">얼굴 인식 방법</a>을 발명한 2000년대 초반부터 얼굴 인식은 대세가 되었습니다. 그런데, 현재는 훨씬 더 신뢰할 수 있는 솔루션이 존재합니다. 우리는 Histogram of Oriented Gradients(또는 줄여서 <strong><em>HOG</em></strong>)라 불리는 2005년에 발명 된 방식을 사용할 것입니다.</p><p>이미지에서 얼굴을 찾기위해 우리는 이미지를 흑백으로 바꾸는 것부터 시작할 것입니다. 왜냐하면 얼굴을 찾는데 색상 데이터는 필요 없기 때문입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/511/1*dP0Ixs4vHGUKCScufH9_Vw.jpeg" /></figure><p>그런 다음 한번에 하나씩 이미지의 모든 단일 픽셀을 살펴 보겠습니다. 모든 단일 픽셀에 대해 이를 직접 둘러싸고 있는 픽셀들 또한 살펴볼 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*GUyJM0sfZudHlSyloT8Sxw.gif" /></figure><p>우리의 목표는 해당 픽셀이 이를 직접 둘러싸고 있는 픽셀들과 비교해서 얼마나 어두운지 알아내는 것입니다. 이를 통해서 이미지가 어두워지는 방향을 나타내는 화살표를 그리고자 합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*luZPVo1tSMoKf3B_7Ikaig.gif" /><figcaption>이 한개의 픽셀과 이 픽셀에 인접한 픽셀들을 살펴 보면, 이미지는 오른쪽 상단으로 갈수록 어두워집니다.</figcaption></figure><p>이미지의 <strong>모든 픽셀</strong>에 대해 이 프로세스를 반복하면 결국 모든 픽셀이 화살표로 바뀌게 됩니다. 이러한 화살표들을 <em>그래디언트(gradients)</em>라고 부르고, 이를 통해 전체 이미지에서 밝은 부분으로부터 어두운 부분으로의 흐름을 알 수 있습니다:</p><blockquote>* 역자주: gradients를 번역할 단어를 찾지 못해 영어 발음 그대로 씁니다. 일반적으로 쓰는 기울기, 변화도, 그라데이션은 절대 아니며, 여기서는 방향성을 갖는 Vector의 특징이 있습니다. 정확한 단어 알고 계시는 분 <a href="mailto:jongdae.lim@oracle.com">연락</a>바랍니다.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*hon65l3ucLaaOVdlfk1_eA.gif" /></figure><p>이것은 아무것도 아닌 일처럼 보일지 모릅니다. 그런데, 픽셀을 그래디언트(gradients)로 바꿔야 하는 정말 좋은 이유가 있습니다. 픽셀을 직접 분석하면, 동일한 사람의 정말 어두운 이미지와 정말 밝은 이미지는 전혀 다른 픽셀값을 갖게 될 것입니다. 그러나 밝기가 변하는 <em>방향</em> 만 고려하면 정말 어두운 이미지와 정말 밝은 이미지에 대한 완전히 동일한 표현(representation)을 얻게 됩니다. 이렇게 하면 문제를 훨씬 더 쉽게 해결할 수 있습니다!</p><p>그러나 모든 단일 픽셀에 대해 그래디언트를 저장하면 너무 자세합니다. 이렇게 되면 결국 우리는 <a href="https://en.wiktionary.org/wiki/see_the_forest_for_the_trees">나무를 보고 숲을 보지 못하게 됩니다</a>. 이미지의 기본 패턴을 알 수 있도록, 높은 수준에서 밝음/어둠의 기본 흐름만을 보는 것이 더 좋습니다.</p><p>이를 위해, 이미지를 각각 16x16 픽셀의 작은 정사각형들으로 분해합니다. 각 정사각형에서, 우리는 얼마나 많은 그래디언트가 주요 방향(윗쪽, 우상쪽, 오른쪽, 등)을 얼마나 가리키고 있는지 세어 볼 것입니다 . 그런 다음 이 사각형을 가장 강한 화살표 방향으로 바꿀 것입니다.</p><p>결과적으로, 우리는 원본 이미지를 얼굴의 기본 구조가 심플한 방법으로 표시되는 매우 간단한 표현(representation)으로 변환하게 됩니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*lsNRg_1oOELFcug_AjlkqQ.gif" /><figcaption>원본 이미지는 이미지 밝기에 관계없이 이미지의 주요 특징들을 캡처한 HOG 표현로 바뀝니다.</figcaption></figure><p>이 HOG 이미지에서 얼굴을 찾기위해서 우리가 해야 할 일은 많은 훈련 얼굴 이미지로부터 추출 된 잘 알려진 HOG 패턴과 가장 유사하게 보이는 부분을 이미지에서 찾는 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*HtgQZ4guaIo8wflbsR1MLw.png" /></figure><p>이 테크닉을 사용하면 어떠한 이미지에서도 얼굴을 쉽게 찾을 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*IWKE3KFq1Bmzah0ydWy2jw.jpeg" /></figure><p>Python과 dlib을 사용해서 이 단계를 직접 해보고 싶다면, 여기 이미지로부터 HOG 표현을 생성하고 확인할 수 있는 <a href="https://gist.github.com/ageitgey/1c1cb1c60ace321868f7410d48c228e1">코드</a>가 있습니다.</p><h4><strong>Step 2: 얼굴의 위치교정(Posing)과 투영(Projection)</strong></h4><p>휴유, 드디어 이미지에서 얼굴들만을 분리해 냈습니다. 그러나 이제 얼굴이 다른 방향을 보고 있으면 컴퓨터에게는 전혀 다르게 보이게 되는 새로운 문제를 해결해야 합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*8296FErEJLrVN891kyItYw.png" /><figcaption>인간은 두 이미지 모두 Will Ferrell이라는 것을 쉽게 인식할 수 있지만, 컴퓨터는 이 사진들을 완전히 다른 두 사람으로 보게됩니다.</figcaption></figure><p>이를 해결하기 위해, 각각의 사진을 비틀어 눈과 입술이 항상 표준 위치에 올 수 있도록 해보려고 합니다. 이렇게하면 다음 단계들에서 얼굴을 좀 더 쉽게 비교할 수 있습니다.</p><p>이렇게 하기 위해서, 우리는 <strong>face landmark estimation</strong>이라고 하는 알고리즘을 사용할 것입니다. 여러 방법이 많이 있지만, <a href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf">2014년에 Vahid Kazemi와 Josephine Sullivan이 발명 한 접근 방법</a>을 사용하려고합니다.</p><p>기본적인 아이디어는 모든 얼굴에 존재하는 68개의 <em>랜드마크(landmarks)</em>라 부르는 특정 포인트(턱의 상단, 눈 바깥의 가장자리, 눈썹 안쪽의 가장자리, 등)를 찾아 내는 것입니다. 이제 우리는 기계 학습 알고리즘을 훈련시켜 어떤 얼굴에서든 이러한 68개의 특정 포인트들을 찾을 수 있도록 할 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/414/1*96UT-D8uSXjlnyvs9DZTog.png" /><figcaption>모든 얼굴에 표시할 68개의 랜드마크(landmarks). 이 이미지는 <a href="https://github.com/cmusatyalab/openface">OpenFace</a>에서 일하는 CMU의 <a href="http://bamos.github.io/">Brandon Amos</a>가 만들었습니다.</figcaption></figure><p>우리의 테스트 이미지에 68개의 얼굴 랜드마크를 표시한 결과입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*uWr1hhpREMdvEfyhGMEfFw.jpeg" /><figcaption><strong>전문가 팁</strong> :이 기술을 동일하게 사용해서 Snapchat의 실시간 3D 얼굴 필터를 당신만의 버전으로 구현할 수도 있습니다.</figcaption></figure><p>눈과 입이 어디에 있는지 알게되었으므로, 이제 눈과 입이 최대한 가운데로 올 수 있도록 이미지를 간단하게 회전(rotate)하고, 크기를 조절(scale)하고 비틀(<a href="https://en.wikipedia.org/wiki/Shear_mapping#/media/File:VerticalShear_m%3D1.25.svg">shear</a>) 것입니다. 여기서 우리는 환상적인 3d 변형(3d warps)를 사용하지는 않을 것입니다. 그렇게 하면 이미지에 왜곡이 생기기 때문입니다. 여기서 우리는 평행선을 보존하는 회전(rotation) 및 크기 조절(scale)과 같은 기본적인 이미지 변환방법(<a href="https://en.wikipedia.org/wiki/Affine_transformation">affine transformations</a>이라 부름) 만을 사용할 것입니다:</p><blockquote>* 역자주: 여기서 3d 변형(3d warps)는 이미지를 3차원 상에서 변형하거나 뒤트는 것을 말합니다. 다음과 같은 <a href="https://www.google.co.jp/search?q=3d+warps&amp;espv=2&amp;biw=1364&amp;bih=678&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjgtdeXs4LRAhVKtJQKHUqeAYUQ_AUIBigB#imgrc=KUE-aiA5I88V3M%3A">이미지</a>를 참고하세요.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*GEOO5onAxlpLY3WbmNgCCA.png" /></figure><p>이제 얼굴이 옆으로 돌아가 있어도, 우리는 눈과 입을 이미지상에서 대략 동일한 위치에 맞출 수 있습니다. 이렇게 함으로써 다음 단계가 훨씬 더 정확해질 것입니다.</p><p>Python과 dlib을 사용해 이번 단계를 직접 해보고 싶다면, 여기 <a href="https://gist.github.com/ageitgey/ae340db3e493530d5e1f9c15292e5c74">얼굴 랜드마크(landmarks)을 찾는 코드</a>와 랜트마크를 사용해 <a href="https://gist.github.com/ageitgey/82d0ea0fdb56dc93cb9b716e7ceb364b">이미지를 변형하는 코드</a>가 있습니다.</p><h4><strong>Step 3 : 얼굴 인코딩</strong></h4><p>이제 문제의 핵심인 실제로 얼굴을 구별하는 단계에 이르렀습니다. 이 단계는 정말 재미있는 곳입니다!</p><p>얼굴 인식을 가장 간단하게 접근하는 방법은 앞서 Step 2에서 찾아낸 모르는 얼굴을 이미 태그가 달린 사람들의 모든 사진과 직접 비교하는 것입니다. 모르는 얼굴과 매우 비슷하게 보이는 이미 태그가 달린 얼굴을 찾게되면, 분명히 같은 사람일 것입니다. 꽤 괜찮은 아이디어 같아 보이죠?</p><p>그런데 이 접근 방법에는 큰 문제가 있습니다. 수십억 명의 사용자와 1조 개의 사진이 있는 Facebook과 같은 사이트는 이미 태그가 달린 모든 얼굴과 새롭게 업로드된 사진을 일일이 반복해서 비교할 수는 없습니다. 너무 오래 걸릴 것입니다. 시간 단위가 아닌 밀리초 단위로 얼굴을 인식 할 수 있어야 합니다.</p><p>우리에게 필요한 것은 각 얼굴에서 몇 가지 기본 측정값을 추출하는 방법입니다. 그런 다음에 모르는 얼굴을 이 방법으로 측정하면 이 측정값과 가장 가까운 값을 가지는 얼굴을 찾을 수 있습니다. 예를 들어, 각 귀의 크기, 눈 사이의 간격, 코의 길이 등을 측정해 볼 수 있을 겁니다. <a href="https://en.wikipedia.org/wiki/CSI:_Crime_Scene_Investigation">CSI</a>와 같은 범죄 드라마를 본 적이 있다면, 무슨 말을하고 있는지 알 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*GoSlTxtPsJ2ep8fp3iSvmA.gif" /><figcaption>TV에서 하는 것처럼! 이거 진짜입니다! #science</figcaption></figure><h4><strong>가장 신뢰할 수 있는 얼굴 측정 방법</strong></h4><p>좋습니다, 그런데 아는 얼굴의 데이터베이스를 만들기 위해 얼굴에서 어떤 측정값을 수집해야 할까요? 귀의 크기? 코의 길이? 눈의 색깔? 아니면 다른 어딘가?</p><p>눈의 색깔과 같이 인간에게는 분명한 것으로 보이는 측정값들이 이미지의 개별 픽셀을 보는 컴퓨터에게는 전혀 의미가 없습니다. 최근에 연구자들은 컴퓨터가 수집할 측정값을 스스로 파악하도록 하는 것이 가장 정확한 접근 방법임을 알게 되었습니다. 딥러닝은 얼굴의 어느 부분이 측정에 있어 중요한 것인지 파악하는 일을 인간보다 더 잘 수행합니다.</p><p>해결책은 바로 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa">Part 3</a>에서 했던 것처럼 딥 컨볼루션 신경망(Deep Convolutional Neural Network)을 훈련시키는 것입니다. 그러나 우리가 마지막으로 했던 것처럼 사진에서 객체를 인식하도록 신경망을 훈련시키는 대신, 각 얼굴에 대해 128개의 측정값을 생성하도록 훈련시킬 것입니다.</p><p>훈련 과정은 3 개의 얼굴 이미지를 차례 대로 확인하는 것입니다.</p><p>1. 훈령용 아는 사람의 얼굴 사진 적재(load)</p><p>2. 동일한 아는 사람의 다른 사진 적재</p><p>3. 전혀 다른 사람의 사진 적재</p><p>그리고 알고리즘은 세 개의 이미지 각각에 대해 현재 생성하고 있는 측정값을 확인합니다. 그런 다음에, #1 과 #2 이미지에 대해 생성한 측정값은 서로 좀더 가깝게 하고 #2와 #3의 측정값은 좀더 멀어지도록 신경망을 조금 조정합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*S8Z07bmdtHT9SI6Qynkcyw.png" /></figure><p>수천 명의 다른 사람들의 수백만 개의 이미지에 대해 이 단계를 수백만 번 반복 하면, 신경망은 각 사람에 대한 128개의 측정값을 신뢰성 있게 생성하는 방법을 배우게 됩니다. 동일한 사람의 10개의 다른 사진에 대해서 대략적으로 동일한 측정값을 제공해야 합니다.</p><p>기계 학습을 하는 사람들은 각 얼굴에 대한 128개의 측정값을 <strong>임베딩(embedding)</strong>이라고<strong> </strong>부릅니다. 사진과 같이 복잡한 원시 데이터(raw data)를 컴퓨터가 생성한(computer-generated) 숫자값의 목록으로 축소한다는 아이디어는 기계 학습(특히 언어 번역)에서 많이 사용되고 있습니다. 우리가 지금 사용하고 있는 얼굴 인식의 정확한 접근 방법은 <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf">2015년에 Google의 연구원에 의해 발명된 것이지만</a> 이와 비슷한 접근 방법은 많이 있습니다.</p><h4><strong>우리의 얼굴 이미지 인코딩 하기</strong></h4><p>임베딩(embeddings)을 생성할 수 있도록 컨볼루션 신경망을 훈련시키는 프로세스는 많은 양의 데이터와 컴퓨터 자원을 필요로 합니다. 비싼 <a href="http://www.nvidia.com/object/tesla-supercomputing-solutions.html">NVidia Telsa 비디오 카드</a>를 사용하더라도, 좋은 정확도를 얻기위해서는 <a href="https://twitter.com/brandondamos/status/757959518433243136">대략 24시간</a>의 지속적인 훈련이 필요합니다.</p><p>그러나 일단 신경망이 훈련을 마치면, 이전에 절대 본적도 없는 얼굴에 대해서도 측정값을 생성할 수 있습니다! 따라서 이 단계는 딱 한 번만 수행하면됩니다. 운좋게도, <a href="https://cmusatyalab.github.io/openface/">OpenFace</a>의 훌륭한 사람들이 이미 이 작업을 진행한 다음, 우리가 바로 사용할 수 있는 <a href="https://github.com/cmusatyalab/openface/tree/master/models/openface">몇 가지 훈련 된 신경망을 공개했습니다</a>. <a href="http://bamos.github.io/">Brandon Amos</a>와 그의 팀에게 감사드립니다!</p><p>따라서 우리가 직접 해야 할 일은 사전 훈련된 신경망을 사용해서 우리의 얼굴 이미지로부터 각 얼굴에 대한 128개의 측정 값을 얻는 것입니다. 다음은 우리의 테스트 이미지에 대한 측정값입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*xwtGXHiWYcrmxjdDHm9JHw.png" /></figure><p>그런데, 이러한 128개의 숫자들이 정확히 얼굴의 어느 부분을 측정한 것일까요? 우리는 전혀 알 수가 없습니다. 하지만, 전혀 문제가 되지 않습니다. 우리가 신경써야 할 것은 동일한 사람의 다른 두개의 사진에 대해 거의 같은 숫자들을 생성한다는 것입니다.</p><p>이 단계를 직접 해보고 싶다면, 폴더 안의 모든 이미지에 대한 임베딩(embeddings)를 생성해주는 OpenFace의 <a href="https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua">lua 스크립트</a>를 사용하면 됩니다. <a href="https://gist.github.com/ageitgey/ddbae3b209b6344a458fa41a3cf75719">이런식으로 실행</a>하면 됩니다.</p><h4><strong>Step 4: 인코딩에서 사람의 이름 찾기</strong></h4><p>이 마지막 단계는 사실 전체 프로세스에서 가장 쉬운 단계입니다. 우리가 해야할 일은 테스트 이미지에 가장 근접한 측정값을 갖고 있는 사람을 우리의 아는 사람에 대한 데이터베이스에서 찾아내는 것입니다.</p><p>기본적인 기계 학습 분류 알고리즘을 사용해서 이 작업을 수행 할 수 있습니다. 대단히 멋진 딥러닝 기술이 필요하지 않습니다. 다양한 분류 알고리즘을 사용할 수 있겠지만, 우리는 간단한 선형 <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVM 분류기(classifier</a>)를 사용하겠습니다.</p><p>우리가 해야 할 일은 새로운 테스트 이미지로부터 측정값을 가져와 가장 매칭되는 아는 사람이 누구인지 알려주는 분류기(classifier)를 훈련시키는 것입니다. 이 분류기의 실행시간은 밀리초 밖에 되지 않습니다. 이 분류기의 결과물은 바로 그 사람의 이름입니다!</p><p>이제 우리 시스템을 시험해 보겠습니다. 먼저, Will Ferrell, Chad Smith 그리고 Jimmy Falon 각각에 대한 20장 정도의 사진들에 대한 임베딩으로 분류기를 훈련시켰습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*RyN_DC_aV4dOJt7QNE-DgQ.jpeg" /><figcaption>아주 멋진 훈련 데이터!</figcaption></figure><p>그런 다음에 <a href="https://www.youtube.com/watch?v=EsWHyBOk2iQ">Will Ferrell과 Chad Smith가 Jimmy Fallon 쇼에서 서로 같은 사람인척 하는 유명한 유튜브 비디오</a>의 모든 프레임에서 분류기를 실행해 봤습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*IHy4EB25kpO9Dh_ugCJp7Q.gif" /></figure><p>정말 동작합니다! 그리고 다른 포즈의 얼굴에 대해서도 얼마나 잘 동작하는지 보세요 — 심지어 옆 모습에서도!</p><h3><strong>직접 실행해 보기</strong></h3><p>지금까지 진행한 단계들을 검토해보겠습니다:</p><p>1. 이미지의 단순화된 버전을 만들어 주는 HOG 알고리즘을 사용해 사진을 인코딩합니다 . 이 단순화된 이미지에서 얼굴의 일반 HOG 인코딩(generic HOG encoding)과 가장 유사하게 보이는 부분을 찾습니다.</p><p>2. 얼굴의 주요 랜드마크(landmarks)를 찾아 얼굴의 포즈(pose)를 알아냅니다. 이 랜드마크를 찾으면, 이를 이용해서 눈과 입이 중앙에 오도록 이미지를 변형시킵니다.</p><p>3. 얼굴의 특징들을 측정하는 방법을 알고 있는 신경망에 중심을 맞춘 얼굴 이미지를 전달합니다. 그리고 128개의 측정값을 저장합니다.</p><p>4. 과거에 측정해 놓은 모든 얼굴에 대해, 이 얼굴의 측정값에 가장 가까운 사람이 누구인지 확인합니다. 이 사람이 바로 그 사람입니다!</p><p>이제 모든 것이 어떻게 동작하는지 알게되었습니다. 다음은 당신의 컴퓨터에서 <a href="https://cmusatyalab.github.io/openface/">OpenFace</a>를 사용해 얼굴 인식의 전체 경로(pipeline)를 실행하는 방법에 대한 처음부터 끝까지 모든 단계별 설명입니다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ec3618758154c20311409ce36edc1c11/href">https://medium.com/media/ec3618758154c20311409ce36edc1c11/href</a></iframe><p>이 글이 마음에 들었다면, 제 <a href="http://eepurl.com/b9fg2T"><strong>Machine Learning is Fun! 이메일 리스트에 가입</strong></a>하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.</p><p>Twitter의 <a href="https://twitter.com/ageitgey">@ageitgey</a>인 저를 팔로우하거나, <a href="mailto:ageitgey@gmail.com">직접 이메일</a>을 보내거나 또는 <a href="https://www.linkedin.com/in/ageitgey">linkedin에서 저를 찾을 수도 있습니다</a>. 기계 학습으로 제가 당신이나 당신의 팀을 도울 수 있다면 언제든 연락 주십시오.</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=63ed781eee3c" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 3]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/928a841a3aa</guid>
            <category><![CDATA[머신러닝]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[딥러닝]]></category>
            <category><![CDATA[인공지능]]></category>
            <category><![CDATA[기계학습]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Fri, 16 Dec 2016 08:28:45 GMT</pubDate>
            <atom:updated>2017-11-09T13:23:10.015Z</atom:updated>
            <content:encoded><![CDATA[<p>딥러닝(Deep Learning)과 컨볼루션 신경망(Convolutional Neural Network 또는 CNN)</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p><em>딥러닝(Deep Learning)</em>에 관한 끝없는 뉴스 기사를 읽고는 있지만 실제로는 무슨 의미인지 잘 모르시나요? 그럼 이제 바꿔봅시다!</p><p>이번에는 딥러닝을 사용해서 사진속에서 객체를 인식하는 프로그램을 작성하는 방법에 대해 알아보도록 하겠습니다. 다시 말해, Google Photos에서 사진 속에 포함된 객체를 기반으로 원하는 사진을 검색할 수 있는 그 흑마법(black magic)에 대해 알려드리겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*yy4ZsTLgVGGiydnGJ2tFtQ.gif" /><figcaption>Google에서는 이제 사진에 태그가 지정되지 않아도! 원하는 사진을 검색 할 수 있습니다! 도대체 어떻게 하는걸까요??</figcaption></figure><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8">Part 1</a>과 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761">Part 2</a>와 마찬가지로, 이 안내서는 기계 학습에 궁금한 점은 있지만 어디서부터 시작해야할지 모르는 사람들을 위한 것입니다. 이 글의 목표는 누구에게나 쉽게 다가가는 데 있습니다 — 이는 글에 많은 일반화가 있음을 의미합니다. 하지만 어떻습니까? 그래서 더 많은 사람들이 ML에 관심을 가지게 된다면, 목표를 달성한 것입니다.</p><p><em>(아직 </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>과 </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>,를 읽지 않았다면, 지금 읽어보세요!).</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/267/1*mlnNWBd7P_TbJB4yhCxUGA.png" /><figcaption>xkcd #1425 (<a href="http://xkcd.com/1425/">원본 보기</a>)</figcaption></figure><p>당신은 <a href="http://xkcd.com/1425/">이 유명한 xkcd 웹툰</a>를 본 적이 있을지도 모릅니다.</p><p>이 웹툰은 3살짜리 아이는 새의 사진을 쉽게 인식 할 수 있어도, 50년 이상 최고의 컴퓨터 과학자들은 컴퓨터로 객체를 인식하는 방법을 알아내기 위해 노력했으나 불가능했다는 것을 보여줍니다.</p><p>지난 몇 년 사이, 우리는 마침내 <em>딥 컨볼루션 신경망(deep convolutional neural networks)</em>을 사용해 드디어 객체 인식에 대한 좋은 접근방법을 발견했습니다. 마치 <a href="https://ko.wikipedia.org/wiki/%EC%9C%8C%EB%A6%AC%EC%97%84_%EA%B9%81%EC%8A%A8">William Gibson</a>의 SF소설에서 따온 인용문처럼 들리지만, 이 아이디어를 하나씩 차례 차례 들여다 보면 완전히 이해할 수 있습니다.</p><p>자 그럼 시작하겠습니다. 새를 인식할 수 있는 프로그램을 만들어 봅시다!</p><h3><strong>심플하게 시작</strong></h3><p>새의 사진을 인식하는 방법을 배우기 전에, 손으로 쓴 숫자 “8”과 같은 좀더 심플한 것을 인식하는 방법을 알아보겠습니다.</p><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761">Part 2</a>에서는 단순한 뉴런(neuron)을 많이 연결함으로써 신경망(neural network)이 어떻게 복잡한 문제를 해결할 수 있는지에 대해 배웠습니다. 그리고 우리는 얼마나 많은 침실이 있는지, 얼마나 큰지, 어느 동네에 있는지에 따라 집의 가격을 추정 할 수있는 작은 신경망을 만들었습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*GkHwF2dInQET_SL8r-BfbA.png" /></figure><p>우리는 또한 기계 학습이라는 아이디어는 동일한 일반 알고리즘을 다른 데이터로 재사용해서 다른 종류의 문제를 해결할 수 있다는 것을 알고 있습니다. 그럼 필기체를 인식하기 위해 이 신경망을 수정하겠습니다. 그러나 일을 정말 심플하게 만들기 위해, 우리는 하나의 글자인 숫자 “8”만을 인식할 해볼 예정입니다.</p><p>기계 학습은 데이터가 있을 때 그것도 아주 많이 있을 때 잘 동작합니다. 그래서 손으로 쓰여진 “8”이 정말 많이 필요합니다. 다행이도, 연구자들이 이와 같은 목적을 위해 필기체 숫자의 MNIST 데이터 세트(<a href="http://yann.lecun.com/exdb/mnist/">MNIST data set of handwritten numbers</a>)를 만들었습니다. MNIST는 하나 당 18x18 픽셀 크기의 이미지로 60,000개의 필기체 숫자 이미지들를 제공합니다. 다음은 데이터 세트에 있는 “8”에 대한 이미지 예입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*500x2s6c8gtQYExfLuhDYQ.jpeg" /><figcaption>MNIST 데이터 세트에 있는 여러가지 8 이미지</figcaption></figure><h4><strong>생각해 보면 모든 것은 그저 숫자일뿐입니다.</strong></h4><p>우리가 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761">Part 2</a>에서 만든 신경망은 입력으로 3 개의 숫자(“3”개의 침실, “2000” 평방 피트 등)만을 사용합니다. 그러나 이제 우리는 우리의 신경망으로 이미지를 처리하려고 합니다. 도대체 어떻게 숫자 대신 신경망에 이미지를 공급(feed)해야 할까요?</p><p>그 해답은 놀라울 정도로 심플합니다. 신경망은 숫자를 입력으로 사용합니다. 컴퓨터에서 이미지는 실제로 각각의 픽셀이 얼마나 어두운지를 나타내는 숫자 그리드일 뿐입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/581/1*6UcQdP-1yFoUotb-QnDWsg.gif" /></figure><p>우리의 신경망에 이미지를 공급(feed)하기 위해서는, 먼저 간단히 18x18 픽셀 이미지를 324개의 숫자들의 배열로 생각합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*mWcpiMO8LM3jTuu9qjl22Q.png" /></figure><p>324개의 입력을 처리하기 위해, 우리의 신경망이 324개의 입력 노드를 갖도록 변경하기만 하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*D4j5Avgq0R_iwGmulh9NXg.png" /></figure><p>우리의 신경망에는 (단 하나가 아닌) 두 개의 출력이 있습니다. 첫 번째 출력은 이미지가 “8”일 가능성이고 두 번째 출력은 “8”이 아닐 가능성을 예측하는 것입니다. 인식하려는 개별 객체에 대해 별도의 출력을 가짐으로써, 우리는 신경망을 이용해 객체를 그룹으로 분류 할 수 있습니다.</p><p>이제 우리의 신경망은 지난 번 보다 훨씬 커졌습니다(3개 대신 324개의 입력!). 그러나 모든 최신의 컴퓨터는 순식간에 수백 개의 노드가있는 신경망을 처리 할 수 있습니다. 심지어 당신의 스마트폰에서도 잘 동작합니다.</p><p>이제 신경망을 “8”과 “8”이 아닌 이미지로 훈련시켜서 이를 구분할 수 있게 하는 것이 남았습니다. 만약 “8”을 제공(feed) 할 때 그 이미지가 “8”일 확율은 100 %이고 “8”이 아닐 확률은 0 %이 되어야 합니다. 반대의 경우도 마찬가지입니다.</p><p>다음은 우리의 훈련 데이터입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*UsARa4r6x_ZuZATcxFqJ6A.jpeg" /><figcaption>음… 아주 멋진 훈련 데이터네요</figcaption></figure><p>우리는 최신 노트북에서 몇 분 안에 이런 종류의 신경망을 훈련시킬 수 있습니다. 훈련이 끝나면 “8”이라는 그림을 꽤 높은 정확도로 인식 할 수있는 신경망을 갖게 될 것입니다. (1980 년대 말) 이미지 인식 세상에 오신 것을 환영합니다!</p><h3><strong>편협한 생각</strong></h3><p>픽셀을 신경망에 제공(feed)하는 것만으로도 이미지 인식을 제작하는데 정말 효과적입니다. 기계 학습은 마술입니다! …정말 그런가요?</p><p><em>네, 실제로 그렇게 간단하지 않습니다.</em></p><p>일단, 좋은 소식은 우리의 “8” 인식기가 글자가 이미지 중간에 있는 단순한 이미지에서는 실제로 잘 작동한다는 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*7jrrLKz5bTKhWm7VGxmcVg.png" /></figure><p>그런데, 이제 정말 나쁜 소식이 있습니다:</p><p>우리의 “8”인식기는 문자가 이미지의 중심에 완벽하게 맞지 않으면 <em>전혀</em> 동작하지 않습니다. 아주 약간 위치만 바뀌어도 모든 것을 망치게 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*yijSTxCgLqMp_vMldo44SA.png" /></figure><p>그 이유는 우리 신경망이 “8”이 완벽하게 중심에있는 패턴만을 배웠기 때문입니다. “8”이 중심에서 벗어난 것이 무엇인지 전혀 알 수 없습니다. 정확히 단지 하나의 패턴 만 알고 있습니다.</p><p>이런 것은 현실 세계에서 전혀 쓸모가 없습니다. 현실 세계의 문제는 결코 명확하고 단순하지 않습니다. 따라서 우리는 “8”이 완벽하게 중심에 있지 않은 경우에도 신경망을 동작시킬 방법을 알아내야 합니다.</p><h4><strong>무차별 대입(Brute Force) 아이디어 # 1 : 슬라이딩 윈도우(Sliding Window)로 찾기</strong></h4><p>우리는 이미 이미지의 중앙에 위치한 “8”을 찾을 수 있는 아주 좋은 프로그램을 만들었습니다.</p><p>그렇다면, 이미지를 작은 섹션으로 나누어 “8”을 찾을 때 까지 한번에 한 섹션씩 모두 스캔하는 방법은 어떨까요?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ru1cEl5bSDzYyD1aX6usuQ.gif" /></figure><p>이 접근 방식을 슬라이딩 윈도우(sliding window)라고 합니다. 또한 이것은 무차별 대입(brute force) 솔루션입니다. 일부 제한된 경우에는 잘 동작하지만 실제로는 매우 비효율적입니다. 다양한 크기의 객체를 찾기 위해 같은 이미지를 계속해서 확인해야합니다. 이것보다 더 잘할 수 있습니다!</p><h4><strong>무차별 대입(Brute Force) 아이디어 # 2 : 더 많은 데이터와 딥 신경망(Deep Neural Net)</strong></h4><p>앞서 우리의 신경망을 훈련시킬때, 완벽하게 중앙에 위치한 “8”만을 보여주었습니다. 그렇다면 이미지안에 여러 크기의 다른 위치에 있는 “8”에 대한 더 많은 데이터로 훈련시키는 것은 어떨까요?</p><p>이를 위해서 새로운 훈련 데이터를 수집 할 필요는 없습니다. 스크립트를 작성해서 이미지내의 여러 다른 위치에 있는 “8” 이미지를 생성하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*JJPzY32YCxUOqbpl498txQ.png" /></figure><p>이미 가지고 있는 훈련 이미지의 다른 버전을 만들어서 합성 훈련 데이터(Synthetic Training Data)를 만들었습니다. 이것은 매우 유용한 테크닉입니다!</p><p>이러한 테크닉을 통해 우리는 훈련 데이터를 손쉽게 무한정 공급할 수 있습니다.</p><p>데이터가 많을수록 당연히 신경망이 해결하기가 더 어려워집니다. 하지만 네트워크를 더 크게 만들어 더 복잡한 패턴을 배울 수 있도록 함으로써 이를 보완 할 수 있습니다.</p><p>신경망을 크게 만들기 위해서 단지 노드의 레이어를 중첩하면 됩니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*V_zDA0v7UBvaGrXSXo0v6A.png" /></figure><p>전통적인 신경망(traditional neural network)보다 더 많은 계층(layers)을 가지고 있기 때문에 우리는 이것을 “딥 신경망(deep neural network)”이라고 부릅니다.</p><p>이 아이디어는 이미 1960년대 말부터 있었습니다. 그러나 최근까지도 이러한 대규모 신경망을 훈련하는 것은 너무 느렸습니다. 그러나 일반적인 컴퓨터 프로세서 대신에 (매트릭스 곱셈을 정말 빠르게하기 위해 고안된) 3D 그래픽 카드를 사용하는 방법을 알게되면서 대규모 신경망으로 작업하는 것이 갑자기 실용적이되었습니다. 실제로 <a href="https://en.wikipedia.org/wiki/Overwatch_%28video_game%29">Overwatch</a>를 플레이 할 때 사용하는 것과 완전하게 동일한 NVIDIA GeForce GTX 1080 비디오 카드를 사용해서 신경망을 매우 빠르게 훈련시킬 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*UEo9-rAT3O71m9SP8yKFRQ.png" /></figure><p>그러나, 우리가 신경 네트워크를 아주 크게 만들고 3D 그래픽 카드로 신속하게 훈련시킬 수는 있겠지만, 여전히 이것이 그대로 솔루션이 되는 것은 아닙니다. 이미지를 우리의 신경망에서 처리하는 방법에 대해서는 좀더 영리해 져야 합니다.</p><p>생각해 보십시오. 두 가지 경우가 완전히 다른 객체라고 보고 그림의 위쪽에 있는 “8”을 인식하도록 신경망을 훈련시키는 것과 그림의 아래쪽에 있는 “8”을 별도로 인식하도록 훈련시키는 것은 정말 말이 안됩니다.</p><p>“8”이 그림의 어느 위치에 있더라도 추가적인 훈련 없이 같은 것이라고 인식할 만큼 충분하게 현명한 신경망을 만들 방법을 찾아야 합니다. 다행이도… 있습니다!</p><h3><strong>그 해결책이 바로 컨볼루션(Convolution) 입니다</strong></h3><p>인간인 당신은 사진에 <em>계층 구조 </em>또는<em> 개념 구조(hierarchy</em> or <em>conceptual structure)</em> 있음을 직관적으로 알 수 있습니다. 다음 그림을 보겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*S1rfKt-nylCFKHjjC7SrXg.jpeg" /><figcaption>우리 아들의 그냥 사진</figcaption></figure><p>인간으로서, 당신은 이 그림에서 계층구조를 즉시 인식하게 됩니다.</p><ul><li>땅은 잔디와 콘크리트로 덮여 있습니다.</li><li>한 아이가 있습니다.</li><li>이 아이는 점핑말(bouncy horse)에 앉아 있습니다.</li><li>점핑말은 잔디 위에 있습니다.</li></ul><p>가장 중요한 것은, 우리는 아이가 어떤 면(surface) 위에 있더라도 <em>아이</em>라는 개념을 인식할 수 있습니다. 우리는 아이가 나타날 수있는 모든 가능한 면(surface)에 대해서 <em>아이</em>라는 개념의 인식을 다시 배울 필요는 없습니다.</p><blockquote>*역자주: 여기서 면(surface)는 앞서 설명한 계층구조(hierarchy)의 하나의 계층 또는 레이어(layer)를 의미합니다.</blockquote><p>그러나 지금 당장 우리의 신경망이 이런 일을 할 수 없습니다. 현재는 이미지의 다른 부분에있는 “8”을 서로 완전히 다른 것이라 생각합니다. 그림상에서 어떤 객체를 움직인다고 해서 바뀌는 건 없다는 것을 이해하지 못합니다. 즉, 가능한 모든 위치에서 각 객체를 식별하는 것을 모두 다시 학습해야 한다는 뜻입니다. 이건 정말이지 끔찍하네요.</p><p>우리는 “8”은 그림에서 어디에 나타나더라도 “8”이라는 <em>이동 불변성(translation invariance)</em>을 우리의 신경망이 이해하도록 해야 합니다.</p><p>컨볼루션(Convolution)이라 부르는 프로세스를 사용해서 이 작업을 수행할 것입니다. 컨볼루션은 컴퓨터 과학과 생물학에서 부분적으로 영감을 받은 아이디어입니다(예를 들어, 어떤 미친 과학자가 고양이가 이미지를 처리하는 방법을 알아 내기 위해 이상한 탐침으로 고양이 두뇌를 진짜로 찔러보면 된다는 생각).</p><h3><strong>컨볼루션(Convolution)의 동작 방식</strong></h3><p>우리의 신경망에 전체 이미지를 한 개의 그리드로써 전체 이미지를 공급(feed)하는 대신에, 객체는 그림의 어디에서 나타나더라도 동일한 것이라는 아이디어를 활용해서 훨씬 더 똑똑한 방법을 사용할 것입니다.</p><p>단계별로 어떻게 동작하는지 알아보겠습니다 —</p><h4><strong>Step 1: 이미지를 중첩된 이미지 타일들로 나누기</strong></h4><p>앞선 슬라이딩 윈도우 검색과 유사하게, 원본 이미지 전체에 슬라이딩 윈도우를 적용해서 각 결과를 별도의 작은 그림 타일로 저장합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ggaJXl0FILguYXkj7oCDbQ.png" /></figure><p>이렇게 해서, 원본 이미지를 동일한 크기의 작은 이미지 타일 77개로 변환했습니다.</p><h4><strong>Step 2: 각 이미지 타일을 작은 신경망에 제공(feed)하기</strong></h4><p>앞서 우리는 “8”인지 확인하기 위해 하나의 이미지를 신경망에 제공(feed)했습니다. 여기서도 똑같은 작업을 할 것입니다. 다만 각 개별 이미지 타일에 대해 이 작업을 수행 할 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/758/1*Z2D0ePPFRlniwSI1G3HEdQ.png" /><figcaption>한번에 한개의 타일에 대해 77번 반복합니다.</figcaption></figure><p>하지만 <strong>한가지 중요한 변화가 있습니다</strong>: 우리는 동일한 원본 이미지의 모든 단일 타일에 대해 <strong>동일한 신경망 가중치(same neural network weights)</strong>를 유지할 것입니다. 다시 말해서, 모든 이미지 타일을 동일하게 취급합니다. 어떤 타일에 무엇인가 흥미로운 것이 나타나면, 그 타일을 흥미있는 것이라고 표시 할 것입니다.</p><h4><strong>Step 3: 각 타일에 대한 결과를 새로운 배열에 저장하기</strong></h4><p>원본 타일의 배열(array) 형태를 버리고 싶지는 않습니다. 따라서 각 타일을 처리 한 결과를 원본 이미지에 대한 동일한 배열 형태로 그리드에 저장합니다. 이 과정은 다음과 같습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*LMyMIyEiHjinp8zWTfv8dw.png" /></figure><p>다시 말해서, 하나의 큰 이미지로 시작해서 원래 이미지의 어느 부분이 가장 흥미로운지를 기록한 조금 크기가 작아진 배열을 얻게 됩니다.</p><h4><strong>Step 4: 시료 채취하기</strong></h4><p>Step 3의 결과는 원본 이미지의 어느 부분이 가장 흥미로운지를 나타내는 배열이었습니다. 그러나 이 배열도 여전히 너무 큽니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*UZLgkNsau_WqMeEgt532Pg.png" /></figure><p>배열의 크기를 줄이기 위해서, 맥스-풀링(<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">max pooling</a>)이라는 알고리즘을 사용해서 시료를 채취하겠습니다. 정말 멋지게 보이는데, 사실 그 뿐 만이 아닙니다!</p><p>우리는 단순히 결과 배열을 2x2 정사각형으로 나누어 각각에서 가장 큰 숫자만을 취합할 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*PQ-8bOxNqWrcj3fzbRfCBg.png" /></figure><p>이 아이디어는 각 2x2 사각형 격자(square grid)를 구성하는 4개의 입력 타일에서 흥미로운 것을 찾으면, 가장 흥미로운 것도 유지할 수 있다는 것입니다. 이를 통해 가장 중요한 부분을 유지하면서도 배열의 크기를 줄일 수 있습니다.</p><h4><strong>마지막 Step: 예측 하기</strong></h4><p>지금까지 커다란 이미지의 크기를 상당히 작은 배열로 줄였습니다.</p><p>자 이제 어떻게 해야 할까요? 이 배열은 결국 숫자들의 집합일뿐입니다. 따라서 이제 우리는 이 작은 배열을 <em>다른 신경망</em>에 제공할 입력으로 사용할 수 있습니다. 이 최종 신경망이 이미지가 일치하는지 또는 일치하지 않는지를 결정해줄 것입니다. 앞선 컨볼루션 단계(Step)들과 구분하기 위해 이를 “완전히 연결된” 망(“fully connected” network)라 부르겠습니다.</p><p>정리하자면, 시작에서부터 끝까지 전체 5 단계의 경로(pipeline)는 다음과 같습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Y16AZm1Ls5A3a5Xe3d_6sw.png" /></figure><h3><strong>더 많은 단계를 추가하기</strong></h3><p>우리의 이미지 처리 경로(pipeline)는 다음과 같은 일련의 단계들로 이루어집니다: 컨볼루션(convolution), 맥스-풀링(max-pooling), 그리고 마지막 완전히 연결된 망(fully-connected network)</p><p>현실 세계의 문제를 해결할 때는 이러한 단계를 원하는만큼 많이 조합하고 반복할 수 있습니다! 2 개, 3 개 또는 심지어 10 개의 컨볼루션 레이어를 가질 수 있습니다. 데이터의 크기를 줄이고 싶다면 어느 단계에서나 맥스-풀링을 실행하면 됩니다.</p><p>기본 아이디어는 큰 이미지로 시작해서 마지막으로 하나의 결과가 나올 때까지 단계적으로 반복해서 데이터를 압축해 가는 것입니다. 더 많은 컨볼루션 단계를 가져갈 수록, 당신의 신경망은 학습을 통해 더욱 복잡한 형상을 인식할 수 있습니다.</p><p>예를 들어, 첫 번째 컨볼루션 단계에서는 날카로운 모서리를 인식하는 법을 배울 수 있고, 두 번째 컨볼루션 단계에서는 이 날카로운 모서리에 대한 지식을 사용해서 부리를 인식 할 수 있으며, 세 번째 단계에서는 부리에 대한 지식 등을 사용해서 전체 새를 인식 할 수도 있을 것입니다.</p><p>(연구 논문에서 찾을 법한) 보다 현실적인 딥 컨볼루션 망은 다음과 같습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*0OqPZGEoDDSfIDdn_44Efg.png" /></figure><p>위 그림은 224x224 픽셀 이미지에서 시작해서, 컨볼루션과 맥스 풀링을 두 번 적용한 다음 컨볼루션을 추가로 3번 더 적용합니다. 그리고 다시 맥스 풀링을 적용한 다음 두 번의 완전히 연결된 망의 단계(layers)를 적용합니다. 최종 결과는 이미지가 1000개의 범주 중 하나로 분류되는 것입니다.</p><h3><strong>정확한 신경망 구축하기</strong></h3><p>그런데 이미지 분류기(image classifier)가 잘 동작할 수 있도록 어떤 단계들을 결합해야 하는지 어떻게 알 수 있을까요?</p><p>솔직히 말해서, 많은 실험과 테스트를 통해서만 답을 얻을 수 있습니다. 해결하려는 문제에 대한 최적의 구조와 매개 변수를 찾기 까지 어쩌면 100개의 신경망를 훈련시켜야 할 수도 있습니다. 기계 학습에는 많은 시행 착오가 뒤따릅니다.</p><h3><strong>새 분류기 만들기</strong></h3><p>이제 우리는 사진이 새인지 아닌지를 결정할 수있는 프로그램을 작성할 수 있습니다.</p><p>항상 그렇듯이 시작하기 위해서는 데이터가 필요합니다. 무료 CIFAR10 데이터 세트(<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10 data set</a>)에는 6,000 장의 새 사진과 52,000 장의 새가 아닌 사진이 포함되어 있습니다. 그러나 더 많은 데이터를 얻기 위해 12,000 장의 새 사진이 있는 Caltech-UCSD Birds-200–2011 데이터 세트(<a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">Caltech-UCSD Birds-200–2011 data set</a>)도 추가 할 것입니다.</p><p>합친 데이터 세트에 있는 새들 사진 중 일부:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*zDwFUE96LS_rdVrPNB1_4w.png" /></figure><p>52,000장의 새가 아닌 사진 중 일부:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*xO0LgAjZuGOJbYyS7oTFaw.png" /></figure><p>이 데이터 세트는 우리의 목적에 맞게 잘 동작하겠지만, 72,000개의 저해상도 이미지는 현실 세계의 애플리케이션에 사용하기에는 여전히 작은 데이터입니다. Google 수준의 성능을 원한다면 <em>수백만 개</em>의 고해상도 이미지가 필요합니다. 기계 학습에서 더 나은 알고리즘을 사용하는 것 보다 더 많은 데이터를 보유하는 것이 언제나 더 중요합니다. 이는 현재 Google이 무제한 사진 저장 용량을 제공하는 이유입니다. 당신의 달콤하고 달콤한 데이터를 원합니다!</p><p>새 분류기를 만들기 위해 <a href="http://tflearn.org/">TFLearn</a>을 사용할 것입니다. TFlearn은 단순화 된 API를 제공하는 Google의 <a href="https://www.tensorflow.org/">TensorFlow</a> 딥러닝 라이브러리의 래퍼(wrapper)입니다. 이를 이용하면 신경망 계층을 정의하는데 단 몇 줄의 코드만 작성하면 되는 수준으로 컨볼루션 신경망을 네트워크 작성을 쉽게 할 수 있습니다.</p><p>신경망을 정의하고 훈련시키기 위한 코드:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/87de8a77d61573abd09eeda493ec6b6d/href">https://medium.com/media/87de8a77d61573abd09eeda493ec6b6d/href</a></iframe><p>Nvidia GeForce GTX 980 Ti 또는 그 상위 제품과 같이 충분한 RAM을 갖춘 좋은 비디오 카드로 훈련시키면, 1시간 이내에 훈련을 완료 할 수 있습니다. 일반 CPU로 훈련시킨다면 시간이 조금더 오래 걸릴 것입니다.</p><p>훈련을 시킬 수록 정확도는 높아집니다. 첫 번째 훈련 후에 75.4%의 정확도를 얻었습니다. 단 10회 훈련한 후 이미 91.7%까지 올라갔습니다. 50회 정도 지나면 95.5 %의 정확도에 이르렀고 추가 훈련은 도움이되지 않았기 때문에 거기에서 멈추었습니다.</p><p>축하합니다! 우리의 프로그램은 이제 이미지에서 새를 식별할 수 있습니다.</p><h3><strong>신경망 테스트 하기</strong></h3><p>이제 우리는 훈련된 신경망을 만들었고 바로 사용할 수 있습니다! 하나의 이미지 파일이 새인지 아닌지 예측하는 <a href="https://gist.github.com/ageitgey/a40dded08e82e59724c70da23786bbf0">간단한 스크립트</a>를 확인해 보세요.</p><p>그러나 우리 신경망이 얼마나 효과적인지를 실제로 확인하기 위해서는 아주 많은 이미지로 테스트해야합니다. 유효성 검사를 위해 15,000개의 이미지를 포함한 데이터 세트를 만들었습니다. 우리의 신경망에 이 15,000 개의 이미지를 실행해보니, 95% 수준으로 정확한 대답을 예측했습니다.</p><p>꽤 괜찮아 보이죠? 글쎄요… 상황에 따라 다르겠지만요!</p><h4><strong>정확도 95%는 얼마나 정확한 것일까요?</strong></h4><p>우리의 신경망은 95% 정확하다 할 수 있습니다. 그런데 자세히 들여다 보면 문제가 있습니다. 95%라는 것은 다른 의미로 볼 수도 있습니다</p><p>예를 들어, 만약 훈련 이미지의 5 %가 새이고 나머지 95%가 새가 아닌 경우 어떻게 될까요? 매번 “새가 아니다”라고 추측하는 프로그램이 있다면 이것은 95% 정확한 것이 됩니다! 그러나 이것은 또한 100% 쓸모 없습니다.</p><p>평균적인 정확도 보다 숫자를 좀 더 자세히 들여다 볼 필요가 있습니다. 분류 시스템이 실제로 얼마나 좋은지 판단하기 위해서, 우리는 실패한 시간의 비율이 아니라 <em>어떻게</em> 실패했는지를 면밀히 조사해야합니다.</p><p>우리의 예측을 “맞다”와 “틀리다”로 생각하는 대신, 다음의 네 가지 범주로 나누어 보겠습니다 —</p><ul><li>첫째, 다음은 우리의 신경망이 올바르게 식별 한 새들 사진의 예입니다. 이것을 <strong>맞힌 긍정(True Positives)</strong>이라고 하겠습니다.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*PMGSeMfV670R6FCwZJCBgw.png" /><figcaption>와우! 우리의 신경망은 여러 종류의 새들을 성공적으로 인식할 수 있습니다!</figcaption></figure><ul><li>둘째, 다음은 “새가 아니다”라고 올바르게 식별 한 이미지들입니다. 이것를 <strong>맞힌 부정(True Negatives)</strong>이라고 하겠습니다.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*nMBBELC-Iqian2wGAuIIhg.png" /><figcaption>말과 트럭 정도에는 속지 않습니다!</figcaption></figure><ul><li>셋째, 새라고 생각했지만 실제 새가 아닌 이미지들 입니다. 이것은 우리의 <strong>틀린 긍정(False Positives)</strong>입니다:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*Cx3mAveuARF-RsJbI0WyQA.png" /><figcaption>많은 비행기를 새라고 착각했습니다! 이해는 됩니다.</figcaption></figure><ul><li>그리고 마지막으로, 우리가 정확하게 새로 인식하지 못한 이미지들입니다. 이것은 우리의 <strong>틀린 부정(False Negatives)</strong>입니다:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*XGuRnNTtYNGqDsvR3NnuHQ.png" /><figcaption>이 새들에게 속았습니다. 망할 타조 같으니라고! 타조도 새라고 해야 합니까?</figcaption></figure><p>15,000개 이미지의 유효성 검사 세트를 사용해서, 우리의 예측이 각 카테고리별로 어떻게 분류 되는지 알아보았습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*p-4GBB_3e1Cxadf-O12fwg.png" /></figure><p>왜 결과를 이런 식으로 세분화해야 할까요? 그 이유는 모든 실수가 같은 원인으로 발생하기 않기 때문입니다.</p><p>MRI 이미지로부터 암을 찾아내는 프로그램을 작성했다고 상상해보십시오. 우리가 암을 찾아낸 경우에 거짓 부정(false negatives)보다 거짓 긍정(false positives)이 더 낫습니다. 프로그램이 실제로 암에 걸린 사람에게 전혀 암에 걸리지 않았다고 말하는 경우를 보면 거짓 부정(false negatives)은 좋지 않을 수 있습니다.</p><p>평균적인 정확도를 보는 대신 <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision과 Recall</a>을 계산했습니다. Precision과 Recall 측정 항목을 통해 얼마나 잘 수행했는지 보다 명확하게 알 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*X3ehvILOqEhpS1q1Co-r0w.png" /></figure><p>위 결과는 우리가 97% 정확도로 “새”를 추측했음을 보여줍니다. 아주 잘 했습니다! 그러나 데이터 세트에서 실제 새의 90%만 발견했다는 사실도 알 수 있습니다. 바꿔 말하자면, 우리는 모든 새를 발견하지 못할 수도 있지만, 발견했을 때는 꽤 확실하게 맞출 수 있습니다!</p><h3><strong>이제 무엇을 해야 하나요?</strong></h3><p>딥 컨볼루션 망(deep convolutional network)의 기본 사항을 알았으니, 이제 다른 신경망 아키텍처에 직접 도전에 보기위해 <a href="https://github.com/tflearn/tflearn/tree/master/examples#tflearn-examples">tflearn에 관한 몇 가지 예제</a>를 해보십시오. 여기에는 데이터 세트도 포함되어 있어 필요한 이미지를 찾을 필요 조차 없습니다.</p><p>당신은 이제 기계 학습의 다른 영역으로 확장해서 학습할 만큼 충분한 지식이 갖추었습니다. <a href="http://karpathy.github.io/2016/05/31/rl/">Atari 게임을 플레이하는 방법을 컴퓨터에 훈련시키기 위한 알고리즘을 어떻게 사용하는지</a> 공부해보는 것은 어떨까요?</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=928a841a3aa" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 2]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/b35f3d327761</guid>
            <category><![CDATA[머신러닝]]></category>
            <category><![CDATA[인공지능]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[기계학습]]></category>
            <category><![CDATA[신경망]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Mon, 12 Dec 2016 04:56:40 GMT</pubDate>
            <atom:updated>2018-06-26T09:10:07.673Z</atom:updated>
            <content:encoded><![CDATA[<p>기계 학습을 사용해서 Super Mario Maker의 레벨 제작하기</p><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8">Part 1</a>에서는 기계 학습(Machine Learning)이 문제를 해결하기 위해 코드를 전혀 작성하지 않고도, 일반 알고리즘(generic algorithm)을 사용해서 주어진 데이터에서 흥미로운 것을 알아 낼 수 있다는 것을 알아봤습니다. (아직 <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8">Part 1</a>을 읽지 않았다면, 지금 읽어보세요!)</p><p>이번에는 이러한 일반 알고리즘 중 하나로 아주 멋진 일을 해내는 것을 보게 될텐데 – 바로 사람들이 만든 것처럼 보이는 비디오 게임의 레벨을 제작하는 것입니다. 우리는 신경망(neural network)를 만들고 기존의 슈퍼 마리오 레벨들을 통해서 새로운 슈퍼 마리오 레벨이 쉽게 만들어지는 것을 살펴 볼 예정입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*aXcI487bc3eB-Gb1ezUQCg.gif" /><figcaption><a href="https://supermariomakerbookmark.nintendo.net/courses/4AC9-0000-0157-F3C3">우리의 알고리즘이 만들어낸 레벨 중 하나</a></figcaption></figure><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8">Part 1</a>과 마찬가지로, 이 안내서는 기계 학습에 궁금한 점은 있지만 어디서부터 시작해야할지 모르는 사람들을 위한 것입니다. 이 글의 목표는 누구에게나 쉽게 다가가는 데 있습니다 – 이는 글에 많은 일반화가 있음을 의미합니다. 하지만 어떻습니까? 그래서 더 많은 사람들이 ML에 관심을 가지게 된다면, 목표를 달성한 것입니다.</p><h3><strong>좀더 영리하게 추측하기</strong></h3><p><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8">Part 1</a>에서, 우리는 주택의 속성을 기반으로 주택 가치를 추정하는 간단한 알고리즘을 만들었습니다. 어떤 주택에 대한 데이터가 다음과 같다고 가정해 봅시다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*tzN3MZMnGMjAX59GGnsPMQ.png" /></figure><p>우리는 간단한 추정 함수(estimation function)를 만들었습니다:</p><pre>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):<br> price = 0</pre><pre># 이건 한 꼬집 넣고<br> price += num_of_bedrooms * <strong>0.123</strong></pre><pre># 그리고 저건 한 스픈 정도 넣고<br> price += sqft * <strong>0.41</strong></pre><pre># 이건 아마도 한 줌 넣고<br> price += neighborhood * <strong>0.57</strong></pre><pre>return price</pre><p>즉, 우리는 각 속성에 <strong>가중치</strong>를 곱하여 주택 가격을 추정했습니다. 그런 다음 이 값들을 더해서 주택의 최종 가치를 얻었습니다.</p><p>코드를 사용하는 대신, 간단한 다이어그램으로 해당 함수를 표현해 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ysIi2_QSIgZIOsznFFBi2w.png" /><figcaption>화살표에 있는 숫자가 우리 함수의 가중치 입니다.</figcaption></figure><p>하지만, 이 알고리즘은 입력데이터와 결과 사이에 <em>선형(linear)</em> 관계가 있는 단순한 문제에 대해서만 동작합니다. 주택 가격이 실제로 그렇게 단순하지 않다면 어떻게 될까요? 예를 들어, 큰 주택과 작은 주택에서는 이웃이 크게 중요 할 수 있지만 중간 크기의 집에서는 중요하지 않을 수 있습니다. 우리 모델에 이런 종류의 복잡한 세부 사항을 적용할 수 있을까요?</p><p>더 영리해지기 위해서, 우리는 각각 다른 경우에 적용되는 서로 다른 가중치를 사용해서 알고리즘을 여러 번 실행해 볼 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*M8HIvJD1zqGobowxbDPEkg.png" /><figcaption>네 가지 다른 방법으로 문제를 풀어 봅시다.</figcaption></figure><p>이제 서로 다른 네가지의 가격 예측이 있습니다. 이 네 가지 가격 예측들을 하나의 최종 예측으로 결합해 보겠습니다. 동일한 알고리즘으로 다시 실행할 것입니다 (다만, 다른 가중치의 세트 사용해서)!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/777/1*ztrtvgUGfEXEdSlAoDKyVw.png" /></figure><p>우리의 새로운 <em>최종 해답</em>은 문제를 해결하기위한 네 가지 시도의 예측들을 결합한 것입니다. 이러한 방법을 이용하면, 하나의 간단한 모델에서 다룰 수 있는 것보다 더 많은 사례에 대한 모델링 할 수 있습니다.</p><h4><strong>신경망(Neural Network)이란 무엇인가요?</strong></h4><p>우리의 네가지 시도를 하나의 큰 다이어그램으로 결합해 보겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*GkHwF2dInQET_SL8r-BfbA.png" /></figure><p>이것이 바로 신경망(neural network)입니다! 각 노드는 일련의 입력을 받아 들여 가중치를 적용하고, 출력 값을 계산하는 방법을 알고 있습니다. 이렇게 많은 노드를 서로 연결함으로써, 우리는 복잡한 함수를 모델링 할 수 있습니다.</p><p>이 글을 쉽게 유지하기 위해서 (<a href="https://en.wikipedia.org/wiki/Feature_scaling">feature scaling</a>과 <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a>을 포함해서) 많은 부분을 건너 뛰었지만, 여기서 가장 중요한 것은 다음의 기본 아이디어입니다.</p><ul><li>우리는 일련의 입력을 받고 가중치를 곱해 출력을 얻는 간단한 추정 함수를 만들었습니다. 이 간단한 함수를 <strong><em>뉴런(neuron)</em></strong>이라고 부르겠습니다.</li><li>단순한 <strong><em>뉴런들(neurons)</em></strong>을 서로 연결함으로써, 우리는 하나의 단일 뉴런으로 모델링하기에는 너무 복잡한 함수를 모델링 할 수 있습니다.</li></ul><p>이것은 LEGO와 같습니다! 하나의 LEGO 블록으로는 많은 모델을 만들 수는 없지만, 조립할 수 있는 기본 LEGO 블록이 충분하다면 어떤 것이라도 모델링 할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/623/1*nC9k4Ae5LlKBkN9a-3B-Ag.png" /><figcaption>이 엄숙한 플라스틱 동물의 미래는? 시간만이 알 수 있겠죠…</figcaption></figure><h4><strong>신경망(Neural Network)에 메모리 주기</strong></h4><p>앞서 우리가 본 신경망은 동일한 입력을 주면 항상 같은 결과을 반환합니다. 메모리(기억 장치)가 없기 때문입니다. 프로그래밍 용어로 말하면, 이것은 <a href="https://en.wikipedia.org/wiki/State_%28computer_science%29">상태 비저장 알고리즘(stateless algorithm)</a>입니다.</p><p>(집값 추정과 같은) 많은 경우에, 이 방식은 당신이 정확히 원하는 것입니다. 그러나 이런 종류의 모델이 할 수없는 한 가지는 시간이 지남에 따라 변하는 데이터의 패턴에 대응하는 것입니다.</p><p>제가 키보드를 건네주고 당신에게 이야기를 하나 써보라고 하는 상황을 상상해보십시오. 그런데 시작하기 전에, 내 직업은 사람들이 타이핑 할 첫 번째 글자를 예측하는 것입니다. 어떤 글자를 예측해야 할까요?</p><p>저는 제 영어 지식을 사용해서 올바른 글자를 예측하는 확률을 높일 수 있습니다. 예를 들어, 당신은 단어의 첫글자에 많이 사용되는 글자를 입력할 지도 모릅니다. 만약 당신이 과거에 쓴 이야기를 봤다면, 아마도 이야기의 시작 부분에서 일반적으로 사용하는 단어를 바탕으로 좀더 좁힐 수도 있을 것입니다. 일단 모든 데이터가 확보되면, 신경망을 구축해서 당신이 글을 시작할 때 특정 글자를 얼마나 사용할 가능성이 있는지를 모델링할 수 있을 것입니다.</p><p>우리는 모델은 아마 다음과 같을 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*4oHvA2epDM8tDKWcZj_r5w.png" /></figure><p>이제 문제를 좀 더 어렵게 만들어 보겠습니다. 당신의 이야기의 어느 부분에서든 입력 할 <em>다음</em> 글자를 추측해야 한다고 가정해 봅시다. 훨씬 더 흥미로운 문제가 되었습니다.</p><p>어니스트 헤밍웨이(Ernest Hemingway)의 The Sun Also Rises의 처음 몇 단어를 예로 들어 보겠습니다.</p><blockquote>Robert Cohn was once middleweight boxi</blockquote><p>자 다음에 올 글자는 무엇일까요?</p><p>당신은 ‘n’을 추측했을 것입니다 — 그 단어는 아마도 <em>boxing</em> 일 것입니다. 우리는 저 문장에서 보이는 글자들과 영어의 일반적인 단어에 대한 지식을 바탕으로 이를 알 수 있습니다. 또한, ‘미들급’(‘middleweight’)이라는 단어는 권투에 대해 이야기하고 있다는 추가 단서를 제공해 줍니다.</p><p>신경망으로이 문제를 해결하려면, 우리 모델에 <em>상태(state)</em>를 추가해야합니다. 우리가 신경망에 응답을 요청할 때마다 우리는 중간 계산 결과들을 저장하고 다음 입력의 일부로써 다시 사용할 수 있습니다. 그렇게 하면, 우리 모델은 최근의 입력을 기반으로 예측을 조정하게 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*MJM6hsMHCIT4nOvSLeV1lg.png" /></figure><p>우리 모델에서 상태를 기록해 두면 단지 그 이야기에서 가장 가능성있는 <em>첫</em> 글자를 예측하는 것이 아니라 이전의 모든 문자를 고려해서 가장 가능성있는 <em>다음</em> 글자를 예측할 수 있게 됩니다.</p><h4><strong>글자 하나 예측하는게 무슨 소용이 있나요?</strong></h4><p>이야기 안에서 다음 글자를 예측하는 것은 꽤 쓸모없는 것처럼 보일 수 있습니다. 여기서 중요한 것은 무엇인가요?</p><p>멋진 예 중에 하나가 바로 휴대 전화 키보드에 있는 자동 예측(auto-predict) 기능입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*DU74IrmABm0W-AZdnss1pQ.png" /><figcaption>다음 글자는 아마도 “t”가 되겠죠.</figcaption></figure><p>이제 이 아이디어를 극단적 끌고 가면 어떻게 될까요? 모델에 다음으로 가장 가능성있는 글자를 계속해서 아니 영원히 예측 해달라고 요청하면 어떻게 될까요? 차라리 완전한 이야기를 하나 만들어 달라고 요청하는 편이 낫겠네요!</p><h3><strong>이야기 만들기</strong></h3><p>우리는 헤밍웨이의 다음 문장을 어떻게 추측 할 수 있는지 알아보았습니다. 이제 헤밍웨이 스타일로 전체 이야기를 만들어 봅니다.</p><p>이를 위해, 우리는 Andrej Karpathy가 작성한 <a href="https://github.com/karpathy/char-rnn">순환 신경망 구현(Recurrent Neural Network implementation)</a>을 사용할 것입니다. Andrej는 Stanford의 Deep-Learning 연구원이며 <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">RNN을 사용하여 텍스트를 생성에 대해 훌륭한 글</a>을 썼습니다. github에서 <a href="https://github.com/karpathy/char-rnn">이 모델에 관한 모든 코드</a>를 볼 수 있습니다.</p><p>84개의 고유한 문자(구두점, 대문자/소문자 등 포함)를 사용해서 총 362,239 글자로 쓰여진 <em>The Sun Also Rises</em> 의 전체 텍스트로부터 모델을 제작할 것입니다. 이 데이터 세트는 현실에서 사용되는 전형적인 애플리케이션에 비하면 아주 적은 양입니다. 헤밍웨이의 스타일의 정말 좋은 모델을 제작하려면, 여러 번에 걸처 많은 샘플 텍스트를 사용하는 편이 훨씬 낫습니다. 뭐 이정도면 예를 들어 해보는데는 그런데로 충분해 보입니다.</p><p>이제 막 RNN을 훈련시키기 시작했기 때문에, 바로 글자를 예측하는 것은 그리 좋지 않습니다. 훈련을 100 회 반복 한 후 작성된 내용은 다음과 같습니다:</p><blockquote>hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h’e btentmuhgehi bcgdltt. gey heho grpiahe.<br>Ddelnss.eelaishaner” cot AAfhB ht ltny<br>ehbih a”on bhnte ectrsnae abeahngy<br>amo k ns aeo?cdse nh a taei.rairrhelardr er deffijha</blockquote><p>단어들 사이에 가끔씩 공백이 있다는 정도는 알아냈지만, 이게 전부입니다.</p><p>1000 회 정도 반복 한 후에는 가능성이 있어 보입니다:</p><blockquote>hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been soms<br>inick stalling that aid.<br> “Hon’t me and acrained on .Hw’s don’t you for the roed,” In’s pair.”<br> “Alough marith him.”</blockquote><p>모델은 이제 기본 문장 구조의 패턴을 식별하기 시작했습니다. 문장의 끝 부분에 마침표를 찍고 심지어 묻고 답하는 대화도 추가했습니다. 몇 마디는 알아볼 수 있지만, 여전히 많은 부분이 말도 안됩니다.</p><p>그러나 수천 번의 훈련을 수행하니 꽤 괜찮아 보입니다:</p><blockquote>He went over to the gate of the café. It was like a country bed.<br> “Do you know it’s been me.”<br> “Damned us,” Bill said.<br> “I was dangerous,” I said. “You were she did it and think I would a fine cape you,” I said.<br> “I can’t look strange in the cab.”<br> “You know I was this is though,” Brett said.<br> “It’s a fights no matter?”<br> “It makes to do it.”<br> “You make it?”<br> “Sit down,” I said. “I wish I wasn’t do a little with the man.”<br> “You found it.”<br> “I don’t know.”<br> “You see, I’m sorry of chatches,” Bill said. “You think it’s a friend off back and make you really drunk.”</blockquote><p>이 시점에서, 알고리즘은 헤밍웨이의 짧고 직접적인 대화 방식의 기본 패턴을 감지해 냈습니다. 몇가지 문장은 대충 말이 됩니다.</p><p>실제 책에 있는 텍스트와 비교해 보겠습니다:</p><blockquote>There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave.<br> “Sit down,” said Harvey, “I’ve been looking for you.”<br> “What’s the matter?”<br> “Nothing. Just looking for you.”<br> “Been out to the races?”<br> “No. Not since Sunday.”<br> “What do you hear from the States?”<br> “Nothing. Absolutely nothing.”<br> “What’s the matter?”</blockquote><p>한 번에 한 글자를 예측하는 패턴을 찾는 것만으로도, 우리의 알고리즘은 올바른 형식으로 그럴듯하게 보이는 산문을 재현해 냈습니다. 정말 놀라운 일입니다!</p><p>텍스트를 처음부터 완전하게 만들 필요는 없습니다. 그냥 알고리즘에 처음 몇 글자를 제공하고 스스로 다음 몇 글자 찾도록 내버려 두면 됩니다.</p><p>재미삼아, 상상의 책을 위한 가짜 책표지를 만들어 보겠습니다. 이를 위해서 “Er”, “He”및 “The S”을 시드(seed)로 해서 새로운 저자와 책제목을 만들어 보겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*frcOndTIVclAzrB1wY5fDw.png" /><figcaption>왼쪽이 진짜 책이고 오른쪽이 우리의 우스꽝스러운 컴퓨터가 만들어낸 말도 안되는 책입니다.</figcaption></figure><p>뭐 그런데로 괜찮네요!</p><p>그런데 <strong>정말로 놀라운 것</strong>은<strong> </strong>이 알고리즘이 모든 일련의 데이터에서 패턴을 파악할 수 있다는 점입니다. 진짜처럼 보이는 <a href="https://gist.github.com/nylki/1efbaa36635956d35bcc">조리법</a>이나 <a href="https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.b4clxzrgf">가짜 오바마 연설문</a>을 쉽게 만들 수 있습니다. 그렇다면 인간의 언어에만 제한을 둘 필요가 있을까요? 아닙니다, 우리는 패턴이있는 모든 종류의 데이터에 이 아이디어를 적용 할 수 있습니다.</p><h4><strong>실제로 마리오 게임 레벨을 만들지 않고 마리오 게임 레벨 만들기</strong></h4><p>2015년, Nintendo사는 Wii U 게임 시스템 용 <a href="http://supermariomaker.nintendo.com/">Super Mario Maker™</a>를 출시했습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/779/1*kTk1ua0xu02Iw2YTfSeGZw.png" /><figcaption>모든 아이들의 꿈!</figcaption></figure><p>이 게임을 사용하면 게임패드에서 자신만의 슈퍼 마리오 브라더스(Super Mario Brothers) 레벨을 그릴 수 있고, 이를 인터넷에 업로드하면 친구들이 이 레벨을 플레이할 수도 있습니다. 자신만의 레벨에 실제 마리오 게임의 모든 기존 파워업(power-ups)과 적을 집어 넣을 수 있습니다. 이는 마치 슈퍼 마리오 브라더스를 플레이하면서 자란 사람들 위한 가상의 레고 세트와 같습니다.</p><p>가짜로 헤밍웨이의 글을 만들어낸 동일한 모델을 사용해서 가짜 슈퍼 마리오 브라더스 레벨을 만들어 낼 수 있을까요?</p><p>우선, 우리의 모델을 훈련시키 데이터 세트가 필요합니다. 1985 년에 출시 된 진짜 슈퍼 마리오 브라더스 게임의 모든 야외 레벨(outdoor levels)을 사용하겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*KoBhR_RpcJ0iH7v4RDW9PQ.png" /><figcaption>가장 행복한 크리스 마스. 엄마 아빠 감사해요!</figcaption></figure><p>이 게임에는 32개의 레벨을 있으며, 그중 대략 70%가 동일한 야외 스타일(outdoor style) 입니다. 그래서 우리는 여기에 집중하겠습니다.</p><p>각 레벨의 디자인을 얻기위해서, 게임 원본의 게임 메모리에서 레벨 디자인을 빼내는 프로그램을 작성했습니다. 슈퍼 마리오 브라더스(Super Mario Bros.)는 30년이나 된 게임이라, 레벨이 게임의 메모리에 어떻게 저장되는 지 알 수 있는 온라인 리소스가 많이 있습니다. 오래된 비디오 게임에서 레벨 데이터를 추출하는 것은 언젠가 시도해 봐야하는 재미있는 프로그래밍 연습입니다.</p><p>게임에서 추출한 첫 번째 레벨입니다 (이 게임을 해본적이 있다면 아마도 기억할 것입니다):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YwdLy9MLGmalfogayPIliQ.gif" /></figure><p>자세히 살펴보면 레벨은 단순한 격자 객체들로 구성되어 있음을 알 수 있습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*BYgGv-u6qebTb4eFVYYbBA.png" /></figure><p>우리는 각 객체를 표현하는 문자를 사용해서 이 격자를 일련의 문자열로 쉽게 나타낼 수 있습니다.</p><pre>--------------------------<br>--------------------------<br>--------------------------<br>#??#----------------------<br>--------------------------<br>--------------------------<br>--------------------------<br>-##------=--=----------==-<br>--------==--==--------===-<br>-------===--===------====-<br>------====--====----=====-<br>=========================-</pre><p>우리는 레벨에 있는 각 객체를 문자로 대체했습니다:</p><ul><li>‘-’ 는 빈공간</li><li>‘=’ 는 딱딱한 블럭</li><li>‘#’ 는 깰 수 있는 벽돌</li><li>‘?’ 는 코인 블럭</li></ul><p>… 등등, 레벨에 있는 여러 객체들에 각각 다른 문자를 사용했습니다.</p><p>결과적으로 다음과 같이 텍스트 파일이 만들어 졌습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*d0k5nJPkUvalKoG2hGjMxg.png" /></figure><p>이 텍스트 파일을 살펴보면, 마리오의 레벨은 줄(line) 단위로 보면 실제로 별다른 패턴이 없다는 것을 알 수 있습니다</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/648/1*ljYHl0LZgRPBYtL08E6bwQ.png" /></figure><p>줄 단위로 읽어 보면 실제로 캡처할 패턴이 없습니다. 많은 줄들이 완전히 비어 있습니다.</p><p>레벨의 패턴은 레벨을 일련의 열(column)으로 생각할 때 드디어 들어나게 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/638/1*Qm8Yfv63c_KGIzZR7OmmCw.png" /></figure><p>열 단위로 보면 실제 패턴이 있습니다. 예를 들어, 각 열은 ‘=’로 끝납니다.</p><p>따라서 알고리즘이 데이터에서 패턴을 찾을 수 있게 하기 위해서는 데이터를 열별로 제공(feed)해야합니다. 입력 데이터의 가장 효과적인 표현을 찾는 것(<a href="https://en.wikipedia.org/wiki/Feature_selection">feature selection</a>이라고 함)은 기계 학습 알고리즘을 잘 사용하기 위한 중요한 요소 중에 하나입니다.</p><p>모델을 훈련시키기 위해서, 텍스트 파일을 90도 회전해야 했습니다. 이를 통해 패턴이 보다 쉽게 들어나는 순서에 따라 모델에 문자열을 제공(feed)할 수 있었습니다.</p><pre>-----------=<br>-------#---=<br>-------#---=<br>-------?---=<br>-------#---=<br>-----------=<br>-----------=<br>----------@=<br>----------@=<br>-----------=<br>-----------=<br>-----------=<br>---------PP=<br>---------PP=<br>----------==<br>---------===<br>--------====<br>-------=====<br>------======<br>-----=======<br>---=========<br>---=========</pre><h4><strong>모델 훈련시키기</strong></h4><p>헤밍웨이의 산문을 위한 모델을 만들 때 확인했듯이, 훈련시킬 수록 모델은 향상됩니다.</p><p>약간의 훈련 후, 우리 모델은 아직 쓸모없는 것을 만들고 있습니다:</p><pre>--------------------------<br>LL+&lt;&amp;=------P-------------<br>--------<br>---------------------T--#--<br>-----<br>-=--=-=------------=-&amp;--T--------------<br>--------------------<br>--=------$-=#-=-_<br>--------------=----=&lt;----<br>-------b<br>-</pre><p>현재는 ‘-’와 ‘=’가 많이 나타나야 한다와 같은 생각을 가지고 있지만, 그 뿐입니다. 아직 패턴을 전혀 알아 내지 못했습니다</p><p>수천 번의 반복을 진행하자, 이제 다음과 같은 걸 보기 시작합니다:</p><pre>--<br>-----------=<br>----------=<br>--------PP=<br>--------PP=<br>-----------=<br>-----------=<br>-----------=<br>-------?---=<br>-----------=<br>-----------=</pre><p>이제 모델은 각 줄이 동일한 길이여야한다는 것을 거의 알아 냈습니다. Mario의 로직 일부를 이해하기 시작했습니다: 마리오에서 파이프는 항상 2 블럭 넓이이고 높이는 최소 2 블럭 이상이므로 데이터에 “P”들은 2x2 클러스터로 나타나야 합니다. 굉장히 멋지군요!</p><blockquote>*역자주: 여기서 2x2 클러스터란 “P”는 2글자 이상이 붙어서 두 줄에 걸처 연결된 형태(cluster)로 나타나야 한다는 의미</blockquote><p>더 많은 훈련을 하면, 모델은 결국 완벽하게 유효한 데이터를 생성하게 됩니다:</p><pre>--------PP=<br>--------PP=<br>----------=<br>----------=<br>----------=<br>---PPP=---=<br>---PPP=---=<br>----------=</pre><p>모델에서 만들어진 전체 레벨의 가치 있는 데이터를 가져와 수평으로 회전시켜 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*UrdOQKyL6Ydt-y5p14Gv6A.png" /><figcaption>우리 모델로 만든 전체 레벨!</figcaption></figure><p>이 데이터는 정말 멋집니다! 그리고 대단한 것들이 몇 가지 있습니다.</p><ul><li>진짜 마리오 레벨처럼 <a href="http://www.mariowiki.com/lakitu">Lakitu</a>(구름을 타고 떠다니는 괴물)를 레벨의 시작되는 하늘에 집어 넣습니다.</li><li>파이프는 그냥 공중에 떠 있는 것이 아니라 반드시 딱딱한 블럭들 위에 있어야 한다는 것을 알고 있습니다.</li><li>적절한 장소에 적을 배치합니다.</li><li>플레이어가 앞으로 나아갈 수 없도록 막는 어떠한 것도 만들지 않습니다.</li><li>게임에 들어 있는 실제 레벨에서 그 스타일을 따왔기 때문에, 슈퍼 마리오 브라더스 1의 실제 레벨 같이 <em>느껴집니다.</em></li></ul><p>자 이제, 이 레벨을 가져와 Super Mario Maker에서 재현해 보겠습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/546/1*mx7knp61S5P4qTz_USc-BA.jpeg" /><figcaption>Super Mario Maker 속으로 들어온 우리의 레벨 데이터</figcaption></figure><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F_-Gc6diodcY%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D_-Gc6diodcY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F_-Gc6diodcY%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/c55d3666a355cb77850e110fec0d6a75/href">https://medium.com/media/c55d3666a355cb77850e110fec0d6a75/href</a></iframe><p>이제 직접 플레이 해보세요!</p><p>Super Mario Maker를 사용하는 경우 <a href="https://supermariomakerbookmark.nintendo.net/courses/4AC9-0000-0157-F3C3">온라인으로 북마크</a>하거나 레벨 코드 <a href="https://supermariomakerbookmark.nintendo.net/courses/4AC9-0000-0157-F3C3">4AC9–0000–0157-F3C3</a>을 사용하여 찾아보면 이 레벨을 직접 플레이 할 수 있습니다.</p><h3><strong>장난감 vs. 실제 애플리케이션</strong></h3><p>우리가 모델을 훈련시키는데 사용한 순환 신경망(recurrent neural network 또는 RNN) 알고리즘은 실제 회사에서 음성 인식이나 번역과 같은 어려운 문제를 해결하는 데 사용하는 알고리즘과 같은 종류입니다. 우리 모델이 최첨단이 아닌 ‘장난감’으로 보이는 이유는 단지 매우 적은 데이터로 생성되었기 때문입니다. 초기 슈퍼 마리오 브라더스 게임은 정말 좋은 모델을 만들기 위한 충분한 데이터를 제공할 만큼 많은 레벨이 없을 뿐입니다.</p><p>닌텐도가 보유하고 있는 수십만 개의 사용자 제작 Super Mario Maker 레벨에 접근할 수 있다면, 우리는 정말 놀라운 모델을 만들 수도 있습니다. 그러나 닌텐도가 이를 제공하지 않기 대문에 불가능합니다. 대기업들은 데이터를 무료로 제공하지 않습니다.</p><p>더 많은 산업 분야에서 기계 학습이 더욱 중요해 짐에 따라, 좋은 프로그램과 나쁜 프로그램의 차이는 모델을 훈련시키는 데 얼마나 많은 양의 데이터를 확보했느냐가 될 것입니다. 그렇기 때문에 Google이나 Facebook과 같은 회사들이 그렇게도 여러분의 데이터를 좋아하는 것입니다.</p><p>예를 들어, Google은 대규모 기계 학습 애플리케이션을 구축하기 위한 소프트웨어 툴킷인 <a href="https://www.tensorflow.org/">TensorFlow</a>를 오픈소스로 공개했습니다. Google이 이처럼 중요하고 뛰어난 기술을 무료로 제공 한 것은 꽤나 큰 사건이었습니다. 이는 Google Translate을 강력하게 하기 위해 이를 공개했던 이유와 같습니다.</p><blockquote>*역자주: 오픈소스를 통해 많은 사람이 사용하면 수많은 훈련 데이터가 축적되어 그 프로그램의 성능이 당연히 강력해지기 때문입니다.</blockquote><p>즉, Google이 수집한 모든 언어에 대한 대규모 데이터가 없다면, 당신은 절대 Google Translate에 대한 경쟁 제품을 만들 수 없습니다. 데이터야 말로 Google의 가장 큰 장점입니다. 나중에 Google 지도 위치 기록(<a href="https://maps.google.com/locationhistory/b/0">Google Maps Location History</a>) 또는 Facebook 위치 기록(<a href="https://www.facebook.com/help/1026190460827516">Facebook Location History</a>)을 열어보면 당신이 이전에 가봤던 모든 장소가 저장되어 있다는걸 알게 될 것입니다.</p><h3><strong>추가 읽을 거리</strong></h3><p>기계 학습에서 문제를 해결할 수있는 유일한 방법은 없습니다. 어떻게 데이터를 사전 처리할지 또는 어떤 알고리즘을 사용할지 결정할 때 우리에겐 무한한 옵션이 있습니다. 때때로 <a href="https://en.wikipedia.org/wiki/Ensemble_learning">여러 접근방법을 결합</a>하면 하나의 접근 방법보다 더 나은 결과를 얻을 수 있습니다.</p><p>이 글을 읽은 분들이 슈퍼 마리오 레벨을 생성하는 또다른 흥미로운 접근 방법에 대한 링크들을 보내왔습니다.</p><ul><li><a href="http://amykhoover.com/">Amy K. Hoover</a> 팀은 <a href="http://julian.togelius.com/Hoover2015Composing.pdf">레벨의 객체(파이프, 바닥, 플랫폼 등) 타입을 전체 교향곡속에서 하나의 목소리으로 나타내는 접근 방식</a>을 사용했습니다. Functional Scaffolding이라는 프로세스를 사용해서, 시스템은 주어진 객체 타입의 블록으로 레벨을 보강해 줄 수 있습니다. 예를 들어, 당신이 레벨의 기본 모양을 스케치 하면, 시스템이 파이프 및 질문 블록을 추가하여 디자인을 완성해 줍니다.</li><li><a href="http://forskning.mah.se/en/id/tsstda">Steve Dahlskog</a> 팀은 레벨 데이터의 각 열을 일련의 n-gram “글자들(words)”로 모델링하면 대형 RNN보다 <a href="http://julian.togelius.com/Dahlskog2014Linear.pdf">훨씬 간단한 알고리즘으로 레벨을 생성 할 수 있다는 것</a>을 보여주었습니다.</li></ul><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b35f3d327761" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[기계 학습(Machine Learning, 머신 러닝)은 즐겁다! Part 1]]></title>
            <link>https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8?source=rss-ec24e1bbe5d8------2</link>
            <guid isPermaLink="false">https://medium.com/p/9a0297198ad8</guid>
            <category><![CDATA[기계학습]]></category>
            <category><![CDATA[인공지능]]></category>
            <category><![CDATA[머신러닝]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Jongdae Lim]]></dc:creator>
            <pubDate>Fri, 09 Dec 2016 06:35:06 GMT</pubDate>
            <atom:updated>2018-10-22T23:21:11.664Z</atom:updated>
            <content:encoded><![CDATA[<h4>(원문 : <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.yz7yrojj9">Machine Learning is Fun!</a> by Adam Geitgey)</h4><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><blockquote>*역자주: 기계 학습에 대해 공부하던 중 좋은 글을 발견했습니다. 기계 학습(Machine Learning)을 공부하는 한국분들을 위해 한국어로도 번역하면 좋겠다고 생각했습니다.</blockquote><blockquote>그래서 저자인 <strong>Adam Geitgey</strong>에게 허락을 받고 번역을 진행했습니다. 가능한 기존에 통용되는 용어로 번역하려고 했습니다만, 잘못되거나 부족한 부분이 있다면 언제든 알려주십시오. 계속해서 정확하게 업데이트하겠습니다.</blockquote><blockquote>이 글과 관련하여 의견이나 질문이 있으시면 <a href="mailto:jongdae.lim@oracle.com">Jongdae.Lim@gmail.com</a>으로 메일 주시기 바랍니다. 감사합니다.</blockquote><p>사람들이 기계 학습(Machine Learning)에 대해 이야기하는 것을 들어 봤지만, 그게 무슨 뜻인지는 명확하지는 않은가요? 동료들과 대화할 때 머리만 끄덕이고 있는 내 모습에 지쳤습니까? 그럼 이제 바꿔봅시다!</p><p>이 글은 기계 학습에 대해 궁금한 점은 있지만 어디서부터 시작해야할지 모르는 분들을 위한 안내서입니다. <a href="http://en.wikipedia.org/wiki/Machine_learning">위키 피디아의 글</a>을 읽고 좌절하고 포기해버린 많은 사람들은 누군가 좀더 쉬운 설명을 해주길 바란다고 생각합니다. 그래서 이글을 쓰게 되었습니다.</p><p>이 글의 목표는 누구에게나 쉽게 다가가는 데 있습니다 — 이는 글에 많은 일반화가 있음을 의미합니다. 하지만 어떻습니까? 그래서 더 많은 사람들이 ML에 관심을 가지게 된다면, 목표를 달성한 것입니다.</p><h3><strong>기계 학습(Machine Learning)이란 무엇인가?</strong></h3><p>기계 학습은 문제를 해결하기 위한 맞춤 코드(custom code)를 작성하지 않고도 일련의 데이터에 대해 무언가 흥미로운 것을 알려줄 수 있는 일반 알고리즘(generic algorithms)이 있다는 아이디어입니다. 코드를 작성하는 대신 데이터를 일반 알고리즘에 공급하면, 데이터를 기반으로 한 자체 로직이 만들어 지게 됩니다.</p><p>예를 들어, 이러한 알고리즘의 한 종류가 분류(classification) 알고리즘입니다. 이 알고리즘을 사용하면 데이터를 서로 다른 그룹으로 분류할 수 있습니다. 손으로 쓴 숫자를 인식하는 데 사용 된 것과 동일한 분류 알고리즘을 그대로 코드 변경없이 사용해서, 이메일을 스팸과 스팸이 아닌 것으로 분류 할 수도 있습니다. 결국 동일한 알고리즘이지만 다른 학습 데이터를 제공하면 다른 분류 로직이 자동으로 만들어지게 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*xa0KGxXvdMY07Vp7F803Xw.png" /><figcaption>이 기계 학습 알고리즘은 서로 다른 많은 분류 문제에 재사용될 수있는 블랙 박스입니다.</figcaption></figure><p>“기계 학습(Machine Learning)”은 이런 종류의 일반 알고리즘(generic algorithms)을 포함하는 포괄적인 용어입니다.</p><h3>두 종류의 기계 학습 알고리즘</h3><p>기계 학습 알고리즘은 <strong>지도 학습(supervised learning)</strong>과 <strong>비지도 학습(unsupervised learning)</strong>의 두 가지 주요 범주 중 하나로 분류될 수 있습니다. 이 차이는 간단하지만 매우 중요합니다.</p><h4><strong>지도 학습(Supervised Learning)</strong></h4><p>당신이 부동산 중개인이라고 가정해 봅시다. 비즈니스가 성장함에 따라 당신을 도와줄 수습 직원을 고용하려고 합니다. 그런데 문제가 있습니다. 당신은 주택을 보기만해도 얼마의 가치가 있는지 잘 알수 있습니다. 그러나 수습 직원들은 당신과 같은 경험이 없으므로 어떻게 주택 가격을 책정해야 하는지 알지 못합니다.</p><p>수습 직원을 돕기 위해 (그리고 어쩌면 당신이 휴가를 즐기기 위해), 당신은 주택의 크기, 지역 등을 기준으로 주택 가치를 추정 할 수 있는 작은 앱을 개발하기로 했습니다.</p><p>그래서 당신은 누군가가 당신의 도시에서 3 개월 동안 주택을 팔 때마다 그 정보를 기록했습니다. 각 주택의 방의 개수, 평방 피트 크기, 지역 등 세부 정보를 기록했습니다. 그러나 가장 중요한 것은 최종 판매 가격을 기록하는 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*IzlJNAXx6Mwt02-W8ca_4Q.png" /><figcaption>이것이 바로 우리의 “훈련 데이터(training data)”입니다.</figcaption></figure><p>이 훈련 데이터를 사용해서 해당 지역의 다른 주택이 얼마의 가치가 있는지 추정 할 수있는 프로그램을 만들려고 합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*tzN3MZMnGMjAX59GGnsPMQ.png" /><figcaption>우리는 훈련 데이터를 사용하여 다른 주택의 가격을 예측하고 싶습니다.</figcaption></figure><p>이것을 <strong>지도 학습(supervised learning)</strong>이라고합니다. 당신은 각각의 주택들이 얼마에 팔렸는지 알고 있습니다. 다시 말해서, 당신은 문제에 대한 답을 알고 있기 때문에 여기에서 반대로 로직을 파악할 수 있습니다.</p><p>앱을 제작하기 위해, 각 주택에 대한 훈련 데이터를 기계 학습 알고리즘에 넣어야 합니다. 이 알고리즘은 해당 숫자의 의미를 알아낼 수 있는 수학이 무엇인지 파악하기 위해 노력하게 됩니다.</p><p>이것은 마치 모든 연산 기호가 지워진 수학 시험에 대한 해답을 갖고 있는 것과 같습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*sLiy7oDqMgLNndKXr2QYyw.png" /><figcaption>오, 이런! 어떤 나쁜 학생이 선생님의 해답지에서 연산 기호를 지워버렸네요!</figcaption></figure><p>여기에서, 당신은 어떤 종류의 수학 문제가 시험에 있었는지 알아낼 수 있습니까? 당신은 오른쪽에 있는 각각의 답을 얻기 위해 왼쪽에있는 숫자들에 “무언인가를 하면” 된다는 것을 알고 있습니다.</p><blockquote><em>*역자주: 모두 눈치채셨겠지만, 위 수학 퀴즈에서 빠진 기호는 순서대로 *, -(곱하기, 빼기)입니다.</em></blockquote><blockquote><em>1) 2 * 4 - 5 = 3, 2) 5 * 2 - 8 = 2, 3) 2 * 2 - 1 = 3, …, 8) 1 * 8 - 1 = 7</em></blockquote><p><strong>지도 학습</strong>에서, 당신은 컴퓨터가 당신을 위해 그 관계를 해결하도록 내버려 두면 됩니다. 그리고 이 특정 문제들을 해결하기 위해 필요한 수학이 무엇인지 알게되면 같은 유형의 다른 문제에 대해서도 답을 얻을 수 있습니다!</p><h4><strong>비지도 학습(Unsupervised Learning)</strong></h4><p>처음 시작했던 부동산 중개업자의 사례로 돌아가 보겠습니다. 각 주택의 판매 가격을 모르는 경우에는 어떻게 될까요? 비록 당신이 알고 있는 정보가 각 집의 크기, 위치 같은 것 뿐이더라도, 당신은 여전히 멋진 일들을 할 수 있습니다. 이것이 바로 <strong>비지도</strong> <strong>학습</strong>입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/616/1*J63ZucZ84Qd-b-ppigOKAw.png" /><figcaption>알려지지 않은 숫자 (예를 들어, 가격)를 예측하려고하지 않더라도 여전히 기계 학습을 통해 흥미로운 일들을 할 수 있습니다.</figcaption></figure><p>이것은 누군가 종이에 숫자 목록을 주고 “나는 이 숫자가 무엇을 의미하는지 알지 못하지만, 어쩌면 당신은 패턴이나 분류(grouping) 또는 무언가가 있는지 알아낼 수도 있을 겁니다 — 행운을 빕니다!”라고 말하는 것과 같습니다.</p><p>그렇다면 이 데이터로 무엇을 할 수 있을까요? 당신은 데이터에서 서로 다른 시장 세그먼트(market segments, 세분시장)를 자동으로 식별하는 알고리즘을 만들 수도 있습니다. 어쩌면 당신은 지역 대학 근처의 주택 구매자는 침실이 많은 작은 집을 좋아지만, 교외의 주택 구매자는 면적이 넓은 침실이 3 개인 집을 선호한다는 사실을 알아낼지도 모릅니다. 이렇게 여러 종류의 고객에 대해 알게되면 마케팅 활동에 직접적으로 도움을 받을 수 있을 것입니다.</p><p>당신이 할 수 있는 또 다른 멋진 일은 자동으로 다른 주택들과 다른 특이한 주택을 식별하는 것입니다. 만약 이 특이한 주택들이 거대한 저택이라면 더 많은 수수료를 받을 수 있기 때문에 당신의 최고의 영업사원들을 이 영역에 집중시킬 수 있습니다.</p><p>이 글의 나머지는 대부분 지도 학습에 집중하게 될텐데, 그렇다고 해서 비지도 학습이 덜 유용하거나 흥미롭지 않다는 것은 아닙니다. 사실, 비지도 학습은 알고리즘들이 향상됨에 따라 더욱더 중요해 지고 있습니다. 왜냐하면, 비지도 학습이 정확한 답을 가진 라벨이 붙여진 데이터가 아닌 경우에 사용될 수 있기 때문입니다.</p><p><em>현학자들을 위한 노트: 네 맞습니다. 많은 </em><a href="http://en.wikipedia.org/wiki/Machine_learning#Algorithm_types"><em>다른 종류</em></a><em>의 기계 학습 알고리즘이 있습니다. 그러나 , 여기서부터 시작하는 것도 나쁘지 않습니다.</em></p><h3><strong>멋지군요, 그런데 정말로 주택 가격이 얼마인지 “학습”을 통해 추정할 수 있습니까?</strong></h3><p>인간이기 때문에, 당신의 두뇌는 대부분의 상황에 대해 이해할 수 있으며, 명시적으로 가르쳐 주지 않아도 상황을 어떻게 해결하면 되는지 배울 수 있습니다. 당신이 오랫동안 주택을 판매한다면, 적절한 주택 가격, 주택을 판매하는 가장 좋은 방법, 관심이 있을 고객의 종류 등에 대한 “느낌(feel)”을 본능적으로 갖게될 것입니다. <a href="http://en.wikipedia.org/wiki/Strong_AI">Strong AI</a> 연구의 목표가 바로 이러한 인간의 능력을 컴퓨터로 복제하는 것입니다.</p><p>그러나 현재의 기계 학습 알고리즘은 그런 수준은 아닙니다 — 매우 구체적이고 제한적인 문제에 초점을 맞춘 경우에만 동작합니다. 이러한 경우 “학습”에 대한 더 나은 정의는 “약간의 샘플 데이터를 기반으로 특정 문제를 해결하기위한 방정식을 알아내는 것”이 될 수 있습니다.</p><p>불행하게도”약간의 샘플 데이터를 기반으로 특정 문제를 해결하기위한 방정식을 알아내는 기계”는 그렇게 좋은 이름은 아닙니다. 그래서 우리는 대신 “기계 학습”이라고 부릅니다.</p><p>물론 당신이 앞으로 50년 후에 이 글을 읽고 있고 이미 우리는 Strong AI를 위한 알고리즘을 알아낸 후라면, 여기 있는 글들은 모두 약간 옛날 얘기처럼 보일 것입니다. 그렇다면 이봐요 미래인간, 이제 그만 읽고 로봇 하인에게 샌드위치나 만들라고 하세요.</p><h4><strong>그럼 프로그램을 작성해 봅시다!</strong></h4><p>그래서, 앞선 예와 같이 주택의 가치를 추정하는 프로그램을 어떻게 작성 하시겠습니까? 다음 내용을 읽기 전에 잠시 생각해 보세요.</p><p>기계 학습에 대해 잘 모른다면, 아마도 당신은 다음과 같이 주택 가격을 추정하기위한 기본 규칙들을 작성하고자 할 것입니다.</p><pre>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):<br>  price = 0</pre><pre><em># 담당 지역내 평균 주택 가격은 평방 피트 당 200 달러이다<br></em>  price_per_sqft = 200</pre><pre>if neighborhood == &quot;hipsterton&quot;:<br>   <em> # 하지만 다른 지역은 조금 더 비싸다</em><br>    price_per_sqft = 400</pre><pre>elif neighborhood == &quot;skid row&quot;:<br>   <em> # 그리고 다른 몇몇 지역은 싸다</em><br>    price_per_sqft = 100</pre><pre><em># 주택의 크기를 기반으로 주택 가격을 추정하는 것으로 시작한다</em><br>  price = price_per_sqft * sqft</pre><pre><em># 이제 침실의 개수로 추정치를 조정한다</em><br>  if num_of_bedrooms == 0:<br>    <em># 원룸형 아파트는 가격이 싸다</em><br>    price = price — 20000<br>  else:<br>   <em> # 일반적으로 많은 침실이 있는 주택이 더 비싸다</em><br>    price = price + (num_of_bedrooms * 1000)</pre><pre>return price</pre><p>이런 과정을 몇 시간이고 계속하다 보면, 뭔가 그럴듯한 것을 만들어 낼 수도 있습니다. 그러나 당신의 프로그램은 결코 완벽하지 않으며 가격이 변함에 따라 로직을 유지하기가 어려울 것입니다.</p><p>당신을 대신해서 컴퓨터가 이 기능을 구현하게 하는 방법이 더 좋지 않을까요? 함수가 정확한 숫자를 반환하는데 누가 뭐라고 하겠습니까:</p><pre>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):<br>  price = &lt;컴퓨터, 나 대신 수학식 좀 만들어줘&gt;</pre><pre>return price</pre><p>이 문제를 해결할 수 있는 또다른 방법은 <strong>가격(price)</strong>이 맛있는 스튜(stew)이며 그 재료는 <strong>침실의 개수(number of bedrooms)</strong>, <strong>평방 피트 면적(square footage)</strong> 및 지역<strong>(neighborhood)이라고 </strong>생각하는 것입니다. 각 재료가 최종 가격에 얼마나 많은 영향을 미치는지 파악할 수 있다면, 이 최종 가격을 만들어줄 혼합 재료의 정확한 비율이 있을 것입니다.</p><p>이렇게하면 정말 간단하게 (if 와 else가 미친듯이 많았던) 원래 함수를 다음과 같이 줄일 수 있을 것입니다.</p><pre>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):<br> price = 0</pre><pre># 이건 한 꼬집 넣고<br> price += num_of_bedrooms * <strong>.841231951398213</strong></pre><pre># 그리고 저건 한 스픈 정도 넣고<br> price += sqft * <strong>1231.1231231</strong></pre><pre># 이건 아마도 한 줌 넣고<br> price += neighborhood * <strong>2.3242341421</strong></pre><pre># 그리고 마지막으로, 약간의 소금을 추가<br> price += <strong>201.23432095</strong></pre><pre>return price</pre><p>굵은 글씨로 쓰여진 마법의 숫자를 주목하세요 — <strong>.841231951398213, 1231.1231231, 2.3242341421</strong> 그리고 <strong>201.23432095.</strong> 이 숫자들이 바로 <strong>가중치</strong> 입니다. 만약 모든 주택에 적용할 수 있는 완벽한 가중치를 찾아낼 수만 있다면, 우리의 함수는 집값을 예측할 수 있을 것입니다!</p><p>최상의 가중치를 알아낼 때 한가지 쉬운 방법은 아마 다음과 같을 것입니다.</p><h4><strong>Step 1:</strong></h4><p>먼저 가중치를 <strong>1.0</strong>로 하고 시작합니다:</p><pre>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):<br> price = 0</pre><pre># 이건 한 꼬집 넣고<br> price += num_of_bedrooms * <strong>1.0</strong></pre><pre># 그리고 저건 한 스픈 정도 넣고<br> price += sqft * <strong>1.0</strong></pre><pre># 이건 아마도 한 줌 넣고<br> price += neighborhood * <strong>1.0</strong></pre><pre># 그리고 마지막으로, 약간의 소금을 추가<br> price += <strong>1.0</strong></pre><pre>return price</pre><h4><strong>Step 2:</strong></h4><p>그리고 알고있는 모든 주택 데이터를 당신의 함수를 통해 실행해보고 각 주택의 정확한 가격과 이 함수가 얼마나 차이가 나는지 살펴보는 것입니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*fiyAFQXASnN8Hr3DF-o1WQ.png" /><figcaption>당신의 함수를 이용해서 각 주택의 가격을 예상해 보세요.</figcaption></figure><p>예를 들어, 첫번째 주택은 실제로는 $250,000에 판매되었고, 당신의 함수는 $178,000에 판매되었다고 추정한것인데, 이 주택 하나만 보더라도 $72,000만큼 차이가 납니다.</p><p>이제 데이터 세트에 있는 각 주택 마다 차이난 가격의 제곱값을 추가해 보겠습니다. 데이터 세트에는 500 개의 주택 판매 정보가 있었고 각 주택에 대해 당신의 함수가 추정한 값과 실제 가격의 차이를 제곱한 값의 총합은 $86,123,373라고 가정해 보겠습니다. 이를 통해 당신이 함수가 현재 얼마나 “잘못되었는지” 알 수 있습니다.</p><p>이제, 이 합계를 500으로 나눠서 각 주택별로 얼마나 차이가 나는지 평균값을 구해보겠습니다. 이 평균 오류 값을 이 함수의 <strong>비용(cost)</strong>이라고 하겠습니다.</p><p>당신이 가중치를 잘 조정해서 이 비용을 0으로 만들 수 있다면, 당신의 함수는 완벽해지게 됩니다. 다시 말해서, 모든 경우에 대해서 당신의 함수가 입력 데이터를 기반으로 주택 가격을 완벽하게 추정할수 있다는 것을 의미합니다. 네, 다른 가중치를 시도해서 가능한 이 비용을 낮추려는 것, 이것이 바로 우리의 목표입니다.</p><h4><strong>Step 3:</strong></h4><p><strong>가능한 모든 가중치를 조합해서</strong> 2 단계를 계속 반복합니다. 어떤 조합의 가중치든 비용을 0에 가깝게 만들어 주는 것을 사용하면 됩니다. 이런 가중치를 찾으면 문제를 해결한 것입니다!</p><h3><strong>깜짝 놀랄 시간</strong></h3><p>아주 간단하죠? 방금 전 한 일에 대해 정리해 보겠습니다. 당신은 약간의 데이터를 가져와 이를 세 가지의 일반적이고 아주 간단한 단계에 제공한 다음, 해당 지역의 모든 집의 가격을 추정할 수 있는 함수를 만들어 냈습니다. Zillow, 조심하는게 좋을꺼야!</p><blockquote>*역자주 : Zillow는 2006년 설립된 미국에서 매우 유명한 온라인 부동산 회사입니다.</blockquote><p>그런데 당신을 깜짝 놀라게 할 몇 가지 사실이 더 있습니다:</p><ol><li>40년 넘게 많은 분야(예를 들어, 언어학/번역)에서 연구한 결과, “숫자로 만든 스튜를 젓는”(제가 막 만들어낸 문장입니다) 이러한 일반 학습 알고리즘이 명시적인 규칙으로 접근하는 실제 사람들의 접근방법을 압도했습니다. 기계 학습의 이 “멍청한” 접근방법이 결국 인간 전문가들을 이기고 말았습니다.</li></ol><blockquote>*역자주: “숫자로 만든 스튜를 젓는” (“stir the number stew”)의 뜻은 앞서 변수들을 스튜의 재료로 가정하고 가중치의 조합을 찾아내는 아주 단순한 과정을 뜻합니다.</blockquote><p>2. 당신이 만든 최종 함수는 실제로 완전히 바보입니다. “평방 피트”또는 “침실”이 무엇인지조차 알지 못합니다. 이 함수가 아는 것이라고는 정답을 얻기위해 이러한 숫자들의 값을 저어줘야 한다입니다.</p><p>3. 당신은 <strong><em>어째서</em></strong> 특정 가중치의 조합이 잘 동작하는지 알지 못하는게 당연합니다. 즉, 당신은 실제로 이해하지 못하는 함수를 작성했지만, 잘 동작하는 것은 증명할 수 있습니다.</p><p>4. “sqft”과 “num_of_bedrooms”와 같은 매개변수를 사용하는 대신 예측 함수에 숫자 배열만을 사용한다고 상상해보십시오. 그리고, 각 숫자가 자동차 위에 장착한 카메라로 촬영한 이미지에서 1 픽셀의 밝기를 나타 낸다고 가정해 보겠습니다. 이제 “price”라는 예측을 출력하는 대신 이 함수는 “degrees_to_turn_steering_wheel”이라는 예측을 출력한다고 상상해보세요. <strong>그래요,</strong> <strong>당신은 방금 스스로 차를 조종 할 수 있는 함수 만들었습니다!</strong></p><p>완전 멋지죠?</p><h4><strong>Step 3에서 “모든 숫자를 시도해보는 것”은 무슨 뜻인가요?</strong></h4><p>물론, 실제로 모든 가중치의 모든 조합을 시도해서 가장 잘 작동하는 조합를 찾을 수는 없습니다. 당신이 시도 할 숫자 조합은 무한하기 때문에 당연하게도 영원히 걸릴 것입니다.</p><p>이것을 피하기 위해서, 수학자들은 많은 것을 시도하지 않고도 이러한 가중치에 대한 좋은 값을 빨리 찾을 수있는 <a href="http://en.wikipedia.org/wiki/Gradient_descent">영리한 방법</a>을 알아냈습니다. 여기 그중 한가지 방법이 있습니다:</p><p>먼저, 위 Step 2를 나타내는 간단한 방정식을 작성합니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*bdODyxl1KZQvFlgPLi1eDw.png" /><figcaption>이 방정식이 바로 당신의 <strong>비용 함수(cost function)</strong> 입니다.</figcaption></figure><p>이제 완전히 똑같은 방정식을 재작성해 볼텐데, 대신 기계 학습의 수학 용어를 사용해 보겠습니다.(모르는 용어들은 지금은 그냥 무시하세요) :</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*hMcuz-MkrxMK7Xuzlb2s3w.png" /><figcaption>θ는 현재 가중치를 나타냅니다. J(θ)는 ‘현재 가중치의 비용’ 을 의미합니다.</figcaption></figure><p>이 방정식은 현재 설정한 가중치에 대해 우리의 가격 추정 함수가 얼마나 차이 나는지를 나타냅니다.</p><p><strong>number_of_bedrooms</strong>와 <strong>sqft</strong>에 대한 모든 가능한 가중치 값에 대해 이 비용 방정식을 그래프로 나타내면 다음과 같습니다:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*4-_XiEYl0l_MqZh9wkoQWQ.png" /><figcaption>우리 비용 함수의 그래프는 그릇(bowl)처럼 보입니다. 세로축이 비용을 나타냅니다.</figcaption></figure><p>이 그래프에서 파란색의 가장 낮은 지점이 비용이 가장 낮은 곳입니다 — 따라서, 우리의 함수는 조금 잘못되었습니다. 가장 높은 지점들이 가장 잘못된 것입니다. 즉, 우리가 이 그래프의 가장 낮은 지점으로 이동할 수 있는 가중치를 찾는다면, 우리는 답을 찾는 겁니다!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*-MrEhKUkNooLEz8shErBcg.png" /></figure><p>이제 그래프에서 가장 낮은 지점을 향해 “언덕을 걸어 내려 갈 수 있도록” 우리의 가중치를 조정해야 합니다. 만약 우리가 가장 낮은 지점으로 이동하도록 가중치를 조금씩 조정해 나간다면, 결과적으로 너무 많은 다른 가중치를 시도하지 않고도 그곳에 도달하게 될 것입니다.</p><p>미적분에 대해 기억을 더듬어 보면, 함수를 미분하면 함수의 특정 지점에 대한 접선의 기울기를 알 수 있다는 것이 생각날 것입니다. 다시 말해서, 미분을 통해 그래프의 특정 지점에 대해 내리막 길이 어느 방향인지 알 수 있습니다. 우리는 이 지식을 이용해 내리막 길로 걸어 내려갈 수 있습니다.</p><p>따라서 각 가중치에 대한 비용 함수의 편미분을 계산하고, 각 가중치에서 해당 값을 뺄 수 있습니다. 이를 통해 우리는 언덕 맨 아래로 한 걸음 더 가까이 가게 됩니다. 이 작업을 계속하면 궁극적으로 언덕 맨 아래에 도달하여 가장 좋은 가중치를 얻게됩니다. (이 부분이 이해가 안되어도 걱정하지 말고 계속 읽세요).</p><p>사실 이 방법은 함수의 적합한 가중치를 찾는 방법 중에 하나인 <strong>배치 기울기 하강(batch gradient descent)</strong>에 대한 개괄적인 설명입니다. 세부적인 내용에 대해 배우고 싶다면 <a href="http://hbfs.wordpress.com/2012/04/24/introduction-to-gradient-descent/">좀더 깊이 공부</a>해보십시오.</p><p>실제 문제를 해결하기 위해 기계 학습 라이브러리를 사용하면 이러한 모든 작업이 자동으로 수행됩니다. 그러나 어떤 일이 일어나는 지에 대해 잘 이해하고 있는 것은 여전히 유용합니다.</p><h3><strong>적당히 건너 뛴 내용은 또 어떤 것들이 있나요?</strong></h3><p>앞서 설명한 3 단계의 알고리즘을 <strong>다변수 선형 회귀(multivariate linear regression)</strong>라고 부릅니다. 사실 당신은 모든 주택 데이터 값에 대해 딱 맞는 직선 방정식을 계산하는 것입니다. 그리고 이 방정식을 사용해서 이전에 보지 못했던 주택의 판매 가격을 추정할 수 있는 것입니다. 이는 정말이지 강력한 아이디어이며, 당신은 이 방법으로 “진짜” 문제를 해결할 수 있습니다.</p><p>그러나 지금까지 설명한 접근 방식이 단순한 경우에는 효과가 있지만, 모든 경우에 동작하지는 않습니다. 한 가지 이유는 집값이 항상 연속적인 직선을 따라갈만큼 단순하지 않기 때문입니다.</p><p>다행히도 이를 처리 할 수 있는 많은 방법들이 있습니다. 비선형 데이터 (예를 들어, <a href="http://en.wikipedia.org/wiki/Artificial_neural_network">신경망(neural networks)</a> 또는 <a href="http://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a> 과 <a href="http://en.wikipedia.org/wiki/Kernel_trick">kernels</a>)를 처리 할 수있는 많은 기계 학습 알고리즘이 있습니다. 선형 회귀(linear regression)를 보다 영리하게 사용해서 더욱 복잡한 선을 맞추는 방법도 있습니다. 이런한 모든 방법에는 최상의 가중치를 찾아야한다는 동일한 기본 아이디어는 여전히 적용되고 있습니다.</p><p>정리하자면, 기본 개념은 매우 단순하지만 기계 학습을 적용하고 유용한 결과를 얻으려면 약간의 기술과 경험이 필요합니다. 하지만, 모든 개발자가 배울 수있는 기술입니다!</p><h3><strong>기계 학습은 마술인가?</strong></h3><p>컴퓨터 학습 기술을 (필기체 인식과 같은 같이) 어려워 보이는 문제에 얼마나 쉽게 적용 할 수 있는지 알게되면, 당신은 이제 충분한 데이터만 있다면 기계 학습을 적용해서 문제를 해결할 수 있다는 느낌을 가지게 될 것입니다. 단지 데이터만 제공하고 컴퓨터가 마술처럼 데이터에 맞는 방정식을 계산해 주는 것을 감상만 하면 됩니다.</p><p>하지만, 기계 학습은 보유한 데이터로 해결 가능한 문제인 경우에만 동작한다는 점을 기억하는 것이 중요합니다.</p><p>예를 들어, 각 주택의 화분 종류에 따라 주택 가격을 예측하는 모델을 만들면, 절대로 동작하지 않을 것입니다. 각 집에있는 화분과 주택 판매 가격 사이에는 아무런 관계가 없습니다. 따라서 컴퓨터가 아무리 노력해도 둘 사이의 관계를 결코 추론 할 수 없습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*YgXmXhgb2yHwZ40jfrs2PQ.png" /><figcaption>실제로 관련이 있는 관계만을 모델링할 수 있습니다.</figcaption></figure><p>따라서 인간 전문가가 수작업으로 문제를 해결하기 위해 데이터를 사용할 수 없다면, 컴퓨터도 아마 동작하지 않을 것입니다. 바꿔말해, 인간이 해결할 수있는 문제 중에서 컴퓨터가 훨씬 빨리 문제를 해결할 수 있는 것에 집중하는 것이 좋습니다.</p><h3><strong>기계 학습에 대해 좀더 배울 수 있는 방법</strong></h3><p>제 생각에, 현재 기계 학습의 가장 큰 문제점은 학계와 상업 연구 그룹의 세상에 있습니다. 실제로 전문가가되지 않고서는 폭 넓은 이해를 하고 싶어하는 사람들을 위한 이해하기 쉬운 자료가 세상에 많지 않습니다. 그러나 매일 조금 나아지고 있습니다.</p><p>앤드류 응 (Andrew Ng)의 <a href="https://www.coursera.org/course/ml">Coursera에 있는 무료 Machine Learning</a> 수업은 매우 훌륭합니다. 여기에서 시작하는 것을 강력히 추천드립니다. 컴퓨터 전공 학위가 있는 사람과 수학을 아주 조금 기억하는 사람이라면 누구나 수강할 수 있습니다.</p><p>또한 <a href="http://scikit-learn.org/stable/">SciKit-Learn</a>을 다운로드해서 설치하면 수 많은 기계 학습 알고리즘을 가지고 놀 수 있습니다. 이것은 모든 표준 알고리즘에 대한 “블랙 박스” 버전의 파이썬 프레임워크(python framework)입니다.</p><p>이 글이 마음에 들었다면, 제 <a href="http://eepurl.com/b9fg2T"><strong>Machine Learning is Fun! 이메일 리스트에 가입</strong></a>하는 것도 좋습니다! 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 제가 이런 종류의 추가 글을 올릴때가 언제인지 알 수 있는 가장 좋은 방법입니다.</p><blockquote>*역자주: 번역글과 관련해 의견 주시려면, 저에게 <a href="mailto:jongdae.lim@gmail.com">직접 이메일</a>을 보내시거나 <a href="https://kr.linkedin.com/in/jongdae-lim-7537445a">LinkedIn에서 저를 찾으셔도 됩니다</a>.</blockquote><ul><li>추가 번역 보기: <a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-1-9a0297198ad8"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-2-b35f3d327761"><em>Part 2</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-3-928a841a3aa"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-4-63ed781eee3c"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-5-83b7a44b797a#.q85uh479e"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-6-eb0ed6b0ed1d?source=linkShare-ec24e1bbe5d8-1495949976"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-7-2435b4a55ccd"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352?source=linkShare-ec24e1bbe5d8-1510230631"><em>Part 8</em></a></li><li>추가 원문 보기: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"><em>Part 1</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3"><em>Part 2</em></a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721"><em>Part 3</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><em>Part 4</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa"><em>Part 5</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=linkShare-ec24e1bbe5d8-1495950104"><em>Part 6</em></a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7"><em>Part 7</em></a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196"><em>Part 8</em></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9a0297198ad8" width="1" height="1">]]></content:encoded>
        </item>
    </channel>
</rss>