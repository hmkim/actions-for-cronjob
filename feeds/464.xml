<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Eunju Son on Medium]]></title>
        <description><![CDATA[Stories by Eunju Son on Medium]]></description>
        <link>https://medium.com/@EJSohn?source=rss-a1aaa843ad8c------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/2*E_6obTq1WAEV9Xnczuw6og.png</url>
            <title>Stories by Eunju Son on Medium</title>
            <link>https://medium.com/@EJSohn?source=rss-a1aaa843ad8c------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 13 May 2019 02:52:33 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/@EJSohn" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[2019 NEA WTM Ambassadors Summit in Tokyo Review]]></title>
            <link>https://medium.com/@EJSohn/2019-nea-wtm-ambassadors-summit-in-tokyo-review-d70a0ac661f7?source=rss-a1aaa843ad8c------2</link>
            <guid isPermaLink="false">https://medium.com/p/d70a0ac661f7</guid>
            <category><![CDATA[women-in-tech]]></category>
            <dc:creator><![CDATA[Eunju Son]]></dc:creator>
            <pubDate>Sat, 04 May 2019 16:06:48 GMT</pubDate>
            <atom:updated>2019-05-04T16:11:12.781Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YBv2qjXjR4gof5C_f33iEA.jpeg" /><figcaption>Google Japan</figcaption></figure><p>I had a great experience in this March, so i decided to write this article to share this experience with everyone. The experience is about the event i participated in, The <strong>NortheastAsia WTM Summit Tokyo </strong>and i had a really good time in there. First, iwant to start by telling you how i got this opportunity, and what WTM is.</p><h3><strong>Connection with Google community</strong></h3><p>I guess i have to introduce myself first. I am Devops engineer who is interested in Platform/Cloud and i participated in Devfest Seoul 2018 as Cloud session speaker. It was also broadcasted via DevFest OnAir and this is my first meet with Google community.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bEvObKqX7zFsoceT3tpu6Q.png" /><figcaption>My humble speaking</figcaption></figure><p>At that time i got caught by the way developers sharing their knowledge with others in Google community. I was not that social in developers community before but this feeling made me keep joining Google developer event. And when i participated in another event as a speaker again, i got ticket to going to WTM ambassador summit.</p><h3>What WTM is?</h3><p>WTM is short for Women Tech Maker. <a href="https://anitab.org/profiles/googles-women-techmakers-program-empowers-women/">ANITA B.ORG</a> cited WTM’s purpose as below.</p><blockquote>“Our goal is to inspire passionate, creative women through discussions with thought leaders, technical workshops, design sprints and networking opportunities,” said Natalie Villalobos, Google’s Women in Technology Advocate leading Women Techmakers. “Overall, we aim to tear down the boundaries inhibiting women from participating in the industry.”</blockquote><p>Women Techmakers is a global program that celebrates women, encouraging them to pursue and excel in technology careers. This past International Women’s Day, 11,000 women benefited from these 128 events across 52 countries. Summits were held at Google offices around the globe and 2019 Tokyo WTM Summit was one of them.</p><h3>WTM Summit Program</h3><p>The morning of WTM Summit day started early. The hotel was close to Google Japan so we were able to walk to there easily. The first thing that caught my eyes when I entered Google Japan was the spectacular view of Tokyo city. One side of the room was made of glasses and we could see outside.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_LiTGlBZPDLlWPahXSUbPw.jpeg" /><figcaption>The view from Google Japan</figcaption></figure><p>After a short photo time, summit’s program started. First, I was able to hear the stories of women working in the technology industry with keynotes and panel talks. The story of this is going to be more detailed later.</p><p>After having lunch, I was able to tour the Google Japan office in more detail. Unfortunately, inside photos could not be shared, but they were good offices where the culture of Japan melted.</p><p>Next we were divided into two groups and choose the workshop to join, so we could participate more actively. One was<strong> IamRemarkable</strong> workshop and the other was Google cloud service <strong>hand-on</strong> workshop. IamRemarkable was really popular so i had to give the opportunity to others. But hand-one workshop was also good and i was able to experience service that i wonder about.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-oKUWUM-jnKrPPDIocW5BA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uiOXsRsIfyfCnYL3rOKvPQ.jpeg" /><figcaption>Brainstorming</figcaption></figure><p>In the last order we had time to brainstorm about the efforts we could make as a woman in the tech industry. There were a number of themes, so participants could come up with ideas for the topics they wanted. Everyone actively participated and really came up with a meaningful idea.</p><p>After the Summit event, a prepared after-party started. Delicious food was displayed and a cake with a WTM mark was revealed. The people of the participating countries distributed the gifts they had taken in their respective countries, and greeted them with a sincere greeting next time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*87kknjkroHN5Yo898J1BVg.png" /></figure><h3>Impressive moments in Panel Discussion</h3><p>The panel discussion consisted of four sessions.</p><ul><li>Women in Tech Retrospective</li><li>Session from an GDE</li><li>Building a community</li><li>Reducing gender bias in Google translate</li></ul><p>All were good sessions and i particularly heard Women in Tech Retrospective impressively. The story of the session made me think about the truth that I always knew but did not really want to face. At present, the proportion of women in the technology industry is significantly lower. I am the only woman in the same department that I work at the moment. If you deduce the reason, you will fall into chicken-egg problem. <br>Is there a perception that women are not professional because the number of female engineers is small? Or is it because women are perceived not to be professional, so the barriers to entry are so high that the number of female engineers is small?</p><p>I don’t think I need to talk more. What I want to talk about in this session is, I think, that we will need more woman role models in the future.</p><h3>Conclude</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3nWbB0cI3So5hN08dTwhRA.jpeg" /><figcaption>WTM Ambassadors — !</figcaption></figure><p>I could had all of this experiences thanks to Google community. i met a lot of cool women and shared unforgettable stories. As a WTM ambassador i want to make more sharable stories in the future.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d70a0ac661f7" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Out Of Memory Killer 회피하기]]></title>
            <link>https://medium.com/@EJSohn/out-of-memory-killer-%ED%9A%8C%ED%94%BC%ED%95%98%EA%B8%B0-9efc65f88c92?source=rss-a1aaa843ad8c------2</link>
            <guid isPermaLink="false">https://medium.com/p/9efc65f88c92</guid>
            <category><![CDATA[memory-management]]></category>
            <category><![CDATA[kernel]]></category>
            <category><![CDATA[linux]]></category>
            <dc:creator><![CDATA[Eunju Son]]></dc:creator>
            <pubDate>Sun, 28 Apr 2019 11:40:59 GMT</pubDate>
            <atom:updated>2019-04-28T11:40:59.526Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5Xgetha9-iIJP-wXwlGx_Q.png" /></figure><p><a href="https://medium.com/@EJSohn/%EB%B2%88%EC%97%AD-linux%EC%97%90%EC%84%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC%EB%A5%BC-%EB%8B%A4-%EC%8D%A8%EB%B2%84%EB%A0%B8%EC%9D%84-%EB%95%8C-%EC%9D%BC%EC%96%B4%EB%82%98%EB%8A%94-%EC%9D%BC-9dadba29c89c">이전에 쓴 번역글</a>에서 메모리가 부족할 때 머신의 내부에서 어떤 일들이 벌어지는지 간단하게 살펴보았습니다. 다시 한 번 요약해보면, 프로세스가 메모리를 필요로 할 때 RAM에 가용한 메모리가 없으면 buff/cache 나 swap file system을 빌려서 사용하게 됩니다. 하지만 이또한 부족해지는 시점이 있는데 그 시점에서 리눅스 커널이 <strong>OOM Killer</strong>라 불리는 작업을 실행해서 현재 실행중인 프로세스를 끈다는 내용이었습니다.</p><p>이 포스트에서는 바로 그 OOM Killer가 무엇이며 어떤 원리로 동작하는지, OOM Killer로부터 꺼져서는 안되는 중요한 프로세스를 어떻게하면 지킬 수 있는지에 대해서 알아보려고 합니다.</p><h3>What is OOM(Out Of Memory) Killer?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*VzsYZgh35Ph2HUxN3-zn0Q.jpeg" /><figcaption>lol</figcaption></figure><p>개발한 프로그램을 운영하다보면 누구든 다음과 같은 에러를 만난 경우가 있을 것입니다. 프로세스들이 필요로 하는 메모리를 할당받지 못하면 시스템은 점점 느려지다가 끝에서는 거의 멈춰버린 것 같은 체감을 줍니다. 심하면 교착 상태(deadlock)에 빠지게 될 수도 있겠습니다. 이런 상황이 일어나지 않기 위해서 커널이 부르는 것이 바로 <strong>OOM Killer</strong>입니다.</p><p>OOM Killer는 시스템을 위해서 하나 이상의 프로세스들을 희생시키는 역할을 합니다. 이 때 희생되는 프로세스와 같은 mm_struct (프로세스가 사용하고 있는 메모리를 저장하는 메모리 디스크립터)를 공유하는 프로세스 또한 같이 희생됩니다. 희생시킬 프로세스를 고르는 로직은 커널 코드에서 확인해볼 수 있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6f6f0a16e0d91f2b43f4c0ac35c56db1/href">https://medium.com/media/6f6f0a16e0d91f2b43f4c0ac35c56db1/href</a></iframe><p>OOM Killer는 메모리를 기준으로 가장 나쁜(?)<strong> </strong>녀석을 선택해 희생시켜서 결과적으로 좋은 상태를 만드는 것을 목표로 합니다. 위의 주석을 통해 OOM Killer가 목표로 하는 것을 알 수 있습니다.</p><ol><li>완료된 작업의 수를 최대로 유지할 수 있게 한다.</li><li>많은 양의 메모리를 복구한다</li><li>무고한(메모리를 많이 소비하지 않는) 프로세스는 죽이지 않는다</li><li>최소한의 프로세스만 희생시킨다</li><li>사용자(여기서는 개발자)가 희생되리라고 기대하고 있는 프로세스를 죽인다</li></ol><h3>OOM Scoring</h3><p>OOM Killer는 희생시킬 프로세스를 고르기 위해서 각 프로세스에 점수를 매기는 과정을 거칩니다. 앞으로 이 점수를 나쁨 점수라고 부르겠습니다. OOM Killer는 결과적으로 메모리가 부족해졌을 때 프로세스 중 가장 높은 나쁨 점수를 받은 순서대로 프로세스를 척살(?)하기 시작합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c7c6099eb1205e746d02ed8c3aae9125/href">https://medium.com/media/c7c6099eb1205e746d02ed8c3aae9125/href</a></iframe><p>나쁨 점수는 기본적으로 프로세스에게 할당된 메모리의 양으로 매겨집니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/78069a8bb037c78f0e12590b0308c9da/href">https://medium.com/media/78069a8bb037c78f0e12590b0308c9da/href</a></iframe><p>거기에 해당 프로세스로부터 fork()된 자식 프로세스들의 메모리를 더해줍니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5b1235f98a45be2431a6300d7d443e60/href">https://medium.com/media/5b1235f98a45be2431a6300d7d443e60/href</a></iframe><p>거기에 오래 돌고 있는 프로세스의 경우 그만큼 점수를 낮춰주고, 나이스된 프로세스는 나쁨 점수를 두배로 올립니다.</p><blockquote>여기서 나이스란 프로세스의 우선 순위를 낮춰주는 점수를 의미합니다. 이에 대해서 더 알고싶으시다면 제 블로그의 <a href="https://ejsohn.github.io/linux/kernel/process-and-kernel/">Process and Kernel</a> 편을 참고해주세요 (깨알홍보)</blockquote><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/fae4058e7fbef46432e3662777cc454d/href">https://medium.com/media/fae4058e7fbef46432e3662777cc454d/href</a></iframe><p>마지막으로 다른 프로세스들보다 비교적 중요하게 취급되어지는 프로세스들 (superuser에 의해서 실행되거나, 하드웨어와 직접적인 관련이 있는)은 값을 크게 낮춰줍니다.</p><p>종합해보면 나쁨 점수는 가장 최근에 시작되서, 스스로와 자식들이 메모리를 많이 사용하고, 우선순위가 낮은 프로세스가 높게 부여받게 됩니다. 프로세스의 나쁨 점수를 직접적으로 보고 싶다면 /proc/&lt;pid&gt;/oom_score파일을 확인해보면 됩니다.</p><pre>$ cat /proc/19862/oom_score<br>12</pre><h3>OOM Killer의 척살 대상으로부터 벗어나기</h3><p>시스템이 멈춰버리는 것을 피하기 위해서 OOM Killer를 끄는 것은 불가능하게 되어 있습니다. 하지만 가끔은 서비스의 안정성을 위해서 꺼지면 안 될 프로세스도 존재하는 법입니다. (예를 들어서 특정한 작업을 하고 있던 worker 프로세스가 이런 경우에 들 수 있겠죠)</p><p>OOM Killer를 직접적으로 끌 수는 없지만, OOM Killer의 scoring 대상에서 벗어나는 것은 가능합니다. 그리고 생각보다 쉽게 할 수 있습니다. 바로 /proc/&lt;pid&gt;/oom_adj값을 -17로 설정해주는 것입니다. -17 값은 OOM_DISABLE의 상수 값입니다.</p><pre>$ echo -17 &gt; /proc/&lt;pid&gt;/oom_adj</pre><p>이 외에도 proc/&lt;pid&gt;/oom_scoring_adj값을 -1000으로 설정하는 방법도 있습니다. 하지만 이 방법은 OOM Scoring을 비활성화한다기보다는 충분히 낮은 값을 주어서 척살 대상에서 벗어나게 하는 식으로 작동합니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9efc65f88c92" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[번역] Linux에서 메모리를 다 써버렸을 때 일어나는 일]]></title>
            <link>https://medium.com/@EJSohn/%EB%B2%88%EC%97%AD-linux%EC%97%90%EC%84%9C-%EB%A9%94%EB%AA%A8%EB%A6%AC%EB%A5%BC-%EB%8B%A4-%EC%8D%A8%EB%B2%84%EB%A0%B8%EC%9D%84-%EB%95%8C-%EC%9D%BC%EC%96%B4%EB%82%98%EB%8A%94-%EC%9D%BC-9dadba29c89c?source=rss-a1aaa843ad8c------2</link>
            <guid isPermaLink="false">https://medium.com/p/9dadba29c89c</guid>
            <category><![CDATA[linux]]></category>
            <category><![CDATA[kernel]]></category>
            <category><![CDATA[operating-systems]]></category>
            <dc:creator><![CDATA[Eunju Son]]></dc:creator>
            <pubDate>Sat, 27 Apr 2019 07:26:36 GMT</pubDate>
            <atom:updated>2019-04-28T01:59:28.629Z</atom:updated>
            <content:encoded><![CDATA[<blockquote><a href="http://pminkov.github.io/blog/what-happens-when-you-run-out-of-memory-in-linux.html"><strong>What happens when you run out of memory in Linux?</strong></a>의 원저자에게 허락을 맡고 번역한 글입니다</blockquote><blockquote>자연스러운 전개를 위해 번역 중 약간의 의역이 들어갔습니다</blockquote><blockquote>consol output의 값들이 밀려 보일 수 있으므로 큰 화면으로 보시는 것을 추천합니다</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ccCqBx_cG-LsiyckcSXMxg.png" /><figcaption>EC2 top</figcaption></figure><p>난 항상 Linux에서 메모리를 다 써버렸을 때 어떤 일이 발생하는지에 대해서 궁금해했었다. 그래서 최근에 이것을 알아내기 위한 실험을 진행했다.</p><p>먼저 AWS의 EC2에 <a href="http://dhbox.org/"><strong>DHBox</strong></a>를 배포하려고 한다. DHBox는 여러 가지 데이터분석 툴을 쉽게 사용할 수 있는 가상 환경을 만드는 것을 도와주는 프로그램이다. 이 가상 환경은 Docker 컨테이너 위에서 돌아간다. 이 때 이 컨테이너가 도는데 많은 메모리가 사용되기 때문에 하나의 EC2에 너무 많은 컨테이너를 올릴 수 없다. 여기까지가 우리가 알고 있는 부분이다. 하지만 이 과정이 실제로 어떻게 일어나는지 살펴보자.</p><p>시작하기 전에, <a href="http://techblog.netflix.com/2015/11/linux-performance-analysis-in-60s.html"><strong>여기</strong></a>에 느려진 Linux 박스를 디버깅하는데 쓰이는 툴들을 소개한 Netflix의 훌륭한 글들이 있다.</p><p>나는 1GB의 메모리를 탑재한 Ubuntu EC2 이미지에 DHBox를 배포했다. 이 때 swap file(가상 메모리)은 사용하지 않도록 했다. 왜일까? swap file을 사용하는 것은 많은 EBS IO를 발생시킬 수도 있기 때문이다. 이건 높은 AWS 요금을 초래한다. 하지만 swap file을 사용하는 것도 흥미로운 디버깅 시나리오가 될 수 있을 것 같다. 아무튼 계속 해보자.</p><p>이것이 시작하는 시점인 지금의 메모리 상황이다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8f91202a08e4ca6e8637507bc980cc98/href">https://medium.com/media/8f91202a08e4ca6e8637507bc980cc98/href</a></iframe><p>“buff/cache” 부분에 392MB가 잡혀있는 것을 볼 수 있다. 이게 대체 뭘까? <a href="http://www.tldp.org/LDP/sag/html/buffer-cache.html"><strong>여기</strong></a>에 이것에 대한 좋은 설명이 있다. buffer cache는 디스크에 들어 있는 캐싱 데이터다(RAM). 예를 들어서 ls 명령어라던지 glibc 라이브러리가 이곳에 캐싱된다고 할 수 있다.</p><p>제일 처음 한 일은 DHBox를 위한 Python 웹 앱을 시작하는 것이었다. 웹 앱은 gunicorn에서 실행되는 간단한 Flask 앱이다. 이것을 실행하고 난 후 다시 free 명령어를 통해 메모리를 조회해봤다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/828ce00508b6f60f255c57e4f789e2a0/href">https://medium.com/media/828ce00508b6f60f255c57e4f789e2a0/href</a></iframe><p>예상하던 대로다. 우리는 아까보다 조금 더 많은 메모리를 쓰고 있다. 디스크로부터 약간 더 많은 데이터를 캐싱했다.</p><p>이제 vmstat을 실행한 다음 네 개의 가상 컨테이너를 차례대로 돌려볼 것이다. 가상 컨테이너가 모두 실행되면 대략 1GB의 메모리를 사용하게 되므로 머신에 어느 정도의 부하를 줄 수 있을 것 같다. vmstat 은 아주 유용한 정보들을 찍어주는 훌륭한 툴이다. 그럼 Docker 컨테이너를 실행하기 전에 vmstat이 찍어주는 정보들을 확인해보자:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cbbc5d25e22e16b9ac45a74ad4f413e1/href">https://medium.com/media/cbbc5d25e22e16b9ac45a74ad4f413e1/href</a></iframe><p>여기서 중요한 부분은 바로 id 열이다. id 값은 CPU가 유휴 또는 대기 상태인 시간의 백분율을 의미한다. 보다시피 지금 CPU는 할 일이 없어 꽤나 편해 보인다.</p><p>그러면 어서 첫번째 Docker 컨테이너를 띄워보자. 몇 개의 값들이 잠시 치솟아 오르기 시작한다. vmstat를 통해 그 값을 살펴보면:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0e330dad2c85b0ca09139fc8e28be29f/href">https://medium.com/media/0e330dad2c85b0ca09139fc8e28be29f/href</a></iframe><p>재미있게도, 보다시피 id 의 값이 감소했다. 덜 한가해진 것이다. 하지만 잠시 후 Docker 컨테이너는 실행되고 상황은 다시 잠잠해진다.</p><p>여기서 free 명령어로 메모리 상태를 다시 체크하자:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/28a6816841e4c9ac552c86ed709d4270/href">https://medium.com/media/28a6816841e4c9ac552c86ed709d4270/href</a></iframe><p>메모리의 사용을 나타내는 used가 127MB에서 350MB로 상승했다. 그리고 buffer cache 또한 같이 늘어났다. 사용할 수 있는 메모리가 적어진 것이다.</p><p>두번째 Docker 컨테이너를 실행시키고 vmstat를 다시 확인하자.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f26ec36bdf3cdad685a19af14b9cfb5b/href">https://medium.com/media/f26ec36bdf3cdad685a19af14b9cfb5b/href</a></iframe><p>비슷한 상황이 일어났다. io가 치솟았고, id가 감소했다. 그리고 다시 잠잠해진다. 메모리 상태를 보자:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/afbf909f601eb4901c11475a319c1656/href">https://medium.com/media/afbf909f601eb4901c11475a319c1656/href</a></iframe><p>메모리가 더 많이 사용되고 있다. 하지만 여기서 buff/cache 값이 낮아진 것을 볼 수 있다. 왤까? 실행되고 있는 프로세스들이 더 많은 메모리를 사용하면서 부족한 메모리를 어딘가로부터 가져와야했기 때문일 것이다. Linux의 커널이 buffer cache로부터 메모리를 가져와 그것을 프로세스들에게 주었다. 역시나 예상하던 대로이다.</p><p>마지막으로 세번째 그리고 네번째(마지막) Docker 컨테이너를 시작한 다음, vmstat 결과를 다시 한 번 보자</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/95bc75241af3da59d49044fb68bf0a43/href">https://medium.com/media/95bc75241af3da59d49044fb68bf0a43/href</a></iframe><p>바로 여기가 시스템 상황이 안좋아지는 구간이다! EC2 머신이 굉장히 비반응적으로 변화했다. ls 같은 아주 간단한 명령어를 실행하는 것도 수 초가 걸린다. free 를 통해 시스템 정보를 확인하고 분석을 이어서 해보자:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a35007c12c3ea60cb4f2abbf6826596b/href">https://medium.com/media/a35007c12c3ea60cb4f2abbf6826596b/href</a></iframe><p>보다시피, 사용할 수 있는 메모리의 양이 거의 없어졌고 buffer cache 또한 더더욱 줄어들었다. 하지만 vmstat에서 일어난 일을 보면 bi(blocks received from a block device) 이 일정한 값에서 멈추었다는 것을 볼 수 있다. 이것은 지금 머신에서 충분히 많은 디스크 읽기가 발생하고 있다는 것을 의미한다.</p><p>왜 이러한 현상이 일어날까? 우리의 프로세스들이 직접 디스크 작업들을 하고 있는 것일까? 그렇다고 볼 수 없다. 여기서 프로세스들은 디스크 위에서 실행되고 있다.</p><p>Linux 커널이 이들을 실행할 때, 커널은 지시어들을 디스크로부터 읽어와야 한다. 만약 우리가 충분한 양의 buffer cache가 있었다면 이 지시어들은 buffer cache에 캐싱되어 반복하여 디스크로부터 읽어올 필요가 없었을 것이다. 그러나 지금 buffer cache의 양은 너무 작다. 그래서 프로세스들이 실행될 때, 커널은 그때 그때 지시어들을 디스크로부터 가져오고 있다. 아마 개별로 봤을 때 각각의 프로세스들은 지시어들을 buffer cache에 저장을 하기는 할 것이다. 하지만 다음의 프로세스가 이전의 프로세스가 저장해놓은 지시어를 치우고 그곳에 다시 자신의 지시어들을 저장해버릴 것이다.</p><p>여기서 끝난 것이 아니다. 이 모든 어지러운 일들이 일어날 때 Linux의 커널은 <a href="https://linux-mm.org/OOM_Killer">OOM Killer</a>이라 불리는 무언가를 호출한다. 무엇이 일어나는지 보고 싶다면 dmesg 명령어롤 통해 운영체제의 로그를 불러와서 확인하면 된다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/72c6723c1fc60799506b39b108063e13/href">https://medium.com/media/72c6723c1fc60799506b39b108063e13/href</a></iframe><p>여기서 이상한 점은 OOM Killer가 계속해서 Jupyter 프로세스들을 죽이고 있다는 것이다. 프로세스들이 죽고 나서 무언가가 그것들을 재실행하고 있다고 추측해볼 수 있다.</p><p>시스템이 이런 상황에 빠졌을 때 할 수 있는 최고의 액션은 간단하게 모든 실행중인 Docker 컨테이너들을 죽여서 EC2를 다시 사용 가능하게 만드는 것이다:</p><pre>docker kill <strong>$(</strong>docker ps -q<strong>)</strong></pre><p>만약 디스크에 swap 파티션이 있었다면, 우리가 보는 것은 조금 달라졌을 것이다. 물론 그것 또한 직접 해볼 수 있다. 생각해보면 swap은 프로세스들에 의해서 사용되는 메모리의 일부를 디스크로 보내는 작업을 했을 것이다. 그건 충분히 효율적이라 볼 수 있는게, 메모리의 일부는 액세스하지 않는 경우가 거의 없기 때문이다. 그러나 swap 없이는 이러한 옵션을 선택할 수 없다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9dadba29c89c" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[번역] 코딩 인터뷰에서 Golang을 사용해야하는 이유]]></title>
            <link>https://medium.com/@EJSohn/%EB%B2%88%EC%97%AD-%EC%BD%94%EB%94%A9-%EC%9D%B8%ED%84%B0%EB%B7%B0%EC%97%90%EC%84%9C-golang%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%B4%EC%95%BC%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0-8b638ab33068?source=rss-a1aaa843ad8c------2</link>
            <guid isPermaLink="false">https://medium.com/p/8b638ab33068</guid>
            <category><![CDATA[golang]]></category>
            <category><![CDATA[developer]]></category>
            <category><![CDATA[coding-interviews]]></category>
            <category><![CDATA[junior-developer]]></category>
            <category><![CDATA[go]]></category>
            <dc:creator><![CDATA[Eunju Son]]></dc:creator>
            <pubDate>Wed, 24 Apr 2019 07:52:09 GMT</pubDate>
            <atom:updated>2019-04-27T06:02:38.482Z</atom:updated>
            <content:encoded><![CDATA[<blockquote><a href="https://briantliao.com/2019/04/14/why-you-should-use-golang-in-interviews/">Why You Should Use Golang in Interviews</a>의 원저자에게 허락을 맡고 번역한 글입니다</blockquote><blockquote>자연스러운 전개를 위해 번역 중 약간의 의역이 들어갔습니다</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xLCB49Yp90nL8eKn-fICLQ.png" /><figcaption>go go gopher!</figcaption></figure><p>요즘 다음에 있을 면접을 위해 코딩 인터뷰를 준비하는데, 연습으로는 대부분 <a href="https://leetcode.com/">Leetcode</a>에 있는 데이터 구조와 알고리즘 문제를 풀고 있다. Python과 Java는 이런 인터뷰를 위해 가장 많이 쓰이는 언어이다.<strong> </strong>Python은 그 간결함과 슬라이싱, 맵과 리스트같이 사용하기 쉬운 자료 구조를 포함한 문법적 유용성 덕분에 인기가 많다. Java는 굉장히 유연한 데다가 자료 구조를 가르치거나 구현할 때 잘 맞는 객체 지향적 추상화 덕분에 많이 쓰인다.</p><p><strong>그렇다면 왜 Go를 사용해야할까?</strong></p><p>Go는 Python과 Java의 장점을 잘 섞어놓은 언어이다. 또한 코딩 인터뷰에 도움이 되는 많은 요소들을 가지고 있다. 여기에 더해서 언어 자체가 가능한 한 단순하게 만들어져있어 쓰고 배우는 데 어렵지 않다</p><h4>1. Go는 놀랄 정도로 단순하다</h4><p>Go는 Python과 C에서 좋은 점들을 가져와 디자인되었다. 코딩 인터뷰를 볼 때에는 단순함이 중요하다. 인터뷰가 시작되면 문제를 생각하고, 설명하고, 알고리즘을 짜는데 오직 45분에서 60분의 시간밖에 주어지지 않는다. 이 때문에 Python이 코딩 인터뷰에 잘 맞는 것일 것이다.</p><p>Go는 문제를 푸는 사람으로 하여금 중요하지 않은 부분을 직접 만들거나 구현체 세부 사항을 생각하는데 신경쓰지 않고도 C 또는 Java 같은 미세한 제어를 수행할 수 있게 해주는 반면 Python의 단순성과 용이성을 제공한다.</p><p>여기에 몇가지 예제가 있다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/12cc4faf2d7defbc866e959254322b60/href">https://medium.com/media/12cc4faf2d7defbc866e959254322b60/href</a></iframe><p>간단하지 않은가?</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e23597c8ed405d12562eb1a7bec99648/href">https://medium.com/media/e23597c8ed405d12562eb1a7bec99648/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9ca111644ceb2d5cfddcba8b851e8ffc/href">https://medium.com/media/9ca111644ceb2d5cfddcba8b851e8ffc/href</a></iframe><h4>2. Go는 자료 구조를 만들기에 좋다</h4><p>위에서 예제를 통해 트리 자료구조를 만들어봤다. 연결 리스트 또한 이와 많이 비슷하다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/02d46b70d43573adc407b7110c5dc8ee/href">https://medium.com/media/02d46b70d43573adc407b7110c5dc8ee/href</a></iframe><p>트리나 연결 리스트에 포인터를 사용하면 Python이나 Java에서의 값의 직접 전달(pass-by-value)이나 숨겨진 포인터를 쓰는 것보다 자료 구조를 더 섬세하게 다룰 수 있다. 이것은 C에서 포인터를 사용하는 것보다 훨씬 쉽다.</p><p>Go는 또한 최대한 표현력을 가지면서도, 최대한 간결할 수 있도록 세 가지의 내장 자료 구조를 가지고 있다. 바로 맵, 배열(고정 크기 배열), 슬라이스 (조정 가능한 배열)이다. 이것들은 Python에 내장된 맵과 리스트, 셋, 튜플과 비슷하다. (셋과 튜플은 Go의 배열과 맵으로 구현할 수 있다) 내장된 자료 구조를 갖고 있거나 이를 구현하기 쉽다는 것은 장황하게 짜여진 Java보다 낫다고 생각된다. 특히 시간이 없을 때는 더욱 그렇다. Go에서는 또한 함수에서 하나 이상의 반환을 할 수 있다. 위의 twoSum 함수에서 그 예를 확인할 수 있다:</p><p>index, ok := m[target — num]</p><p>이러한 특징은 index 가 존재 할 때(ok가 참일 때) 와 그렇지 않을 때를 다루는 것을 정말 쉽게 만들어준다.</p><h4>3. Go는 타입을 가지고 있다 (정적 언어이다)</h4><p>Python으로 코딩 인터뷰를 준비하다보면 자주 타입의 부재 때문에 곤경을 겪는다. 그리고 그 곤경은 스트레스가 높은 환경에서 수정과 디버깅을 하기 어렵게 만든다. 그 예중에 하나는 코드 내에서 내가 슬라이싱하고 있는 대상이 문자열인지, 맵인지, 리스트인지, 리스트라면 변수 값은 어떤 것이 들었는지같은 것들을 전혀 알수 없다는 것이다. Go에서 타입을 활용하는 것은 정말 쉽다. 하나의 함수로 예를 들어 보자면:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d2ccdf245c33470eee7cb7f3ef915af0/href">https://medium.com/media/d2ccdf245c33470eee7cb7f3ef915af0/href</a></iframe><p>이 함수를 Python으로 변환하면 다음과 같다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b7cf3b5505f3e35f3a1d793730b89b78/href">https://medium.com/media/b7cf3b5505f3e35f3a1d793730b89b78/href</a></iframe><p>Python의 한줄짜리 답변은 무시하자, Go로 짜여진 함수를 보면 꽤나 깔끔하다! 설명을 조금 하자면, sumSlice 는 []int 타입의 slice 를 입력으로 받아 int 값을 반환하는 함수이다. tot 변수는 int 타입으로 추론되기 때문에 우리가 굳이 직접 타입을 선언할 필요가 없다. range slice 는 Python의 enumerate(slice) 와 같은 역할을 한다. 여기서 _ (index값을 무시한 것)은 int 타입으로 추론되고 val 또한 int 타입으로 추론된다. 결국 이것은 간결함을 유지하면서도 필요한 타입 정보를 모두 가질 수 있다는 것을 의미한다.</p><h4>4. Go는 포인터를 가지고 있다</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/524/1*sVwIO2XkB1BO6NriNZu3aQ.gif" /><figcaption>연결 리스트를 뒤집어보라는 요청을 받고 나면 왜 포인터가 중요한지 알 수 있을 것이다</figcaption></figure><p>위에서 포인터를 통해 무거운 데이터 구조를 보다 세밀하게 제어할 수 있는 방법에 대해 언급했다.</p><p>여기에 추가로, 포인터를 활용하면 쉽게 메모리를 관리할 수 있고 구조체를 수정하거나, 구조체를 복사해서 사용하고 싶지 않을 때 더욱 효과적일 수 있다. 코딩 인터뷰에서는 빠르고 효율적인 메모리 관리가 중요하다.</p><h4>5. Go는 Generic이 없다.</h4><p>이것은 Go의 흥미로운 디자인 선택이다. 나는 예쁜 추상화를 위해서(가끔 지나치게 장황한 추상화를 하기도 한다) generic을 잘 활용하는 편이다. 당신이 generic에 대해서 어떻게 생각하는지와 상관 없이, 그것을 없애는 것은 언어를 단순하게 만든다. 그리고 코딩 인터뷰에서는 보통 generic이 중요하지 않다. 코딩 인터뷰에서는 integer나 string같이 하나의 타입만 다루는데 이것은 generic 없이도 코드를 더 깔끔하고 쉽게 짤 수 있게 해준다. 살다 보면 가끔은 지나친 추상화가 필요하지 않을 때가 있는데, 특히 코딩 인터뷰에서 그렇다.</p><h3>다른 장점들</h3><p>상단에서 언급된 대부분은 클래식한 자료 구조나 알고리즘 코딩 인터뷰를 위한 것이다. 물론 그 외에 다른 장점들도 많다!</p><h4>6. Go를 통해 면접자에게 인상을 남길 수 있다</h4><p>솔직히 이 점이 내가 Go로 코딩 인터뷰를 준비하는 이유이다. 면접자들은 보통 정말 많은 사람들을 본다. 그 사이에서 눈에 띄고 싶다면? 답은: Go를 사용하는 것이다! 만약 당신이 수백 명의 다른 개발자들과 똑같다면 면접자는 아마 Python이나 Java를 사용한 사람들은 기억하지 않을 수도 있다. 하지만 아름답고, 간결함을 자랑하는 Golang을 사용한 한 명의 괴짜(혹은 천재?)는 기억할 수 있다.</p><p>이 글이 나온 이후에 많은 사람이 golang을 사용하게 되어 여기로부터 오는 이득이 희석될 수 있다. 그러면 나는 OCaml이나 F#같은 것을 사용해야 할 것이다. 이것이 바로 <a href="https://en.wikipedia.org/wiki/Efficient-market_hypothesis">Efficient Interviewing Language Hypothesis</a> 에서 나오는 끝없는 순환이다. (하하)</p><h4>7. Go는 훌륭한 동시성 지원을 가지고 있다</h4><p>또한 무엇이 1x의 지원자를 10x의 지원자와 구분할 수 있을까? 바로 단순히 알고리즘 문제를 푸는 것을 넘어서서 하드웨어적이거나 실제 실무의 문제들을 더욱 효과적으로 구현할 수 있도록 고려하는 것이다. 예를 들어 동시성, 캐시, 한정적인 메모리, 다수의 머신을 지원하는 등의 문제가 있을 수 있다.</p><p><a href="https://www.janestreet.com/">Jane Street</a>는 이것을 그들의 <a href="https://blog.janestreet.com/interviewing-at-jane-street/">인터뷰 가이드</a>의 <strong>Think about Real Computer </strong>문단에서 이렇게 묘사했다.</p><blockquote>하지만 시스템 개발에 대한 몇몇의 직무에서 우리는 깊은 지식을 기대한다. 당신이 캐시의 효과나 IO 패턴 혹은 메모리, CPU의 가용성과 같은 것을 고려한다면 일반적으로 그것은 큰 가점이 될 것이다.</blockquote><p>Go는 동시성을 염두에 두고 서버/시스템 언어로 쓰이도록 디자인되어 있다. CPU의 성능을 최대한으로 쓸 수 있도록 하는 것은 Go의 목표이다. 만약 면접자에게 큰 인상을 주고 싶다면 goroutine을 활용해 알고리즘을 더욱 빠르게 만들어 볼 수 있다.</p><p>여기에 <a href="https://tour.golang.org/concurrency/7">A Tour of Go</a>에서 가져온 goroutine으로 만든 이진 트리의 예가 있다:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a640f0b9444034ce8fc9172a8b2aa2d8/href">https://medium.com/media/a640f0b9444034ce8fc9172a8b2aa2d8/href</a></iframe><p>goroutine으로 해볼 수 있는 다른 예로 이론적으로 빠른 점근적(asymptotic) 속도 향상을 위한 <a href="https://courses.cs.washington.edu/courses/cse332/19wi/lectures/cse332-19wi-lec15-AnalysisForkJoin.pdf">병렬 합</a>과 <a href="https://courses.cs.washington.edu/courses/cse332/19wi/lectures/cse332-19wi-lec16-PrefixAndSorting.pdf">병렬 정렬</a>이 있다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*S_0dHbmlVdx9-XPVGIu8YA.png" /><figcaption>병렬 합</figcaption></figure><p>면접자들은 동기화에 대해서도 관심이 많다. 동기화를 하다보면 자주 어려운 버그들을 만나게 되어 기능을 제대로 구현하는 것이 쉽지 않다. Go는 안전한 코드를 쉽게 짤 수 있게 해준다. 만약 당신이 mutex를 활용하거나 워커 풀을 잘 만들어 쓰고 싶다면 Go로 해보는 것을 추천한다.</p><h4>8. Go는 시스템 설계 질문에 도움이 된다</h4><p>시스템 설계 문제는 인터뷰를 볼 때 자주 나오는 문제중에 하나이다. 면접자는 어떤 문제를 효과적으로 풀기 위해서 어떻게 시스템을 구성할 것인지 물어본다. Go는 서버/시스템을 위해 만들어진 언어이고 아키텍쳐를 효율적으로 만들며 간단하게 묘사하기 쉽다. 예를 들어서, Go에서는 HTTP 엔드포인트와 API를 만들기 쉽다. 만약 높은 성능이 필요하다면 goroutine과 동기화를 사용하면 된다. Go는 또한 시스템을 위한 실제 서비스에서도 많이 쓰인다. 가장 유명한 것들 중 하나로 <a href="https://kubernetes.io/">Kubernetes</a>가 있다.</p><h4>9. 다시 한 번 생각할 수 있다</h4><p>이건 연습을 할 때 큰 도움이 된다. 가끔 나는 문제를 풀다 막히거나 답을 생각해 낼 수 없을 때가 있다. 다행히 인터넷에는 도움이 되는 자료가 많지만 대부분 Java나 Python으로 이루어져 있다. Go는 이들과 문법이 비슷해서 이를 Go로 변환하는 것은 크게 어렵지 않지만 다른 언어로 문제를 푼다는 것은 단순히 복사하는 것에서 벗어나 지금 쓰고 있는 것에 대해서 다시 한 번 생각하게 만든다. 바로 그 생각이 실력을 향상시키는 비법이다.</p><h4>10. Go로 일을 구할 수 있다</h4><p>나는 Go를 좋아한다. Go는 굉장한 언어이다. 혹시 Google 혹은 Go를 사용하는 회사가 이 글을 보고 있다면, 나는 그곳에 가서 이 굉장한 언어를 계속해서 사용할 수 있다. (연락 주세요!)</p><h3>그래서 왜 Golang을 사용해야 할까?</h3><p>지금까지 왜 Go로 프로그래밍 인터뷰를 봐야 하는가에 대해서 알아봤다. 하지만 그렇다고 모든 사람이 Python이나 Java를 사용하지 않고 (아니면 Javascript나 C++ 혹은 그 무엇) Go를 선택한다는 이야기는 아니다. 요약하자면, Go는 Python의 단순함, C의 세밀한 컨트롤, 그리고 Java의 추상화와 조직화 특성을 가지고 있다. 거의 프로그래밍 인터뷰로 유명한 회사가 프로그래밍 인터뷰를 위한 완벽한 언어를 만든 것 같다. 이것은 Interviewer wow factor와 더불어 Go를 프로그래밍 인터뷰에서 사용하기에 훌륭한 언어로 만들어 준다.</p><h3>Go LeetCode Examples</h3><p>여전히 증명이 필요한가? 최고의 방법은 한 번 사용해보는 것이다! 다음은 몇가지의 Leetcode 예제들이다.</p><h4>Basic: <a href="https://leetcode.com/problems/two-sum/description/">Two Sum</a></h4><p>list (slice)인 num 을 받아서, 합해서target 이 될 수 있는 값의 index를 반환해준다. 만약 답이 없으면 nil 을 반환한다. 이건 실제로 존재하는 인터뷰 문제이므로 당신이 Go가 얼마나 대단한지 알 수 있는 좋은 예제가 될 수 있다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8c5f23bfe1e2d8c9e7f42b2d4fd4b321/href">https://medium.com/media/8c5f23bfe1e2d8c9e7f42b2d4fd4b321/href</a></iframe><h4>LeetCode Easy: <a href="https://leetcode.com/problems/remove-duplicates-from-sorted-list/">Remove Duplicates from Sorted List</a></h4><p>정렬된 연결 리스트를 받아서 중복된 모든 요소들을 지워준다. 결과적으로 모든 요소들은 유일한 값을 갖게 된다. 아래는 포인터를 활용한 단순한 답이다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/06c32c6db67bccdd7091e8ebd32bfbda/href">https://medium.com/media/06c32c6db67bccdd7091e8ebd32bfbda/href</a></iframe><h4>LeetCode Medium: <a href="https://leetcode.com/problems/find-first-and-last-position-of-element-in-sorted-array/">Find First and Last Position of Element in Sorted Array</a></h4><p>오름차순으로 정렬된 integer 배열인 nums 와 integer의 target 을 받아서 target 값의 시작하는 위치와 끝나는 위치를 찾는다. 만약 target 이 발견이 안되면 [-1, -1]를 반환하면 된다.</p><p>시작점과 끝점을 찾는데 두 개의 이진 탐색으로 풀었다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f89122e8f3eced0d47121cf1d08fbc73/href">https://medium.com/media/f89122e8f3eced0d47121cf1d08fbc73/href</a></iframe><h4>LeetCode Hard: <a href="https://leetcode.com/problems/lru-cache/">LRU Cache</a></h4><p>O(1)의 get/set 기능을 하는 LRU 캐시를 만들어라. 답은 해시 맵과 연결 리스트를 사용했다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/18c60335e0595dd67c7550c8bac5e09c/href">https://medium.com/media/18c60335e0595dd67c7550c8bac5e09c/href</a></iframe><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8b638ab33068" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Udacity Deep Learning Nanodegree 수료 후기]]></title>
            <link>https://medium.com/@EJSohn/udacity-deep-learning-nanodegree-%EC%88%98%EB%A3%8C-%ED%9B%84%EA%B8%B0-51449857fc24?source=rss-a1aaa843ad8c------2</link>
            <guid isPermaLink="false">https://medium.com/p/51449857fc24</guid>
            <category><![CDATA[tensorflow]]></category>
            <category><![CDATA[udacity]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[review]]></category>
            <dc:creator><![CDATA[Eunju Son]]></dc:creator>
            <pubDate>Sat, 22 Dec 2018 06:06:48 GMT</pubDate>
            <atom:updated>2018-12-22T06:59:17.908Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DpH-jkGQnKsWKlWIU7cMrg.png" /></figure><p>상업적 홍보와 관련없는, 글쓴이가 자발적으로 작성한 포스팅입니다. 안심하고 읽으세요!</p><p>얼마 전에 부모님 댁의 인터넷 업체를 바꿨더니 사은품으로 LG 기가지니를 주었습니다. 집에 인공지능 스피커를 들인 건 처음이라, 가족들과 기가지니에게 이런 저런 질문을 하며 재미있게 놀았던 기억이 납니다.</p><p>이처럼 요즘은 인공 지능 스피커, 인공 지능 앱, 인공 지능 전자기기 등 우리의 생활 곳곳에서 AI (Artificial Intelligence, 인공 지능)를 만나볼 수 있는데요, 이러한 인공지능을 개발하는 방법 중의 하나가 바로 Deep Learning (심층 학습) 입니다.</p><blockquote><strong>딥 러닝</strong>(<a href="https://ko.wikipedia.org/wiki/%EC%98%81%EC%96%B4">영어</a>: deep learning), <strong>심층학습</strong>(深層學習)은 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 <a href="https://ko.wikipedia.org/wiki/%EB%A8%B8%EC%8B%A0_%EB%9F%AC%EB%8B%9D">기계학습</a>(machine learning) 알고리즘의 집합으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있다.</blockquote><blockquote>- <a href="https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D">Wikipedia</a></blockquote><p>Deep Learning을 통해 우리는 특정한 과제를 수행하는 인공지능 신경망 (Neural Network)을 만들 수 있습니다. 물론 그러기 위해서는 공부가 필요합니다. 대중의 관심이 많아졌다고는 하지만 그래도 아직 메이저한 기술 분야가 아니기 때문에, 많은 분들이 Deep Learning을 공부하기 위해 무엇을 해야 하나 헤매고는 합니다.</p><p>저는 이 포스팅을 통해 Udacity의 Deep Learning Nanodegree 코스로 Deep Learning을 학습하고 느낀 점들을 공유해보려고 합니다.</p><blockquote>방금 막 수료한, 아주 따끈따끈한 후기입니다! 😃</blockquote><h3>Udacity Nanodegree 코스란 무엇인가?</h3><p>만약 개발자라면 인터넷을 하면서 개발 역량을 기를 수 있는 여러 사이트나, 서비스를 본 적이 있을 것입니다. 그런 종류의 서비스에는 영어로 된 Udacity, Udemy, Coursera 등과 우리나라에서 한국어로 이용할 수 있는 인프런, 코드잇, 패스트캠퍼스 등의 사이트들이 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/537/1*ZT67jLc6zp00ouiSCYrhKA.png" /></figure><p>Udacity는 그 중에서도 비교적 적은 수강 코스를 운영하면서 여러 대학, 기업들과 협업해 높은 수준의 강의를 만드는 서비스라고 할 수 있겠습니다. 코스는 프로젝트를 중심으로 진행되고 해당 기술의 기본적인 개념, 이론, 실무에서 어떻게 사용되는지 등을 저명한 현업 교수, 개발자가 직접 동영상을 통해서 지도해줍니다.</p><p>Udacity Nanodegree 코스는 Udacity가 운영하는 자격 프로그램입니다. 보통 주 12시간을 기준으로 4개월가량 이어지고, 코스에서 제공하는 일정한 수의 프로젝트를 완료하면 Udacity 발급의 자격증을 받을 수 있습니다. 자격증을 발급해주는 코스라 그런지 일반적인 코스보다 가격이 더 나가고 난이도도 높지만, 그만큼 장점들도 많습니다. 장점은 밑에서 천천히 소개해보도록 하겠습니다.</p><p>저는 한양대학교의 Deep Learning 파일럿 프로그램에 선정되어 학교의 지원을 받고 해당 코스를 수강할 수 있었습니다.</p><h3>어떤 것을 배우는가?</h3><p>Udacity Deep Learning Nanodegree 코스는 총 6개의 챕터로 이루어져 있습니다.</p><ul><li>Introduction</li><li>Neural Networks</li><li>Convolutional Neural Networks (CNN)</li><li>Recurrent Neural Netoworks (RNN)</li><li>Generative Adversarial Networks (GAN)</li><li>Deep Reinforcement Learning</li></ul><p>먼저<strong> Introduction</strong>에서는 개발을 하기 위한 기본적인 기술이나 방법론을 소개하고 익숙해질 수 있게 해줍니다. 클라우드 서비스인 AWS의 딥러닝 AMI를 사용해 직접 머신을 띄워서 사용할 수 있도록 credit을 지원하고, 이것이 어려운 학생들을 위해 Udacity의 내부에 있는 GPU 머신을 이용해 웹상에서 jupyter를 통해 개발을 할 수 있게도 해줍니다. 또한 tensorflow, keras, numpy 등 Deep Learning 알고리즘 개발에 필요한 패키지들이 어떤 일을 하고, 이것들을 어떻게 사용하면 좋은지 가이드를 해줍니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/791/0*-AEB6viLdaA6G9RW.png" /></figure><p><strong>Neural Networks</strong> 챕터는 전체 챕터중에서 가장 이론적, 수학적 강의가 많은 것 같습니다. 아무래도 Deep Learning 방법론들이 전부 Neural Network이기 때문에 기초를 단단히 다지기 위해서 그럴 것이라고 생각합니다. 구체적이고 간단한 예제를 통해서 어려운 수학적 개념을 쉽게 풀어서 설명주고, 해당 이론을 수학적 Lemma로 풀어서 익힐 수 있게 해줍니다. (물론 이러한 수학적 접근 방법은 아래의 챕터들에서도 모두 포함이 됩니다) 또한 복잡한 방법론을 사용하지 않고, 기본적 신경망 이론들만 사용해 데이터 기반 미래 예측, 감정 분석(Sentiment Analysis)등을 할 수 있도록 프로젝트를 제공합니다. 중요하게 다뤄지는 개념은 Backpropogation, feed forward 등이 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/949/0*V9TYudEWAhAAttHz.jpg" /></figure><p><strong>CNN</strong> 챕터에서는 이전에서부터 배운 내용들을 이용해서 Convolutional Network를 구성하는 방법을 배웁니다. CNN에서 주로 사용되는 MaxPolling, Dropout layer 등이 왜 사용되는지 하나 하나 이해하고 넘어갈 수 있어서 좋고, 여러 Activation Function들을 비교해볼 수 있습니다.</p><p>그리고 배운 CNN으로 Autoencoder도 개발해보며, 이 때 Weight Initialize, Transfer Learning 등의 방법론을 통해 CNN을 강화할 수 있는 방법들도 개별의 소챕터로 구성되어있습니다. 이론들은 tensorflow, keras를 통해 직접 구현하고 테스트해 볼 수 있습니다.</p><p><strong>메인 프로젝트</strong>로는 개의 종을 구별할 수 있는 Classifier와 Cancer Detection 모델을 만들어봅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yh-uwTScgQn8g185.png" /></figure><p><strong>RNN</strong> 챕터에서는 BPTT(BackPropogation Trough Time) ~ LSTM(Long Short Term Memory)의 발전과 구현법을 이론적으로 학습한 뒤 이를 tensorflow로 직접 개발합니다. Backpropogation, FeedForward가 다시 등장해 RNN에서 어떻게 동작하는지 다시 볼 수 있고, Vanishing Gradient Problem 등의 여러 문제들을 어떻게 해소할 수 있는지 알 수 있습니다. 또한 각각의 hyperparameter들을 어떤 식으로 조정하면 좋을 지 가이드를 제공하고, word2vec을 만들며 embeddings에 대한 개념도 배울 수 있습니다.</p><p><strong>메인 프로젝트</strong>로는 RNN으로 하는 감정 분석과, TV 대본 데이터를 학습해 새로운 대본을 생산해내는 프로젝트가 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/261/0*85rebu5NEuDTPM7s.png" /><figcaption>Faces generated by GAN</figcaption></figure><p><strong>GAN</strong> 챕터에서는 원하는 데이터를 생산하는 Generator, 그리고 이를 진짜 데이터와 구별하는 Discriminator를 만들고 Loss, Optimizer 학습을 하며 진짜와 구별할 수 없는 데이터를 생산하는 방법을 배웁니다. 그리고 이 GAN 모델에 CNN을 적용하면 퍼포먼스가 어떻게 바뀌는지 체험해가며 볼 수 있고, <strong>Ian Goodfellow</strong>가 직접 semi-supervised 학습에 대해 해주는 강의를 들을 수 있습니다 🤩.</p><p><strong>메인 프로젝트</strong>로는 CelebA 데이터셋으로 가상의 얼굴을 직접 만들어봅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ckIsghco_6O5tJiX.png" /></figure><p><strong>Deep Reinforcement Learning</strong> 챕터에서는 MDP (Marcov Decision Process)로 episodic, continuous task에서 어떻게 value-based, policy-based optimal를 찾아낼 수 있는지 배웁니다. 이 때 등장하는 Method들이 다수 있습니다. 아마 분량만 따지면 전체 챕터 중 가장 큰 것 같습니다.</p><p>Github에 공개되어 있는 강화학습 알고리즘 학습 플랫폼인 <a href="https://gym.openai.com/docs/">OpenAI Gym</a>의 여러 프로젝트를 사용해 Environment, Agent가 다른 여러 상황을 다뤄보며 배운 이론을 실습해볼 수 있습니다.</p><p><strong>메인 프로젝트</strong>로는 발이 네개 달린 QuadCopter의 각 앵글과 파워를 조절해서 특정한 액션을 수행하게 하는 프로젝트를 진행합니다. (Hover!)</p><p>각 챕터에서 하게 되는 프로젝트들은 전부 Udacity의 Github 레포지토리에 공개되어 있습니다. 코스를 듣지 않아도 해당 레포지토리를 조회하고 클론해서 실습해볼 수 있으니 꼭 들어가서 보시기를 추천드립니다! 프로젝트는 모두 Jupyter notebook을 이용해 만들어져있습니다.</p><blockquote><a href="https://github.com/udacity/deep-learning">Deep Learning Nanodegree course project repository</a></blockquote><h3>누구에게 배울 수 있는가?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IcjuaL3ldFmXb-za_VTmLg.png" /></figure><p>각각의 챕터는 여러 명의 연사들의 강의로 구성이 됩니다. 보통 이론이나 수학적 개념을 배울 때는 미국의 유명 대학 교수 혹은 위와 같은 특정 분야를 개발하거나 큰 프로젝트를 운영하고 있는 연사들이 나와 적절한 픽토그램과 함께 강의를 합니다. 물론 그 외의 개념 설명, 직접적인 개발 등에도 엄청난 석박 출신 현업 개발자들이 등장합니다. 연사 분들이 자기소개를 시작할 때마다 점점 위축이 되는 걸 실시간으로 경험할 수가 있었습니다.</p><blockquote>강의진에 대한 자세한 정보는 <a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101">여기</a>서 조회할 수 있습니다.</blockquote><h3>어떻게 배우는가?</h3><p>제가 생각하기에 새로운 기술을 배울 때 가장 힘든 부분은 세가지인 것 같습니다. <strong>첫째로</strong> 개념, 이론에 대한 부족함을 계속 느끼는 것. <strong>두번째로</strong> 개념은 숙지한 것 같으나 익숙한 실무자들이 어떻게 해당 기술을 사용하는지에 대한 정보가 부족하다는 것. <strong>마지막으로</strong> 기술은 끊임없이 배워야하는 것인데 하던 공부를 마쳤을 때 다음에 무엇을 봐야 할 지 알 수 없다는 것. 다음의 요소들만 갖춰진다면 그 이후에 실력을 키워나가는 것은 본인의 의지에 달린 것일 것입니다.</p><p>해당 코스는 다음의 세가지 필요를 충족해준다고 할 수 있을 것 같습니다. 이론이나 실무 개발 중 하나에만 편향되게 스킬을 기르는 것이 아닌, 골고루 필요한 만큼 지도를 해 줄 뿐더러 하나씩 개념과 강의가 끝나갈 때마다 추가로 읽어보면 좋은 논문이나 자료를 링크와 함께 추천해줘 관심이 있다면 더 파볼 수 있는 깊은 길을 열어줍니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RDi7yA4QNg0YEwOdPA7b2A.png" /><figcaption>GAN Project Workspace (GPU enable)</figcaption></figure><p>코스의 콘텐츠는 다음의 요소들로 구성되어있습니다.</p><ul><li>강의 동영상</li><li>개념 및 가이드 노트</li><li>퀴즈</li><li>코드랩</li><li>프로젝트</li><li>질문 및 강의 수강자들과의 네트워킹 게시판</li></ul><p>챕터의 끝마다 필수로 이수해야하는 프로젝트는 외부(Google..?)의 도움 없이 하기엔 난이도가 있는 주제와 내용을 가지고 있습니다. 하지만 Udacity에서는 완벽하게 구현된 제출물이 아니더라도 따로 실력 있는 개발자를 학생에게 1:1로 매칭해줘 적절한 조언과 코드 리뷰를 받을 수 있도록 합니다.</p><p>그렇기에 내가 구현한 모델이 어디가 부족하고, 어디를 개선할 수 있는지, 또한 어떤 개념을 추가로 학습해서 적용하면 퍼포먼스가 올라갈 것인지 하는 직접적인 조언을 받을 수 있습니다. 물론 질문 노트를 통해 구체적인 질문을 하는 것도 가능합니다. 이러한 제출-답변 Cycle은 6시간 이내에 이루어집니다.</p><p>리뷰어는 친절하지만, 프로젝트 제출에 있어서는 그리 관대하지 않아서 어느정도의 퀄리티를 달성하지 않으면 해당 프로젝트는 끊임없는 Reject을 받을 수 있습니다. 😂</p><h3>다른 방법 대비 장점이 무엇인가?</h3><p>리스트로 정리해보았습니다.</p><ul><li>GPU 머신을 지원해준다</li><li>강사진의 전문성을 믿을 수 있다</li><li>적절한 논문들을 추천받을 수 있다</li><li>픽토그램과 동영상의 특성을 잘 활용해 어려운 이론을 놓치지 않고 쉽게 설명해준다</li><li>강의 동영상을 다운로드받아 복습에 쓸 수 있다</li><li>검증된 프로젝트가 많아 이외의 경험 없이도 실무 감각을 늘릴 수 있다</li><li>적절하고 빠른 피드백을 받을 수 있다</li></ul><h3>다른 방법 대비 단점은 무엇인가?</h3><p>또한 리스트로 정리해보았습니다.</p><ul><li>강의부터 노트까지 모두 영어로 이루어져있다</li><li>가격이 비싸다 (1000 USD)</li><li>Tensorflow 1.0.0 버젼을 주로 사용한다</li><li>코스를 들을 수 있는 기간이 정해져있다</li><li>최소 고등 이과 수준의 수학지식이 필요하다</li></ul><p>물론 시간이 지나며 Udacity도 강의를 재구성해 Tensorflow 버젼을 높일 수 있겠지만, 곧 Tensorflow 2가 나오는 지금 이 시점에서는 낮은 Tensorflow 버젼을 사용해 개발을 했던 것이 가장 아쉬웠던 것 같습니다.</p><h3>결론</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tS9pNwXY01Ub5g5LxGApOw.png" /><figcaption>수료증은 LinkedIn에 연결해 사용할 수 있어요</figcaption></figure><p>저는 인터넷 강의를 통한 학습을 신뢰하지 않는 학생이었습니다. 수험 생활을 하던 고등학생 때엔 남들이 다 듣는 인강을 듣지 않고 종이책으로 공부를 했고, 대학에 올라와서도 인터넷 강의를 듣는 것은 학비, 시간 낭비라고 생각을 해 오프라인 강의만 수강해서 학점을 채웠었습니다.</p><p>하지만 Udacity을 수강해보니 인터넷 강의 중에서도 신뢰하고 이용할 수 있는 것이 있구나라는 것을 알게 되었습니다. 그만큼 아쉬운 것이 크게 없었고, 오히려 그 시간동안 더 열심히 이용할 수 있었을텐데 하는 아쉬움만 남았습니다. 저는 현재 인터넷 강의에 대한 편견을 부수고 Coursera의 Data Structures and Algorithms Advanced 강의를 수강중입니다. 또한 Deep Learning 코스를 통해 배운 것들로 새로운 프로젝트를 시작하려고 준비중입니다.</p><p>이 글을 통해 앞으로 Deep Learning을 공부하려고 하는 분들이 그 방법을 정하는데 도움이 되었으면 좋겠습니다. Udacity를 통한 학습은 비교적 큰 금액이 들지만, 그만큼 효율적으로 짧은 시간에 질 좋은 자료를 쓸 수 있기에 좋다고 생각합니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=51449857fc24" width="1" height="1">]]></content:encoded>
        </item>
    </channel>
</rss>