<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Junghoon Song on Medium]]></title>
        <description><![CDATA[Stories by Junghoon Song on Medium]]></description>
        <link>https://medium.com/@gaemi?source=rss-d4008c147c0e------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/0*30MLLN0Wod6QkfTN.jpg</url>
            <title>Stories by Junghoon Song on Medium</title>
            <link>https://medium.com/@gaemi?source=rss-d4008c147c0e------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 13 May 2019 04:15:21 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/@gaemi" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Spring Cloud Stream 에서 Confluent Schema Registry 를 사용할 때 주의사항! 이라고 적혀있는 부분을 보시면 됩니다. ㅎㅎ]]></title>
            <link>https://medium.com/@gaemi/spring-cloud-stream-%EC%97%90%EC%84%9C-confluent-schema-registry-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%A0-%EB%95%8C-%EC%A3%BC%EC%9D%98%EC%82%AC%ED%95%AD-%EC%9D%B4%EB%9D%BC%EA%B3%A0-%EC%A0%81%ED%98%80%EC%9E%88%EB%8A%94-%EB%B6%80%EB%B6%84%EC%9D%84-%EB%B3%B4%EC%8B%9C%EB%A9%B4-%EB%90%A9%EB%8B%88%EB%8B%A4-%E3%85%8E%E3%85%8E-4450ea114f8?source=rss-d4008c147c0e------2</link>
            <guid isPermaLink="false">https://medium.com/p/4450ea114f8</guid>
            <dc:creator><![CDATA[Junghoon Song]]></dc:creator>
            <pubDate>Thu, 21 Mar 2019 06:45:55 GMT</pubDate>
            <atom:updated>2019-03-21T06:45:55.801Z</atom:updated>
            <content:encoded><![CDATA[<p>Spring Cloud Stream 에서 Confluent Schema Registry 를 사용할 때 주의사항! 이라고 적혀있는 부분을 보시면 됩니다. ㅎㅎ</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4450ea114f8" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ConfluentAvroSchemaRegistryClientMessageConverter 로 대체하는 이유에 대해서도 정리가 되어 있는데 놓치신것 같네요. ^^]]></title>
            <link>https://medium.com/@gaemi/confluentavroschemaregistryclientmessageconverter-%EB%A1%9C-%EB%8C%80%EC%B2%B4%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C%EB%8F%84-%EC%A0%95%EB%A6%AC%EA%B0%80-%EB%90%98%EC%96%B4-%EC%9E%88%EB%8A%94%EB%8D%B0-%EB%86%93%EC%B9%98%EC%8B%A0%EA%B2%83-%EA%B0%99%EB%84%A4%EC%9A%94-a748a7263d?source=rss-d4008c147c0e------2</link>
            <guid isPermaLink="false">https://medium.com/p/a748a7263d</guid>
            <dc:creator><![CDATA[Junghoon Song]]></dc:creator>
            <pubDate>Thu, 21 Mar 2019 06:43:51 GMT</pubDate>
            <atom:updated>2019-03-21T06:43:51.791Z</atom:updated>
            <content:encoded><![CDATA[<p>ConfluentAvroSchemaRegistryClientMessageConverter 로 대체하는 이유에 대해서도 정리가 되어 있는데 놓치신것 같네요. ^^</p><p>간략하게 댓글로 요약하자면 Spring Cloud Stream 에서는 자체적으로 제공하는 AvroSchemaRegistryClientMessageConverter 는 Confluent 의 SchemaRegistry 를 정상적으로 지원하지 못합니다.</p><p>요 이슈는 코멘트를 남기고 있는 지금까지도 해결되지 않은 상태이네요. (<a href="https://github.com/spring-cloud/spring-cloud-stream/issues/850">https://github.com/spring-cloud/spring-cloud-stream/issues/850</a>)</p><p>그래서 이 부분을 수정했던 ConfluentAvroSchemaRegistryClientMessageConverter 를 사용하게 된 것이구요.</p><p>이것은 Spring Cloud Stream 을 사용하는 Publisher 와 Consumer 모두 동일하게 적용되어야 합니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a748a7263d" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Java 와 Spring 의 Validation]]></title>
            <link>https://medium.com/@gaemi/java-%EC%99%80-spring-%EC%9D%98-validation-b5191a113f5c?source=rss-d4008c147c0e------2</link>
            <guid isPermaLink="false">https://medium.com/p/b5191a113f5c</guid>
            <category><![CDATA[spring]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[jsr-380]]></category>
            <category><![CDATA[validation]]></category>
            <category><![CDATA[hibernate]]></category>
            <dc:creator><![CDATA[Junghoon Song]]></dc:creator>
            <pubDate>Sat, 18 Aug 2018 12:54:37 GMT</pubDate>
            <atom:updated>2018-08-18T12:54:37.821Z</atom:updated>
            <content:encoded><![CDATA[<p>이 글은 Bean Validation 과 그 구현체인 Hibernate Validator 그리고 Spring Validation 에 대해서 다루고 있습니다.</p><h3>Java Bean Validation</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/650/0*xaxuUXlJp7FZgoM5.png" /><figcaption>출처 <a href="http://docs.jboss.org/hibernate/stable/validator/reference/en-US/html_single/">Hibernate Validator 6.0.11.Final — JSR 380 Reference Implementation: Reference Guide</a></figcaption></figure><p>일반적으로 데이터 검증 (Validation) 은 여러 계층에 걸쳐서 이루어지게 됩니다.<br>거의 동일한 내용의 검증로직이 각 계층별로 구현된다면 그것은 중복이고 낭비가 심한 작업일것입니다.<br>또한 그러한 경우 각 계층별로 구현된 검증로직간 불일치로 인하여 오류가 발생하기도 쉽습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/650/0*ZP1-gltlXubb8SSP.png" /><figcaption>출처 <a href="http://docs.jboss.org/hibernate/stable/validator/reference/en-US/html_single/">Hibernate Validator 6.0.11.Final — JSR 380 Reference Implementation: Reference Guide</a></figcaption></figure><p>이를 해결하기 위하여 데이터 검증을 위한 로직을 도메인 모델 자체에 묶어서 표현하는 방법이 있습니다.<br>실제 코드로 표현된다면 너무 장황하고 복잡할것이기 때문에, Java 에서는 Bean Validation 라는 이름으로 애노테이션 (Annotation) 을 데이터 검증을 위한 메타데이터로 사용하는 방법을 제시하고 있습니다.</p><p>Bean Validation 명세는 현재 2.0 (JSR 380) 까지 나와있습니다.</p><ul><li><a href="https://jcp.org/en/jsr/detail?id=303">Bean Validation 1.0 (JSR-303)</a></li><li><a href="https://jcp.org/en/jsr/detail?id=349">Bean Validation 1.1 (JSR-349)</a></li><li><a href="https://www.jcp.org/en/jsr/detail?id=380">Bean Validation 2.0 (JSR-380)</a></li></ul><h4>Hibernate Validator</h4><p><a href="http://hibernate.org/validator/">Hibernate Validator</a> 는 Bean Validation 명세에 대한 구현체입니다.</p><p>Bean Validation 2.0 에 대한 구현은 Hibernate Validator 6.0.1.Final 이며 Spring Boot 2.0 이상에서 이것을 사용하고 있습니다.</p><h4>제약조건의 작성</h4><p>위에서 언급했던것처럼 Bean Validation 은 애노테이션을 사용하여 제약조건을 명시하게 됩니다.<br>즉 아래와 같은 모습이 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/dc72feea6706f4992e2e5019c0cf299b/href">https://medium.com/media/dc72feea6706f4992e2e5019c0cf299b/href</a></iframe><p>위 제약조건에 의하면 name 은 null 이거나 빈문자열이어서는 안되며, age 는 0 이상이어야만 합니다.</p><p>Java 에서 기본적으로 제공해주는 제약조건들이 있으며, Hibernate 에서 추가적으로 제공하는 제약조건이나 필요한 경우 직접 제약조건을 만드는것도 가능합니다.</p><p>필드, 클래스, 메소드 또는 파라메터에 애노테이션을 작성하는게 가능하며, 위치에 따라 제약조건의 적용 범위가 달라지게 됩니다.</p><p>Java 와 Hibernate 에서 제공하는 제약조건들은 <a href="http://docs.jboss.org/hibernate/stable/validator/reference/en-US/html_single/#section-builtin-constraints">Hibernate Validator 6.0.12.Final#builtin-constraints</a> 에서 확인하실 수 있습니다.</p><h4>제약조건에 대한 유효성검증</h4><p>제약조건의 확인은 javax.validation.Validator 를 사용해서 이루어집니다.</p><p>가장 쉽고 빠르게 Validator 를 가져오는 방법은 ValidatorFactory 를 사용하는 방법입니다.</p><p>그리고 validate() 를 사용해서 빈의 유효성검증을 실행합니다.<br>제약조건을 위반한 내용은 ConstraintViolation 인터페이스로 반환됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ea12a7f2f6d9e0b84c30c9ae00505973/href">https://medium.com/media/ea12a7f2f6d9e0b84c30c9ae00505973/href</a></iframe><h4>제약조건 위반내용에 대한 확인</h4><p>위반내용은 아래와 같이 ConstraintViolation 인터페이스에 포함되어 있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/dd245cc531cf9337dd7cb46151db269a/href">https://medium.com/media/dd245cc531cf9337dd7cb46151db269a/href</a></iframe><p>위에 언급했던 Person 을 사용한 예제는 아래와 같습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d47387d3894ec0ac4fd58f281d253b0d/href">https://medium.com/media/d47387d3894ec0ac4fd58f281d253b0d/href</a></iframe><h4>그룹핑된 제약조건</h4><p>Bean Validation 1.1 (JSR-349) 부터 포함된 내용으로, 그룹을 사용하여 유효성검증을 위한 세트를 제한할 수 있습니다.</p><p>그룹에 해당하는 인터페이스 (혹은 클래스) 를 생성하고, 해당 그룹세트에 해당하는 제약조건 애노테이션의 groups 에 인터페이스를 넣으면 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b018125385dd43d1f0f728fca21475ba/href">https://medium.com/media/b018125385dd43d1f0f728fca21475ba/href</a></iframe><p>위 코드는 PersonGroups.Driver 그룹세트에 포함된 경우, age 가 18 이상이어야 하고 driverLicenseNumber 가 있어야 한다는 제약조건을 명시하고 있습니다.</p><p>제약조건은 복수의 그룹세트에 포함될 수 있습니다.</p><p>그리고 name 에 선언된 @NotEmpty 또는 age 에 선언된 @Positive 와 같이 groups 가 지정되지 않은 제약조건의 경우 javax.validation.groups.Default 그룹세트에 포함되어 동작하게됩니다.</p><p>특정 그룹세트에 위반되는 사항들을 확인하기 위해서는 validate() 에 PersonGroups.Driver 그룹클래스를 추가적으로 전달하면 됩니다.</p><p>하지만 대부분의 경우는 javax.validation.groups.Default 에 해당하는 제약조건들도 같이 확인하기를 원할것입니다.</p><p>따라서 javax.validation.groups.Default 와 PersonGroups.Driver 2개 그룹세트를 넣어주는 방식으로 사용하시면 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e7fdae6a68e9b1e13c38fbd4cdc21ed7/href">https://medium.com/media/e7fdae6a68e9b1e13c38fbd4cdc21ed7/href</a></iframe><p>매번 javax.validation.groups.Default 그룹세트를 추가하는것이 번거롭다면, PersonGroups.Driver 가 javax.validation.groups.Default 를 확장클래스로 만들면 됩니다.</p><p>그렇다면 validate() 시 PersonGroups.Driver동 만을 넘겨주더라도 javax.validation.groups.Default 포함하여 유효성검증을 하게 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d7a46929552804f234b1474619564541/href">https://medium.com/media/d7a46929552804f234b1474619564541/href</a></iframe><h4>필드에 대한 제약조건과 속성에 대한 제약조건</h4><p>유효성검사에서 필드와 속성에 대한 제약조건을 구분해서 생각할 필요성이 있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e6103313f0a785c8322b105be32334d4/href">https://medium.com/media/e6103313f0a785c8322b105be32334d4/href</a></iframe><p>위와 같은 코드에서 1번 제약조건의 경우는 필드에 대한 제약조건입니다. getName() 이라는 name 필드에 대한 접근자가 있지만 무시되며 “송정훈” 이라는 값에 대해서 유효성검사가 진행되게 됩니다.</p><p>2번 제약조건은 속성에 대한 제약조건입니다. “이름은 송정훈” 이라는 값에 대해서 유효성검사가 진행됩니다.</p><p>getXXX 또는 isXXX 와 같은 이름의 응답값을 가진 메소드들이 속성에 속하게되고, 빈에 대한 유효성검사를 진행하면 속성들에 대해서도 검사가 진행이 되게 됩니다.</p><h4>계단식 유효성검사 (cascaded validation)</h4><p>계단식 유효성검사를 위해서는 해당되는 필드에 @Valid 를 추가해 주셔야 합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/67c7715e072b3c6340ac1cca30ee8516/href">https://medium.com/media/67c7715e072b3c6340ac1cca30ee8516/href</a></iframe><p>위 예제에서는 driver 필드에 @Valid 애노테이션이 선언되어 있습니다.</p><p>따라서 driver 객체에 대한 유효성검사도 이뤄지게 됩니다.</p><h4>컨테이너 요소에 대한 유효성검사</h4><p>Iterable (Set, List 등), Map, Optional 과 같은 컨테이너형의 요소들에 대한 유효성검사가 Bean Validation 2.0 (JSR-380) 에 포함되었습니다.</p><p>유효성검사를 진행하기 위한 요소 앞에 애노테이션을 추가하면 되는데, 위에 언급한 Car 에서 여러명의 (하나 이상의) 운전자가 있어야 한다면 아래와 작성하게 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8ddc3ffbf60557d6b1b32f9040684412/href">https://medium.com/media/8ddc3ffbf60557d6b1b32f9040684412/href</a></iframe><h4>그룹전환</h4><p>우리는 Car 에 대해서 유효성검사를 진행할때 drivers 는 PersonGroups.Driver 그룹세트로 유효성검사가 이루어지기를 바라겠지만, 아쉽게도 Car 와 동일하게 javax.validation.groups.Default 에 대한유효성검사만 이뤄지게 됩니다.</p><p>이런경우에 @ConvertGroup 을 사용하여 그룹전환을 제약조건에 추가하는것이 유용합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/92663fde6269cc97153d2c09ad328869/href">https://medium.com/media/92663fde6269cc97153d2c09ad328869/href</a></iframe><p>참고로 @ConvertGroup 에서 from 은 아무것도 적지 않으면 javax.validation.groups.Default 를 가지게 됩니다.</p><p>따라서 위의 경우 from 을 생략하셔도 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/79ce71a1c9eb97c8c2fdea80efd0d057/href">https://medium.com/media/79ce71a1c9eb97c8c2fdea80efd0d057/href</a></iframe><h4>매개변수에 대한 유효성검사</h4><p>메소드의 매개변수 또는 응답값에 대한 유효성검사도 가능합니다.</p><p>먼저 매개변수에 대한 제약조건은 아래와 같이 작성하면 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a5d3e71ed5e950a6f40002225998d8d6/href">https://medium.com/media/a5d3e71ed5e950a6f40002225998d8d6/href</a></iframe><p>매개변수에 대한 유효성검사는 ExecutableValidator 를 사용하게 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/65052aee1cc006b73bd1e30fd1bbade1/href">https://medium.com/media/65052aee1cc006b73bd1e30fd1bbade1/href</a></iframe><h4>응답값에 대한 유효성검사</h4><p>메소드에 응답값에 대한 유효성검사는 아래와 같이 가능합니다.</p><p>위에서 나온 속성에 대한 유효성검사와는 다르게, 이 경우에는 ExecutableValidator를 사용하게 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d34391e63e937a357fecf97613ef8d2e/href">https://medium.com/media/d34391e63e937a357fecf97613ef8d2e/href</a></iframe><h4>복수의 매개변수에 대한 유효성검사</h4><p>매개변수 하나하나가 아닌, 여러 매개변수에 대한 유효성검증이 필요할 경우가 있습니다.</p><p>이럴때는 사용자정의 애노테이션과 ConstraintValidator 를 만드는 방법으로 해결이 가능합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e3434921258b514a4b1126e3e5c0d636/href">https://medium.com/media/e3434921258b514a4b1126e3e5c0d636/href</a></iframe><p>위 update 메소드는 18세 미만의 경우 운전면허번호는 null 만 허용해야 합니다. 즉 2개의 매개변수를 같이 검증해야 합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/81aac7383d67826652814424508246d7/href">https://medium.com/media/81aac7383d67826652814424508246d7/href</a></iframe><p>우선 검증을 위한 사용자정의 애노테이션을 추가하였습니다.</p><p>이 제약조건에 대한 유효성을 검증하기 위하여 PersonUpdateValidator 를 사용하도록 되어있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/782f8ec4394c5fff515e603b06b8233e/href">https://medium.com/media/782f8ec4394c5fff515e603b06b8233e/href</a></iframe><p>이 검사기는 age (첫번째 매개변수) 가 18 이상인경우 유효하지만, 미만인 경우에는 운전면허번호 (두번째 매개변수) 가 null 이어야만 유효한것으로 판단합니다.</p><p>이 사용자정의 ConstraintValidator 은 @SupportedValidationTarget 을 ValidationTarget.PARAMETERS 로 하여 매개변수에 대한 유효성검증기임을 나타내고 있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/669b3017b07119337fbb02fd9dd67f1b/href">https://medium.com/media/669b3017b07119337fbb02fd9dd67f1b/href</a></iframe><h4>제약조건의 합성</h4><p>여러 제약조건 애노테이션을 합성한 애노테이션을 만드는것도 가능합니다.</p><p><a href="https://docs.jboss.org/hibernate/stable/validator/reference/en-US/html_single/#section-boolean-constraint-composition">boolean-constraint-composition</a> 를 참고하시기 바랍니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b4868ec62065adb3514a963171c9bd0f/href">https://medium.com/media/b4868ec62065adb3514a963171c9bd0f/href</a></iframe><h4>스크립트를 사용한 유효성검증</h4><p>Hibernate 에서 제공하는 @ScriptAssert 와 같은것을 사용하여, 스크립트를 사용한 유효성검증도 가능합니다.</p><p>예를든다면 18세 미만인 경우 운전면허번호가 null 이어야 하는 제약조건은 아래와 같이 작성하는게 가능합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/167b9cab02b610016c28d5aec155d8a6/href">https://medium.com/media/167b9cab02b610016c28d5aec155d8a6/href</a></iframe><h3>Spring Validation</h3><p>Spring 에서는 Java Bean Validation 을 완벽하게 지원하면서, 검사기를 직접 사용하지 않고 AOP 와 같은 방식으로 더 편리하게 유효성검사를 할 수 있는 장치들을 제공해주고 있습니다.</p><p>그것들에 대해 알아보도록 하겠습니다.</p><h4>Validated</h4><p>Bean Validation 에서 @Valid 를 사용하여 유효성검사가 진행될 대상을 지정하고는 하였습니다.</p><p>하지만 이것은 어떤 그룹세트에서 유효성검사가 일어날지 표현하기 적합하지는 않습니다.</p><p>Spring 에서는 Validator.validate() 를 사용하여 유효성검사를 진행하기보다는 AOP 와 같은 방법을 사용하므로 진입점이 되는 애노테이션에 그룹세트를 명시적으로 지정해야 합니다.</p><p>따라서 스프링에서는 유효성검사에 진입하게되는 지점에 @Validated 라는 애노테이션을 사용하는 방법을 제공하고 있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/94e7e141424981d5d6c820b3d4670b61/href">https://medium.com/media/94e7e141424981d5d6c820b3d4670b61/href</a></iframe><h4>Spring 에서 메소드에 대한 유효성검사</h4><p>AOP 를 사용하여 메소드를 실행할때 유효성검사를 실행하는것이 가능합니다.</p><p>MethodValidationPostProcessor 를 빈을 정의하고, 필요한 클래스 혹은 메소드에 @Validated 애노테이션을 추가하면 됩니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0fba81115a9645bfd8811504cc90d3f5/href">https://medium.com/media/0fba81115a9645bfd8811504cc90d3f5/href</a></iframe><p>만약 제약조건에 위반되는 내용이 발견된다면 ConstraintViolationException 이 발생하게 됩니다.</p><p>SpringBoot 의 ValidationAutoConfiguration 를 살펴보면 이미 MethodValidationPostProcessor 빈이 정의되어 있다는것을 알 수 있습니다.</p><h4>Spring Validator</h4><p>Spring 내부에서 사용하는 독자적인 Validator 인터페이스가 있습니다.</p><p>org.springframework.validation.Validator 로서 아래와 같은 코드를 가지고 있습니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a44581c7ea2f5afd52eb2bc098817044/href">https://medium.com/media/a44581c7ea2f5afd52eb2bc098817044/href</a></iframe><p>기본적으로 Spring 에서는 Java Bean Validation 을 지원하는 완벽하게 지원하는 LocalValidatorFactoryBean 과 SpringValidatorAdapter 을 제공하고 있습니다.</p><p>즉 SpringValidatorAdapter 는 org.springframework.validation.Validator 이외에도 javax.validation.ValidatorFactory, javax.validation.Validator 의 구현체로 동작합니다.</p><p>SpringBoot 의 ValidationAutoConfiguration 를 살펴보면 LocalValidatorFactoryBean 가 defaultValidator 로 동작한다는것을 알 수 있습니다.</p><h4>Spring MVC 에서 유효성검사</h4><p>Spring MVC 에서 Data Binding 시점에 유효성검사를 실행하게 됩니다.</p><blockquote>이 부분은 WebMvcConfigurationSupport 또는 WebFluxConfigurationSupport 를 보시면 좀 더 자세하게 아실 수 있습니다. ConfigurableWebBindingInitializer 를 생성할때 Validator 를 주입하는걸 확인하실 수 있습니다.</blockquote><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/80c2d6a02daea02ff4b0e7186e143cda/href">https://medium.com/media/80c2d6a02daea02ff4b0e7186e143cda/href</a></iframe><p>바인딩에 실패한경우 org.springframework.web.bind.MethodArgumentNotValidException 이 발생하게 되는데, 만약 필요하다면 이 예외에 대한 ExceptionHandler 를 만들어서 처리하는것이 가능합니다.</p><p>ExceptionHandler 를 사용하지 않고 직접 BindingResult 를 받아서 처리하는것도 가능합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6bf9b9befeba0cd208c344b874e0e932/href">https://medium.com/media/6bf9b9befeba0cd208c344b874e0e932/href</a></iframe><p>사용자정의 Validator 를 사용하여 처리하는것도 가능합니다.</p><p>org.springframework.validation.Validator 를 확장한 Validator 를 만들어서 아래와 같이 추가해주도록 합니다.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/00174c19900c2e77dae2ade14ffe6f06/href">https://medium.com/media/00174c19900c2e77dae2ade14ffe6f06/href</a></iframe><p>여기까지 Java Bean Validation 과 Spring Validation 을 사용한 유효성검사에 대해서 간단하게 알아보았습니다.</p><p>이 글은 아래 자료들을 참고하였습니다.</p><ul><li><a href="http://docs.jboss.org/hibernate/stable/validator/reference/en-US/html_single">Hibernate Validator 6.0.12.Final — JSR 380 Reference Implementation: Reference Guide</a></li><li><a href="https://docs.spring.io/spring/docs/4.1.x/spring-framework-reference/html/validation.html#validation-beanvalidation">7. Validation, Data Binding, and Type Conversion</a></li><li><a href="https://www.baeldung.com/javax-validation-method-constraints">Method Constraints with Bean Validation 2.0 | Baeldung</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b5191a113f5c" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kafka 와 Confluent Schema Registry 를 사용한 스키마 관리 #3]]></title>
            <link>https://medium.com/@gaemi/kafka-%EC%99%80-confluent-schema-registry-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%8A%A4%ED%82%A4%EB%A7%88-%EA%B4%80%EB%A6%AC-3-96b0f070d0f1?source=rss-d4008c147c0e------2</link>
            <guid isPermaLink="false">https://medium.com/p/96b0f070d0f1</guid>
            <category><![CDATA[kafka]]></category>
            <category><![CDATA[confluent]]></category>
            <category><![CDATA[schemaregistry]]></category>
            <category><![CDATA[springcloudstream]]></category>
            <category><![CDATA[spring-kafka]]></category>
            <dc:creator><![CDATA[Junghoon Song]]></dc:creator>
            <pubDate>Sat, 21 Jul 2018 15:05:07 GMT</pubDate>
            <atom:updated>2018-07-23T11:16:08.648Z</atom:updated>
            <content:encoded><![CDATA[<h3>Kafka 와 Confluent Schema Registry 를 사용한 스키마 관리 #3</h3><p><a href="https://medium.com/@gaemi/kafka-%EC%99%80-confluent-schema-registry-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%8A%A4%ED%82%A4%EB%A7%88-%EA%B4%80%EB%A6%AC-2-bfa96622a974">Kafka 와 Confluent Schema Registry 를 사용한 스키마 관리 #2 — Junghoon Song — Medium</a> 에 이어지는 내용입니다.</p><p>여기에 나오는 코드들은 <a href="https://github.com/gaemi/confluent-kafka-samples">GitHub — gaemi/confluent-kafka-samples</a> 에 공개되어 있습니다.</p><h3>Spiring Kafka 에서 Producer 와 Consumer</h3><p>SpringBoot 의 AutoConfigration 은 정말 편합니다.</p><p>만약 SpringBoot 를 쓰고 계신다면 Spring Kafka 에 대한 의존성을 추가하는것 만으로도 거의 모든 설정이 끝난 상태라고 보시면 됩니다.</p><p>우리는 그냥 application.yml 파일을 수정하기만 하면됩니다.</p><pre>spring:<br>  kafka:<br>    bootstrap-servers: localhost:9092<br>    producer:<br>      keySerializer: org.apache.kafka.common.serialization.StringSerializer<br>      valueSerializer: io.confluent.kafka.serializers.KafkaAvroSerializer<br>      properties:<br>        schema:<br>          registry:<br>            url: <a href="http://localhost:8081">http://localhost:8081</a></pre><p>producer 에서는 vanilla-kafka 와 동일하게 value 에 대한 serializer 로 KafkaAvroSerializer 을 사용 설정하였습니다.</p><pre>spring:<br>  kafka:<br>    bootstrap-servers: localhost:9092<br>    consumer:<br>      group-id: spring-kafka<br>      auto-offset-reset: earliest<br>      keyDeserializer: org.apache.kafka.common.serialization.StringDeserializer<br>      valueDeserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer<br>      properties:<br>        schema:<br>          registry:<br>            url: <a href="http://localhost:8081">http://localhost:8081</a><br>        specific:<br>          avro:<br>            reader: true</pre><p>consumer 도 역시 vanilla-kafka 와 동일하게 KafkaAvroDeserializer 를 사용합니다.</p><pre>@RestController<br>@RequiredArgsConstructor(onConstructor = @__(@Autowired))<br>public class ShipmentController {<br>    private final KafkaTemplate&lt;Object, Object&gt; kafkaTemplate;</pre><pre>    @PostMapping(&quot;/shipments&quot;)<br>    public void send(@RequestBody Shipment shipment) {<br>        kafkaTemplate.send(&quot;shipment&quot;, shipment);<br>    }<br>}</pre><p>Spring Kafka 에서는 Producer 를 보다 편리하게 사용할 수 있도록 KafkaTemplate 를 제공하고 있습니다.<br>이것을 사용하면 ProducerRecord 를 사용하지 않고 좀 더 쉽게 메시지를 전송할 수 있습니다.</p><pre>@Slf4j<br>@Component<br>public class ShipmentProcessor {<br>    @KafkaListener(topics = &quot;shipment&quot;)<br>    public void process(ConsumerRecord&lt;String, Shipment&gt; record) {<br>        log.info(&quot;received a message. {}&quot;, record);<br>    }<br>}</pre><p>Consumer 도 @KafkaListener 를 사용하여 메시지를 받고 처리하는게 가능합니다.</p><p>개인적으로 Spring Kafka 는 Spring 환경에서 Kafka 를 사용하기 위한 적절한 추상화를 제공한다고 생각하고 있습니다.</p><p>만약 Confluent Schema Registry 를 사용할 예정이라면, 현재 몇가지 문제가 있는 Spring Cloud Stream 보다는 Spring Kafka 를 사용하기를 추천합니다.</p><h3>Spring Cloud Stream 에서 Producer 와 Consumer</h3><p>Spring Cloud Stream 은 MSA 환경에서 사용가능한 메시징 시스템으로 최근 많은 관심을 받고 있습니다.</p><p>그리고 자체적으로 사용가능한 Schema Registry Server 도 제공하고 있습니다. <a href="https://docs.spring.io/spring-cloud-stream/docs/Elmhurst.RELEASE/reference/htmlsingle/#_schema_registry_server">Spring Cloud Stream Reference Guide</a></p><p>하지만 우리는 Confluent Schema Registry 와 Spring Cloud Stream Kafka 를 연동하는 방법에 대해서 고민해보도록 하겠습니다.</p><p>시작에 앞서서 한가지 고백을 하자면 Confluent Schema Registry 와 Spring Cloud Stream 연동이 불가능한것은 아니지만 썩 매끄럽지는 못한 모양새입니다.</p><pre>@Configuration<br>@EnableSchemaRegistryClient // Spring Cloud Stream 에서 Schema Registry 에 Client 로 연결하여 사용하기 위하여 추가한다.<br>public class SpringCloudStreamKafkaProducerConfig {</pre><pre>    // Confluent 의 Schema Registry 를 사용하기 위해서 필요<br>    @Bean<br>    public ConfluentSchemaRegistryClient schemaRegistryClient(@Value(&quot;\${spring.cloud.stream.schema-registry-client.endpoint&quot;) String endpoint) {<br>        ConfluentSchemaRegistryClient client = new ConfluentSchemaRegistryClient();<br>        client.setEndpoint(endpoint);<br>        return client;<br>    }<br>}</pre><p>먼저 @EnableSchemaRegistryClient 를 추가하면 Schema Registry 에 Client 도 연동하게 됩니다.</p><p>이때 default 로 Schema Registry 는 Spring Cloud Stream 에 포함된 Schema Registry 입니다.</p><p>우리는 Confluent 의 Schema Registry 를 사용할 예정이므로 SchemaRegistryClient Bean 을 재정의 해야 합니다.<br>ConfluentSchemaRegistryClient 는 Confuent Schema Registry 에 연결하기 위한 SchemaRegistryClient 입니다.</p><pre>spring:<br>  cloud:<br>    stream:<br>      schema-registry-client:<br>        endpoint: http://localhost:8081<br>      kafka:<br>        binder:<br>          brokers: localhost:9092<br>      bindings:<br>        shipment:<br>          content-type: application/*+avro</pre><p>Spring Cloud Stream 에서는 application.yml 의 bindings 에 바인딩할 채널명을 쭉 넣게 됩니다.</p><p>우리는 shipment 라는 채널을 사용할 예정이고 content-type 을 위와 같이 적어주므로서 avro 를 사용한다는것을 알리게 됩니다.<br>(별도 지정한 destination 이 없다면 채널명과 토픽명은 동일합니다.)</p><p>content-type 은 Spring Cloud Stream 에서 사용하기 위한 적절한 MessageConverter 를 찾는데 도움을 줍니다.</p><p>Spring Cloud Stream 의 바인더가 어떻게 동작하는지가 궁금하다면 <a href="https://docs.spring.io/spring-cloud-stream/docs/Elmhurst.RELEASE/reference/htmlsingle/#spring-cloud-stream-overview-binders">Spring Cloud Stream Reference Guide</a> 를 참고하시기 바랍니다.</p><pre>public interface ShipmentChannels {<br>    String OUTPUT = &quot;shipment&quot;;</pre><pre>    @Output(ShipmentChannels.OUTPUT)<br>    MessageChannel output();<br>}</pre><p>Producer 에서는 Output 에 해당하는 채널만 정의해줍니다. (위에서 언급했던 shipment 라는 이름의 채널입니다.)</p><pre>@Configuration<br>@EnableSchemaRegistryClient<br>@EnableBinding(ShipmentChannels.class) // 바인딩해준다.<br>public class SpringCloudStreamKafkaProducerConfig {<br>    ...<br>}</pre><p>인터페이스 클래스를 바인딩하는것만으로 우리는 이제 shipment 채널을 사용할 수 있게 됩니다.</p><pre>@RestController<br>@RequiredArgsConstructor(onConstructor = @__(@Autowired))<br>public class ShipmentController {</pre><pre>    private final ShipmentChannels shipmentChannels;</pre><pre>    @PostMapping(&quot;/shipments&quot;)<br>    public void send(@RequestBody Shipment shipment) {<br>        shipmentChannels.output().send(MessageBuilder.withPayload(shipment).build());<br>    }<br>}</pre><p>채널을 통해서 우리는 Message 객체를 주고 받을 수 있습니다.<br>MessageBuilder 를 사용하면 Message 를 편리하게 생성할 수 있습니다.</p><pre>public interface ShipmentChannels {<br>    String INPUT = &quot;shipment&quot;;</pre><pre>    @Input(ShipmentChannels.INPUT)<br>    SubscribableChannel input();<br>}</pre><p>Consumer 에서는 Input 채널을 생성하시면 됩니다.</p><pre>@Slf4j<br>@Component<br>public class ShipmentProcessor {<br>    @StreamListener(ShipmentChannels.INPUT)<br>    public void process(Shipment shipment) {<br>        log.info(&quot;received a message. {}&quot;, shipment);<br>    }<br>}</pre><p>Spring Kafka 와 같이 Annotation 형태로 쉽게 사용이 가능합니다.<br>@StreamListener 로 사용하고자 하는 Input 채널명을 넣어주시면 됩니다.</p><h3>Spring Cloud Stream 에서 Confluent Schema Registry 를 사용할 때 주의사항!</h3><p>안타깝게도 위 예제에는 큰 문제가 한가지 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OortllduCOkKV6_s4dZ6fg.png" /></figure><p>저번장에서 설명하였던 Confleunt 에서 제공한 KafkaAvroSerializer 를 사용하였을때 데이터 흐름을 나타낸 그림입니다.</p><p>(4번에 해당하는 내용입니다) 카프카로 전송되는 데이터의 앞에 Magic Byte 와 Schema ID 가 포함된다는 이야기를 했던걸 기억하실 겁니다. 그리고 이것은 Confluent 의 독자 규격이라는 이야기도 기억하실 겁니다.</p><p>이것이 정상적인 데이터 흐름이지만 안타깝게도 우리가 보았던 Spring Cloud Stream 예제코드에서는 Magic Byte 와 Schema ID 가 포함되지 않은 Data 만 카프카로 전송되게 됩니다.</p><p>Spring Cloud Stream Kafka 에서는 ByteArraySerializer 를 사용하며, Message Converter 에서 데이터 포맷을 변경하는 작업을 하게 됩니다.</p><p>이런 현상이 발생하는 원인은 SchemaRegistryClient 의 구현체로 Confluent 를 지원하는 ConfluentSchemaRegistryClient 를 지원하기는 하지만, Message Converter 에서는 Confluent 의 독자 규격을 지원하고 있지 않기 때문입니다.<br>(우리가 Avro 의 메시지를 보내기 위해서 사용하는 Mesaage Converter 는 org.springframework.cloud.stream.schema.avro.AvroSchemaRegistryClientMessageConverter 입니다.)</p><p>현재 Spring Cloud Stream 에서는 해당 이슈가 아직 해결되지 않은 상태입니다.<br><a href="https://github.com/spring-cloud/spring-cloud-stream/issues/850">Improve compatibility with the Confluent Platform · Issue #850 · spring-cloud/spring-cloud-stream · GitHub</a></p><p>우리가 정상적으로 Confluent 의 Schema Registry 와 연동하기 위하여서는 Message Converter 를 적절하게 재구현해야 합니다.</p><p><a href="https://github.com/gaemi/confluent-kafka-samples/blob/master/spring-cloud-stream-kafka/producer/src/main/java/io/github/gaemi/producer/ConfluentAvroSchemaRegistryClientMessageConverter.java">confluent-kafka-samples/ConfluentAvroSchemaRegistryClientMessageConverter.java at master · gaemi/confluent-kafka-samples · GitHub</a> 에서 이런 부분들을 수정한것을 확인하실 수 있습니다.<br>(대부분 AvroSchemaRegistryClientMessageConverter 와 동일한 코드이며, Magic Byte 와 Schema ID 에 대한 처리가 들어가 있습니다.)</p><p>하지만 가능하다면 이와 같은 방법보다 Spring Kafka 를 사용할것을 권하고 싶습니다.<br>(개인적으로 여러 종류의 broker 를 사용하지 않는 상황에서 Spring Cloud Stream 의 장점을 확인하지 못했습니다.)</p><h3>Kafka REST Proxy — 그 외 환경에서 Confluent Schema Registry 연동하기</h3><p>안타깝게도 모든 환경들이 전부 Avro 나 Confluent Schema Registry 를 매끄럽게 지원하고 있지는 못합니다.</p><p>Confluent 에서는 Kafka REST Proxy 를 통해 이런 문제를 해결할 수 있도록 도와줍니다.</p><p>Kafka REST Proxy 를 사용하는 경우 우리는 아래와 같이 HTTP 환경에서 연동되는 Producer 와 Consumer 를 구현하여 사용할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1016/1*SxbRwpX-JTGLm6ny_1hGgQ.png" /></figure><p>여기서는 해당 내용은 포함시키지 않았습니다.<br>Kafka REST Proxy 에 관심이 있는 분들은 <a href="https://docs.confluent.io/current/kafka-rest/docs/index.html">Kafka REST Proxy — Confluent Platform</a> 를 참고하시기 바랍니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=96b0f070d0f1" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kafka 와 Confluent Schema Registry 를 사용한 스키마 관리 #2]]></title>
            <link>https://medium.com/@gaemi/kafka-%EC%99%80-confluent-schema-registry-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%8A%A4%ED%82%A4%EB%A7%88-%EA%B4%80%EB%A6%AC-2-bfa96622a974?source=rss-d4008c147c0e------2</link>
            <guid isPermaLink="false">https://medium.com/p/bfa96622a974</guid>
            <category><![CDATA[schemaregistry]]></category>
            <category><![CDATA[kafka]]></category>
            <category><![CDATA[confluent]]></category>
            <dc:creator><![CDATA[Junghoon Song]]></dc:creator>
            <pubDate>Wed, 18 Jul 2018 13:55:53 GMT</pubDate>
            <atom:updated>2018-07-18T14:01:46.281Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://medium.com/@gaemi/kafka-%EC%99%80-confluent-schema-registry-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%8A%A4%ED%82%A4%EB%A7%88-%EA%B4%80%EB%A6%AC-1-cdf8c99d2c5c">Kafka 와 Confluent Schema Registry 를 사용한 스키마 관리 #1 — Junghoon Song — Medium</a> 에 이어지는 내용입니다.</p><p>여기에 나오는 코드들은 <a href="https://github.com/gaemi/confluent-kafka-samples">GitHub — gaemi/confluent-kafka-samples</a> 에 공개되어 있습니다.</p><h3>Avro 스키마 작성하기</h3><p>Avro 스키마는 JSON 으로 작성하게 됩니다.<br>우선 여기서 예제로 사용할 스키마는 아래와 같습니다. (실제로 사용하는 스키마는 아니고 예제용으로 급하게 만들어 봤습니다.)</p><pre>[<br>  {<br>    &quot;type&quot;: &quot;record&quot;,<br>    &quot;namespace&quot;: &quot;io.github.gaemi.model&quot;,<br>    &quot;name&quot;: &quot;Sender&quot;,<br>    &quot;doc&quot;: &quot;보내는 사람&quot;,<br>    &quot;fields&quot;: [<br>      {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;이름&quot;},<br>      {&quot;name&quot;: &quot;address1&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;주소&quot;},<br>      {&quot;name&quot;: &quot;address2&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;상세주소&quot;},<br>      {&quot;name&quot;: &quot;phoneNumber&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;휴대폰번호&quot;}<br>    ]<br>  },<br>  {<br>    &quot;type&quot;: &quot;record&quot;,<br>    &quot;namespace&quot;: &quot;io.github.gaemi.model&quot;,<br>    &quot;name&quot;: &quot;Recipient&quot;,<br>    &quot;doc&quot;: &quot;받는 사람&quot;,<br>    &quot;fields&quot;: [<br>      {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;이름&quot;},<br>      {&quot;name&quot;: &quot;address1&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;주소&quot;},<br>      {&quot;name&quot;: &quot;address2&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;상세주소&quot;},<br>      {&quot;name&quot;: &quot;phoneNumber&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;휴대폰번호&quot;}<br>    ]<br>  },<br>  {<br>    &quot;type&quot;: &quot;record&quot;,<br>    &quot;namespace&quot;: &quot;io.github.gaemi.model&quot;,<br>    &quot;name&quot;: &quot;Shipment&quot;,<br>    &quot;doc&quot;: &quot;수송품&quot;,<br>    &quot;fields&quot;: [<br>      {&quot;name&quot;: &quot;sender&quot;, &quot;type&quot;: &quot;io.github.gaemi.model.Sender&quot;, &quot;doc&quot;: &quot;보내는 사람&quot;},<br>      {&quot;name&quot;: &quot;recipient&quot;, &quot;type&quot;: &quot;io.github.gaemi.model.Recipient&quot;, &quot;doc&quot;: &quot;받는 사람&quot;},<br>      {&quot;name&quot;: &quot;contents&quot;, &quot;type&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;}, &quot;default&quot;: [], &quot;doc&quot;: &quot;내용물&quot;},<br>      {&quot;name&quot;: &quot;charge&quot;, &quot;type&quot;: &quot;long&quot;, &quot;default&quot;: 0, &quot;doc&quot;: &quot;배송비&quot;},<br>      {&quot;name&quot;: &quot;paymentMethod&quot;, &quot;type&quot;: {&quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;PaymentMethod&quot;, &quot;symbols&quot;: [&quot;Cash&quot;, &quot;CreditCard&quot;, &quot;COD&quot;]}, &quot;default&quot;: &quot;Cash&quot;, &quot;doc&quot;: &quot;지불수단&quot;}<br>    ]<br>  }<br>]</pre><p>스키마를 아래와 같은 방법으로 정의할 수 있습니다.</p><ul><li><strong>name</strong> : 스키마명, Java 에서 클래스명을 의미합니다.</li><li><strong>namespace</strong> : Java 에서 패키지명을 의미합니다.</li><li><strong>doc</strong> : 설명</li><li><strong>aliases</strong> : 별칭</li><li><strong>fields</strong></li><li><strong>name</strong> : 필드명</li><li><strong>doc</strong> : 설명</li><li><strong>type</strong> : 필드 타입</li><li><strong>default</strong> : 기본값</li></ul><p>아래는 사용 가능한 필드에 타입으로 사용 가능한 primitive type 입니다.</p><ul><li><strong>null</strong></li><li><strong>boolean</strong></li><li><strong>int </strong>: 32-bit singed integer</li><li><strong>long </strong>: 64-bit singed integer</li><li><strong>float </strong>: single precision (32-bit) IEEE 754 floating-point number</li><li><strong>double</strong> : double precision (64-bit) IEEE 754 floating-point number</li><li><strong>bytes </strong>: sequence of 8-bit unsinged bytes</li><li><strong>string </strong>: unicode character sequence</li></ul><p>&quot;type&quot;: [&quot;null&quot;, &quot;string&quot;] 와 같이 복수개의 type 을 지정하는것도 가능한데, 이것은 Union 이라는 타입으로 complex type 에 속합니다.<br>주로 null 을 같이 포함시켜 Nullable 한 타입을 만들고자 할 때 사용합니다. (만약 null 이 포함되지 않았으면 Not Null 타입이라고 보시면 됩니다.)<br>Union 이외에도 Enum, Array, Map 과 같은 complex type 이 존재합니다.</p><p>logical type 도 존재하는데요. 예를든다면 datetime 을 표현하기 위하여 timestamp-mills 라는 logical type 을 사용할 수 있습니다.<br>logical type 은 일부 환경 (또는 라이브러리) 에서 지원하지 못하는 경우가 있으니 사용에 주의하시기를 바랍니다.</p><p><a href="https://avro.apache.org/docs/1.8.0/spec.html">Apache Avro™ 1.8.0 Specification</a> 에서 보다 자세한 내용을 확인하실 수 있습니다.</p><h3>주의사항!</h3><p>현재 Java Avro Library 1.8.2 에서는 union type 과 logical type 을 함께 사용하는경우 오류가 발생하는 경우가 있습니다. — <a href="https://issues.apache.org/jira/browse/AVRO-1891">[AVRO-1891] Generated Java code fails with union containing logical type — ASF JIRA</a></p><p>저는 nullable 한 DateTime 필드를 사용하기 위하여 사용해봤었는데, Java 코드는 잘 생성이 되지만 사용하는 과정에서 오류가 발생하였었습니다.</p><p>1.8.3 버전에서 수정이 될것으로 보입니다만, 현재로서는 logical type 을 사용하실때 주의가 필요합니다.</p><h3>Avro 스키마에서 Java 코드 생성하기</h3><p>Maven, Gradle 등에서 사용가능한 플러그인 중, Avro 스키마 정보를 바탕으로 Java 코드를 생성해주는 플러그인이 있습니다.</p><p>여기서는 Gradle 용 플로그인을 기준으로 설명하도록 하겠습니다.<br>아래와 같이 플러그인을 활성하 할 수 있습니다. (<a href="https://plugins.gradle.org/plugin/com.commercehub.gradle.plugin.avro">Gradle — Plugin: com.commercehub.gradle.plugin.avro</a>)</p><pre>plugins {<br>  id &quot;com.commercehub.gradle.plugin.avro&quot; version &quot;0.9.1&quot;<br>}</pre><pre>// 또는</pre><pre>buildscript {<br>  repositories {<br>    maven {<br>      url &quot;https://plugins.gradle.org/m2/&quot;<br>    }<br>  }<br>  dependencies {<br>    classpath &quot;com.commercehub.gradle.plugin:gradle-avro-plugin:0.9.1&quot;<br>  }<br>}</pre><pre>apply plugin: &quot;com.commercehub.gradle.plugin.avro&quot;</pre><p>저희는 model 이라는 모듈을 만들고 build.gradle 아래 내용들을 추가로 넣어보도록 하겠습니다.</p><pre>// 상세한 설정은 아래에 추가하게 됩니다.<br>avro {<br>    createSetters = false<br>    fieldVisibility = &quot;PRIVATE&quot;<br>}</pre><pre>// 필요한 의존성들을 추가해줍니다.<br>dependencies {<br>    compile &quot;org.apache.avro:avro&quot; // 자동생성된 클래스들은 해당 의존성의 org.apache.avro.specific.SpecificRecord 를 상속받습니다.<br>}</pre><p>/src/main/avro/shipment.avsc 라는 스키마 파일을 만든 후, 위 예제의 코드를 추가하였습니다.<br>이제 generateAvroJava Task 를 실행하게 되면 자동으로 코드가 생성이 됩니다.</p><pre>$ gradlew generateAvroJava</pre><p>생성된 Java 코드는 아래와 같습니다.</p><pre>/**<br> * Autogenerated by Avro<br> *<br> * DO NOT EDIT DIRECTLY<br> */<br>package io.github.gaemi.model;</pre><pre>import org.apache.avro.specific.SpecificData;</pre><pre>@SuppressWarnings(&quot;all&quot;)<br>/** 받는 사람 */<br>@org.apache.avro.specific.AvroGenerated<br>public class Recipient extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {<br>...<br>}</pre><p>앞으로는 스키마가 변경될 때 shipment.avsc 파일을 수정한 후 generateAvroJava Task 를 실행하시면 됩니다.</p><h3>Vanilla Kafka 에서 Producer 와 Consumer</h3><p>Confluent 에서는 Shcema Registry 와 연동하여 사용가능한 Kafka Avro Serializer 라이브러리를 제공하고 있습니다.<br>아래와 같은 방식으로 동작합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OortllduCOkKV6_s4dZ6fg.png" /></figure><ol><li>Producer 는 메시지를 직렬화 하기 위하여 KafkaAvroSerializer 를 사용하게 됩니다.</li><li>KafkaAvroSerializer 는 SchemaRegistryClient 라는것을 사용하여 Schema Registry 에 스키마정보를 등록하게 됩니다.</li><li>Schema Registry 에 정상적으로 스키마가 등록되면 SchemaID 를 반환하게 됩니다.</li><li>KafkaAvroSerializer 는 SchemaID 와 메시지 본문을 포함한 데이터를 직렬화 하게 됩니다.<br>정확하게는 Magic Byte (1byte) + SchemaID (4byte) + Data로 구성이 된 데이터가 직렬화되어 전송됩니다.</li></ol><p>여기서 눈여겨봐야하는 내용은 4번으로 Kafka 에 전송되는 데이터 앞에 5바이트는 매직바이트와 SchemaID 라는 점 입니다.<br>이 내용은 Confluent Schema Registry 독자적인 방식이므로 다른 Schema Registry 들 와 호환되는 내용이 아닙니다. (다른 Schema Registry 들도 각자 독자적인 방식으로 SchemaID 를 전달하는 방법을 채용하고 있습니다.)</p><p>자세한 내용은 <a href="https://docs.confluent.io/current/schema-registry/docs/serializer-formatter.html#wire-format">Serializer and Formatter — Confluent Platform</a> 를 참고하시기 바랍니다.</p><p>아무튼 이제 우리는 Confluent 에서 제공해주는 라이브러리를 사용하여 메시지를 주고 받는 코드를 작성해보도록 하겠습니다.<br>(모든 예제는 SpringBoot 를 기준으로 작성하였습니다.)</p><p>vanilla-kafka/producer 와 vanilla-kafka/consumer 모듈을 추가하도록 합니다.</p><p>먼저 vanilla-kafka/producer 코드를 살펴보도록 하겠습니다.</p><pre>apply plugin: &quot;org.springframework.boot&quot;</pre><pre>dependencies {<br>    compile project(&quot;:model&quot;) // shipment 가 포함된 모듈<br>    compile &quot;org.springframework.boot:spring-boot-starter-web&quot;<br>    compile &quot;org.apache.kafka:kafka_2.12&quot; // kafka library<br>    compile &quot;io.confluent:kafka-avro-serializer&quot; // confluent 에서 제공하는 avro serializer (schema registry 를 지원)<br>}</pre><p>build.gradle 에서는 confluent 에서 제공하는 avro serilizer library 를 추가하였습니다.</p><pre>schema:<br>  registry:<br>    url: http://localhost:8081<br>bootstrap:<br>  servers: localhost:9092</pre><p>application.yml 은 kafka broker, schema registry url 을 적어줍니다.<br>예제이기 때문에 세부적인 설정들은 모두 생략하였습니다.</p><pre>@Configuration<br>public class VanillaKafkaProducerConfig {<br>    @Bean<br>    public Producer&lt;String, Shipment&gt; producer(@Value(&quot;${bootstrap.servers}&quot;) String bootstrapServers, @Value(&quot;${schema.registry.url}&quot;) String schemaRegistryUrl) {<br>        Properties props = new Properties();<br>        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);<br>        props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;KafkaExampleProducer&quot;);<br>        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());<br>        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());<br>        props.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl);<br>        return new KafkaProducer&lt;&gt;(props);<br>    }<br>}</pre><p>Conflguration 코드입니다.<br>기존에 Kafka 를 사용하셨던 분들이라면 Producer 를 생성하는 방법을 아실겁니다.<br>다른것은 Serializer 로 KafkaAvroSerializer 를 사용한다는 점과 (여기서는 Value 에 대해서만 적용한 상태입니다.) schema registry url 도 properties 에 추가하여 넘긴다는 점 입니다.</p><pre>@RestController<br>@RequiredArgsConstructor(onConstructor = @__(@Autowired))<br>public class ShipmentController {<br>    private final Producer&lt;String, GenericRecord&gt; producer;</pre><pre>    @PostMapping(&quot;/shipments&quot;)<br>    public void send(@RequestBody Shipment shipment) {<br>        producer.send(new ProducerRecord&lt;&gt;(&quot;shipment&quot;, shipment));<br>    }<br>}</pre><p>Controller 에서는 사용자의 요청을 받아서 메시지를 발행하고 있습니다.</p><p>다음은 vanilla-kafka/consumer 의 코드를 살펴보도록 합시다.<br>build.gradle 과 application.yml 은 크게 다른 부분이 없어서 생략하도록 하겠습니다.</p><pre>@Configuration<br>public class VanillaKafkaConsumerConfig {<br>    @Bean<br>    public Consumer&lt;String, Shipment&gt; consumer(@Value(&quot;${bootstrap.servers}&quot;) String bootstrapServers, @Value(&quot;${schema.registry.url}&quot;) String schemaRegistryUrl) {<br>        Properties props = new Properties();<br>        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);<br>        props.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;KafkaExampleProducer&quot;);<br>        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());<br>        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());<br>        props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl);<br>        return new KafkaConsumer&lt;&gt;(props);<br>    }<br>}</pre><p>역시 KafkaAvroDeserilizer 를 사용한다는 점과 schema registry url 을 전달하는 것 외에 특별히 다른 부분은 없습니다.</p><pre>@Slf4j<br>@Component<br>@RequiredArgsConstructor(onConstructor = @__(@Autowired))<br>public class ShipmentProcessor {</pre><pre>    private final Consumer&lt;String, Shipment&gt; consumer;</pre><pre>    @PostConstruct<br>    public void subscribe() {<br>        consumer.subscribe(Collections.singletonList(&quot;shipment&quot;));</pre><pre>        while (true) {<br>            consumer.poll(1000).forEach(record -&gt; {<br>                log.info(&quot;received a message. {}&quot;, record);<br>            });</pre><pre>            consumer.commitAsync();<br>        }<br>    }</pre><pre>    @PreDestroy<br>    public void destroy() {<br>        consumer.close(5, TimeUnit.SECONDS);<br>    }<br>}</pre><p>Consumer 에서는 메시지를 폴링하고 로그로 찍어줍니다.</p><pre>$ curl --header &quot;Content-Type: application/json&quot; -X POST   --data &#39;{&quot;sender&quot;:{&quot;name&quot;:&quot;송정훈&quot;, &quot;address1&quot;:&quot;경기도 성남시 분당구&quot;, &quot;address2&quot;:&quot;정자동&quot;, &quot;phoneNumber&quot;:&quot;010-1111-1111&quot;}, &quot;recipient&quot;:{&quot;name&quot;:&quot;송마루&quot;, &quot;address1&quot;:&quot;경기도 구리시&quot;, &quot;address2&quot;:&quot;갈매동&quot;, &quot;phoneNumber&quot;:&quot;010-2222-2222&quot;}, &quot;contents&quot;:[&quot;또봇&quot;], &quot;charge&quot;:5000, &quot;paymentMethod&quot;:&quot;Cash&quot;}&#39;   <a href="http://localhost:8080/shipments">http://localhost:8080/shipments</a></pre><p>이제 각각 어플리케이션을 실행한 후 curl 로 메시지 발행요청을 하면 consumer application 에서 로그를 확인할 수 있습니다.</p><p>SchemaRegistry 에 등록된 스키마 정보도 확인이 가능합니다.</p><pre>$ curl -X GET http://localhost:8081/subjects<br>[&quot;shipment-value&quot;]</pre><pre>$ curl -X GET http://localhost:8081/subjects/shipment-value/versions<br>[1]</pre><pre>$ curl -X GET http://localhost:8081/subjects/shipment-value/versions/1<br>{&quot;subject&quot;:&quot;shipment-value&quot;,&quot;version&quot;:1,&quot;id&quot;:1,&quot;schema&quot;:&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Shipment\&quot;,\&quot;namespace\&quot;:\&quot;io.github.gaemi.model\&quot;,\&quot;doc\&quot;:\&quot;수송품\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;sender\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Sender\&quot;,\&quot;doc\&quot;:\&quot;보내는 사람\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;이름\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address1\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address2\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;상세주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;phoneNumber\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;휴대폰번호\&quot;,\&quot;default\&quot;:null}]},\&quot;doc\&quot;:\&quot;보내는 사람\&quot;},{\&quot;name\&quot;:\&quot;recipient\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Recipient\&quot;,\&quot;doc\&quot;:\&quot;받는 사람\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;이름\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address1\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address2\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;상세주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;phoneNumber\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;휴대폰번호\&quot;,\&quot;default\&quot;:null}]},\&quot;doc\&quot;:\&quot;받는 사람\&quot;},{\&quot;name\&quot;:\&quot;contents\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}},\&quot;doc\&quot;:\&quot;내용물\&quot;,\&quot;default\&quot;:[]},{\&quot;name\&quot;:\&quot;charge\&quot;,\&quot;type\&quot;:\&quot;long\&quot;,\&quot;doc\&quot;:\&quot;배송비\&quot;,\&quot;default\&quot;:0},{\&quot;name\&quot;:\&quot;paymentMethod\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;enum\&quot;,\&quot;name\&quot;:\&quot;PaymentMethod\&quot;,\&quot;symbols\&quot;:[\&quot;Cash\&quot;,\&quot;CreditCard\&quot;,\&quot;COD\&quot;]},\&quot;doc\&quot;:\&quot;지불수단\&quot;,\&quot;default\&quot;:\&quot;Cash\&quot;}]}&quot;</pre><p>받는사람에서 default value 없이 email 필드를 추가하면 어떻게 될가요?</p><pre>  {<br>    &quot;type&quot;: &quot;record&quot;,<br>    &quot;namespace&quot;: &quot;io.github.gaemi.model&quot;,<br>    &quot;name&quot;: &quot;Recipient&quot;,<br>    &quot;doc&quot;: &quot;받는 사람&quot;,<br>    &quot;fields&quot;: [<br>      {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;이름&quot;},<br>      {&quot;name&quot;: &quot;address1&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;주소&quot;},<br>      {&quot;name&quot;: &quot;address2&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;상세주소&quot;},<br>      {&quot;name&quot;: &quot;phoneNumber&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;default&quot;: null, &quot;doc&quot;: &quot;휴대폰번호&quot;},<br>      {&quot;name&quot;: &quot;email&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;], &quot;doc&quot;: &quot;이메일주소&quot;}<br>    ]<br>  }</pre><p>이것은 BACKWARD 을 만족하지 못하며 메시지를 발행하려하면 409 오류와 함께 메시지가 발행되지 않는것을 확인할 수 있습니다.</p><pre>io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Schema being registered is incompatible with an earlier schema; error code: 409</pre><p>default value 가 지정되어 있다면 BACKWARD 조건을 만족하면서 스키마가 변경이 됩니다.</p><pre>$ curl -X GET http://localhost:8081/subjects/shipment-value/versions<br>[1,2]</pre><pre>$ curl -X GET http://localhost:8081/subjects/shipment-value/versions/2<br>{&quot;subject&quot;:&quot;shipment-value&quot;,&quot;version&quot;:2,&quot;id&quot;:2,&quot;schema&quot;:&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Shipment\&quot;,\&quot;namespace\&quot;:\&quot;io.github.gaemi.model\&quot;,\&quot;doc\&quot;:\&quot;수송품\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;sender\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Sender\&quot;,\&quot;doc\&quot;:\&quot;보내는 사람\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;이름\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address1\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address2\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;상세주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;phoneNumber\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;휴대폰번호\&quot;,\&quot;default\&quot;:null}]},\&quot;doc\&quot;:\&quot;보내는 사람\&quot;},{\&quot;name\&quot;:\&quot;recipient\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Recipient\&quot;,\&quot;doc\&quot;:\&quot;받는 사람\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;이름\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address1\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;address2\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;상세주소\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;phoneNumber\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;휴대폰번호\&quot;,\&quot;default\&quot;:null},{\&quot;name\&quot;:\&quot;email\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}],\&quot;doc\&quot;:\&quot;이메일주소\&quot;,\&quot;default\&quot;:null}]},\&quot;doc\&quot;:\&quot;받는 사람\&quot;},{\&quot;name\&quot;:\&quot;contents\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;string\&quot;,\&quot;avro.java.string\&quot;:\&quot;String\&quot;}},\&quot;doc\&quot;:\&quot;내용물\&quot;,\&quot;default\&quot;:[]},{\&quot;name\&quot;:\&quot;charge\&quot;,\&quot;type\&quot;:\&quot;long\&quot;,\&quot;doc\&quot;:\&quot;배송비\&quot;,\&quot;default\&quot;:0},{\&quot;name\&quot;:\&quot;paymentMethod\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;enum\&quot;,\&quot;name\&quot;:\&quot;PaymentMethod\&quot;,\&quot;symbols\&quot;:[\&quot;Cash\&quot;,\&quot;CreditCard\&quot;,\&quot;COD\&quot;]},\&quot;doc\&quot;:\&quot;지불수단\&quot;,\&quot;default\&quot;:\&quot;Cash\&quot;}]}&quot;}</pre><p>이것은 Confluent 에서 제공하는 Java Library 를 사용하여 Schema Registry 와 연동하는 방법을 살펴보았습니다.</p><p>다음장에서는 Spring Kafka 와 Spring Cloud Stream 을 사용하는 방법을 살펴보도록 하겠습니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bfa96622a974" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kafka 와 Confluent Schema Registry 를 사용한 스키마 관리 #1]]></title>
            <link>https://medium.com/@gaemi/kafka-%EC%99%80-confluent-schema-registry-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%8A%A4%ED%82%A4%EB%A7%88-%EA%B4%80%EB%A6%AC-1-cdf8c99d2c5c?source=rss-d4008c147c0e------2</link>
            <guid isPermaLink="false">https://medium.com/p/cdf8c99d2c5c</guid>
            <category><![CDATA[schemaregistry]]></category>
            <category><![CDATA[kafka]]></category>
            <category><![CDATA[confluent]]></category>
            <dc:creator><![CDATA[Junghoon Song]]></dc:creator>
            <pubDate>Tue, 10 Jul 2018 16:24:57 GMT</pubDate>
            <atom:updated>2018-07-18T13:10:58.559Z</atom:updated>
            <content:encoded><![CDATA[<h3>진화하는 메시지</h3><p>우리는 서비스에서 특정한 이벤트가 발생되면 Kafka 를 통하여 해당 이벤트에 관한 메시지를 전달하고 있습니다.<br>그리고 이 메시지들은 하나이상의 소비자들에게 전달이 되고, 적절한 서비스로직이 수행되는 것을 기대할 것입니다.</p><p>예를 한가지 들어봅시다.</p><p>먼저 서비스에 사용자가 가입하거나(추가), 탈퇴하거나(삭제), 또는 사용자정보를 변경하는것도 이벤트로 보고, Kafka 와 같은 Broker 를 통해서 아래와 같은 형태로 메시지를 전달한다고 가정해봅니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*emhXYa-lOza0SSh4Qaj7AQ.png" /><figcaption>사용자 정보에 대한 이벤트 메시지 v.1</figcaption></figure><p>이것은 매우 단순한 형태의 메시지인데 생산자(Producer)는 특정 사용자에게 어떤 이벤트가 발생하였고 이 사용자는 현재 어떤 상태인지를 전달하고 있습니다.<br>그리고 소비자(Consumer)들은 어떤 형태의 메시지를 받을것인지를 잘 알고 있고, 이 메시지 형태에 적합한 서비스로직을 각자 구현을 하게 될 것입니다.</p><p>하지만 비즈니스 요구사항은 계속 변하고 있고, 어느순간 새로운 요구사항을 만족시키기 위하여 메시지 형태가 변경이 되어야 하는 순간이 오게됩니다.<br>더 많은 정보를 담아야 할 필요성이 생기기도 하고, 아니면 더이상 사용하지 않는 불필요한 정보는 삭제해야 할 필요성도 있습니다.</p><p>요구사항이 변하였습니다.<br>앞으로는 사용자의 성과 이름을 따로 받아서 저장하게 되었고 이벤트 발생시 전달되는 메시지도 변경이 되었습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zKpLwVZwD4uhE8aBPhFRpQ.png" /><figcaption>사용자 정보에 대한 이벤트 메시지 v.2</figcaption></figure><p>기존에 사용하던 Name 이라는 필드가 사라졌고 firstName, lastName 이라는 필드가 새로 추가되었습니다.<br>이벤트 생산자(Producer)는 앞으로 위와 같은 형태의 메시지를 발행하게 되겠지만, 아직 소비자(Consumer)들은 변경된 메시지의 형태를 알지 못하고 있습니다.<br>소비자들은 firstName 이나 lastName 은 알지 못하며, 여전히 name 이라는 필드를 통해 사용자 이름에 대한 데이터를 전달받을것을 기대하고 있습니다.<br>결국은 적절하게 서비스로직이 수행되지 못하게되며 오류가 발생하는 상황에 이르게 됩니다.</p><p>우리는 이러한 상황(소비자들에게서 오류가 발생하는)을 피하고 싶습니다.<br>그러면서도 메시지의 형태를 지속적으로 안전한 방향으로 진화시켜나가기를 원하고 있습니다.<br>(예를 들자면 Name 이라는 필드를 삭제하지 않고 유지하면서 firstName, lastName 필드만 추가하는 방식으로 말이죠.)</p><p>지속적으로 메시지가 변경되어야 하는 상황에서는, 메시지의 스키마를 관리하고 안전한 방향으로 진화하고 있는지를 확인하는것이 중요합니다.<br>그렇지 못한다면 다수의 소비자(Consumer)들이 에러를 마주하게 됩니다.</p><h3>Confluent Schema Regsitry</h3><p>Confluent (<a href="https://www.confluent.io/">Confluent: Apache Kafka &amp; Streaming Platform for the Enterprise</a>) 는 LinkedIn 에서 Apache Kafka 를 개발한 팀이 세운 회사입니다.</p><p>Apache Kafka 와 더불어 다양한 플랫폼들을 연결시켜 오픈소스로 공개하였는데, 그 중 Schema Registry 도 같이 포함되어 있으며 이것을 Confluent Schema Registry 라고 부르고 있습니다.</p><p>Confluent Schema Registry 는 RESTful 인터페이스를 사용하여 스키마(Schema)를 관리하거나 조회하는 기능을 제공합니다.<br>그리하여 생산자와 소비자는 Apache Avro (<a href="https://avro.apache.org/">Welcome to Apache Avro!</a>) 포맷의 메시지를 Kafka 를 통해서 전달하고 받을수 있도록 도와줍니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/952/1*6crlNRdKaB7B1P82rE10UQ.png" /><figcaption>Confluent Schema Registry 를 사용하였을때 메시지 흐름</figcaption></figure><p>소비자(Consumer)는 Kafka 로부터 바이너리 데이터를 받는데 이 데이터에는 스키마 ID 가 포함되어 있습니다.<br>이 ID 정보를 가지고 로컬캐시 혹은 Confluent Schema Registry 에서 스키마정보를 탐색하여 가져오고, 스키마정보를 사용하여 역직렬화하여 사용하게 됩니다.</p><p>Confluent 에서 제공하는 Schema Registry 만 있는것은 아닙니다.<br>Hortonworks 에서도 Schema Registry 를 제공하고 있고 Spring Cloud Stream 에도 Schema Registry Server 가 포함되어 있습니다.<br>하지만 구글에서 검색을 해보시면 Confluent Schema Registry 가 압도적으로 많이 노출되는것을 보실 수 있습니다. (2018.07 현재)<br>그만큼 가장 많이 사용된다고 판단하였고 저희는 Confluent Schema Registry 를 선택했었습니다.</p><p><a href="https://medium.com/@stephane.maarek/introduction-to-schemas-in-apache-kafka-with-the-confluent-schema-registry-3bf55e401321">Introduction to Schemas in Apache Kafka with the Confluent Schema Registry</a> 은 Confluent Schema Registry 를 이해하는데 많은 도움을 줄 것입니다. 강력하게 추천합니다.</p><h3>스키마 진화 전략</h3><p>Confluent Schema Registry 에선 아래와 같은 패턴으로 호환성을 유지하게 됩니다. (<a href="https://docs.confluent.io/current/avro.html#schema-evolution">Data Serialization and Evolution — Confluent Platform</a> 참고)</p><ul><li><strong>Backward</strong> : 새로운 스키마로 이전 데이터를 읽는것이 가능한것을 의미합니다.<br>새로운 스키마에 필드가 추가되었는데 해당 필드에 default value 가 없다면 오류가 발생할 것이므로 스키마 등록을 허용하지 않게 됩니다.<br>새로 추가되는 필드에 default value 를 지정할때에만 스키마 등록이 허용이 됩니다.<br>Confluent Schema Registry 는 기본적으로 Backward 로 동작합니다. (물론 설정에서 변경할 수 있습니다.)</li><li><strong>Forward</strong> : 이전 스키마에서 새로운 데이터를 읽는것이 가능한것을 의미합니다.<br>새로운 스키마에서 특정 필드가 삭제된다면, 해당 필드는 이전 스키마에서 default value 를 가지고 있었어야 합니다.</li><li><strong>Full</strong> : Backward 와 Forward 을 모두 만족함을 이야기합니다. 가능하다면 이것을 사용하기를 권장합니다.</li><li><strong>None</strong> : Backward 와 Forward 어느한장 만족하지 못합니다. 아주 예외적인 상황을 제외한다면 사용하지 않기를 권장합니다.</li></ul><p>만약 항상 모든 Consumer 가 먼저 배포되고 이후에 Producer 가 배포된다면 Backward 로 충분합니다.<br>하지만 그것을 확신할 수 없다면 Full 을 사용하는것을 권장합니다.</p><p>스키마를 설계할때는 아래와 같은 내용들을 고려하도록 합시다.</p><ul><li>삭제될 가능성이 있는 필드라면 default value 를 지정합니다. 해당 필드에 데이터가 들어오지 않더라도 에러가 발생하지 않습니다.</li><li>추가되는 필드라면 default value 가 지정되어야 합니다.</li><li>Enum 은 변경될 가능성이 없을때에만 사용하도록 합니다.</li><li>필드의 이름은 변경하지 않도록 합니다.</li></ul><h3>Schema Registry 사용의 장점과 단점</h3><p>우리는 Schema Registry 를 사용하면서 메시지의 스키마를 보다 안전하게 진화해나갈 수 있습니다.<br>만약 잘못된 스키마를 가진 메시지를 전달하고자 한다면 Schema Registry 에 등록되는 과정에서 실패가 되고, Kafka 에 메시지가 전달되지 않게 됩니다.<br>소비자들은 잘못된 스키마의 메시지를 받을일이 없기 때문에 에러는 발생하지 않을것입니다.</p><p>또 한가지 장점은 Kafka 에 전달되는 바이너리 데이터에 스키마 정보가 포함되어 있지 않기 때문에, 상대적으로 적은 용량의 데이터가 전달된다는 점 입니다.<br>따라서 JSON 등과 비교하여 Kafka 시스템에서 더 적은 공간만 차지하게 되고 네트워크 대역폭도 절약할 수 있습니다.</p><p>물론 단점도 있습니다.</p><p>가장 큰 단점은 Schema Registry 의 역할이 굉장히 중요하고 Schema Registry 의 장애가 발생하는 경우 정상적으로 메시지를 전달하지 못하게 됩니다.<br>이것은 Kafka 만 운영하였을때와 비교하여 운영포인트가 증가한다는것을 의미합니다.</p><p>그 외에 Avro 포맷은 JSON등과 비교하여 직렬화과정에서 퍼포먼스가 조금 떨어진다고 알려져있습니다. (개인적으로는 큰 문제는 아니라고 생각합니다.)</p><h3>Confluent Schema Registry 설치</h3><p><a href="https://www.confluent.io/download/">Download — Confluent</a> 에서 배포판을 받아서 압축을 해제하는것만으로 설치는 끝납니다.<br>해당 배포판에는 Kafka 를 비롯하여 Schema Registry, REST Proxy 등등이 포함되어 있습니다.<br>아래와 같은 방법으로 실행하시면 됩니다.</p><pre># 압축해제<br>$ tar xvfz confluent-oss-4.1.1-2.11.tar.gz</pre><pre># Zookeeper 실행<br>$ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties</pre><pre># Kafka 실행<br>$ ./bin/kafka-server-start  ./etc/kafka/server.properties</pre><pre># Schema Registry 실행<br>$ ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties</pre><pre># Schema Registry 에 등록된 스키마 조회 (기본포트 8081)<br>$ curl -X GET http://localhost:8081/subjects<br>[] </pre><pre>$ curl -X GET <a href="http://localhost:8081/config">http://localhost:8081/config</a><br>{&quot;compatibilityLevel&quot;:&quot;BACKWARD&quot;} # 기본적으로 BACKWARD 를 사용하고 있습니다.</pre><pre>$ curl --header &quot;Content-Type: application/json&quot; -X PUT   --data &#39;{&quot;compatibility&quot;: &quot;FULL&quot;}&#39;   <a href="http://localhost:8081/config">http://localhost:8081/config</a><br>{&quot;compatibility&quot;:&quot;FULL&quot;} # FULL 로 변경되었습니다.</pre><p>schema-registry.properties 의 자세한 설정은 <a href="https://docs.confluent.io/current/schema-registry/docs/config.html">Schema Registry Configuration Options — Confluent Platform</a> 를 참고하시면 됩니다.</p><p>제공되는 API 는 <a href="https://docs.confluent.io/current/schema-registry/docs/api.html">API Reference — Confluent Platform</a> 에서 확인이 가능합니다.</p><p>Docker (<a href="https://docs.confluent.io/current/installation/docker/docs/quickstart.html">Docker Quick Start — Confluent Platform</a>) 를 사용하는 방법도 있으니 편리한 방법으로 설치해서 사용하시길 바랍니다.</p><p>다음장에서는 Avro 를 사용하여 스키마를 설계하고, Java (특히 Spring Framework를 사용한) 어플리케이션에서 Schema Registry 를 사용하는 방법에 대해서 알아보도록 하겠습니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cdf8c99d2c5c" width="1" height="1">]]></content:encoded>
        </item>
    </channel>
</rss>