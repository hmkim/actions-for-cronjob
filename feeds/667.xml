<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>나의 큰 O는 logx야..</title>
<link>https://bab2min.tistory.com/</link>
<description>제가 안 것의 대부분은 인터넷으로부터 왔으니,
다시 인터넷에게 돌려주어야 합니다.</description>
<language>ko</language>
<pubDate>Mon, 13 May 2019 15:16:30 +0900</pubDate>
<generator>TISTORY</generator>
<managingEditor>∫2tdt=t²+c</managingEditor>
<image>
<title>나의 큰 O는 logx야..</title>
<url>http://cfile27.uf.tistory.com/image/2503E44757131F760D9FEE</url>
<link>https://bab2min.tistory.com</link>
<description>제가 안 것의 대부분은 인터넷으로부터 왔으니,
다시 인터넷에게 돌려주어야 합니다.</description>
</image>
<item>
<title>단순하지만 강력한 Smooth Inverse Frequency 문장 임베딩 기법</title>
<link>https://bab2min.tistory.com/631</link>
<description>&lt;p&gt;자연언어처리 분야에서 임베딩 기법은 자연언어를 수치의 형태로 효과적으로 표현한다는 강점 때문에 최근 널리 사용되고 있습니다. 대표적인 것이 단어 임베딩 기법인데, Word2Vec, GloVe, FastText 등이 있지요. 이들은 단어의 의미를 벡터 공간 상의 점으로 표현하는데, 그 점이 단어의 실제 의미를 반영한다는 점에서 의미가 크지요. 그러나 많은 텍스트 처리 기법들은 단어 이상의 단위를 처리할 것을 요구받습니다. 문장이나 문단, 혹은 문헌처럼 말이지요. 따라서 당연히 문장 전체나 문단 등 그보다 큰 단위에 대해 임베딩을 실시하려는 시도가 있었습니다.&lt;/p&gt;&lt;p&gt;본 포스팅에서는 유명한 문장 임베딩 기법들과 함께,&amp;nbsp;간단하지만 강력한 문장 임베딩 기법인 SIF(Smooth Inverse Frequency)&lt;sup class=&quot;footnote&quot;&gt;&lt;a id=&quot;footnote_link_631_1&quot; href=&quot;#footnote_631_1&quot; onmouseover=&quot;tistoryFootnote.show(this,631,1)&quot; onmouseout=&quot;tistoryFootnote.hide(631,1)&quot; style=&quot;color:#f9650d;font-family:Verdana,Sans-serif;display:inline;&quot;&gt;&lt;span style=&quot;display:none&quot;&gt;[각주:&lt;/span&gt;1&lt;span style=&quot;display:none&quot;&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;에 대해 살펴보고자 합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;문장 임베딩 기법&lt;/h2&gt;&lt;p&gt;문장 임베딩(Sentence Embedding 혹은 Phrase나 Paragraph Embedding이라고도 함)은 문장 전체를 벡터 공간 상의 점으로 표현하는 기법을 말합니다. 특히 벡터 공간 상에 점으로 문장을 옮겼을때, 의미적으로 유사한 문장끼리는 유사한 지점에 모이게 되는데, 이를 통해 분류나 클러스터링과 같은 기법 뿐만 아니라, 자동 질의 응답과 같은 더 복잡한 작업도 수행할 수 있습니다. 근데 문제는 &quot;&lt;b&gt;어떻게 문장을 벡터 공간으로 옮길 것인가&lt;/b&gt;&quot; 입니다. 일단 단어 임베딩 기법이 주어졌으므로 이를 이용하는 방법을 생각해볼 수 있겠구요, 최근 유행하는 RNN과 같은 신경망을 이용할 수도 있겠습니다. 몇 가지 대표적인 방법을 살펴보시지요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;단어 임베딩 기법을 활용하기&lt;/h3&gt;&lt;p&gt;제일 쉬운 것은 문장 내의 각 단어의 임베딩을 다 합하고 평균을 구하는 것입니다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;I like an&amp;nbsp;apple, not a pear.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;예를 들어 위와 같은 문장이 있다면 I, like, an, apple, not, a, pear의 임베딩을 얻고 이들의 &lt;b&gt;평균을 구하자&lt;/b&gt;는 것이지요. 짧은 문장의 경우, Word2vec을 개발한 Mikolov에 따르면&lt;sup class=&quot;footnote&quot;&gt;&lt;a id=&quot;footnote_link_631_2&quot; href=&quot;#footnote_631_2&quot; onmouseover=&quot;tistoryFootnote.show(this,631,2)&quot; onmouseout=&quot;tistoryFootnote.hide(631,2)&quot; style=&quot;color:#f9650d;font-family:Verdana,Sans-serif;display:inline;&quot;&gt;&lt;span style=&quot;display:none&quot;&gt;[각주:&lt;/span&gt;2&lt;span style=&quot;display:none&quot;&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, 이 방법으로 구한 문장 임베딩도 효과적으로 쓰일 수 있다고 합니다.&lt;/p&gt;&lt;p&gt;근데 이 방법의 한계는 자명하게도, 문장이 길어질수록 다양한 중요하지 않은 단어들이 포함되고, 이들이 평균계산에 다 포함되기 때문에 문장 임베딩이 가리키는 의미가 희석될 수 밖에 없다는 것입니다. 위의 예시에서도 문장에서 제일 중요한 건 like, apple, pear이지, an, a 같은게 아니지요. 문장이 길어질수록 이런 불용어의 비중도 증가하기 때문에 이는 골치 아픈 문제라고 할 수 있습니다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이를 개선하는 것도 가능하겠죠. 입력 문장의 전처리를 잘 해서 불용어를 사전에 제거하는 것도 좋은 접근법입니다. 혹은 조금 더 정교하게, 각 단어의 가중치를 다르게 부여하는 방법도 있겠습니다. &lt;b&gt;tf-idf&lt;/b&gt; 등의 가중치를 사용할 수도 있겠네요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;RNN 모형을 활용하기&lt;/h3&gt;&lt;p&gt;RNN은 자신의 출력을 다시 입력으로 받는 신경망 셀을 여러 개를 겹쳐 다양한 길이의 입력을 다룰 수 있도록 확장한 신경망 모형입니다. 이를 이용하면 가변적인 길이의 입력을 고정된 크기의 숫자로 압축할 수도 있고, 고정된 크기의 숫자를 가변적 길이의 출력으로 풀어낼 수도 있습니다. 이 둘을 결합하면 seq2seq과 같이 가변 길이의 입력을 받아서 가변 길이의 출력을 생성하는 모형을 만들수도 있습니다. 이 모형은 기계번역이나 자동 질의&amp;nbsp;응답과 같은 작업에 널리 쓰이고 있는데요, 이를 이용해 문장을 임베딩하는 것도 가능합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;대표적인 것으로 &lt;b&gt;Skip-thought vector&lt;/b&gt;&lt;sup class=&quot;footnote&quot;&gt;&lt;a id=&quot;footnote_link_631_3&quot; href=&quot;#footnote_631_3&quot; onmouseover=&quot;tistoryFootnote.show(this,631,3)&quot; onmouseout=&quot;tistoryFootnote.hide(631,3)&quot; style=&quot;color:#f9650d;font-family:Verdana,Sans-serif;display:inline;&quot;&gt;&lt;span style=&quot;display:none&quot;&gt;[각주:&lt;/span&gt;3&lt;span style=&quot;display:none&quot;&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;가 있습니다. 이는 여러 문장을 포함한 문헌을 입력으로 받는데요, seq2seq 모형이 문장을 입력 받으면, 그 바로 앞의 문장과 바로 뒤의 문장을 출력하도록 학습을 진행합니다. 대게 붙어 있는 문장은 의미적으로 연결되는 경우가 많기 때문에, 이를 이용해 앞 뒤의 문장을 예측하도록 한 것이죠. 따라서 한 문장이 입력되면, 이 문장은 벡터 공간의 점으로 변환되고, 이 벡터 공간 상의 점은 그 문장의 바로 앞, 뒤 문장을 예측할 수 있어야 하므로, 그 의미를 반영하게 됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이와 비슷하게 &lt;a href=&quot;https://bab2min.tistory.com/628&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;&lt;b&gt;seq2seq을 이용해 Autoencoder&lt;/b&gt;를 구현하는 방법&lt;/a&gt;(이전 포스팅에서 소개한 바 있음)도 있습니다. 이는 입력 문장을 고정 길이의 숫자로 변환하고 이를 다시 복원하여 원래 문장이 나오도록 seq2seq 모형을 학습함으로써, 가변 길이 문장에 대한 고정 길이 표현을 발견합니다. 이 경우는 Skip-thought vector와는 다르게 입력 문장을 그대로 복원하는 것을 목표로 하므로, 문장에 담긴 내재적인 의미보다도 문장의 표현 그 자체에 좀 더 집중한다는 차이가 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;간단하지만 강력한 SIF 기법&lt;/h2&gt;&lt;p&gt;저자들이 아예 논문 제목에&amp;nbsp;'Simple but Tough-To-Beat'라는 표현을 썼기에, 저자들의 의견을 존중해 '간단하지만 강력하다'는 수식어를 붙였습니다. 본 논문에 제안된 SIF 기법은 정말 매우 간단한데요, &lt;b&gt;단어 임베딩 기법을 활용하&lt;/b&gt;되 가중치를 조금 다르게 주고, 이를 다듬어주는 것이 전체 기법의 전부입니다. 구체적으로 적으면 다음과 같습니다.&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;먼저 단어 임베딩 기법을 통해 각 단어의 임베딩을 얻어낸다.&lt;/li&gt;&lt;li&gt;문장 내의 모든 단어에 대해, 그 단어 임베딩을 합하되&amp;nbsp;가중치를 &lt;b&gt;a / (a + P(w))&lt;/b&gt;로 둔다&lt;/li&gt;&lt;li&gt;2번의 방법으로 전체 문장들의 문장 임베딩을 구한뒤, 이를 합쳐 행렬을 구성한다. 이 행렬에 대해 &lt;b&gt;SVD&lt;/b&gt;를 진행하여 각 문장 임베딩에서 &lt;b&gt;공통 부분&lt;/b&gt;을 구하고, 전체 문장 임베딩에서 이 공통 부분을 제거해준다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2번에서 &lt;b&gt;a&lt;/b&gt;는 SIF 기법의 파라미터로 논문에 따르면 약 0.0001 ~ 0.001의 값이 적절하다고 합니다. &lt;b&gt;P(w)&lt;/b&gt;는 해당 단어가 출현할 확률로, 이 값이 클수록 전체 가중치는 작아집니다. 즉 일반적으로 자주 등장하는 단어에 대해서는 낮은 가중치를 부여하고, 드물게 등장하는 단어에 대해서는 높은 가중치를 부여하는 것이죠.&lt;/p&gt;&lt;p&gt;3번에서는 문장 임베딩에서 공통부분을 제거합니다. 전체 문장 임베딩을 모아서 SVD를 실시하면 각 문장 임베딩에 대해 n개의 특잇값(Singular Value)이 나오는데, 이 중 가장 큰 첫번째 특잇값을 문장 임베딩에서 제거해줍니다. 즉 모든 문장 임베딩이 공유하는 요소를 제거해줌으로써 문장 임베딩 간의 공통점을 줄이고 차이점을 키우는 과정이라고 볼 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;2번은 단어 임베딩 기법에 IDF 가중치를 적용한 것과 매우 유사합니다. 다만 그 가중치 값이 IDF가 아니라 a / (a + P(w))라는게 차이점이구요. 3번은 SIF에서 처음 제안된 방법인데 의외로 높은 성능을 내는 것을 확인할 수가 있습니다. 이 방법이 우연히 얻어진 것은 아니구요,&amp;nbsp;사실 다음과 같은 수식으로부터 유도된 것입니다.&lt;/p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/993D4F415CC0769227&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28Pr%28w%7Cc%29%7E%20%3D%7E%20%5Calpha%20P%28w%29+%281-%5Calpha%20%29%5Cfrac%20%7B%20exp%28%5Cupsilon%20_%7B%20w%20%7D%5Ccdot%20%5Chat%20%7B%20c%20%7D%20%29%20%7D%7B%20Z%20%7D%20%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;345&quot; height=&quot;64&quot;&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/994937485CC076E117&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28%5Chat%20%7B%20c%20%7D%20%7E%20%3D%7E%20%5Cbeta%20c_%7B%200%20%7D+%281-%5Cbeta%20%29c%2C%7E%20c_%7B%200%20%7D%5Cbot%20c%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;233&quot; height=&quot;31&quot;&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;복잡해 보이는 식입니다만 풀어서 보면 별거 없습니다. 여기서 c가 문장 임베딩이고, w는 그 문장에 포함되는 단어 중 하나입니다. &lt;b&gt;Pr(w|c)&lt;/b&gt;는 문장 임베딩이 c일때 w라는 단어가 문장에서 출현할 확률을 계산합니다. 확률 식은 총 2가지 항으로 계산되는데, 전자는 단어 w가 일반적으로 출현할 확률 &lt;b&gt;P(w)&lt;/b&gt;이고, 후자는 해당 단어 w의 벡터 u와 공통 부분을 포함한 임베딩 벡터 ^c을 내적한 값입니다. Z는 모든 종류의 단어에 대해 &lt;b&gt;exp(u_w * ^c)&lt;/b&gt;의 합을 가리킵니다. 현재 단어 w에 대한 exp값을 전체 exp 합으로 나눠 정규화함으로써 저 분수부분은 0~1사이의 확률 값을 가지게 되지요.&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;따라서 전자는 문장 임베딩과 상관없이 단어 w가 출현할 확률, 후자는 문장 임베딩과 현재 단어 w가 잘 어울리는 정도를 바탕으로 계산한 w가 출현할 확률입니다. 이 둘을 적절한 정도&amp;nbsp;α로 조합하여 전체 확률을 계산한게 최종적으로 단어 w가 문장 임베딩 c에서 출현할 확률이지요.&amp;nbsp;w가 출현할 확률을 두 가지 항을 합하여 계산하는 것에는&amp;nbsp;나름대로 합리적인 이유가 있습니다. 문장 임베딩 c가 문장 내에서 등장하는 단어들과 밀접한 연관을 가져야하는 것은 당연합니다. 다만 모든 단어가 문장 임베딩과 관련이 있지는 않을 겁니다. 문장 내에는 문장의 핵심 의미와는 관련 없는 불용어들도 많이 등장하기 때문이죠. 따라서 문장에 어떤 단어가 등장하는 것에는 크게 두 가지 이유가 있겠죠.&amp;nbsp;&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;&lt;p style=&quot;text-align: left;&quot;&gt;원래 여기저기 자주 등장하는 단어라서 해당 문장에 등장하거나&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p style=&quot;text-align: left;&quot;&gt;문장의 핵심 의미와 관련이 있어서 해당 문장에 등장하거나&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p style=&quot;text-align: left;&quot;&gt;위의 Pr(w|c)는 이를 잘 반영하는 확률 계산법이라고 볼 수 있지요.&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;한 가지 미심쩍은&amp;nbsp;부분은 ^c입니다. ^c는 문장 임베딩에 공통 부분 임베딩인 c0를 섞어서 만든 임베딩입니다. 이게 왜 필요한지는 이론적으로 좀더 고민해볼 여지가 있지만, 주어진 학습 데이터의 편향을 반영하기 위한게 아닐까 싶습니다.&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;c값을 구하기 위해서는 문장 내에 등장하는 모든 단어 w_i들에 대해서 &lt;b&gt;Pr(w_i|c)&lt;/b&gt;가 최대가 되는 c를 찾아야합니다. 이를 위해 양변에 로그를 취하고 미분을 하여 전체 우도가&amp;nbsp;최대가 되는 c를 찾으면 앞서 설명한 SIF 계산식이 나오게 됩니다. 꽤 치밀하지요?&lt;/p&gt;&lt;p style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;성능은?&lt;/h3&gt;&lt;div&gt;논문을 보면 단순 단어 임베딩 평균이나 tf-idf 가중치를 적용한 단어 임베딩 평균뿐만 아니라 Skip-thought와 같이 체계적인 RNN 모형보다도 문장 유사도(similarity), 추론(entailment), 분류(classification) 등의 과제에서&amp;nbsp;SIF 기법이 5~10% 정도 더 높은 성능을 보인다는 것을 확인할 수 있습니다. 기반이 되는 단어 임베딩 기법으로는 Word2Vec보다는 GloVe가 더 높은 성능을 보였구요, Paragram-SL999이라는 반지도 학습 단어 임베딩 기법이 가장 높은 성능을 보였습니다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;p&gt;SIF 기법은 문장 임베딩을 계산할 때 전혀 어순을 고려하지 않기에, 어순을 고려하는 RNN 모형에 비하면 여러 모로 한계가 있을 수 밖에 없습니다. 그런데 SIF가 RNN 기반 모형보다 높은 성능을 보였다는 점은 흥미로울 수 밖에 없죠. 두 가지 가능성이 있을 겁니다. 1) 제시된 자연언어처리 과제에서 높은 성능을 내는 데에&amp;nbsp;어순이 필요없는 것이거나&amp;nbsp;2)&amp;nbsp;RNN 모형이 어순을 고려하는데에 생각보다 한계가 있다던가 말이죠.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;SIF 기법은 그 자체로는 단어 출현 확률을 제외하면, 학습 데이터도 전혀 필요없고, 비지도 방법이라는 장점이 있습니다. 따라서 다양한 과제에 쉽게 적용될 수 있다는 것인데요, SIF 기법을 보면서 때론 간단한 기법들이 간단하기 때문에 더 강력할 수 있는게 아닌가 생각해보게 됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;코드&lt;/h3&gt;&lt;p&gt;사실 워낙 간단한 모형이기 때문에 직접 구현해도 크게 복잡하지 않습니다만, 저자들이 친히 코드 또한 공개해주었습니다.&amp;nbsp;&lt;a href=&quot;https://github.com/PrincetonML/SIF&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://github.com/PrincetonML/SIF&lt;/a&gt;&amp;nbsp;코드 역시 어렵지 않으므로 필요한 곳에 쉽게 적용하여 사용 가능할 듯합니다.&lt;/p&gt;&lt;div class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes&quot;&gt;
&lt;li id=&quot;footnote_631_1&quot;&gt;Arora, S., Liang, Y., &amp; Ma, T. (2016). A simple but tough-to-beat baseline for sentence embeddings. &lt;a href=&quot;#footnote_link_631_1&quot;&gt;[본문으로]&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;footnote_631_2&quot;&gt;Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119). &lt;a href=&quot;#footnote_link_631_2&quot;&gt;[본문으로]&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;footnote_631_3&quot;&gt;Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., &amp; Fidler, S. (2015). Skip-thought vectors. In Advances in neural information processing systems (pp. 3294-3302). &lt;a href=&quot;#footnote_link_631_3&quot;&gt;[본문으로]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/631&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>그냥 공부</category>
<category>word2vec</category>
<category>단어 임베딩</category>
<category>자연언어처리</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/631</guid>
<comments>https://bab2min.tistory.com/631#entry631comment</comments>
<pubDate>Wed, 24 Apr 2019 21:18:54 +0900</pubDate>
</item>
<item>
<title>[c++] CRTP를 이용한 다단계 정적 상속으로 코드 최적화하기</title>
<link>https://bab2min.tistory.com/630</link>
<description>&lt;p&gt;상속은 객체지향 프로그래밍의 꽃이라고 할 수 있습니다. 상속을 통해 공통되는 코드를 통합하고, 다형성을 확보하는 등 다양한 작업이 가능하지요. C++에서는 일반적으로 클래스와 가상 함수, 상속이라는 문법적 장치를&amp;nbsp;통해 이러한 개념들이 구현됩니다. 어떤 Data에 대한 처리를 수행하는 클래스 ModelA이 있다고 생각해봅시다.&lt;/p&gt;
&lt;p&gt;&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;struct Data
{
    int foo, bar;
};

class ModelA
{
    std::vector&amp;lt;Data&amp;gt; myData;
public:
    virtual void loadData()
    {
        // myData에 값들을 채워넣는다~~
    }

    virtual void work()
    {
        // myData를 가지고 어떠한 처리를 한다~~
    }

    void loadAndWork()
    {
        loadData();
        work();
    }

    virtual ~ModelA() {}
};
&lt;/textarea&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;ModelA는 두 종류의 가상 함수를 가집니다. loadData()는 어딘가로부터 데이터를 읽어와 myData에 채워넣는 일을 하고, work()는 채워진 myData를 가지고 어떠한 작업을 수행하는 일을 합니다. loadAndWork()를 호출하면 loadData()와 work()가 호출되는 식이죠. loadData()와 work()는 가상함수이기 때문에 상속받은 클래스에서 이 녀석들을 override할 수 있습니다.&lt;/p&gt;
&lt;p&gt;그래서 만약 Data에 대해 조금 다른 처리를 해야하는 경우가 있다고 하면 ModelA2와 같이 상속을 통해서 ModelA의 기능을 확장할 수 있습니다.&lt;/p&gt;
&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;class ModelA2 : public ModelA
{
public:
    virtual void work()
    {
        // myData를 가지고 조금 다른 처리를 한다~~
    }

    virtual ~ModelA2() {}
};
&lt;/textarea&gt;
&lt;p&gt;ModelA2는 ModelA가 될 수 있기 때문에 ModelA이든 ModelA2이든 ModelA*로 추상화하여 ModelA::loadAndWork()를 호출할 수 있습니다. ModelA냐 ModelA2냐에 따라 내부 동작은 달라질 수 있겠지만, 그것까지 복잡하게 고민할 필요가 전혀 없습니다. 상속을 사용하여 아주 훌륭한 설계를 했다고 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;그런데 문제가 조금 복잡해지기 시작합니다. ModelA가 Data를 가지고 작업을 잘 수행하고 있는데, 기존의 Data에다가 몇개의 추가 필드를 필요로하는 ModelB를 만들어야 합니다.&lt;/p&gt;
&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;struct DataB : public Data
{
    // int foo, bar; // Data가 가지고 있는 기본 필드들
    float boo; // 이 녀석을 추가적으로 필요로 한다
};

/* ModelA와 겹치는 부분들이 많지만, 
myData가 std::vector&amp;lt;DataB&amp;gt; 타입이 되어야해서
상속을 사용하지 못했습니다. */
class ModelB
{
    std::vector&amp;lt;DataB&amp;gt; myData;
public:
    virtual void loadData()
    {
        // myData에 값들을 채워넣는다~~
    }

    virtual void work()
    {
        // myData를 가지고 어떠한 처리를 한다~~
    }

    void loadAndWork()
    {
        loadData();
        work();
    }
};
&lt;/textarea&gt;
&lt;p&gt;ModelB는 ModelA와 대부분 하는 작업이 동일하고 그 중 몇 가지 기능만 확장된 거라서 대부분의 코드를 재사용하고 싶습니다. 그런데 &lt;b&gt;ModelA::myData&lt;/b&gt;는 &lt;b&gt;std::vector&amp;lt;Data&amp;gt;&lt;/b&gt;이기 때문에 DataB를 담을 수 없습니다. 그래서 전체 클래스를 일일히 새로 작성했습니다. 설계가 뭔가 잘못된것 같습니다. 어떻게 하면 ModelA를 재사용하여 ModelB를 만드는데에 쓸 수 있을까요? 여러 가지 선택지가 있겠죠.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;그냥 &lt;b&gt;Data&lt;/b&gt;에 &lt;b&gt;float boo&lt;/b&gt;를 추가하고, &lt;b&gt;ModelB&lt;/b&gt;는 &lt;b&gt;ModelA&lt;/b&gt;를 상속받는다&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Data&lt;/b&gt;에 &lt;b&gt;virtual &lt;/b&gt;소멸자를 추가하고, &lt;b&gt;ModelA::myData&lt;/b&gt;를 &lt;b&gt;std::vector&amp;lt;Data*&amp;gt;&lt;/b&gt;로 바꾸어 &lt;b&gt;myData&lt;/b&gt;가 &lt;b&gt;Data &lt;/b&gt;및 &lt;b&gt;Data&lt;/b&gt;의 하위 타입을 담을 수 있게 한다.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ModelA&lt;/b&gt;와 &lt;b&gt;ModelB&lt;/b&gt;에 공통적으로 사용되는 코드들을 &lt;b&gt;ModelBase &lt;/b&gt;추상 클래스로 따로 뽑아내고 &lt;b&gt;ModelA &lt;/b&gt;및 &lt;b&gt;ModelB&lt;/b&gt;가 이를 상속받도록 한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1번의 경우, 추가적으로 들어가야할 것들이 더 생길 때마다 Data 구조체를 수정해야합니다. 그에 따라 전체 클래스의 내용도 다 바뀌게 됩니다. 지금은 추가된 데이터가 float boo 하나지만, 이런게 더 늘어날수록,&amp;nbsp;ModelA의 경우 사용하지도 않을 데이터들을 myData에 다 담고 있어야합니다. 올바른 선택지는 아닌듯합니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2번의 경우 Data를 POD에서 가상함수를 사용하는 class로 바꾸어버립니다. myData가 Data의 하위타입들을 저장할 수 있게 되어 매우 유연한 구조를 띄게 됩니다. 성능을 제외한다면 완벽한 선택지가 될겁니다. 성능을 제외한다면 말이죠... 큰 문제는 Data가 더이상 POD 타입이 아니게 되고, myData는 Data*를 들고 있게 되어 성능 상의 손실이 크다는 것입니다. Data*를 생성할 때마다 메모리할당이 일어나게 되고, Data* 내의 필드에 접근하기 위해서 간접 참조가 발생하죠. 또한 ModelB에서는 Data*를 사용할 때마다 DataB*인지 검사하여 캐스팅하는 작업이 필요합니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3번의 경우 ModelA와 ModelB가 일부 다를 경우 이를 묶어내기가 까다롭다는 문제가 있습니다. ModelA::loadData()는 Data타입인 벡터에 대해 로딩 작업을 수행할 것이고, ModelB::loadData()는 DataB 타입인 벡터에 대해 로딩 작업을 수행할 것입니다. 그렇기 때문에 두 작업의 공통 부분을 묶어내기 위해서는 std::vector&amp;lt;Data&amp;gt;에 대한 접근과&amp;nbsp;std::vector&amp;lt;DataB&amp;gt;에 대한 접근을 추상화해야 합니다. 지금은 2가지라서 문제 없지만 DataC, DataD가 더 생긴다면 쉽지 않은 작업이 되겠죠..?&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;높은 성능을 추구하면 구조가 지저분해지고, 깔끔한 구조를 찾으면 성능을 포기하게 되는 딜레마에 봉착하게 됐습니다. (사실 객체 지향 프로그래밍이라는 것이 본디 성능을 약간 포기하여 유지보수가 편한 코드를 짜기 위한것이긴 합니다.) 다행히도 C++에는 template이라는 강력한 메타 프로그래밍 문법이 있어서 성능과 구조 두 마리 토끼를 모두 잡는 것이 가능합니다. (그대신 코딩 난이도와 컴파일 시간은 놓쳐버리게 되지만요)&lt;/p&gt;
&lt;p&gt;&lt;b&gt;CRTP&lt;/b&gt;(Curiously Recurring Template Pattern; 기묘하게 재귀하는 템플릿 패턴)이라 불리는 c++ 테크닉이 있습니다. 이는 다음과 같이 클래스 상속과 템플릿을 꼬아서 사용하는 방법을 가리키는 표현입니다.&lt;/p&gt;
&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;// 부모 클래스를 템플릿 클래스로 구현
template&amp;lt;class _Derived&amp;gt;
class Foo
{
protected:
    void impl()
    {
        // 상속 가능한 작업들..
    } 
public:
    void call()
    {
// 다음 구문에서 어떤 impl()이 호출될지는 컴파일 타임에 결정됨
        static_cast&amp;lt;_Derived*&amp;gt;(this)-&amp;gt;impl();
    }
};

// 자식 클래스는 자신을 템플릿 파라미터로 넣은 부모 클래스를 상속받음
class Bar : public Foo&amp;lt;Bar&amp;gt;
{
    friend Foo&amp;lt;Bar&amp;gt;;
protected:
    void impl()
    {
        // Foo::impl을 상속받아서 더 복잡한 작업을 수행
    }
};
&lt;/textarea&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;class Bar : public Foo&amp;lt;Bar&amp;gt;&lt;/b&gt;&amp;nbsp;가 이 기법의 핵심입니다. 굉장히 기묘해보이지만, 사실 class Bar 부분에서 이미 Bar가 선언되었으므로 뒤의 Foo&amp;lt;Bar&amp;gt;에서 Bar를 파라미터로 사용하는게 합법입니다. 따라서 &lt;b&gt;Foo&amp;lt;Bar&amp;gt;::call()&lt;/b&gt;에서는 자연히 &lt;b&gt;Bar::impl()&lt;/b&gt;을 호출하게 되고, 이를 통해 다형성을 성취할 수 있게 됩니다. 이 기법을 사용할 경우 virtual 함수가 없어지므로 모든 함수 호출은 정적으로 바인딩되어, 간접 참조가 줄어들고, 또한 컴파일 타임에 어떤 함수가 불릴지 알 수 있으므로 추가적인 컴파일 타임 최적화가 가능해집니다. 짜잔!&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이 기법을 활용해서 위의 문제를 해결해보도록 하겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;struct Data
{
    int foo, bar;
};

/*
_Derived 파라미터는 상속받을 타입이고, 
_Data 파라미터는 내부적으로 사용할 데이터 타입입니다.
*/
template&amp;lt;class _Derived, class _Data&amp;gt;
class ModelA
{
    std::vector&amp;lt;_Data&amp;gt; myData;
public:
    void loadData()
    {
        // myData에 값들을 채워넣는다~~
    }

    void work()
    {
        // myData를 가지고 어떠한 처리를 한다~~
    }

    void loadAndWork()
    {
        static_cast&amp;lt;_Dervied*&amp;gt;(this)-&amp;gt;loadData();
        static_cast&amp;lt;_Dervied*&amp;gt;(this)-&amp;gt;work();
    }
};

class ModelA2 : public ModelA&amp;lt;ModelA2, Data&amp;gt;
{
    friend ModelA&amp;lt;ModelA2, Data&amp;gt;;
public:
    void work()
    {
        // myData를 가지고 조금 다른 처리를 한다~~
    }
};
&lt;/textarea&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이렇게 해서 ModelA2가 ModelA를 상속받게 할 수 있습니다. 그런데 그냥 ModelA를 쓰고 싶을때는 어떻게 해야할까요..?? ModelA&amp;lt;??, Data&amp;gt;라고 써야할텐데, 상속받은게 아니라 자기 자신을 그대로 쓰는거라서 ??에 무엇을 넣어야할지 모르겠습니다. 이를 해결하기 위해 다음과 같이 코드를 고쳐봅시다.&lt;/p&gt;
&lt;p&gt;&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;struct Data
{
    int foo, bar;
};

/*
_Derived 파라미터는 상속받을 타입이고, 
_Data 파라미터는 내부적으로 사용할 데이터 타입입니다.
*/
template&amp;lt;class _Derived = void, class _Data = Data&amp;gt;
class ModelA
{
// _Derived == void이면 Dervied를 ModelA&amp;lt;&amp;gt;로 설정, 그 외에는 Dervied를 _Derived로 설정
    using Derived = typename std::conditional&amp;lt;
        std::is_same&amp;lt;_Derived, void&amp;gt;::value,
        ModelA&amp;lt;&amp;gt;, _Derived&amp;gt;::type;
    std::vector&amp;lt;_Data&amp;gt; myData;
public:
    void loadData()
    {
        // myData에 값들을 채워넣는다~~
    }

    void work()
    {
        // myData를 가지고 어떠한 처리를 한다~~
    }

    void loadAndWork()
    {
/* 다음 구문들은 _Derived가 void일 경우에는 ModelA&amp;lt;&amp;gt;, 
즉 자기자신의 loadData()와 work()를 호출합니다*/
        static_cast&amp;lt;Dervied*&amp;gt;(this)-&amp;gt;loadData();
        static_cast&amp;lt;Dervied*&amp;gt;(this)-&amp;gt;work();
    }
};

class ModelA2 : public ModelA&amp;lt;ModelA2, Data&amp;gt;
{
    friend ModelA&amp;lt;ModelA2, Data&amp;gt;;
public:
    void work()
    {
        // myData를 가지고 조금 다른 처리를 한다~~
    }
};

// 이제 다음과 같이 ModelA와 ModelA2를 사용할 수 있음
ModelA&amp;lt;&amp;gt; a;
ModelA2 a2;
&lt;/textarea&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;여기서는 using 구문이 포인트가 됩니다. _Derived가 void이면 자기 자신의 멤버 함수들을 호출하고, void가 아니면 상속하는 클래스의 멤버 함수를 호출하도록 컴파일 타임에 분기합니다. 참고로 using 구문은 c++11에서 지원하는, 타입 이름에 별칭을 붙여주는 구문입니다. typedef이 업그레이드 버전이라고 생각하시면 될것 같습니다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;std::conditional&amp;lt;A, B, C&amp;gt;&lt;/b&gt;의 경우 A가 참이면 B 타입, 거짓이면 C타입을 돌려주는 템플릿 세계의 삼항 연산자(?:) 같은 녀석입니다. &lt;b&gt;std::is_same&amp;lt;A, B&amp;gt;&lt;/b&gt;는 그 이름 그대로&amp;nbsp;A, B가 같은 타입이면 참, 다른 타입이면 거짓을 돌려주는 템플릿 세계의 비교 연산자이구요. 즉 일반 연산자를 이용해서 표현하자면 &lt;b&gt;Derived = (_Derived == void) ? ModelA : _Derived;&lt;/b&gt; 와 같은 의미의 템플릿 구문입니다.&lt;/p&gt;
&lt;p&gt;마지막으로 &lt;b&gt;std::condtional&lt;/b&gt; 앞에 &lt;b&gt;typename&lt;/b&gt;을 붙인것은, 이것이 템플릿 파라미터에 종속되어 있는 scope의 타입(type in dependent scope)이기 때문에, 뒤에 오는 것이 type이라는것을 알려주기 위해 붙인 것입니다. 컴파일러는 코드 해석 중&amp;nbsp;이름(identifier)들을 만나면 이게 타입인지&amp;nbsp;변수명인지를 구분하려고 합니다. 그래야 int a;와 같은 구문이 변수 선언 구문인지를 알 수 있으니까요. 그런데 템플릿 클래스 scope 안에 속한 이름의 경우 템플릿을 해석하기 전까지는 이게 타입을 가리키는 이름인지 변수를 가리키는 이름인지 알 수가 없습니다. 일단 코드 전체를&amp;nbsp;해석해야 템플릿을 해석할 수 있는데, 코드 해석 단계에서 해당 이름이 타입 이름인지 모르므로 코드 해석 자체가 불가능해집니다. 이런 경우를 막기 위해서, 템플릿에 종속되어 있는 scope의 타입은 반드시 그 앞에 &lt;b&gt;typename&lt;/b&gt;을 붙여 이것이 타입을 가리킨다는 것을 사전에 명시하도록 되어있습니다. 설명은 여기까지 하고 ModelB도 같은 방법으로 구현해봅시다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;struct DataB : public Data
{
    // int foo, bar; // Data가 가지고 있는 기본 필드들
    float boo; // 이 녀석을 추가적으로 필요로 한다
};

class ModelB : public ModelA&amp;lt;ModelB, DataB&amp;gt;
{
    friend ModelA&amp;lt;ModelB, DataB&amp;gt;;
public:
    void loadData()
    {
        // myData에 값들을 채워넣는다~~
    }

    void work()
    {
        // myData를 가지고 어떠한 처리를 한다~~
    }
};
&lt;/textarea&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;그런데, 한 가지 문제가 또 있습니다. ModelB를 상속받는 ModelB2를 만들고 싶은데, ModelB는 템플릿 클래스가 아니라 일반 클래스라 CRTP 패턴을 사용할 수가 없습니다. 이를 해결하기 위해선 어떻게 ModelB를 고쳐야할까요?&lt;/p&gt;
&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;template&amp;lt;class _Derived = void, class _Data = DataB&amp;gt;
class ModelB : public ModelA&amp;lt;
    typename std::conditional&amp;lt;
        std::is_same&amp;lt;_Derived, void&amp;gt;::value, 
        ModelB&amp;lt;&amp;gt;, _Derived&amp;gt;::type,
    _Data&amp;gt;
{
    using Derived = typename std::conditional&amp;lt;
        std::is_same&amp;lt;_Derived, void&amp;gt;::value, 
        ModelB&amp;lt;&amp;gt;, _Derived&amp;gt;::type;
    using Base = ModelA&amp;lt;Derived, _Data&amp;gt;;
    friend Base;
public:
    void loadData()
    {
        // myData에 값들을 채워넣는다~~
    }

    void work()
    {
        // myData를 가지고 어떠한 처리를 한다~~
    }
};

class ModelB2 : public ModelB&amp;lt;ModelB2&amp;gt;
{
    using Base = ModelB&amp;lt;ModelB2&amp;gt;;
    friend Base;
    friend Base::Base;
public:
// ...
};
&lt;/textarea&gt;
&lt;p&gt;정답은 위와 같이 ModelB에도 템플릿 파라미터를 붙이는 것입니다. 그러면 이를 이용해 ModelB2도 정적 상속이 가능합니다. ModelB2의 상속 계보도를 보면, 먼저 ModelB2는 ModelB&amp;lt;ModelB2&amp;gt;를 상속받습니다. 그리고 ModelB&amp;lt;ModelB2&amp;gt;는 ModelA&amp;lt;ModelB2, DataB&amp;gt;를 상속받지요. 따라서&amp;nbsp;&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p style=&quot;text-align: center;&quot;&gt;ModelA&amp;lt;ModelB2, DataB&amp;gt; → ModelB&amp;lt;ModelB2&amp;gt; →&amp;nbsp;ModelB2&lt;/p&gt;&lt;/div&gt;&lt;p&gt;로 핏줄이 이어집니다. 여기에 추가적으로 상속이 더 필요하다면, ModelB2도 템플릿 클래스화하면 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;마지막 문제가 하나 남았는데, 이렇게 상속받은 각각의 클래스들은 실제로는 공통의 조상이 하나도 없습니다. 따져보면&lt;/p&gt;&lt;ul style=&quot;list-style-type: disc;&quot;&gt;&lt;li&gt;ModelA&amp;lt;&amp;gt; : 최종 조상 ModelA&amp;lt;void, Data&amp;gt;&lt;/li&gt;&lt;li&gt;ModelA2 : 최종 조상 ModelA&amp;lt;ModelA2, Data&amp;gt;&lt;/li&gt;&lt;li&gt;ModelB&amp;lt;&amp;gt; : 최종 조상 ModelA&amp;lt;ModelB&amp;lt;&amp;gt;, DataB&amp;gt;&lt;/li&gt;&lt;li&gt;ModelB2&amp;nbsp;: 최종 조상 ModelA&amp;lt;ModelB2, DataB&amp;gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;와 같이 각자의 조상들이 다릅니다. 따라서 이들을 하나의 포인터 타입 ModelA*에 담을 수 없다는 문제가 있지요. 정적 상속을 통해 정적인 다형성을 확보함과 동시에 동적인 다형성을 잃어버린 것이죠. 만약 동적인 다형성까지 필요로 한다면, 여기에 공통적인 인터페이스(Interface)를 추가해주는 방법을 사용할 수 있습니다.&lt;/p&gt;
&lt;textarea name=&quot;code&quot; class=&quot;c&quot;&gt;class IModel
{
public:
    // 인터페이스에 필수적으로 필요한 가상 소멸자!
    virtual ~ModelInterface() {}

    // 외부로 드러내고 싶은 method들
    virtual void loadAndWork() = 0;
};

template&amp;lt;class _Derived, class _Data&amp;gt;
class ModelA : public IModel
// ... 하략 ...
&lt;/textarea&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이렇게 Model의 공통 함수들을 묶어낼 IModel 인터페이스를 정의한 후 최종 조상인 ModelA 템플릿 클래스가 IModel을 구현하도록 코드를 수정하면, ModelA, ModelA2, ModelB, ModelB2가 모두 공통 조상인 IModel을 가지게 됩니다. 따라서 IModel*로 모든 종류의 Model을 다룰 수 있게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이렇게 CRTP와 템플릿 상속을 이용하면 기존의 class 상속만으로는 불가능했던 코드 재사용 및 최적화가 가능해집니다. (Java 같은 언어에는 없는, 대단히 강려크한&amp;nbsp;기능..! 이 정도는 되어야 코드 좀 재사용해봤다고 할 수 있는거죠.) 물론 이 방법도 단점이 있습니다. 크게 정리하면 다음과 같은 단점들이 있겠네요.&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;부모 클래스가 템플릿 클래스여야만 하므로, 모든 부모 클래스 구현이 헤더 파일에 들어가야한다. 즉, 구현체 전체가 다 헤더로 공개될 수 밖에 없다.&lt;/li&gt;&lt;li&gt;템플릿이 복잡하게 얽혀있을 경우 오류가 발생하면 이를 잡기가 까다롭다. (유지 보수 난이도 상승)&lt;/li&gt;&lt;li&gt;각 클래스에 따른 실행 코드가 따로 생성되므로 실행 파일의 크기가 커질 수 있다.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;따라서 결론은 &quot;적재적소에 적절하게 사용하면 좋다&quot;가 되겠습니다.&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/630&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>테크닉</category>
<category>c++</category>
<category>템플릿 메타프로그래밍</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/630</guid>
<comments>https://bab2min.tistory.com/630#entry630comment</comments>
<pubDate>Thu, 07 Mar 2019 20:41:00 +0900</pubDate>
</item>
<item>
<title>[Python] Segmented Least Squares를 이용해 구간 나누기</title>
<link>https://bab2min.tistory.com/629</link>
<description>&lt;p&gt;최소제곱법(Least Square Approximation)은 데이터를 근사하는 모형을 찾는데 흔히 사용하는 방법입니다. 참값과 근사값의 오차의 &lt;b&gt;제곱&lt;/b&gt;합이 &lt;b&gt;최소&lt;/b&gt;가 되게한다고 해서 최소제곱법이라고 부르지요. 대표적인 사례가 선형회귀입니다. 두 변수가 가지는 관계를 좌표평면 상에 늘어놓고, 데이터의 분포를 최대한 잘 근사하는 직선을 찾는 일입니다. 그 형태가 간단하고&amp;nbsp;닫힌 해가 알려져 있어서 통계학에서는 기초 중의 기초로 널리 쓰이고 있습니다.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:600px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/999666375C7645AA24&quot; filemime=&quot;image/jpeg&quot; filename=&quot;600px-Linear-regression.svg.png&quot; height=&quot;480&quot; width=&quot;600&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;근데 때로는 전체 데이터의 분포가 하나의 직선으로 표현하기에 어려운 경우도 있습니다. 이 경우 선형이 아닌 좀 더 복잡한 모형을 사용하는 방법을 쓸 수도&amp;nbsp;있고, 선형 모형 여러개를 결합하여 데이터를 표현하는 방법을 쓸 수도 있습니다.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/999CF3405C7648D319&quot; filemime=&quot;image/jpeg&quot; filename=&quot;regression.PNG&quot; height=&quot;331&quot; original=&quot;yes&quot; width=&quot;800&quot;/&gt;&lt;span class=&quot;cap1&quot; style=&quot;display:block;max-width:100%;width:800px;&quot;&gt;(왼쪽은 비선형 모형을 사용하는 경우, 오른쪽은 선형 모형 2개를 결합하는 경우)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;오늘의 주제는 오른쪽과 같이 복잡한 형태를 쪼개어 직선 여러 개로 표현하는 기법에 대한 것입니다.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;Segmented Least Squares Problem&lt;/h2&gt;&lt;p&gt;위와 같이 임의의 데이터에 대해 이를 최대한 잘 근사하는 직선의 조합을 찾는 문제를 Segmented Least Squares Problem이라고 합니다. 이 때 전체 데이터를 쪼개는 개수에는 제한을 두지 않습니다. 즉 위의 그림의 경우 2개로 쪼갠것이지만, 필요에 따라 더 많이 쪼갤수도 있고 더 적게 쪼갤수도 있겠죠. 물론 당연히 더 잘게 쪼갤수록 더 정확하게 근사가 가능할테니 너무 많이 쪼갤 경우에는 페널티를 주어야 합니다. 이를 염두해두면서 이 문제를 조금더 수학적으로 정밀하게 써보도록 하겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;&lt;b&gt;x&lt;small&gt;1&lt;/small&gt; &amp;lt;= x&lt;small&gt;2&lt;/small&gt;&amp;nbsp;&amp;lt;= ... &amp;lt;= x&lt;small&gt;N&lt;/small&gt;&lt;/b&gt; 으로 순서대로 정렬된&amp;nbsp;&lt;b&gt;P =&amp;nbsp;{(x&lt;small&gt;1&lt;/small&gt;, y&lt;small&gt;1&lt;/small&gt;), (x&lt;small&gt;2&lt;/small&gt;, y&lt;small&gt;2&lt;/small&gt;), ... (x&lt;small&gt;N&lt;/small&gt;, y&lt;small&gt;N&lt;/small&gt;)}&lt;/b&gt;가 있고, &lt;b&gt;P&lt;/b&gt;의 부분집합&amp;nbsp;&lt;b&gt;S(a,b)&amp;nbsp;= {(x&lt;small&gt;i&lt;/small&gt;, y&lt;small&gt;i&lt;/small&gt;) | a &amp;lt;= i &amp;lt; b}&lt;/b&gt;에 대한 근사 오차를 &lt;b&gt;Err(S(a,b))&lt;/b&gt;라고 하자. 이 때 전체 오차&amp;nbsp;&lt;b&gt;∑&lt;small&gt;i&lt;/small&gt;&amp;nbsp;{Err(S(t&lt;small&gt;i&lt;/small&gt;, t&lt;small&gt;i+1&lt;/small&gt;)) + C}&lt;/b&gt;를 최소로 하는&amp;nbsp;&lt;b&gt;L&lt;/b&gt;개의 분할지점 &lt;b&gt;t&lt;small&gt;1&lt;/small&gt; &amp;lt; t&lt;small&gt;2&lt;/small&gt; &amp;lt; ... &amp;lt; t&lt;small&gt;L&lt;/small&gt;&lt;/b&gt; 을 찾으시오. (단, &lt;b&gt;t&lt;small&gt;1&lt;/small&gt; = x&lt;small&gt;1&lt;/small&gt;, t&lt;small&gt;L&lt;/small&gt; = x&lt;small&gt;N&lt;/small&gt;&lt;/b&gt;)&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;여기서 &lt;b&gt;C&lt;/b&gt;는 분할에 대한 페널티 값입니다. 전체 분할의 개수가 늘어날수록 전체 오차에 합쳐지는 C값이 많아지므로, 분할을 키우지 못하게 막습니다. 따라서 C값이 클수록 적게 분할하게 되고, C값이 작을수록 많이 분할하게 되겠죠. 복잡해 보이는 이 문제는 다행히도 동적 프로그래밍(Dynamic Programming)을 이용하여 다항시간 내에 풀 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;동적 프로그래밍을 이용한 풀이&lt;/h2&gt;&lt;p&gt;동적 프로그래밍이란 이름은 참&amp;nbsp;거창하지만, 사실 이 기법은 단순히 이전 값을 저장해두고 다시 사용하겠다는 개념일 뿐입니다. 우리가 원하는 최소 전체 오차를 &lt;b&gt;Opt&lt;/b&gt;라고 정의하면 다음과 같은 식이 성립합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;&lt;b&gt;Opt( {} ) = 0&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Opt(S(1, u)) = min &lt;small&gt;0&amp;lt;= i &amp;lt;&amp;nbsp;u&lt;/small&gt;{ Opt(S(1, i)) + Err(S(i+1, u)) + C&amp;nbsp;}&lt;/b&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;먼저 당연하게도 공집합에 대해서는 그 최소&amp;nbsp;오차값은 0입니다. 그리고 1~u까지의 데이터를 포함하는 집합에 대한 오차는, 1~i까지의 데이터에 대한 최소 오차에 나머지 i+1~u까지의 데이터에 대한 오차, 그리고 분할 페널티 값인 C를 합한 값으로 계산할 수 있습니다. i가 몇이냐에 따라 그 값이 바뀔텐데 모든 가능한 i에 대해 계산을 실시하고 가장 작은 값을 Opt(S(1, u))로 정의하면 최소 오차를 구할 수 있겠죠.&lt;/p&gt;&lt;p&gt;따라서 u = 1에서부터 시작하여 전체 데이터의 개수인 N까지 키워가며 Opt를 계산하면 전체 데이터 집합에 대한 최소 오차값을 구할 수 있습니다. 참 쉽죠?&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;기출 변형..! 글자 크기에 따라 단락을 구분해라!&lt;/h2&gt;&lt;p&gt;OCR을 수행하다 보면 한 문서 내에서도 글자 크기가 다 다른 경우를 만날수가 있습니다. 지금 보고 계신 이 문서만 하더라도 헤드라인이 나오고 그 다음에는 작은 글자들이 반복되다가 또 다른 헤드라인이 나오는 것을 볼 수 있습니다. 이 연속된 행들을 동질적인 녀석들끼리 묶어서 단락을 구분지어주려면 어떻게 해야할까요?&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;문서가 디지털로 깔끔하게 정리되어 있지 못하여 OCR을 수행하는 경우 각 글자마다 그 높이값이 다 다르게 측정됩니다. 10pt로 쓴 글일지라도 9.8pt, 10.1pt, 10.5pt ... 와 같이 측정되는 일이 비재합니다. 따라서 단순히 값이 같은 경우만 한 단락으로 묶기에는 어려운 경우가 많죠.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이 경우 위에서 설명했던 Segmented Least Squares 풀이법을 활용할 수 있습니다. 단 데이터가 (x, y) 형태의 평면 상의 점이 아니라 (n, x)처럼 행 번호와 해당 행의 글자 크기로 표현되는 식이죠. 그리고 이 때 Err는 해당 집합 내의 글자 크기들의 평균으로부터의 오차 제곱합으로 정의할 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;간단하네요! 그럼 바로 Python3 코드로 구현해보도록 하겠습니다.&lt;/p&gt;&lt;p&gt;&lt;textarea name=&quot;code&quot; class=&quot;python&quot;&gt;import numpy as np

arr = [12.1, 12.7, 12.0, 10, 10.1, 9.8, 9.9, 13.1, 9.8, 9.9, 10.0]


def segmentLS(arr, C):
    arr = np.array(arr)
    err = np.zeros((len(arr), len(arr)))
    opt = np.zeros(len(arr) + 1)
    slicePoint = [()] * (len(arr) + 1)
    # Err 을 먼저 계산하여 저장해둔다
    for j in range(len(arr)):
        for i in range(j):
            err[i, j] = np.std(arr[i:j + 1]) ** 2 * (j+1-i)
            # 그 후 Opt를 차례대로 계산해나간다
    for j in range(1, len(arr) + 1):
        opt[j], minP = min((err[i, j - 1] + C + opt[i], i) for i in range(j))
        # slicePoint에는 분할지점을 저장해둔다
        slicePoint[j] = slicePoint[minP] + (minP,)
    final = slicePoint[-1] + (len(arr),)
    # 최소 오차값과 분할된 리스트를 함께 반환한다
    return opt[-1], [list(arr[a:b]) for a, b in zip(final[:-1], final[1:])]


print(*segmentLS(arr, 10))
# 27.3872 [[12.1, 12.7, 12.0, 10.0, 10.1, 9.8, 9.9, 13.1, 9.8, 9.9, 10.0]]
print(*segmentLS(arr, 1))
# 4.35666 [[12.1, 12.7, 12.0], [10.0, 10.1, 9.8, 9.9], [13.1], [9.8, 9.9, 10.0]]
print(*segmentLS(arr, 0.1))
# 0.669999 [[12.1], [12.7], [12.0], [10.0, 10.1, 9.8, 9.9], [13.1], [9.8, 9.9, 10.0]]
&lt;/textarea&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;십 여 줄의 코드로 간단하게 구현되었습니다. (역시 간단한 실험 돌려볼때는 파이썬만한게 없죠)&lt;/p&gt;&lt;p&gt;보시면 입력 데이터는 [&lt;span style=&quot;color: rgb(255, 0, 0);&quot;&gt;12.1, 12.7, 12.0&lt;/span&gt;, &lt;span style=&quot;color: rgb(0, 85, 255);&quot;&gt;10, 10.1, 9.8, 9.9&lt;/span&gt;, &lt;span style=&quot;color: rgb(255, 0, 0);&quot;&gt;13.1&lt;/span&gt;, &lt;span style=&quot;color: rgb(0, 85, 255);&quot;&gt;9.8, 9.9, 10.0&lt;/span&gt;]으로, 처음에는 12와 같이 큰 값이 3개 연속 등장하다가, 그 다음에는 비교적 작은 값이 4개, 그리고 다시 큰값 1개, 다시 작은값 3개가 등장하고 있습니다. C값을 적절하게 잘 부여할 경우, 이 차이를 반영하여 전체 데이터셋을 4개로 분할해줄 수 있습니다.&lt;/p&gt;&lt;p&gt;본 테스트에서는 C=10일때는 전체를 하나로 묶어 버렸고, C=1일때는 우리의 예상대로 4단계로 분할했습니다. 마지막으로 C=0.1일때는 과도하게 많이 분할해버렸구요.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;실제로 이 기법을 통해서 OCR된 문서에서 글자 크기가 달라지는 줄을 찾아서 단락을 구분해줄 수 있었습니다. (사실 단순히 글자 크기만 사용해서는 원하는것처럼 깔끔한 결과가 나오진 않았는데요, 다른 자질들을 좀더 이용하도록 다른 방법도 찾아봐야겠습니다.)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;참고 문헌&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://kartikkukreja.wordpress.com/2013/10/21/segmented-least-squares-problem/&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://kartikkukreja.wordpress.com/2013/10/21/segmented-least-squares-problem/&lt;/a&gt; SLS 문제에 대해서 설명이 깔끔하게 잘 되어있는 블로그입니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/629&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>프로그래밍</category>
<category>python</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/629</guid>
<comments>https://bab2min.tistory.com/629#entry629comment</comments>
<pubDate>Wed, 27 Feb 2019 18:23:48 +0900</pubDate>
</item>
<item>
<title>Seq2seq를 이용한 텍스트 Autoencoder + 이를 이용한 클러스터링</title>
<link>https://bab2min.tistory.com/628</link>
<description>&lt;h2&gt;이론&lt;/h2&gt;&lt;p&gt;오토인코더(Autoencoder)라는 개념이 있습니다. 쉽게 말하면, 어떤 상자 안에 값 x를 넣으면 그와 동일한 값인&amp;nbsp;x를 출력하도록 하는 녀석을 말합니다. 입력값을 그대로 출력해주면 되는 것인데, 이게 어떤 의미가 있냐 의아할 수 있습니다. 하지만 데이터 x가 단순히 하나의 숫자가 아니라, 여러 개의 숫자로 표현되는 복잡한 값(행렬이라던지, 이미지라던지, 음성이라던지...)이고, 상자 안의 저장 공간이 해당 입력을 전부 저장하기엔 부족하다면 이는 쉽지 않은 작업이 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;좀더 쉬운 예를 들자면, 숫자를 10개까지밖에 기억못하는 사람(오토인코더)한테 동시에 숫자 100개를 들려주고, 그대로 그걸 다시 말해보라고 하는 것이죠. 기억력에 한계가 있기 때문에 숫자 100개를 다 못 외울것 같지만, 계속 이걸 외우라고 협박하면, 갖은 방법을 동원해서 어떻게든 100개의 숫자를 비슷하게 기억하게 되겠죠.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;이게 가능해지면 다양한 작업이 편해집니다. 일단 숫자 10개만 기억하면 여기서 숫자 100개를 복원해낼 수 있으므로, 숫자 100개의 데이터를 숫자 10개의 데이터로 표현할 수 있습니다&amp;nbsp;(차원 축소). 게다가 이때의 숫자 10개는 원래 숫자 100개를 복원하는데에&amp;nbsp;가장 핵심적인 정보들만 뽑혀 있을 것이므로, 이 값을 전체 숫자 100개를 표현하는 주요 자질로 사용할 수 있습니다 (자질 추출). 그리고 핵심적인 정보인 숫자 10개의 값을 조절하면 다양한 100개짜리 숫자를 만들어 낼수도 있구요 (생성 모형).&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;좀더 엄밀하게 말하면, n개로 이뤄진 벡터 집합&amp;nbsp;X = {x1, x2, x3, ... , x_n}의 모든 원소에 대해&amp;nbsp;f(x_i)의 값과&amp;nbsp;x_i 값 사이의 오차를 최소화하도록 하는 함수 f를 찾는 것입니다. 이 때 f의 구조는 x_i 전체를 저장할 수 없게 하면, f는 x_i의 패턴 중 중요한 것을 찾아 이를 압축하는 표현을 찾게 됩니다. 이를 활용하는 것이 Autoencoder의 핵심적인 개념입니다. 주로 인공 신경망을 여러 층 쌓는 방법으로 구현하며, 차원 축소 및 자질 추출, 생성 모형에 유용하게 널리 사용되고 있습니다. (&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://en.wikipedia.org/wiki/Autoencoder&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;하지만, 흔히 알려진 Autoencoder 모형은 텍스트 데이터에 적용하기에 적합하지 않습니다. 텍스트는 전형적인 시간에 종속적인 데이터 타입이고, 그 길이가 가변적이기 때문에 고정된 크기의 네트워크에 입력이 불가능하기 때문이죠. 그 대신에 seq2seq를 이용해서 autoencoder와 유사한 녀석을 만들수가 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/990975415C66DCE216&quot; filemime=&quot;image/jpeg&quot; filename=&quot;basic_seq2seq.png&quot; height=&quot;177&quot; original=&quot;yes&quot; width=&quot;800&quot;/&gt;&lt;span class=&quot;cap1&quot; style=&quot;display:block;max-width:100%;width:800px;&quot;&gt;그림 출처:
https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/seq2seq/&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;seq2seq 모형은 RNN 네트워크 2종류를 이어붙인 모형으로&amp;nbsp;기계 번역, 질문 답변 등의 작업을 처리하는데 널리 쓰입니다. encoder 부분에 A, B, C가 차례로 들어가면서 RNN이 이를 차례대로 인코딩하고, 최종적으로 인코딩 결과가 디코더로 넘겨져서 이를 W, X, Y, Z로 문장을 생성해 나갑니다. 충분한 양의 데이터를 학습시키면, 인코더와 디코더의 가중치들이 적절하게 학습되어 복잡한 문장에서도 그 구조를 추출하고 의미있는 패턴을 학습하는게 가능해집니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이때 A-&amp;gt;B-&amp;gt;C가 인코더에 들어간 후의 인코더의 상태값만으로 W, Y, Y, Z를 생성한다는 점이 매우 중요한데요, 다른 말로하면 가변적 길이의 문장을 통해 고정된 크기의 값을 생성하고, 또 고정된 크기의 값을 통해 가변적인 길이의 문장을 생성할 수 있다는 것입니다. 그렇다면, A-&amp;gt;B-&amp;gt;C가 입력됐을때, 출력으로 그대로 A, B, C를 생성하도록 학습시키면, 이는 결국 텍스트버전의 Autoencoder가 될 수 있겠죠.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;실전&lt;/h2&gt;&lt;p&gt;과연 텍스트 버전의 오토인코더가 잘 작동할지 궁금해져서, 관련 구현들을 찾아봤습니다. Tensorflow로 구현된 코드들이 몇몇 있더라구요. (&lt;a href=&quot;https://github.com/erickrf/autoencoder/&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://github.com/erickrf/autoencoder/&lt;/a&gt;) 이 코드를 바탕으로 조금 개량하여&amp;nbsp;Bidirectional GRU Encoder에 GRU Decoder를 사용해 텍스트 오토인코더를 구현하고 실제로 실험을 진행해보았습니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;실험에 사용한 텍스트는 Expedia 호텔 영어 리뷰 데이터&amp;nbsp;148만개를 사용했습니다. 각 리뷰들에&amp;nbsp;대문자를 소문자로 통일시키고, 토큰화 처리한 것 이외의 전처리는 수행되지 않았습니다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;i loved this location due to the proximity to shops and food and would stay there again .&lt;/p&gt;
&lt;p&gt;i would stay again due to cleanliness and central location .&lt;/p&gt;
&lt;p&gt;the staff are friendly and helpful .&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;전처리된 문장들 예시&amp;gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Hidden Size = 150, Hidden Layer = 3, Word Embedding Size = 200으로 설정하고, Tensorflow GPU 버전에서 전체 데이터에 대해 1 epoch 학습을 진행시켰습니다. 학습 후 오토인코더의 복원 정확도는 약 98%로, 사실상 입력 문장을 그대로 복원하는데에 성공해냈구요. 150 크기의 Hidden Layer가 3개 있으므로, 인코딩된 상태의 크기는 총 450이고, 양방향 데이터를 모두 사용했으므로, 총 900개의 숫자로 전체 문장을 표현하는 방법을 학습한것이죠.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이렇게 학습된 오토인코더를 사용해 문장을 인코딩하면 다음과 같이 900차원의 벡터값을 돌려줍니다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;the staff are really nice / polite , breakfast was ok . [-0.287, -0.986, -1, 0.001, 0.006, -0.001, 0.021, -0.996, ...]&lt;/p&gt;
&lt;p&gt;the staff are friendly and helpful . [0.236, -0.992, -1, -0.005, 0.006, -0.000, -0.732, -0.997, ...]&lt;/p&gt;
&lt;p&gt;i don't recommend this hotel at all .&amp;nbsp;[0.104, -0.989, -1, -0.004, 0.009, -0.001, -0.587, -0.997, ...]&lt;/p&gt;&lt;/div&gt;&lt;p&gt;혹은 거꾸로 900차원의 벡터값을 넣어주면 문장을 생성하는것도 가능하구요.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;오토인코더 학습이 잘 진행되었다면, 비슷한 문장끼리는 분명 비슷한 임베딩 값을 가질 것입니다. 그래서 임베딩 값을 바탕으로 클러스터링을 실시해서 실제로 비슷한 문장끼리 묶이는지를 확인해봤습니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;10만개의 문장을 랜덤으로 추출해서 k-means 클러스터링을 실시했습니다. K = 3000으로 두었으니, 평균적으로 한 클러스터에 33개의 문장이 들어갈 겁니다. 그리고 비교를 위해 같은 데이터를 대상으로 텍스트 오토인코더가 아닌 일반 tf-idf를 이용한 텍스트 클러스터링도 실시해 보았습니다.&lt;/p&gt;
&lt;p&gt;먼저 클러스터링 결과의 통계를 살펴보면 다음과 같습니다.&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:572px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/991163355C66EAD70D&quot; filemime=&quot;image/jpeg&quot; filename=&quot;Chart1.PNG&quot; height=&quot;439&quot; width=&quot;572&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:573px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9937F83C5C66EADF01&quot; filemime=&quot;image/jpeg&quot; filename=&quot;Chart2.PNG&quot; height=&quot;439&quot; width=&quot;573&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Autoencoder를 사용한 경우 생성하는 벡터값이 dense하다보니 좀더 균일한 크기의 클러스터들이 많이 생성된 반면, tf-idf를 사용한 경우는 벡터값이 sparse하기 때문에 5개 이하의 소규모 클러스터도가 많이 생기고, 또 120개 이상의 대규모 클러스터도 잔뜩 생겼습니다. (크기가 300개 이상인 클러스터도 3개나 있었습니다.)&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;
&lt;style&gt;.std-table td p {line-height:110%; text-indent:-.5em;padding-left:.5em;margin-bottom:8px !important;word-wrap:normal;word-break:keep-all;}&lt;/style&gt;
&lt;table class=&quot;std-table&quot;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;
&lt;th&gt;오토인코더&lt;/th&gt;&lt;th&gt;TF-IDF&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;td&gt;&lt;p&gt;good facility .
&lt;/p&gt;
&lt;p&gt;view great .
&lt;/p&gt;
&lt;p&gt;good value .
&lt;/p&gt;
&lt;p&gt;good service .
&lt;/p&gt;
&lt;p&gt;good location .
&lt;/p&gt;
&lt;p&gt;good price .
&lt;/p&gt;
&lt;p&gt;fine location .&lt;/p&gt;
&lt;p&gt;&amp;lt;하략&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;good value .&lt;/p&gt;
&lt;p&gt;it was a spacious room , good value ...&lt;/p&gt;
&lt;p&gt;very good value breakfast .&lt;/p&gt;
&lt;p&gt;good value , clean and comfortable .&lt;/p&gt;
&lt;p&gt;good location snd good value .&lt;/p&gt;
&lt;p&gt;good value for downtown hotel&lt;/p&gt;
&lt;p&gt;good value for the location of the hotel .&lt;/p&gt;
&lt;p&gt;&amp;lt;하략&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;td&gt;&lt;p&gt;best environment , best stuff , best breakfast !&lt;/p&gt;
&lt;p&gt;average hotel and meet our accommodations for the night .&lt;/p&gt;
&lt;p&gt;good hotel , fair conditions for what you paid .&lt;/p&gt;
&lt;p&gt;good stay with a comfortable bed and free breakfast .&lt;/p&gt;
&lt;p&gt;good - except that the wifi connection was spotty .&lt;/p&gt;
&lt;p&gt;good hotel w / good service .&lt;/p&gt;
&lt;p&gt;good enough for a single night stay .&lt;/p&gt;
&lt;p&gt;&amp;lt;하략&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;clean room , good service .&lt;/p&gt;
&lt;p&gt;service : good ;&lt;/p&gt;
&lt;p&gt;good hotel w / good service .&lt;/p&gt;
&lt;p&gt;service seemed good .&lt;/p&gt;
&lt;p&gt;service at the hotel was extraordinarily good .&lt;/p&gt;
&lt;p&gt;the service was good .&lt;/p&gt;
&lt;p&gt;good service .&lt;/p&gt;
&lt;p&gt;they people and service was good .&lt;/p&gt;
&lt;p&gt;&amp;lt;하략&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;&lt;td&gt;&lt;p&gt;this was a great location and a great stop over motel .&lt;/p&gt;
&lt;p&gt;it was a great stay and we would definitely go back .&lt;/p&gt;
&lt;p&gt;it was a beachfront property and a great location .&lt;/p&gt;
&lt;p&gt;it was a great stay , will definately return again&lt;/p&gt;
&lt;p&gt;this was a nice suprise , and i will stay there again .&lt;/p&gt;
&lt;p&gt;it was a great price , which is why we stayed there .&lt;/p&gt;
&lt;p&gt;&amp;lt;하략&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;louis and will stay there again as this was our first time there .&lt;/p&gt;
&lt;p&gt;this was a nice suprise , and i will stay there again .&lt;/p&gt;
&lt;p&gt;&amp;lt;끝. 2개가 전부인 클러스터&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;&lt;td&gt;&lt;p&gt;overall , a good place to stay at a good price .&lt;/p&gt;
&lt;p&gt;this is a good one night hotel near airport .&lt;/p&gt;
&lt;p&gt;this is a good hotel for a one night stay near the airport .&lt;/p&gt;
&lt;p&gt;this is a good hotel for the night before you&lt;/p&gt;
&lt;p&gt;this is a great place to stay with your dog .&lt;/p&gt;
&lt;p&gt;overall , a good choice for access to terminal 4 .&lt;/p&gt;
&lt;p&gt;this hotel was good for one night in guatemala city .&lt;/p&gt;
&lt;p&gt;this hotel was amazing , in looks and service .&lt;/p&gt;
&lt;p&gt;this hotel is clean and very convenient to the dfw airport .&lt;/p&gt;
&lt;p&gt;&amp;lt;하략&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;i stayed one night so i could sleep near the airport .&lt;/p&gt;
&lt;p&gt;this is a good one night hotel near airport .&lt;/p&gt;
&lt;p&gt;this is a good hotel for a one night stay near the airport .&lt;/p&gt;
&lt;p&gt;this hotel was good for one night in guatemala city .&lt;/p&gt;
&lt;p&gt;&amp;lt;끝. 4개가 전부인 클러스터&amp;gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;위의 예시는 양쪽 클러스터링 결과 중 비슷한 클러스터 4쌍을 뽑아 놓은 것입니다.&amp;nbsp;&lt;/p&gt;
&lt;div&gt;TF-IDF를 이용한 클러스터링 결과는 기법 그대로 단어가 얼마나 일치하는지를 바탕으로 유사도를 계산합니다. 그 때문에 공유하는 단어가 많은 문장들끼리 묶이게 됩니다. 반면, 오토인코더의 경우 겹치는 단어가 없더라도 의미적으로 유사하다면 충분히 같이 묶일 수 있습니다.&amp;nbsp;&lt;/div&gt;&lt;div&gt;1번에서 good location과 view great 이 한데 묶인것처럼 말이지요. 또한 오토인코더의 경우 문장의 형태에 굉장히 민감한 것으로 보입니다. 3번의 경우 it was 혹은 this was로 시작하고, and 혹은 관계절로 문장을 덧이은 경우들이 주로 묶이는 것을 확인할 수 있었습니다.&amp;nbsp;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;일단 작은 데이터셋을 돌려보고 내린 결론은 다음과 같습니다.&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;텍스트 오토인코더는 의미적인 유사성을 어느 정도 반영할 수 있다.&lt;/li&gt;&lt;li&gt;또한 문장 구조의 유사성을 반영할 수도 있다.&lt;/li&gt;&lt;li&gt;다만 문장 구조가 유사해도 의미적으로는 전혀 상관이 없을수도 있으므로, 임베딩 결과가 항상 의미를 반영한다고 해석하긴 어려운 것 같다.&lt;/li&gt;&lt;li&gt;이와는 별개로 임베딩 값을 가지고 거의 완벽한 문장을 생성해낼수 있다는 건 아주 유용한 특징이다!&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;실험을 다 돌리고 난 다음에 생각해보니, 어쩌면 Hidden Layer 사이즈나 개수에 따라서도 반영하는 정도에 차이가 있을지도 모르겠다는 생각이 들었습니다. 흥미로운 주제지만, 사이즈를 더 키우기에는 집 컴퓨터의 GPU가 벅차하므로 컴퓨터를 먼저 업그레이드하고선 해봐야겠습니다.&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/628&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>잉여</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/628</guid>
<comments>https://bab2min.tistory.com/628#entry628comment</comments>
<pubDate>Sat, 16 Feb 2019 02:15:56 +0900</pubDate>
</item>
<item>
<title>단어 임베딩을 이용한 추출적 텍스트 요약 기법</title>
<link>https://bab2min.tistory.com/627</link>
<description>&lt;p&gt;오늘 살펴볼 논문은 추출적 텍스트 요약 기법(Extractive Text Summarization)에 단어 임베딩을 적용하는 간단한 방법을 통해 비지도 방법으로 높은 텍스트 요약 성능을 보인 기법에 대한 것입니다. 아이디어는&amp;nbsp;정말로 간단한데 성능이 강력하니 이보다 좋을 수 없지요. 간략하게 어떤 기법인지 살펴보도록 하겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Rossiello, G., Basile, P., &amp;amp; Semeraro, G. (2017). Centroid-based text summarization through compositionality of word embeddings. In&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;&amp;nbsp;(pp. 12-21).&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;추출적 텍스트 요약 기법&lt;/h2&gt;&lt;p&gt;앞서 &lt;a href=&quot;https://bab2min.tistory.com/625&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;텍스트 요약 기법에 대해 정리&lt;/a&gt;했었지만, 추출적 요약 기법에 대해 간략하게 다시 설명하도록 하겠습니다. 추출적 텍스트 요약 기법이라 함은&amp;nbsp;1) 원본 문헌을 특정 단위(대게 문장)로 &lt;b&gt;분할&lt;/b&gt;하고, 2) 분할된 각 단위의 &lt;b&gt;중요도를 평가&lt;/b&gt;한 뒤, 3) 중요도가 높은 단위들만을 &lt;b&gt;추출&lt;/b&gt;하여 요약문을 구성하는 방법을 말합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;여기서 중요도를 어떻게 평가하느냐가 추출적 텍스트 자동 요약 기법의 성능을 가르는 핵심이 됩니다. 당연히 문헌 전체의 내용을 잘 드러낼 수 있는 문장일수록 그 중요도가 높다고 할 수 있을텐데요, 이를 위해 문장들 간의 유사도를 계산하고, 그것들을 바탕으로 모든 문장들 중에서 가장 중심성이 높은 문장을 찾는 방법을 사용할 수 있겠습니다.&lt;/p&gt;&lt;p&gt;그러면 두 문장이 유사한지는 어떻게 판단할 수 있을까요? 가장 원초적인 방법으로는 두 문장 사이에 겹치는 단어가 얼마나 있는지 세는 것일 겁니다. 조금 더 고급스럽게 이야기하면, 용어 빈도(term frequency, tf)를 이용한 유사도 척도라는 표현을 쓰기도 하죠. 그리고 이를 좀더 고도화하여 중요한 용어에는 더 높은 가중치를 부여하도록 한 tf-idf를 이용한 유사도 척도를 쓸 수도 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이런 고전적인 단어 빈도 기반의 접근법은 두 문장 사이에 일치하는 단어가 있어야만 그 유사도가 계산이 됩니다. 따라서 두 문장이 아무리 유사한 이야기를 하더라도 둘 사이에 겹치는 단어가 하나도 없다면 둘의 유사도는 0이 될 수 밖에 없습니다. 이는 단어주머니 모델(Bag of Words Model)의 큰 한계입니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;단어 임베딩을 이용한 문장 유사도 계산&lt;/h2&gt;&lt;p&gt;BOW 모델의 한계를 지적하면서 뒤따라 나오는 이야기는 늘 &lt;b&gt;단어 임베딩&lt;/b&gt; 기법입니다. (패턴이 뻔하죠..) 단어 임베딩 기법은 개별적인 단어를 특정한 차원의 벡터(벡터 공간 상의 점)로 매핑해주기에, 서로 다른 단어들끼리의 거리 계산, 여러 산술 연산이 가능해집니다. 따라서 이를 이용하면 두 문장 사이에 공통 단어가 존재하지 않더라도 의미적으로 유사하다면 그 유사도를 구해낼 수 있습니다.&amp;nbsp;이러한 단어 임베딩 기법 중 대표적인 것엔&amp;nbsp;Word2Vec이 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;본격적으로 저자들이 어떻게 단어 임베딩을 자동 요약에 적용했는지를 살펴보도록 하겠습니다. 먼저 미리 학습된 Word2Vec 임베딩을 이용하면 문장 내의 각 단어들을 하나의 벡터로 표현할 수 있습니다. 그런데 단어를 벡터로 만드는 것은 간단했지만, 여러 단어가 모여 만들어진 문장을 벡터로 만드는 것은 생각보다 어렵습니다. 저자들은 정말 단순하게 문장 내에 등장하는 모든 단어들의 벡터 값을 합하는 것으로 문장 전체의 임베딩을 구하는 방식을 제안했습니다. 수식으로 옮기자면 다음과 같겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99ED274D5C5C4EB034&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28E%28S_%7B%20j%20%7D%29%7E%20%3D%7E%20%5Csum%20_%7B%20w%5Cin%20S_%7B%20j%20%7D%20%7D%20V_%7B%20w%20%7D%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;153&quot; height=&quot;56&quot;&gt;&lt;/p&gt;&lt;p&gt;(여기서&amp;nbsp;S_j는 임의의 문장, E(S_j)는 S_j의 N차원 임베딩, V_w는 단어 w에 대한 N차원 임베딩)&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그리고 이 때 두 문장 간의 유사도는 다음과 같이 정의합니다.&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/996606495C5C4F5E33&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28sim%28S_%7B%20j%20%7D%2CS_%7B%20k%20%7D%29%7E%20%3D%7E%20%5Cfrac%20%7B%20E%28S_%7B%20j%20%7D%29%5Ccdot%20E%28S_%7B%20k%20%7D%29%20%7D%7B%20%7C%7CE%28S_%7B%20j%20%7D%29%7C%7C%5Ccdot%20%7C%7CE%28S_%7B%20k%20%7D%29%7C%7C%20%7D%20%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;301&quot; height=&quot;62&quot;&gt;&lt;/p&gt;&lt;p&gt;(수식으로는 복잡해보이지만, 사실은 코사인 유사도 공식입니다.)&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;중심점(Centroid)을&amp;nbsp;이용한 문장의 중요성 판단&lt;/h2&gt;&lt;p&gt;사실 본 논문에서는 문장 간의 유사도를 가지고 중요성을 판단하기보다는, 문헌의 중심점(Centroid)와 문장의 유사도를 가지고 그 중요성을 판단합니다. 여기서 문헌의 Centroid라고 함은, 전체 문헌의 내용의 중심이 되는 임베딩 벡터를 가리킵니다. 즉 이 중심점에 가까울수록 문헌의 주 내용과 유사한 것이라고 판단할 수 있고, 이런 문장들만 추려내면 전체 문헌의 내용을 잘 표현하는 요약문이 생성될 수 있다고 가정할 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그럼 문헌의 Centroid는 어떻게 구할 수 있을까요? 저자들은 고전적이면서 간단한 방법을 제안했습니다. 문헌 내에 등장하는 단어들 중 tf-idf 값이 특정한 값 이상인 경우들만 골라내, 이 단어들의 임베딩 값을 합한 것을 Centroid로 쓰기로 한 것이죠. &lt;b&gt;tf-idf 가중치&lt;/b&gt; 값은&amp;nbsp;문헌 집합 내에서 골고루 등장하는 것이 아니라&amp;nbsp;&lt;b&gt;현재 문헌에서만 집중적으로 자주 등장&lt;/b&gt;하는 용어들에 대해서 높은 점수를 부여하는 특성이 있습니다. 따라서 tf-idf값이 높은 용어는 그 문헌의 중요 단어, 즉 키워드일 가능성이 높습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;중심점 계산 공식을&amp;nbsp;형식적으로 쓰자면 다음과 같습니다.&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99897E385C5C5C5932&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28C%7E%20%3D%7E%20%5Csum%20_%7B%20w%5Cin%20D%2C%5C%3A%20f%28w%29%26gt%3Btt%20%7D%20V_%7B%20w%20%7D%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;165&quot; height=&quot;56&quot;&gt;&lt;/p&gt;&lt;p&gt;(여기서 D는 문헌, f(w)는 단어 w의 tf-idf 가중치 값, tt는 가중치 임계값)&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;마지막으로 문장의 중요성은 중심점과 해당 문장과의 유사도로 계산할 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/995E943B5C5C527F1B&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28Score%28S_%7B%20j%20%7D%29%7E%20%3D%7E%20sim%28S_%7B%20j%20%7D%2CC%29%7E%20%3D%7E%20%5Cfrac%20%7B%20E%28S_%7B%20j%20%7D%29%5Ccdot%20C%20%7D%7B%20%7C%7CE%28S_%7B%20j%20%7D%29%7C%7C%5Ccdot%20%7C%7CC%7C%7C%20%7D%20%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;382&quot; height=&quot;62&quot;&gt;&lt;/p&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;p&gt;실제 예시를 봅시다. 다음 예시는 논문에서 가져온 것입니다.&lt;/p&gt;&lt;table class=&quot;std-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;arcade&lt;/th&gt;&lt;th&gt;donkey&lt;/th&gt;&lt;th&gt;kong&lt;/th&gt;&lt;th&gt;game&lt;/th&gt;&lt;th&gt;nintendo&lt;/th&gt;&lt;th&gt;coleco&lt;/th&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;&lt;i&gt;Cent&lt;/i&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;arcades&lt;/td&gt;&lt;td&gt;goat&lt;/td&gt;&lt;td&gt;hong&lt;/td&gt;&lt;td&gt;gameplay&lt;/td&gt;&lt;td&gt;mario&lt;/td&gt;&lt;td&gt;intellivision&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;i&gt;nes&lt;/i&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;pac-man&lt;/td&gt;&lt;td&gt;pig&lt;/td&gt;&lt;td&gt;macao&lt;/td&gt;&lt;td&gt;multiplayer&lt;/td&gt;&lt;td&gt;wii&lt;/td&gt;&lt;td&gt;atari&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;i&gt;gamecube&lt;/i&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;console&lt;/td&gt;&lt;td&gt;monkey&lt;/td&gt;&lt;td&gt;fung&lt;/td&gt;&lt;td&gt;videogame&lt;/td&gt;&lt;td&gt;console&lt;/td&gt;&lt;td&gt;nes&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;i&gt;konami&lt;/i&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;위의 arcade, donkey, kong, game, nintendo, coleco라는 단어는&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Donkey_Kong_(video_game)&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;Donkey Kong&lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Donkey_Kong_(video_game)&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;이라는 영어 위키백과 문헌&lt;/a&gt;에서 tf-idf 가중치가 0.3 이상인 것만 골라낸 결과입니다. 그 아래에는 Word2Vec을 통해 계산한 각 키워드별 유사 단어 Top3이구요. 제일 오른쪽의 &lt;i&gt;Cent&lt;/i&gt;는 arcade, donkey, kong, game, nintendo, coleco의 임베딩 벡터를 합하여 Centroid를 구성하였을때&amp;nbsp;이와 Word2Vec 상에서 가장 유사한 단어 3개를 보여줍니다. 이를 통해 Centroid가 문헌 전반의 의미를 비교적 잘 잡아내고 있다는 것을 알 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그럼 실제 Centroid와 제일 유사도가 높은 문장들에는 어떤 것들이 뽑히는지 살펴봅시다.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/9919BA365C5C5A7F2E&quot; filemime=&quot;image/jpeg&quot; filename=&quot;sim.PNG&quot; height=&quot;270&quot; width=&quot;800&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;이 역시 논문에서 가져온 예시인데요, 보시다시피 Donkey Kong이라는 게임에 대한 내용과 관련이 깊은 문장들이 주로 선택된 것을 확인할 수 있습니다.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;문장 추출 및 요약문 생성&lt;/h2&gt;&lt;p&gt;각 문장의 중요성 점수 Score(S_j)를 계산했다면, 이제 Score가 높은 문장들을 추려 요약문을 생성할 단계입니다. 이는 간단합니다. 모든 문장들의 Score를 계산한 뒤, 이를 내림차순으로 정렬하여 Score가 높은 값부터 차례대로 후보에 추가해나갑니다. 이를&amp;nbsp;후보들 전체의 길이가 원하는 길이에 도달할때까지 반복합니다. 최종적으로 후보에 있는 문장들을 이어서 출력하면 이것이 요약문이 됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;사실, 이 방법은 한 가지 문제가 있습니다. Score가 매우 높은 문장들은 모두 Centroid와 유사도가 높으니, 서로 간의 유사도도 높을 수 밖에 없습니다. 이 문장들을 그대로 다 요약문에 넣을 경우 요약문에&amp;nbsp;유사한 문장들이 반복되어 장황함(redundancy)이 높아지겠죠. 따라서 후보에 이미 비슷한 문장이 있다면 또 추가하지 않도록 배제하는 것이 조금 더 나은 요약문을 생성할 수 있습니다. 이를 고려해 문장 추출 알고리즘을 정리하자면 다음과 같이 되겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;모든 문장들의 집합&amp;nbsp;&lt;b&gt;S&lt;/b&gt;에 대해 각각 &lt;b&gt;Score&lt;/b&gt;(&lt;b&gt;s&lt;/b&gt;)를 계산&lt;/li&gt;&lt;li&gt;&lt;b&gt;Score&lt;/b&gt;(&lt;b&gt;s&lt;/b&gt;)를 기준으로 &lt;b&gt;S&lt;/b&gt;를 내림차순 정렬&lt;/li&gt;&lt;li&gt;&lt;b&gt;Cand&lt;/b&gt;는 빈 리스트로 설정&lt;/li&gt;&lt;li&gt;&lt;b&gt;S &lt;/b&gt;내의 각 문장 &lt;b&gt;s&lt;/b&gt;에 대해 다음을 반복&lt;/li&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;(&lt;b&gt;Cand&lt;/b&gt;&amp;nbsp;전체의 길이) &amp;gt; &lt;b&gt;LIMIT&lt;/b&gt; 이면 반복문을 탈출&lt;/li&gt;&lt;li&gt;&lt;b&gt;sim&lt;/b&gt;(&lt;b&gt;s&lt;/b&gt;, &lt;b&gt;t&lt;/b&gt;) &amp;gt; &lt;b&gt;ST&lt;/b&gt;인 &lt;b&gt;t&lt;/b&gt;∈&lt;b&gt;Cand&lt;/b&gt;가 존재하지 않는다면&lt;/li&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;&lt;b&gt;Cand&lt;/b&gt;에 문장 &lt;b&gt;s&lt;/b&gt;를 추가&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;&lt;b&gt;Cand&lt;/b&gt; 내의 문장들을 연결하여 요약문 출력&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;이때 &lt;b&gt;LIMIT&lt;/b&gt;은 전체 요약문의 길이를 지정하는 상수값이며, &lt;b&gt;ST&lt;/b&gt;는 중복 용인 한계를 지정하는 상수값입니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;실험 결과&lt;/h2&gt;&lt;p&gt;그러면 실제 실험 결과를 살펴보도록 합시다. 다음은 DUC-2004에 대해 진행한 실험입니다.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:507px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/995AAC3F5C5C5B6819&quot; filemime=&quot;image/jpeg&quot; filename=&quot;res1.PNG&quot; height=&quot;359&quot; width=&quot;507&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;R1는 Rouge-1 Score, R2는 Rouge-2 Score를 의미합니다.&lt;/p&gt;&lt;p&gt;C_BOW는 Centroid BOW로 단어임베딩을 적용안 한 모델, C_GNEWS는 Google News Datasets을 이용한 Word2Vec을 사용한 모델, C_CBOW는 입력데이터로 Word2Vec CBOW를 학습하여 사용한 모델, C_SKIP은 입력데이터로 Word2Vec Skipgram을 학습하여 사용한 모델입니다. Skipgram을 사용한 경우가 기존보다 높은 성능을 보였으며, 이 때 tt = 0.3, st = 0.94였습니다.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;모델이 단순한 것치고는 기존의 방법들보다 높은 성능을 보였는데, 이는 단어 임베딩 덕분이라고 볼 수 있습니다. 또한 discussion에서 나온 이야기로는 tt값을 낮춰서 문헌 내의 모든 단어를 centroid에 만드는데에 사용할 경우 C_BOW만도 못한 성능을 보인다고 합니다. 즉 다시 말하면, 좋은 centroid를 찾는 것이 성능을 높이는데에 필수적이라는 것이죠. 이 부분은 단순히 tf-idf를 이용하는 것보다 더 세련된 방법을 이용할 수 있다면 개선의 여지가 있을 것 같습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99D92B395C5C5E6A2B&quot; filemime=&quot;image/jpeg&quot; filename=&quot;res2.PNG&quot; height=&quot;207&quot; width=&quot;800&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;위의 결과는 MultiLing MSS 2015 데이터셋에 대해 실험을 진행할 결과입니다. LEAD와 C_BOW는 Baseline이라고 볼 수 있으며, ORACLE은 자동 요약 시스템이 낼 수 있는 성능의 상한점이라고 볼 수 있습니다. C_W2V가 제안한 방법을 가지고 얻은 실제 점수인데, 영어의 경우 최고 시스템을 앞지르는 결과를 냈으며, 독일어, 스페인어, 프랑스어의 경우 최저 시스템을 앞서는 점수를 냈습니다. 반면 이탈리아어의 경우는 최저 시스템에 못 미치는 결과를 냈습니다.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이탈리아어의 성능이 낮은 원인은 이탈리아어의 특성상 굴절이 많아 어휘의 수가 많은데 이를 Word2Vec이 학습하는데에 어려움을 겪기 때문이라고 저자들은 설명했습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://multiling.iit.demokritos.gr/file/view/1629/mssmultilingual-single-document-summarization-submissionand-automatic-score&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;MSS에 제출된 시스템&lt;/a&gt;들의 경우 높은 성능을 달성하기 위해서 복잡한 기법들을 사용했어야 했는데, 본 논문에서 제안하는 기법은 단순한 시스템에 단어 임베딩을 조합하는 것만으로 복잡한 기법들과 유사한 성능을 달성했다는 데에서 의미가 있다고 볼 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;저자들이 친절하게 코드도 공개해주었습니다. 사실 방법이 단순해서 직접 구현해도 어렵지 않아보이지만 코드 공개해주는것처럼 감사한 일도 없지요.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/gaetangate/text-summarizer&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://github.com/gaetangate/text-summarizer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;마무리&lt;/h2&gt;&lt;p&gt;장황하게 설명했지만, 결국 한 줄 요약하자면, Word2Vec을 이용해서 문장의 임베딩과 Centroid의 임베딩을 얻고, Centroid와의 코사인 유사도가 높은 문장들을 뽑아서 요약문을 생성하면 의외로 좋은 성능을 얻을 수 있다는 것입니다. 기법이 참 간단하긴 한데, 종종 이렇게 해도 되는건지 아쉬운 지점들도 있었습니다. 정리해보니 크게 다음과 같네요&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;Centroid나 문장의 임베딩을 계산할때 단순히 각 단어들의 임베딩을 합하여도 되는 것인지?&lt;/li&gt;&lt;li&gt;tf-idf 가중치로 단어를 골라냈는데, 이게 Centroid를 계산하는 최선의 방법인 것인지?&lt;/li&gt;&lt;li&gt;tt, st 값에 전체 성능이 크게 좌우되는데, 적절한 tt, st 값을 어떻게 선정할 수 있는지?&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;특히 1번이 많이 걸렸습니다. 그저 단어의 임베딩을 더하여 전체 문장의 임베딩을 구하기 때문에, 단어들 간의 순서가 무시되고, 임베딩 벡터를 더하는 과정에서 각 단어의 의미들이 희석될 수 밖에 없다는 염려를 떨칠 수가 없었네요. LSTM을 이용해&amp;nbsp;문장을 바로 임베딩하는 식의 방법으로 조금더 정밀하게 개선할 수 있을 것 같습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그리고 Centroid가 반드시 필요한지도 의문입니다. TextRank를 이용한 요약 기법에서는 굳이 Centroid를 가정하지 않고, 문장 간의 유사도 네트워크에서 PageRank를 수행하여 각 문장의 중요성을 계산하는 방식을 사용했습니다. 이런 방법을 적용하면 어떤 다른 결과가 나올지도 흥미로운 부분입니다. 시간이 되면 실험해봐야겠네요.&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/627&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>그냥 공부</category>
<category>word2vec</category>
<category>자동요약</category>
<category>자연언어처리</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/627</guid>
<comments>https://bab2min.tistory.com/627#entry627comment</comments>
<pubDate>Fri, 08 Feb 2019 01:58:03 +0900</pubDate>
</item>
<item>
<title>[c++] 빠른 log sigmoid 계산</title>
<link>https://bab2min.tistory.com/626</link>
<description>&lt;p&gt;sigmoid 함수는 연속이고 미분가능하면서 비선형이라는 특징 덕분에 여러 기계 학습 및 신경망에 두루 쓰이는 함수이죠. 최근에 word2vec을 확장한 모형을 공부하면서 목적함수에 log sigmoid를 잔뜩 사용하는 코드를 구현한 적이 있었는데, log sigmoid 계산 비용이 꽤나 컸기에 전반적인 속도 향상에 어려움이 있었습니다. 그래서 Look Up Table을 이용해 최적화를 실시했었는데 그 결과를 포스팅으로 공유하고자 합니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;먼저 log sigmoid 함수는 다음과 같이 정의됩니다.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99ADF54C5C2B851F3E&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28s%28x%29%7E%20%3D%7E%20-%5Clog%20%20%5Cleft%28%201+e%5E%7B%20-x%20%7D%20%5Cright%29%20%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;191&quot; height=&quot;28&quot;&gt;&lt;/p&gt;
&lt;p&gt;이름처럼 sigmoid 함수에 log를 취한 형태이구요, 그 그래프 모양은 다음과 같습니다.&lt;/p&gt;
&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:206px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99A5D74E5C2B85F107&quot; filemime=&quot;image/jpeg&quot; filename=&quot;ls.PNG&quot; height=&quot;140&quot; width=&quot;206&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;전 범위에서 음수 값을 가지며, x &amp;gt; 0일는 그 값이 0에 극도로 가까워집니다. 이 함수를 계산하기 위해서는 exp 연산 한 번과 log 연산 한 번을 수행해야 하는데, 딥러닝의 특성상 수많은 가중치들에 대해 log sigmoid를 계산하게 되므로 그 부하가 보통이 아닙니다. 따라서 최적화가 필요한 부분이라고 할 수 있겠습니다.&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;빠른 계산을 위해서 log sigmoid의 특성을 살펴보도록 합시다. 먼저 x&amp;lt;0인 부분을 살펴봅시다. t = -x라고 하면&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/992D3D435C2B871209&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28s%28x%29%7E%20%3D%7E%20-%5Clog%20%20%5Cleft%28%201+e%5E%7B%20t%20%7D%20%5Cright%29%20%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;184&quot; height=&quot;28&quot;&gt;&lt;/p&gt;
&lt;p&gt;가 되는데, t가 커질수록 exp(t)의 값은 말 그대로 지수적으로 커집니다. 따라서 충분히 큰 t에 대해&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99DE09445C2B87502F&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%281+e%5E%7B%20t%20%7D%7E%20%5Capprox%20%7E%20e%5E%7B%20t%20%7D%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;110&quot; height=&quot;31&quot;&gt;&lt;/p&gt;
&lt;p&gt;라고 할 수 있으므로&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99632E435C2B879D01&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28s%28x%29%7E%20%3D%7E%20-%5Clog%20%20%5Cleft%28%201+e%5E%7B%20t%20%7D%20%5Cright%29%20%7E%20%5Capprox%20%7E%20-%5Clog%20%20%5Cleft%28%20e%5E%7B%20t%20%7D%20%5Cright%29%20%7E%20%3D%7E%20-t%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;363&quot; height=&quot;31&quot;&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99F08B4C5C2B87AB0A&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28s%28x%29%7E%20%5Capprox%20%7E%20x%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;93&quot; height=&quot;31&quot;&gt;&lt;/p&gt;
&lt;p&gt;x가 충분히 작은 값일때 log sigmoid의&amp;nbsp;값은 x로 근사할 수 있습니다. 그러먼 이 때 근사값과 참값의 차이는 어느 정도 될까요?&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99B7A2505C2B88742E&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28s%28x%29-x%7E%20%3D%7E%20-%5Clog%20%20%5Cleft%28%201+e%5E%7B%20-x%20%7D%20%5Cright%29%20-x%7E%20%3D%7E%20-%5Clog%20%20%5Cleft%28%201+e%5E%7B%20-x%20%7D%20%5Cright%29%20-%5Clog%20%20e%5E%7B%20x%20%7D%5C%5C%20%7E%20%3D%7E%20-%5Clog%20%20%281+e%5E%7B%20-x%20%7D%29%28e%5E%7B%20x%20%7D%29%7E%20%3D%7E%20-%5Clog%20%20%5Cleft%28%201+e%5E%7B%20x%20%7D%20%5Cright%29%20%7E%20%3D%7E%20s%28-x%29%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;446&quot; height=&quot;59&quot;&gt;&lt;/p&gt;
&lt;p&gt;여기서 log sigmoid 함수의 재미난 특성이 드러납니다. s(x)에서 x를 빼면 그 값이 s(-x)와 일치하게 됩니다. 다르게 말하면 s(x)를 x로 근사할 때 그 오차는 s(-x)와 같다는 것이고, 따라서 우리는 x&amp;gt;=0인 부분의 s(x)값을 알면 전 범위의 x값에 대해서 s(x)를 구할 수 있습니다.&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;그런데 x가 충분히 크다면 s(x)는 0과 거의 같습니다. 그렇기에 우리는 적당히 큰 수 n에 대해 0 &amp;lt;= x &amp;lt; n 범위의 s(x)값만 안다면 사실상 거의 모든 범위의 s(x)를 빠르게 계산할 수 있습니다.&lt;/p&gt;
&lt;table class=&quot;std-table&quot;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;x&lt;/th&gt;&lt;th&gt;s(x)&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;-0.6931&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;-4.539E-5&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;-2.061E-9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;30&lt;/td&gt;&lt;td&gt;-9.357E-14&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;-4.248E-18&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;몇 개의 x값에 대한 log sigmoid 값은 위와 같습니다. 보다시피 x가 30을 넘어가면 사실상 0이라고 봐도 될 정도입니다. 그러면 적당히&amp;nbsp;n = 32로 정하고 0 &amp;lt;= x &amp;lt; 32인 x에 대해 s(x)의 값을 미리 계산하여 테이블에 넣어두고 사용하면 되겠네요.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;다음은 LUT을 생성하는 c++ 코드입니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;textarea name=&quot;code&quot; class=&quot;cpp&quot;&gt;#include &amp;lt;array&amp;gt;
#include &amp;lt;cmath&amp;gt;
/*
_Func : LUT를 생성할 함수
N : LUT 의 크기
S : 분할 개수
*/
template&amp;lt;class _Func, size_t N, size_t S&amp;gt;
class SimpleLUT
{
protected:
	std::array&amp;lt;double, N&amp;gt; points;
	static constexpr double P = 1. / S;
	SimpleLUT()
	{
		_Func fun;
		for (size_t i = 0; i &amp;lt; N; i++)
		{
			points[i] = fun(i * P);
		}
	}

	double _get(double x) const
	{
		size_t idx = (size_t)(x * S);
		if (idx &amp;gt;= N) return _Func{}.forLarge(x);
		return points[idx];
	}
public:
	static const SimpleLUT&amp;amp; getInst()
	{
		static SimpleLUT lg;
		return lg;
	}

	static double get(double x)
	{
		return getInst()._get(x);
	}
};

/*
LUT 계산에 사용할 함수
*/
struct F_logsigmoid
{
	double operator()(double x) { return -log(1 + exp(-x)); }
	double forLarge(double x) { return -0.f; } // 충분히 큰 x에 대해선 -0을 돌려줌
};

/*
LUT를 사용해 빠르게 logsigmoid를 계산
[0, 32) 범위를 1/128 단위로 나누어 LUT를 계산하고,
이를 조회하여 logsigmoid의 근사값을 반환
*/
inline double logsigmoid(double x)
{
	if (x &amp;gt;= 0) return SimpleLUT&amp;lt;F_logsigmoid, 32 * 128, 128&amp;gt;::get(x);
	return SimpleLUT&amp;lt;F_logsigmoid, 32 * 128, 128&amp;gt;::get(-x) + x;
}

&lt;/textarea&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;이 log sigmoid 근사 함수의 평균 오차를 계산해보면 다음과 같습니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;b&gt;상대 오차 = |(근사값 - 참값) / 참값|&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;table class=&quot;std-table&quot;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;x범위&lt;/th&gt;&lt;th&gt;평균 상대 오차 (%)&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-32~-16&lt;/td&gt;&lt;td&gt;0%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-16~-8&lt;/td&gt;&lt;td&gt;0.0000018%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-8~-4&lt;/td&gt;&lt;td&gt;0.00036%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-4~-2&lt;/td&gt;&lt;td&gt;0.0079%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-2~0&lt;/td&gt;&lt;td&gt;0.102%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;0~2&lt;/td&gt;&lt;td&gt;0.332%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;2~4&lt;/td&gt;&lt;td&gt;0.381%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4~8&lt;/td&gt;&lt;td&gt;0.391%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;8~16&lt;/td&gt;&lt;td&gt;0.443%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;16~32&lt;/td&gt;&lt;td&gt;1.539%&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;x가 커질수록 상대 오차가 점점 커지는 것을 확인할 수 있습니다. 사실 x가 음수인 경우 거의 오차가 없다고 봐도 될 정도인 반면, x가 양수일수록 상대 오차가 커지고 있습니다. 그런데 이는 상대오차의 특성도 고려해야합니다. 상대오차는 참값과 근사값의 차이가 참값 대비 얼마나 큰지를 보는 것이기 때문에 참값이 작을수록 오차가 크게 계산됩니다. 16~32 범위의 평균 오차가 1%가 나온 이유는 사실, 이 범위에서 참값과 오차값의 차이가 1e-11 이하 임에도 참값이 사실상 0이다보니 상대 오차가 크게 계산된 것이지요. 따라서 실제 사용에는 전혀 지장이 없다고 볼 수 있겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;그럼 속도 차이는 어느 정도 날까요? 기존의 log 및 exp 연산을 이용한 logsigmoid 구현과 LUT를 이용한 logsigmoid 구현을 각각 5억번 호출하여 그 실행시간 총합을 비교하여 보았습니다.&lt;/p&gt;
&lt;table class=&quot;std-table&quot;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;log, exp 사용(ms)&lt;/th&gt;&lt;th&gt;LUT 사용(ms)&lt;/th&gt;&lt;th&gt;속도 향상&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;21934.1&lt;/td&gt;&lt;td&gt;8359.91&lt;/td&gt;&lt;td&gt;+162.4(%)&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;거의 2~3배 빨라졌다고 볼 수 있겠습니다. 실제로 제 word2vec 구현에 적용해본 결과 evaluation 시간 향상은 약 1.5배 정도 있었구요. 룩업테이블로 32*128크기의 double 배열만 사용하므로, 4096 * 8Byte = 32KB의 메모리만 추가로 소비한다는 것도 아주 마음에 드네요. double 대신 float를 사용할 경우 16KB를 더 아낄수도 있겠습니다. 마음에 드는 LUT 최적화였습니다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/626&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>프로그래밍</category>
<category>c++</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/626</guid>
<comments>https://bab2min.tistory.com/626#entry626comment</comments>
<pubDate>Wed, 02 Jan 2019 01:26:16 +0900</pubDate>
</item>
<item>
<title>자동 요약 기법의 연구 동향 정리</title>
<link>https://bab2min.tistory.com/625</link>
<description>&lt;p&gt;사람들은 대체로 긴 글을 읽는 것을 좋아하지 않습니다. 모든 것이 빨리 변하고, 새로운 것이 하루가 멀다 쏟아지는 세상에서, 읽어야할 글 역시 폭발적으로 늘어나고 있기 때문에 천천히 모든 글을 읽어가며 따라가는 것이 현대인들에게는 특히 더 버거운 것 같습니다. 게시물에 세 줄 요약을 달아놓는 것이 괜히 웹 커뮤티니 상에서의 미덕이 된것이 아니겠지요. 귀찮은 일이 있으면 그걸 도구를 사용해서 간편하게 바꾸려고 하는것 또한 인간의 본성입니다. 그래서 당연히 긴 글을 자동적으로 요약해주는 시스템을 만들고자 여러 학자들이 수십년 동안 연구해왔는데요, 이번 포스팅에서는 자동 요약 기법 전반에 대해서 간단하게 살펴보고, 최근 연구 동향은 어떤지 정리해보고자 합니다.&lt;/p&gt;&lt;p&gt;주로 다음 논문의 내용을 참조하였습니다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Gambhir, M., &amp;amp; Gupta, V. (2017). Recent automatic text summarization techniques: a survey.&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Artificial Intelligence Review&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;,&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;47&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;(1), 1-66.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Allahyari, M., Pouriyeh, S., Assefi, M., Safaei, S., Trippe, E. D., Gutierrez, J. B., &amp;amp; Kochut, K. (2017). Text summarization techniques: a brief survey.&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;arXiv preprint arXiv:1707.02268&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;.&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Kasture, N. R., Yargal, N., Singh, N. N., Kulkarni, N., &amp;amp; Mathur, V. (2014). A survey on methods of abstractive text summarization.&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Int. J. Res. Merg. Sci. Technol&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;,&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;1&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;(6), 53-57.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;자동 요약(Automatic Text Summarization)이란&lt;sup class=&quot;footnote&quot;&gt;&lt;a id=&quot;footnote_link_625_1&quot; href=&quot;#footnote_625_1&quot; onmouseover=&quot;tistoryFootnote.show(this,625,1)&quot; onmouseout=&quot;tistoryFootnote.hide(625,1)&quot; style=&quot;color:#f9650d;font-family:Verdana,Sans-serif;display:inline;&quot;&gt;&lt;span style=&quot;display:none&quot;&gt;[각주:&lt;/span&gt;1&lt;span style=&quot;display:none&quot;&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;&lt;p&gt;자동 요약은 말 그래도 '요약' 작업을 컴퓨터가 대신해주도록 하는 기술입니다. 요약이란 어떤 문헌을, 그 문헌의 정보를 잘 표현할 수 있는 압축된 문장들로 표현하는 작업을 말하구요. 사실 설명은 간단해보이지만, 요약이라는 작업은 그 관점에 따라 굉장히 다양한&amp;nbsp;종류로 나누어지는 녀석입니다. 주로 연구된 자동 요약의 종류를 정리해보자면 다음과 같습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;문서 수에 따라: 단일 문서 요약 vs 다중 문서 요약&lt;/h3&gt;&lt;p&gt;가장 기본적인 것으로, 요약하려는 문서가 1개인 경우와 여러 개인 경우 사용할 수 있는 기법이 크게 달라집니다. 물론 &lt;b&gt;단일 문서 요약&lt;/b&gt;을 여러 문서에 대해 적용하여 다중 문서 요약을 수행할 수도 있겠지만, 이 경우 여러 문서들에서 공통적인 내용이 등장할 때 중복을 제거하고 핵심만 뽑아내는데 실패하게 됩니다. 따라서 &lt;b&gt;다중 문서 요약&lt;/b&gt;에서는 여러 문서들 사이에서 발생하는 중복을 제거하는 기법에 집중합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;문장 생성 방식에 따라: 추출적(extractive) 요약 vs 추상적(abstractive, 혹은 생성적) 요약&lt;/h3&gt;&lt;p&gt;요약문을 생성하는 방법에 따라 &lt;b&gt;추출적 요약&lt;/b&gt;과 &lt;b&gt;추상적 요약&lt;/b&gt;을 생각해 볼 수 있습니다. &lt;b&gt;추출적 요약&lt;/b&gt;은 문헌 내에서 중요한 핵심 문장들을 뽑아서 그 문장들로 요약문을 만드는 방법입니다. 반면 &lt;b&gt;추상적 요약&lt;/b&gt;은 문헌의 내용을 잘 반영할 수 있는 추상적인 문장을 직접 생성함으로써 요약문을 만듭니다. &lt;b&gt;전자&lt;/b&gt;의 경우 어떤 문장이 핵심인지만을 파악하여 그걸 골라내고 예쁘게 잘 이어붙이면 되는 반면에, &lt;b&gt;후자&lt;/b&gt;의 경우 전체 내용을 이해하고 그 내용을 잘 표현할 수 있는 간결한 문장을 직접 작성해야하므로 상당히 고난도의 작업이 됩니다. 따라서 현재까지는 주로 &lt;b&gt;추출적 요약&lt;/b&gt;에 대한 연구가 자동 요약 분야의 대세를 이루었으나, 최근 딥 러닝 기반의 자연언어처리 기술이 발전하면서 &lt;b&gt;추상적 요약&lt;/b&gt;에 대한 도전이&amp;nbsp;등장하고 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;내용 범위에 따라: 포괄적(generic) 요약 vs 질의집중적(query-focused) 요약&lt;/h3&gt;&lt;p&gt;이 경우는 요약문에 어떤 내용을 포함시킬지를 바탕으로 요약 기법을 나눈 것입니다. 일반적으로 생각하는 요약은 &lt;b&gt;포괄적 요약&lt;/b&gt;으로, 이는 문헌 내에 있는 모든 측면을 고르게 포함시키는 요약을 말합니다. 반면 사용자가 문헌 전반적인 내용을 알고 싶기보다는, 특정한 관심사만 알고 싶어할 수 있습니다. 신문을 보더라도 스포츠 부분만 보고 싶어하는 사람이 있듯이, 요약 결과도 특정 주제에 관한 것만 받아보고 싶어하는 경우라고 할 수 있죠. 이런 경우 이용자의 요구, 질의에 집중하여 요약문을 생성해야 하는데, 이를 &lt;b&gt;질의집중적(query-focused)&amp;nbsp;요약&lt;/b&gt;, 혹은 주제집중적(topic-focused), 이용자집중적(user-focused) 요약이라고 부르기도 합니다. 이 경우는 개인화 요약과도 연결된다고 볼 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;학습 방법에 따라: 지도학습(supervised) vs 비지도학습(unsupervised)&lt;/h3&gt;&lt;p&gt;요약을 위해서는 문헌 내에서 어떤 내용이 중요한 내용이고, 어떤 내용이 사소한 내용인지를 구분하는 작업이 필요합니다. 이를 구분하는 것을 어떻게 학습시킬지에 따라 자동 요약 기법을 &lt;b&gt;지도학습 기반&lt;/b&gt;의 요약과 &lt;b&gt;비지도학습 기반&lt;/b&gt;으로 나눠볼 수 있습니다. &lt;b&gt;지도학습 기반&lt;/b&gt;의 경우 수 많은 문헌 데이터에서 각각의 문장에 대해 중요한지, 사소한지를 라벨을 달아 훈련 데이터를 생성해 놓습니다. 그리고 이 훈련데이터를 바탕으로 새로운 문장을 입력했을때 이 문장이 중요한지 사소한지를 분류할 수 있게 분류기를 학습합니다. 이 때 분류 자질(문장의 중요도를 판별하기 위해 사용될&amp;nbsp;특성)을 선정하는 것이 중요합니다. 반면 &lt;b&gt;비지도학습 기반&lt;/b&gt;의 경우 훈련 데이터가 필요하지 않습니다. 주어진 문헌의 특성만을 가지고 다양한 특성을 바탕으로 중요한 문장을 추출해내도록 합니다. 이에는 주로 클러스터링 기법이 활용됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;요약문의 스타일에 따라: 지시적(indicative) 요약 vs 정보적(informative) 요약&lt;/h3&gt;&lt;p&gt;&lt;b&gt;지시적 요약&lt;/b&gt;은 문헌의 내용이 아니라 문헌 자체에 대한 설명을 통해 문헌을 요약하는 방법을 말합니다. 예를 들어 지금 이 블로그의 포스팅을 다음과 같이 요약할 수 있습니다:&amp;nbsp;&lt;/p&gt;&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;i&gt;&lt;span style=&quot;color: rgb(53, 53, 53);&quot;&gt;자동 요약 기법에 대해서 설명하는 포스팅으로, bab2min이 2018년 12월 28일 새벽에 &lt;/span&gt;&lt;span style=&quot;color: rgb(53, 53, 53);&quot;&gt;작성했음.&lt;/span&gt;&lt;/i&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;이 포스팅을 한 줄로 잘 표현한 요약이죠. 포스팅의 내용이 아니라 포스팅 자체에 대한 설명만 있는 겁니다. 흔히 말하는 포스팅에 대한 &lt;b&gt;메타데이터&lt;/b&gt;를 표현하는 요약이라고 말할 수 있겠습니다. 반면 &lt;b&gt;정보적 요약&lt;/b&gt;은 문헌의 내용을 설명하는 요약문을 생성하는 것을 가리킵니다. 위와 마찬가지로 이 포스팅을 정보적으로 요약한다면 다음과 같겠죠:&amp;nbsp;&lt;/p&gt;&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;i&gt;&lt;span style=&quot;color: rgb(53, 53, 53);&quot;&gt;자동 요약은 컴퓨터를 통해 텍스트를 요약하는 기술을 가리키며, 문서 수, 문장 생성 방식, 내용 범위 등에 따라 다양한 종류의 기법이 있다. (하략...)&lt;/span&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;이것이&amp;nbsp;우리가 흔히 생각하는 요약에 걸맞는 개념이라고 할 수 있습니다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;언어에 따라: 단일언어(mono-lingual)&amp;nbsp;요약, 다중언어(multi-lingual) 요약, 교차언어(cross-lingual) 요약&lt;/h3&gt;&lt;p&gt;요약하려는 문헌의 언어에 따라 요약 기법을 3가지로 분류해 볼 수도 있습니다. 먼저 &lt;b&gt;단일언어 요약&lt;/b&gt;의 경우는, 요약하려는 문헌이 하나의 언어로 쓰여져 있고, 요약문 역시 원 문헌과 같은 언어인 경우를 가리킵니다(예: 한국어 문헌을 요약하여 한국어 요약문을 생성). 반면 &lt;b&gt;다중언어 요약&lt;/b&gt;은 요약하려는 문헌이 여러 언어로 섞여 쓰여있는 경우에 사용합니다. 이 경우 요약문 역시 원 문헌에 있는 언어들로 쓰여야겠죠(예:한국어 &amp;amp;&amp;nbsp;영어가 섞인 문헌을 요약하여, 한국어 &amp;amp;&amp;nbsp;영어 요약문을 생성).&amp;nbsp;마지막으로 &lt;b&gt;교차언어 요약&lt;/b&gt;의 경우는 요약하려는 문헌과 요약문이 다른 언어일 경우를 가리킵니다. 영어 논문을 읽고 한국어 요약문을 작성하는 경우가 딱 맞는 예입니다. 이 경우 기계 번역과 자동 요약 양쪽에 걸쳐&amp;nbsp;복잡한 작업을 수행해야하겠죠.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;요약 대상에 특화: 웹 기반 요약, 이메일 기반 요약&lt;/h3&gt;&lt;p&gt;요약 대상이 일반 문헌이 아니라 &lt;b&gt;웹 페이지&lt;/b&gt;이거나 &lt;b&gt;이메일&lt;/b&gt;일수도 있습니다. 이들의 경우 일반 문헌과는 다른 특성을 가지기 때문에 좀더 특화된 요약 기법을 사용하는 것이 가능합니다. 먼저 &lt;b&gt;웹 페이지&lt;/b&gt;의 경우 한 웹 페이지가 다른 웹 페이지들과 유기적으로 연결되어 있다는 특성을 가집니다. 특히 하이퍼 링크로 연결된 페이지들의 경우 서로 관련 있는 경우가 많기 때문에 이런 관련된 페이지들을 바탕으로 해당 페이지에서 말하는 내용 중 무엇이 핵심인지 파악하기 용이할 수 있습니다. 웹 페이지 요약에서는 이런 특성들을 십분 활용하고자 합니다.&lt;/p&gt;&lt;p&gt;&lt;b&gt;이메일&lt;/b&gt;은&amp;nbsp;오고 가는 대화가 기록되는, 구어체와 문어체의 특성이 공존하는 매체입니다. 이런 대화체의 문헌들에서 중요한 내용을 끄집어 내기 위해서는 일반 문헌과는 다른 기법이 필요합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;이용자 맞춤: 개인화(Personalized) 요약, 업데이트(Update) 요약&lt;/h3&gt;&lt;p&gt;이 경우는 요약 내용을 이용자의 입맛에 맞추어주는 기법들입니다. 사람마다 관심사가 다르기 때문에 같은 문헌의 요약을 받아보더라도, 그중에서 더 관심 있는 부분과 관심없는 부분은 사람에 따라&amp;nbsp;다르게 마련이죠. 따라서 이용자가 선호하는 패턴이나 주제를 기억해두었다가 그 내용에 알맞게 더 요약을 생성하는 것을 &lt;b&gt;개인화 요약&lt;/b&gt;이라고 합니다.&lt;/p&gt;&lt;p&gt;또한 이용자가 특정 분야에 대해 어느 정도 지식이 있는 경우, 요약문에서 기존에 알고 있는 내용 말고 새로운 내용만을 보고 싶을 수 있습니다. 이런 경우 이용자가 이미 아는 것 말고 새로 업데이트된 내용만을 바탕으로 요약문을 생성할 수 있겠습니다. 이를 &lt;b&gt;업데이트 요약&lt;/b&gt;이라고 합니다.&amp;nbsp;이런 시스템이 잘 구추된다면, 전문직 분야의 종사하는 사람들이 해당 분야의 새로운 소식들을 알아보는데 유용하게 사용할 수 있을 것이라 기대됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;감성기반(sentiment-based) 요약&lt;/h3&gt;&lt;p&gt;소셜 텍스트의 경우, 이용자들의 감성이 담겨 있는 경우가 많습니다. SNS에서 자신이 사용한&amp;nbsp;제품의 후기를 올리는 경우를 생각해보시면 좋습니다. 각각의 후기는 정말 짧고 별 내용이 없을 수 있지만, 수 십억 인구가 이렇게 올리는 SNS 내용을 보면 해당 제품에 대한 통찰력을 얻을테지요. 그렇지만 사람이 그것들을 일일히 볼수 없으니, 이를 감성에 따라 요약하여 받아보고자 하는 경우가 많습니다. 이런 경우 &lt;b&gt;감성기반 요약&lt;/b&gt;이 사용되게 됩니다. 감성 분석(Sentiment Analysis)을 통해 주관성 여부를 판단한 뒤, 극성을 판단하여 긍정/중립/부정으로 텍스트를 분류하고 다중 문서 요약을 수행하는 식으로 요약을 수행하는 경우가 많습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;추출적 요약의 주요 접근 방법&lt;sup class=&quot;footnote&quot;&gt;&lt;a id=&quot;footnote_link_625_2&quot; href=&quot;#footnote_625_2&quot; onmouseover=&quot;tistoryFootnote.show(this,625,2)&quot; onmouseout=&quot;tistoryFootnote.hide(625,2)&quot; style=&quot;color:#f9650d;font-family:Verdana,Sans-serif;display:inline;&quot;&gt;&lt;span style=&quot;display:none&quot;&gt;[각주:&lt;/span&gt;2&lt;span style=&quot;display:none&quot;&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;&lt;p&gt;앞서 설명했듯이, 추출적 요약은 문헌 내의 문장들 중에서 중요한 문장만 골라냄으로써 요약을 수행합니다. 이는 바꿔말하면, 각각의 문장이 중요한지 사소한지를 분류하는 이진 분류 문제와 동일합니다. 그리고 이는 정보 검색의 주요 분야 중 하나인 자동 색인과 매우 유사합니다. 자동 색인은 결국 문헌의 내용을 가장 잘 들어내는 키워드를 뽑는 문제인데, 이는 문헌 내에 등장하는 수 많은 단어들이 주제어인지 비주제어인지 분류하는 이진 문제이기 때문이죠. 즉 자동 색인이 단어를 대상으로 중요한 것을 뽑는 것이라면, 자동 요약은 문장을 대상으로 중요한 것을 뽑는 문제라는 겁니다. 따라서 자동 색인에서 사용하던 여러 통계적 기법을 거의 그대로 자동 요약에 사용할 수 있습니다. 따라서 추출적 요약의 가장 기본적인 접근 방법은 통계적 방법이라고 할 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;통계 기반&amp;nbsp;접근법&lt;/h3&gt;&lt;p&gt;중요한 단어를 찾아내는 가장 간단한 통계적 방법은 빈도 조사입니다. 이 외에도 좀더 정교한 방법으로 TF*IDF, 정보 획득량(information gain), 상호 정보량(mutual information), 기타 IDF를 변형한 척도들이 사용될 수도 있겠죠. 그리고 이렇게 얻어진 단어의 중요도를 바탕으로 전체 문장의 중요도도 계산이 가능합니다.&lt;/p&gt;&lt;p&gt;단어의 중요도 외에도 자주 사용되는 자질에는, 문장의 위치, 문장 내 긍정 키워드(중요문장에 자주 등장하는 키워드)의 빈도, 부정 키워드(비중요문장에 자주 등장하는 키워드)의 빈도, 문장의 중심성, 제목과의 유사도, 문장의 길이, 문장 내 수치 데이터의 유무, 개체명의 유무 등이 있습니다. 이에 대해서는 다음 논문에서 좀더 상세히 살펴볼 수 있다고 합니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126–144. doi:10.1016/j.csl.2008.04.002&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이런 통계적 자질들은 그 자체로 비지도학습 방법으로&amp;nbsp;중요 문장을 변별하는데 쓰일수도 있고, 이를 기계학습의 입력으로 사용하여 지도학습 방법으로 분류 모델을 구축하는데에 쓰일수도 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;주제 기반 접근법&lt;/h3&gt;&lt;p&gt;문헌 내에는 문헌의 주제와 관련된 문장도 있고, 그렇지 않은 문장도 섞여 있을 겁니다. 결국 우리가 문헌에서 알고싶은 부분은 문헌의 주제와 관련된 부분이므로, 이들만 추려내면 좋은 요약문을 만들수 있을 것이라는게 주제기반 접근법의 아이디어입니다. 그렇다면 어떤 문장이 주제와 얼만큼 연관되어 있는지를 파악하는게 이 기법의 핵심이 될 겁니다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;이를 측정하는 대표적인 개념이 &lt;b&gt;Topic Signature&lt;/b&gt;인데, 이는 각 단어가 특정 주제를 얼마나 드러내는지를 보여주는 지표가 됩니다. 예를 들어 &quot;유성우&quot;라는 주제가 있다면, 이 주제를 드러내는 단어들에는 {천체, 별똥별, 운석, 혜성, 대기} 등이 있을 겁니다. 이런 단어들이 많이 등장하는 문장이 있다면 그 문장은 &quot;유성우&quot;와 관련된 문장이라고 판단할 수 있겠죠? Topic Signature를 일일히 수작업으로 만들수는 없고, 대게 통계적인 방법을 바탕으로 TS의 목록을 구합니다. 가장 간단한 것은 &quot;유성우&quot;와 관련된 문헌을 수집하고, 또 &quot;유성우&quot;와 관련없는 문헌을 수집한 다음, 관련 없는 문헌보다 관련된 문헌에서 더 자주 등장하는 단어의 목록을 추출하면 이 단어들을 TS라고 볼 수 있을 겁니다. 엄밀성을 위해서는 관련있는 문헌을 수작업으로 분류해야 하겠지만, 사실 &quot;유성우&quot;라는 키워드로 문헌 검색을 해서 나온 결과를 가지고도 만족할 만한 결과를 얻을수 있습니다.&lt;/p&gt;&lt;p&gt;이외에도 좀더 정교한 방법으로 문장의 주제 연관 여부를 측정하는 방법들이 제안되었습니다. Enhanced topic signatures, Thematic signatures 등이 있고 이에 대해서는 다음 논문에서 자세히 살펴볼 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIR’ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202–209&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;주제 기반 접근법의 경우 위에서 설명했던 포괄적 요약뿐만 아니라 질의집중적 요약을 수행하는데 유용하게 쓰일 수 있습니다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;그래프 기반 접근법&lt;/h3&gt;&lt;p&gt;그래프 기반 접근법에서는 텍스트 내의 단어 혹은 문장을 각각의 정점(node)으로 보고, 이들 사이의 관계를 간선(edge)으로 표현하여 그래프 분석 기법을 적용하는 접근방법을 가리킵니다. 이에 대해서는 앞서 &lt;a href=&quot;https://bab2min.tistory.com/552&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;TextRank라는 기법을 소개&lt;/a&gt;한 적이 있습니다. &lt;b&gt;TextRank&lt;/b&gt;에서는 문헌 요약을 위해 문헌을 문장 단위로 쪼개서 각 문장을 node로 설정하였고, 문장들 간의 유사도를 edge로 설정하였습니다. 그리고 PageRank를 통해 문장의 중요도 점수를 매겼죠. 이를 통해 상위 N개의 중요 문장만 골라내면 이것이 요약 결과가 되는 식입니다.&lt;/p&gt;&lt;p&gt;좀더 정교한 방법으로는 2013년 등장한 &lt;b&gt;GRAPHSUM&lt;/b&gt;이 있습니다. GRAPHSUM을 간략히 설명하자면, 문헌들 내의 문장들에서 단어들을 추출하여, 단어들 간의 상관그래프를 구축합니다. 이 그래프에 PageRank를 적용하여 단어들의 중요도 점수를 계산합니다. 최종적으로 문장의 중요도를 계산하는데 이는 문장의 각 단어들의 중요도를 합한 것과 문장의 그래프 coverage를 조합하여 얻습니다. 자세한 내용은 다음 논문에서 확인할 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Baralis, E., Cagliero, L., Mahoto, N., &amp;amp; Fiori, A. (2013). GRAPHSUM: Discovering correlations among multiple terms for graph-based summarization. Information Sciences, 249, 96-109.&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그래프 기반 접근법은 대게 비지도학습적 방법이며, 별도의 학습 과정이&amp;nbsp;없다는 점 때문에 새로운 도메인에 쉽게 적용할 수 있다는 장점이 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;담화 기반 접근법&lt;/h3&gt;&lt;p&gt;이는 &lt;b&gt;수사구조이론(Rhetorical structure theory)&lt;/b&gt;에 바탕을 두고 있습니다. 이는 인간이 쓴 글들이 가지고 있는 구조를 분석하는 이론입니다. 논문이나 소설, 보고서, 하다못해 감상문 같은 글을 생각해봐도 대체로 그 구조는 비슷합니다. 먼저 제일 앞에 제목이 나오고, 그 다음 서론, 다음에는 핵심이 나오고 마지막에는 전체 내용을 마무리하는 내용이 등장하면서 전체 글이 끝나죠. 이렇게 어떤 글이라 할지라도&amp;nbsp;여러 부분으로 이뤄져 있고,&amp;nbsp;각 부분이 다른 부분과 특정한 역할을 맺고 있는데 이를 분석하는게 RST라고 할 수 있습니다. 이런 특성을 활용하면 핵심 부분을 찾아서 전체 글을 요약하는 작업도 할 수 있지 않을까 하는게 담화 기반 접근법의 요지입니다. 언어학 전공이 아니라서 더 깊이는 모르겠네요.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Mann W, Thompson S (1988) Rhetorical structure theory: toward a functional theory of text organization. Text 8:243–281&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;기계학습 기반 접근법&lt;/h3&gt;&lt;p&gt;분류 문제를 해결할 수 있는 최신 방법은 역시 기계학습입니다. 중요한 문장인지 사소한 문장인지 라벨이 매겨진 수 많은 문장쌍을 입력으로 받아 새로 입력 받은 문장이 중요한지 사소한지 분류할 수 있는 분류기를 학습시킬 수 있습니다. 이에는 &lt;b&gt;SVM&lt;/b&gt;, &lt;b&gt;Naive Bayes&lt;/b&gt;, &lt;b&gt;의사결정트리&lt;/b&gt;&amp;nbsp;등의 분류기를 사용할 수 있습니다. 단, 문장은 텍스트로 이루어져 있으므로, 그 자체를 분류기의 학습으로 입력할 수는 없습니다. 그 대신 문장에서 추출한 자질들을 분류기의 학습으로 입력하게 되는데, 주로 사용하는 자질들은 위의 통계적 기법에서 소개한 자질들이 있습니다. 높은 성능을 위해서는 자질들 잘 선택해야하는데, 자질 선택의 어려움을 피하기 위해, &lt;b&gt;신경망&lt;/b&gt;을 이용한 분류기도 적용되고 있는 추세구요. 특히 신경망을 활용한 딥러닝의 경우 아래에서 소개할 추상적 요약 기법에서 더욱 활용되고 있습니다. 비지도학습 방법도 또한 여럿 시도되고 있는데 대표적인 것엔&amp;nbsp;&lt;b&gt;클러스터링&lt;/b&gt;, &lt;b&gt;은닉 마르코프 모델&lt;/b&gt; 등이 있다고 합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;추상적 요약의 주요 접근 방법&lt;/h2&gt;&lt;p&gt;추상적 요약은 자연언어처리의 어려움 때문에 추출적 요약보다는 상대적으로 적게 연구가 되었습니다. 추출적 요약 기법이 잘 정제된 문헌에 대해서는 쓸만한 결과를 내면서도, 추상적 요약보다는 그 난이도가 상대적으로 낮기 때문이죠. 그런데 텍스트 길이가 짧고, 같은 의미에 대해서도 여러가지 표현 방법이&amp;nbsp;있는 소셜 텍스트들에 대해서는 추출적 요약 기법이 잘 작동하지 않습니다. 같은 의미를 뜻하는 문장이더라도 그 표현 방법이 너무나도 자유롭기에, 중복 제거가 잘 되지 않기 때문입니다. 따라서 이런 경우엔&amp;nbsp;좀더 간결하면서도 함축적으로 많은 정보를 전달할 수 있는 추상적 요약이 좋은 해결책이 될 것입니다. 추성적 요약의 주요 접근 방법에는 크게 다음과 같이 나눌 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;구조 기반 접근법&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Prior Knowledge&lt;/b&gt; 기반, 혹은 &lt;b&gt;템플릿(template) 기반 접근법&lt;/b&gt;이라고 봐도 무방합니다. 이는 사람이 직접 구축한 지식을 바탕으로 요약문을 생성하는 것을 가리킵니다. 문헌의 패턴에 따라 적절한 요약문의 템플릿을 미리 생성해두고, 전체 문헌 내에서 템플릿에 들어갈 내용들을 찾아서 채워넣는 것이죠. 템플릿을 수작업으로 만들기에 깔끔한 결과물이 나올 수 있지만, 얼마나 다양한 패턴에 맞춰서 템플릿을 만들지, 템플릿에 채워넣을 내용을 얼마나 잘 추출할지에 따라 전체 성능이 크게 좌우됩니다. 꽤나 품이 많이 드는 작업이라고 할 수 있겠죠. 뉴스 기사 생성이나 요약 등에 주로 사용된다고 합니다. 템플릿에 채워 넣을 내용을 추출하기 위해서는 &lt;b&gt;규칙&lt;/b&gt;을 사용하거나(Rule-based), &lt;b&gt;온톨로지&lt;/b&gt;를 활용하거나, 구문을 파싱하여 &lt;b&gt;tree&lt;/b&gt;를 이용합니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Harabagiu, S. M., &amp;amp; Lacatusu, F. (2002, July). Generating single and multi-document summaries with gistexter. In Document Understanding Conferences (pp. 11-12).&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;small&gt;Kasture, N. R., Yargal, N., Singh, N. N., Kulkarni, N., &amp;amp; Mathur, V. (2014). A survey on methods of abstractive text summarization. Int. J. Res. Merg. Sci. Technol, 1(6), 53-57.&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;그래프 기반 접근법&lt;/h3&gt;&lt;p&gt;추출적 요약에서와 유사하게 그래프 기반으로 접근하는 방법 역시 시도되었습니다. &lt;b&gt;Opinosis&lt;/b&gt;의 경우 제품 리뷰와 같이 다양하면서 여러 의견이 등장하는 문헌 집합을 요약하기 위해 그래프를 사용하였습니다. 여기에서는 각 단어를 node로 보고, 문장에서 등장하는&amp;nbsp;각 단어와 그 단어의 다음 단어를 edge로 연결하여 방향성 그래프를 생성했습니다. 이렇게 여러 opinion의 내용을 하나의 그래프로 표현하면 여러 edge들이 하나의 node로 모이고, 또 여러 edge로 갈라져 나가는&amp;nbsp;hub를 발견할 수 있고, 이를 바탕으로 weight가 높은 경로를 골라낼 수도 있습니다. weight가 높으면서 문법적으로 알맞는 경로를 골라내 문장으로 조합하는 방식으로 요약을 실시했습니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Ganesan, K., Zhai, C., &amp;amp; Han, J. (2010, August). Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions. In&amp;nbsp;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif; font-size: 13px;&quot;&gt;Proceedings of the 23rd international conference on computational linguistics&lt;/i&gt;&amp;nbsp;(pp. 340-348). Association for Computational Linguistics.&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;의미 기반 접근법&lt;/h3&gt;&lt;p&gt;의미 기반 접근법은 문장의 의미를 해석하고, 축약하여 요약문을 생성하는 작업입니다. 이는 결국 &lt;b&gt;자연언어이해(Natural Language Understanding)&lt;/b&gt;와 &lt;b&gt;자연언어생성(Natural Language Generation)&lt;/b&gt;이 요구되는 접근법이라고 할 수 있겠죠. 이 방법은 다음과 같이 사람이 문헌을 요약하는 것과&amp;nbsp;유사하게 작업이 진행됩니다.&amp;nbsp;&lt;/p&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;문헌을 읽고선 이를 해석하여 중간 형태로 표현한다.&lt;/li&gt;&lt;li&gt;중간 형태를 간략하게 압축한다.&lt;/li&gt;&lt;li&gt;압축된 중간 형태를 다시 문장의 형태로 바꾸어 작성한다.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;1번과 3번도 충분히 어렵지만, 2번에서 중간 형태를 어떻게 설정할지도 난감한 문제입니다.&lt;/div&gt;&lt;p&gt;고전적인 방법에서는 중간형태를 언어학의 &lt;b&gt;의미론(Semantics)&lt;/b&gt;에서 빌려옵니다. 의미론에서 하나의 문장은 최소 하나 이상의 Predicate과 Predicate에 딸리는 Argument를 가진다고 이야기합니다. 따라서 문장을 Predicate과 Argument로 분해할수도 있구요. 다음 예시를 보면 이해가 쉬울겁니다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;나는 오늘 영희가 좋아하던 철수를 만났다. =&amp;gt; &lt;b&gt;만나다&lt;/b&gt;(나, 철수) ^ &lt;b&gt;좋아하다&lt;/b&gt;(영희, 철수)&lt;/p&gt;&lt;/div&gt;&lt;p&gt;만나다, 좋아하다는 각각 Predicate를 이루고, 이 Predicate의 Argument로 문장 내에 등장하는 Entity가 들어가게 됩니다. 이를 통해 문장은 트리 혹은 그래프의 형태로 표현이 가능하게 됩니다. Liu의 최근 연구에서는 Abstract Meaning Representation을 바탕으로 압축된 형태의 그래프를 형성하기 위해 통계적 기법의 모델을 사용하였습니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Liu, F., Flanigan, J., Thomson, S., Sadeh, N., &amp;amp; Smith, N. A. (2018). Toward abstractive summarization using semantic representations.&amp;nbsp;&lt;i&gt;arXiv preprint arXiv:1805.10399&lt;/i&gt;.&lt;/small&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;최근에는 중간 형태를 인간이 직접 정의해주는 대신, &lt;b&gt;딥러닝(Deep Learning)의 임베딩&lt;/b&gt;을 활용하는 기법도 등장하고 있습니다. 중간 의미로 N차원의 벡터가 들어가지만, 이게 어떤 의미인지는 모릅니다. 다만 인풋과 아웃풋을 적절하게 넣어주고 이를 학습하도록 하여 최적의 모델을 만들도록 하는거죠. 이때 입력되는 문헌도 여러 단어로 이뤄져 있고, 출력되는 문헌도 여러 단어로 이뤄져 있으므로 many-to-many를 다룰수 있는 &lt;b&gt;RNN&lt;/b&gt; 모델이 애용됩니다. 특히 &lt;b&gt;LSTM을 이용한 encoder-decoder sequence model&lt;/b&gt;이 적절할 겁니다. Paulus의 최근 연구는 강화학습을 이용해 추상적 요약 문제를 해결하고자 시도했습니다.&lt;/p&gt;&lt;p&gt;&lt;small&gt;Paulus, R., Xiong, C., &amp;amp; Socher, R. (2017). A deep reinforced model for abstractive summarization.&amp;nbsp;&lt;i&gt;arXiv preprint arXiv:1705.04304&lt;/i&gt;.&lt;/small&gt;&lt;/p&gt;&lt;h2&gt;자동 요약의 평가&lt;sup class=&quot;footnote&quot;&gt;&lt;a id=&quot;footnote_link_625_3&quot; href=&quot;#footnote_625_3&quot; onmouseover=&quot;tistoryFootnote.show(this,625,3)&quot; onmouseout=&quot;tistoryFootnote.hide(625,3)&quot; style=&quot;color:#f9650d;font-family:Verdana,Sans-serif;display:inline;&quot;&gt;&lt;span style=&quot;display:none&quot;&gt;[각주:&lt;/span&gt;3&lt;span style=&quot;display:none&quot;&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;&lt;p&gt;자동 요약 기법을 평가하는 것도 쉬운 일이 아닙니다. 제일 정확한것은 기계가 요약해낸 내용을 여러 사람이 직접 읽고 점수를 매겨주는 것이겠지만, 사람마다 맞다고 여기는 요약의 내용이 다를 수 있기 때문에 객관적이며&amp;nbsp;체계적이기 어렵습니다. 따라서 자동요약의 결과를 체계적으로 평가하기 위해 &lt;b&gt;SUMMAC&lt;/b&gt;(1996~1998), &lt;b&gt;DUC&lt;/b&gt;(2000~2007), &lt;b&gt;TAC&lt;/b&gt;(2008~) 등의 컨퍼런스에서 표준적인 evaluation set을 구축하려 노력해왔습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;인간에 의한 평가&lt;/h3&gt;&lt;div&gt;DUC에서는 요약문의 &lt;b&gt;coverage&lt;/b&gt;를 주 평가 기준으로 사용했습니다. 즉 요약이 원&amp;nbsp;문헌의 내용을 얼마나 넓게 다루는지를 바탕으로 성능을 평가한 것이죠. 최근 TAC에서는 &lt;b&gt;문법 적합성&lt;/b&gt;, &lt;b&gt;비 중복성&lt;/b&gt;, &lt;b&gt;중요 정보들의 통합 여부&lt;/b&gt;, &lt;b&gt;구조&lt;/b&gt;,&amp;nbsp;&lt;b&gt;일관성 &lt;/b&gt;등을 바탕으로 요약문을 평가하도록 제안했습니다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;자동 평가 방법&lt;/h3&gt;&lt;p&gt;아무래도 사람이 매번 결과를 직접 평가하기는 어렵습니다. 따라서 자동적으로 평가할 수 있는 방법들이 제안되었는데, 대표적인 것으로 &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;ROUGE(Recall Oriented Understudy of Gisting Evaluation&lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;)&lt;/a&gt;가 있습니다.&lt;/p&gt;&lt;p&gt;ROUGE의 아이디어를 간략하게 정리하자면, 먼저 모범 답안 요약문을 만들어놓고, 자동 요약된 요약문이 모범 답안과 얼마나 유사한지를 비교하자는 것입니다. 단, 두 요약문 사이의 유사도를 계산할때 단어 단위에서 일치도를 보게 되는데, 다음 3종류가 제일 자주 사용되는 방법이라고 하네요.&lt;/p&gt;&lt;h4&gt;ROUGE-n&lt;/h4&gt;&lt;p&gt;n-gram 기반으로 두 요약문 사이의 유사도를 계산합니다. n-gram이라함은 연속된 n개의 단어를 하나의 단위로 보겠다는 겁니다. 모범 답안이 다음과 같다고 해봅시다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(243, 197, 52); background-color: rgb(254, 254, 184); padding: 10px;&quot;&gt;&lt;p&gt;the cat was on my keyboard&lt;br /&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;그리고 자동 요약 결과로 생성된 요약문은 다음과 같다고 해봅시다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(243, 197, 52); background-color: rgb(254, 254, 184); padding: 10px;&quot;&gt;&lt;p&gt;cat sat on the keyboard&lt;br /&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;ROUGE-1에서는 1-gram, 즉 단어 단위로 두 요약문을 비교합니다. 이 때 모범답안은 {&lt;b&gt;the&lt;/b&gt;, &lt;b&gt;cat&lt;/b&gt;, was, &lt;b&gt;on&lt;/b&gt;, my, &lt;b&gt;keyboard&lt;/b&gt;}이고, 자동 요약은 {&lt;b&gt;cat&lt;/b&gt;, sat, &lt;b&gt;on&lt;/b&gt;, &lt;b&gt;the&lt;/b&gt;, &lt;b&gt;keyboard&lt;/b&gt;}가 됩니다. Precision과 Recall은 다음과 같이 계산됩니다.&lt;/p&gt;&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;b&gt;Precision = (양쪽 모두 일치하는 단어 개수) / (모범 답안의 단어 개수)&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;b&gt;Recall = (양쪽 모두 일치하는 단어 개수) / (자동 요약 결과의 단어 개수)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;따라서 ROUGE-1의 Precision = 4 / 6, Recall = 4 / 5가 됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;반면 ROUGE-2에서는 2-gram을 기준으로 계산하므로, 모범 답안은 {^the, the cat, cat was, was on, on my, my keyboard, &lt;b&gt;keyboard$&lt;/b&gt;}, 자동 요약은 {^cat, cat sat, sat on, on the, the keyboard, &lt;b&gt;keyboard$&lt;/b&gt;} 입니다.&lt;/p&gt;&lt;p&gt;따라서 이때의 Precision = 1 / 7, Recall = 1 / 6이 됩니다.&lt;/p&gt;&lt;p&gt;n이 커질수록 연속된 단어가 일치해야하므로, 좀더 빡빡한 평가 척도가 됩니다. 반면 n = 1일 경우 어순에 상관없이 단어가 등장하기만 해도 맞다고 카운트해주겠죠.&lt;/p&gt;&lt;h4&gt;ROUGE-L&lt;/h4&gt;&lt;p&gt;ROUGE-L은 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%9E%A5_%EA%B3%B5%ED%86%B5_%EB%B6%80%EB%B6%84_%EC%88%98%EC%97%B4&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;최장 공통 부분 수열&lt;/a&gt;을 가지고 평가하는 방법입니다. 좀더 쉽게 풀어쓰자면, 두 요약문에서 공통으로 등장하는 연속된 단어열 중 가장 길이가 긴 녀석을 찾겠다는 겁니다. 예를 들어 A&lt;b&gt;BCD&lt;/b&gt;EF와 &lt;b&gt;BCD&lt;/b&gt;ADF의 최장 공통 부분 수열은 &lt;b&gt;BCD&lt;/b&gt;가 됩니다. BCD가 연속해서 등장한 수열 중 가장 길이가 길기 때문이지요. ROUGE-L에서의 Precision과 Recall은 다음과 같이 정의됩니다.&lt;/p&gt;&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;b&gt;Precision = (최장 공통 부분 수열의 길이) / (모범 답안의 단어 개수)&lt;/b&gt;&lt;/p&gt;&lt;p style=&quot;margin-left: 2em;&quot;&gt;&lt;b&gt;Recall = (최장 공통 부분 수열의 길이) / (자동 요약 결과의 단어 개수)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h4&gt;ROUGE-SU&lt;/h4&gt;&lt;p&gt;ROUGE-SU는 Skip-bigram과 Unigram을 포함하는 변형 척도입니다. Skip-bigram은 연속된 두 단어뿐만 아니라, 하나 건너 뛴 두 단어도 고려하는 skip-gram 기법을 말합니다. 즉 ABCD라는 단어열이 있으면 이것의 모든 skip-bigram은 AB, BC, CD 뿐만 아니라 AC, BD (사이에 단어 하나를 건너뛴 경우)도 포함하게 됩니다.&lt;/p&gt;&lt;p&gt;ROUGE-N에서와 같은 예문을 가지고 ROUGE-SU의 예시를 보이면 다음과 같습니다. 모범 답안의 경우 {&lt;b&gt;the&lt;/b&gt;, &lt;b&gt;cat&lt;/b&gt;, was, &lt;b&gt;on&lt;/b&gt;, my, &lt;b&gt;keyboard &lt;/b&gt;(unigram), &lt;span style=&quot;color: rgb(255, 0, 0);&quot;&gt;^the, the cat, cat was, was on, on my, my keyboard, &lt;b&gt;keyboard$&lt;/b&gt; (bigram)&lt;/span&gt;, &lt;span style=&quot;color: rgb(0, 0, 255);&quot;&gt;the was, &lt;b&gt;cat on&lt;/b&gt;, was my, &lt;b&gt;on keyboard&lt;/b&gt; (skip-bigram)&lt;/span&gt;}가 되고, 자동 요약 결과는 {&lt;b&gt;cat&lt;/b&gt;, sat, &lt;b&gt;on&lt;/b&gt;, &lt;b&gt;the&lt;/b&gt;, &lt;b&gt;keyboard &lt;/b&gt;(unigram), &lt;span style=&quot;color: rgb(255, 0, 0);&quot;&gt;^cat, cat sat, sat on, on the, the keyboard, &lt;b&gt;keyboard$&lt;/b&gt; (bigram)&lt;/span&gt;, &lt;span style=&quot;color: rgb(0, 0, 255);&quot;&gt;&lt;b&gt;cat on&lt;/b&gt;, sat the, &lt;b&gt;on keyboard&lt;/b&gt; (skip-bigram)&lt;/span&gt;}가 됩니다.&lt;/p&gt;&lt;p&gt;따라서 이 경우엔&amp;nbsp;Precision = 7 / 17, Recall = 7 / 14 가 됩니다.&lt;/p&gt;&lt;p&gt;ROUGE-SU는 한 단어를 건너뛰는 경우도 포함하기 때문에 두 단어 사이에 다른 단어가 끼어들거나, 바뀐 경우를 용인해주는, ROUGE-2보다는 관대하지만, ROUGE-1보다는 엄격한 척도라고 볼 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;지금까지 간략하게나마 자동 요약 기법에 대해서 정리해봤습니다. 사실 금방 끝날줄 알았는데, 이 분야가 생각보다 오래전부터 연구가 시작됐고, 다양하게 분화되어서 겉핥기만 하는데도 꽤 걸렸네요. 아마 이 분야 공부를 시작한다면 최근 많이 떠오르고 있는 딥러닝의 seq2seq 모델을 적용한 추상적 요약 쪽을 살펴보는게 좋을 것 같습니다. 개인적으로 이 기법의 향후 발전도 기대가 많이 되고 있어서 좀더 관련 연구를 찾아보려 하고 있어요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;div class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes&quot;&gt;
&lt;li id=&quot;footnote_625_1&quot;&gt;Gambhir (2017)의 2. Various types of text Summarization 의 내용을 바탕으로 요약하였음 &lt;a href=&quot;#footnote_link_625_1&quot;&gt;[본문으로]&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;footnote_625_2&quot;&gt;Gambhir (2017)의 3. Classification of extractive approaches for summary generation 의 내용을 바탕으로 요약하였음 &lt;a href=&quot;#footnote_link_625_2&quot;&gt;[본문으로]&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;footnote_625_3&quot;&gt;Allahyari(2017)의 7. Evaluation 부분을 바탕으로 요약하였음. &lt;a href=&quot;#footnote_link_625_3&quot;&gt;[본문으로]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/625&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>그냥 공부</category>
<category>자동 요약</category>
<category>자연언어처리</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/625</guid>
<comments>https://bab2min.tistory.com/625#entry625comment</comments>
<pubDate>Fri, 28 Dec 2018 03:22:15 +0900</pubDate>
</item>
<item>
<title>[Kiwi] 지능형 한국어 형태소 분석기 0.6버전 업데이트</title>
<link>https://bab2min.tistory.com/624</link>
<description>&lt;p&gt;최근 Kiwi 형태소 분석기 0.6 버전 업데이트를 실시했습니다. 사실 엄청 바뀐건 없고, 이전 버전에서 사용하던 최적화를 좀더 한 단계 끌어올리는 작업을 수행했습니다. &lt;a href=&quot;https://bab2min.tistory.com/580&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;0.4버전에서 도입한 그래프 기반 경로 탐색 알고리즘&lt;/a&gt;의 경우, 이상하게도 끝쪽에서 앞쪽으로 분석을 진행했습니다. 당시에 왜 이렇게 설계했는지 모르겠는데, 개발하고 보니, 역방향으로 추적을 진행하더라구요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;사실 경로 탐색을 진행하는게 순방향인지 역방향인지 자체는 성능에 영향을 미치지 않습니다만, n-gram 언어모델을 캐싱하는데에는 순방향이 훨씬 유리합니다. 0.5버전에서 Kneser-Ney smoothing을 적용한 n-gram 언어 모델을 바탕으로 형태소가 출현할 확률을 계산하도록 알고리즘을 교체하였는데요, 이게 약 2만 여개의 형태소를 대상으로 trigram을 계산해야하는 작업이었습니다. 가능한 총 조합의 수는 최대 20000^3이므로, 이 전체가 메모리에 올라갈 수는 없습니다. 따라서 trie를 활용하여 trigram의 KN smoothing 값을 계산하도록 구현했습니다.&amp;nbsp;&lt;/p&gt;&lt;p&gt;trigram을 trie로 표현하는건 메모리를 아낄 순 있지만, 전반적인 탐색 속도는 느려지게 된다는 문제가 있습니다.&amp;nbsp;이 때문에 형태소 분석의 속도를 좌우하는게 이 trie의 순회 속도였는데, 이를 최적화하는게 이번 업데이트의 핵심이었습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이번 포스팅에서는 간략하게 그 최적화 방법에 대해서 설명하도록 하겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;Trie와 Trigram 확률 계산&lt;/h2&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:568px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99B57B475C0D475031&quot; filemime=&quot;image/jpeg&quot; filename=&quot;trie0.PNG&quot; height=&quot;646&quot; style=&quot;&quot; width=&quot;568&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Trigram&amp;nbsp;언어모델이라 함은 어떤 단어가 등장할 확률은 이전 두 단어에 영향을 받는다는 가정하에 다음 단어를 예측하도록 생성한 모델이라고 설명할 수 있습니다. 즉 AB 뒤에 C가 나올 확률을 보는 식이죠. 이는 AB가 등장하는 전체 빈도 대비 ABC가 등장하는 빈도를 계산함으로써 알 수 있습니다. 앞서 설명했다시피, 단어의 수가 수 만에 육박할때, 조합이 가능한 전체 trigram 수는 단어 수의 세제곱배가 되므로, 이 모든 경우를 배열에 저장할 수는 없습니다. 따라서 위와 같이 tree의 형태로 각 경우를 표현하는 방법을 쓰게됩니다. tree의 각 노드가 단어(혹은 글자)이고, 각 노드가 어떤 문자열(prefix)를 나타내게 될 경우 이를 trie라고 부릅니다.&amp;nbsp;위의 경우 A, B, C가 각 단어가 될것이고, 이 Trie는 &lt;b&gt;&lt;span style=&quot;color: rgb(9, 0, 255);&quot;&gt;A, B&lt;/span&gt;, &lt;span style=&quot;color: rgb(255, 94, 0);&quot;&gt;AA, AB, AC, BA, BC,&lt;/span&gt;&amp;nbsp;&lt;span style=&quot;color: rgb(31, 218, 17);&quot;&gt;AAA, AAB, ABA, ACC, BAA, BAB, BCC&lt;/span&gt;&lt;/b&gt;&amp;nbsp;라는 문자열을 표현하게 됩니다.&lt;/p&gt;&lt;p&gt;이 Trie의 각 노드에 확률값을 저장해두면, 쉽게 Trigram 언어모델을 계산하는데에 쓸 수가 있습니다. 위의 그림을 보시면 이해가 쉽습니다. 만약 ABA라는 문자열이 연속해서 등장할 확률을 이 Trie를 가지고 계산한다면, 먼저 Root노드에서 시작해서 A노드를 찾습니다. A노드는 &lt;b&gt;P(A)&lt;/b&gt;값을 가지고 있겠죠. 그 다음 B노드를 찾습니다. 이 노드는 &lt;b&gt;P(B|A)&lt;/b&gt;값을 갖고 있습니다. 마지막으로 A노드를 찾습니다. 이 노드는 &lt;b&gt;P(A|AB)&lt;/b&gt; 값을 가지고 있습니다. A, B, A를 차례로 순회하면서 얻은 이 확률값들을 모두 곱하면 &lt;b&gt;P(A) * P(B|A) * P(A|AB) = P(ABC)&lt;/b&gt;를 얻을 수 있는거죠.&lt;/p&gt;&lt;p&gt;꽤 효율적인 방법입니다. 다만 각 노드가 가지는 자식이 수 만개가 될 경우 생각보다 다음 노드를 찾는게 어려워집니다. 각 노드가 자식 노드 수 만개를 다 가지고 있지는 않을테니 이를 vector(array)가 아닌 map(dictionary) 형태로 저장할텐데, 이를 탐색하는데에는&amp;nbsp;O(log(N)) 시간이 소요됩니다. (당연히 이때 N은 전체 어휘의 수겠죠.) Trigram이므로 최소 3번의 탐색을 거쳐야 원하는 확률을 얻을 수 있을 것이므로 평균적으로 탐색에 걸리는 시간은 3*log(N)이 될것이구요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이제 여기서 ABAAB 문자열이 등장할 확률을 계산한다고 가정해봅시다. trigram이므로 P(ABAAB) = P(A) * P(B|A) * P(A|AB) * P(A|BA) * P(B|AA)가 될텐데요, P(A), P(B|A), P(A|AB)는 자식 노드를 따라가며 한번에 발견할 수 있지만 (총 3회의 탐색), P(A|BA), P(B|AA)는 앞의 경로와는 연결된게 없으므로, root부터 시작해서 다시 탐색해야하므로 각각 3회의 탐색이 필요합니다. 총 9회의 탐색이 필요한거죠. 이게 기존 0.5버전의 알고리즘이었습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:568px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/997CFB4A5C0D4B380C&quot; filemime=&quot;image/jpeg&quot; filename=&quot;trie1.PNG&quot; height=&quot;659&quot; style=&quot;&quot; width=&quot;568&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;/p&gt;&lt;p&gt;0.6버전에서는 상위 노드를 가리키는 빨간선이 추가되었습니다. abc라는 문자열이 있을때 앞의&amp;nbsp;a를 제거한 bc라는 문자열을 표현하는 노드를 가리키도록 한 것입니다. 예를 들어 ABA의 경우, 빨간선이 BA를 가리키도록 했고, BAA의 경우 AA를 가리키도록 했습니다. 이 빨간선은 자신의 부모노드를 가리키는 것과는 다릅니다. 자신이 가리키는 제일 앞의 문자를 제거하여 연속적으로 다음 문자가 등장할 확률을 탐색할 수 있도록 한것입니다.&lt;/p&gt;&lt;p&gt;위와 같이 ABAAB라는 문자열을 탐색한다고 가정해봅시다. AB&lt;b&gt;A&lt;/b&gt;까지 왔을 때 포인터는 P(A|AB) 지점을 가리키고 있을겁니다. 이제 다음문자로 A를 찾아야하는데, A에 해당하는 자식 노드가 없으므로 빨간선을 따라 상위 노드로 이동합니다. 그러면 P(A|B)노드에 도달하는데 이 노드는 A에 해당하는 자식을 가지고 있으므로 P(A|BA)를 얻을 수 있습니다. 이제 그 다음 B에 해당하는 자식 노드를 찾아야하는데, 역시 자식 노드가 없으므로 다시 빨간선을 타고 P(A|A)에 도달하면 자식 노드 B를 찾아서 P(B|AA)를 찾을 수 있습니다. 즉 연속하는 단어열을 검색할때 Root에서부터 찾지 않고 계속해서 빨간선과 파란선만 이동하면서 모든 확률값을 찾을 수 있게 된 것이죠. 따라서 다음 단어 한 개를 찾는데 평균적인 시간은 log(N)이 걸리게 됩니다. (3*log(N)이 아니라. 즉 이론상으로 3배 빨라질 수 있다는겁니다.)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;실제 성능 개선 결과는?&lt;/h2&gt;&lt;p&gt;이를 적용해 Kiwi의 성능을 끌어올렸는데요, 이유는 모르겠지만 정확도도 같이 상승했습니다. 아마 0.5버전 구현 어딘가에 사소한 실수가 있었거나, 속도를 보장하기 위해서 이런저런 트릭을 사용한게 정확도를 낮추는 문제가 있었나 봅니다. 0.6버전으로 넘어오면서 그런 트릭들 없이도 충분한 성능이 나와서 트릭들은 다 제거했기에 정확도도 상승한것 같구요.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/991031425C0D4D7B40&quot; filemime=&quot;image/jpeg&quot; filename=&quot;KiwiChart.PNG&quot; height=&quot;505&quot; style=&quot;&quot; width=&quot;800&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;이건 실행 속도를 비교한 결과를 그래프로 나타낸 것입니다. 이전버전보다 속도가 (3배까지는 아니고) 약 1.5~2배 정도 상승하여서 다른 분석기들과 비교해도 꿀리지 않는 속도를 보이고 있습니다. (Mecab이 제일 빠른 속도를 보인다고 알고 있는데, macOS가 아니라 구동이 불가한 관계로 속도 평가에서 제외됐습니다. 아마 다른 벤치마킹 결과들을 종합해볼때 Kiwi가 Mecab보다는 느릴것으로 예상됩니다.)&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;정확도의 경우 사전에 데이터가 없는 단어가 많은 비문학, 신문 기사 등에서는 평균 92%, 문학 작품에서는 96%를 보이는 것으로 측정되었구요. 자세한 결과는 &lt;a href=&quot;https://github.com/bab2min/Kiwi&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;Github &lt;/a&gt;리포지터리에서 확인하실 수 있습니다. (&lt;a href=&quot;https://lab.bab2min.pe.kr/kiwi&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;실행 데모는 여기&lt;/a&gt;에서!)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;처음에 개발을 시작했을때는, Kiwi는 c++로 개발할 것이므로 당연히 java로 만든 분석기들은 쉽게 따라잡을 수 있을거라고 생각했었습니다. 그런데 무려 1년이 넘게 개발해도 java로 짠 녀석들을 따라잡을 수 없었습니다... Java가 생각보다 느리지 않다는 것과 언어보다는 효율적인 알고리즘이 더 중요하다는 것을 다시 한 번 깨닫는 나날이었죠. 그래도 드디어 Java로 만든 분석기들을 따라잡게 되어 매우 기쁘네요. 아직 정확도 및 기능 면에서 부족한 점이 많지만, 당분간 Kiwi 개발에는 잠시 여유를 두고&amp;nbsp;다른 공부들을 더 진행해보려 합니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;물론 버그 제보나 기능 제안은 언제든지 환영이니, 메일이나 github을 통해서 전달 부탁드려요!&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/624&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>NLP</category>
<category>c++</category>
<category>kiwi</category>
<category>자연언어처리</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/624</guid>
<comments>https://bab2min.tistory.com/624#entry624comment</comments>
<pubDate>Sun, 09 Dec 2018 23:23:53 +0900</pubDate>
</item>
<item>
<title>[기계 번역] 이중 언어 데이터에서의 단어 임베딩 (Bilingual Word Embeddings from Non-Parallel Document Data)</title>
<link>https://bab2min.tistory.com/623</link>
<description>&lt;h2&gt;서론&lt;/h2&gt;&lt;p&gt;기계 번역은 (사실은 꽤 오래전부터지만) 최근 엄청나게 떠오르고 있는 Hot한 연구 분야입니다. 특히 몇년전 Google이 Google 번역에 딥러닝을 도입하고, 네이버가 파파고를 출시하면서 외국어 공부가 필요없는 세상이 다가오는 것처럼 보이기도 합니다. 기계 번역 분야는 정말 미래가 기대되는 연구 분야라고 할 수 있겠습니다.&lt;/p&gt;&lt;p&gt;딥러닝을 통한 기계 번역 기술의 비약적인 발전에는 단어 임베딩(Word Embedding)이라는 뿌리가 있습니다. 단어 임베딩 기법에 대해서는 예전에 &lt;a href=&quot;https://bab2min.tistory.com/608&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;Gaussian LDA를 소개&lt;/a&gt;하면서 잠깐 언급한 적이 있습니다. 단어 임베딩이란 각각의 단어를 숫자로 표현하는데, 그 때 그 숫자들이&amp;nbsp;단어의 의미를 반영할 수 있도록 하는 것이라고 한 줄 요약할 수 있겠지요.&lt;/p&gt;&lt;p&gt;결국 컴퓨터는 모든 데이터를 숫자로 처리하고, 딥러닝과 같은 복잡한 인공신경망도 숫자(입력 신호의 강도)만을 입력받을 수 있으니, 자연언어처리에 딥러닝을 적용하기 위해서는 각 단어들을 적절하게 숫자로 바꿔주는게 필수적입니다. Word Embedding이 중요한 이유라고 할 수 있죠.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h3&gt;언어 중립적인 단어 임베딩의 가능성&lt;/h3&gt;&lt;p&gt;기존의 Word2Vec과 같은 기법은 한 언어에 대해서만 단어 임베딩을 진행하는 기법이었습니다. 영어 텍스트를 가지고 학습을 진행한다면 Word2Vec은 텍스트 내의 단어들 간의 관계를 학습하여 &lt;b&gt;Girl-Boy&lt;/b&gt;, &lt;b&gt;Woman-Man&lt;/b&gt; 등의 관계를 파악할 수 있게 해줍니다. Word2Vec을 통해 각 단어들의 적절하게 숫자로 변환되었으니, 의미가 유사한 단어들은 좌표 상으로도 비슷한 위치에 존재할 것이고, 이를 통해서 다양한 자연언어처리 기법(텍스트 분류, 감성 분석, 개체명 인식, 질문 답변 등등)을 사용할 수가 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;다만 위와 같이 영어에 대해서 Word Embedding을 사용하고, 그 결과를 바탕으로 감성 분석 모델을 학습했다면, 그 모델은 영어에 대해서만 사용할 수 있습니다. 만약 한국어에 대해서 감성 분석 모델을 학습하고 싶다면, 다시 한국어 텍스트를 모으고, Word2Vec을 돌리고, 그 결과를 바탕으로 한국어 감성 모델을 학습해야하죠.&lt;/p&gt;&lt;p&gt;그런데 곰곰히 생각해보면, 단어 임베딩의 결과로 나타나는 숫자들이 실제 단어의 의미를 반영한다면, 언어에 독립적인 값을 가질 겁니다. (정확히는 단어가&amp;nbsp;가리키는 어떤 개념을 값으로 가지겠죠.) 예를 들어 &lt;b&gt;덥다-춥다&lt;/b&gt;라는 개념 간의&amp;nbsp;관계는 언어에 상관없이 반의어일 겁니다. 이 개념들 간의 관계는 대부분 언어와는 독립적인 것이고, 각각의&amp;nbsp;언어들은 이 개념을 다른 어휘로 표현할 뿐이라는 겁니다. 즉, &lt;b&gt;단어 임베딩의 값은 중립적인 중간 언어(Interlingual)&lt;/b&gt;가 될 수 있다는 얘기죠.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;우리가 적절한 형태의 중간 언어 임베딩을 구할 수 있다면, 이 임베딩으로 감성 분석 모델(을 비롯한 여러 가지 자연언어처리 모델)을 학습하고, 각각의 개별언어를 중간 언어의 임베딩으로 바꾸어서 이 모델에 넣는 것만으로 감성 분석을 실시할 수 있습니다. 언어 독립적인 감성 분석 모델을 학습할 수 있으니, 개별 언어에 대한 감성 분석 모델을 굳이 만들 필요가 없어집니다. 기계 번역도 아주 간단해지죠. A언어에서 중간 언어로 encoding을 하고, 중간 언어에서&amp;nbsp;B언어로 decoding해줌으로써 번역을 실시하는게 가능해집니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;문제는 이 중간 언어의 임베딩을 어떻게 구하느냐겠죠. 여러 가지 기법들이 제시되고 있는데 본 포스팅에서는&amp;nbsp;다음 논문의 내용을 바탕으로 그 기법들에 대해 살펴보도록 하겠습니다.&lt;/p&gt;&lt;div class=&quot;txc-textbox&quot; style=&quot;border-style: dashed; border-width: 1px; border-color: rgb(203, 203, 203); background-color: rgb(255, 255, 255); padding: 10px;&quot;&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;Vulić, I., &amp;amp; Moens, M. F. (2015). Bilingual word embeddings from non-parallel document-aligned data applied to bilingual lexicon induction. In&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;&amp;nbsp;(Vol. 2, pp. 719-725).&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;Duong, L., Kanayama, H., Ma, T., Bird, S., &amp;amp; Cohn, T. (2016). Learning crosslingual word embeddings without bilingual corpora.&amp;nbsp;&lt;/span&gt;&lt;i style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;arXiv preprint arXiv:1606.09403&lt;/i&gt;&lt;span style=&quot;color: rgb(34, 34, 34); font-family: Arial, sans-serif;&quot;&gt;.&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;기존의 방법들&lt;/h2&gt;&lt;p&gt;가장 대표적인 기존 방법은 선형 변환(Linear Transform)입니다. &lt;b&gt;Girl-Boy, Woman-Man&lt;/b&gt;&amp;nbsp;같이&amp;nbsp;개념들 사이의 관계는 어떤 언어에서나 유사할 것입니다. 다만 Word Embedding 기법의 특성 상 코드를 돌릴때마다 각 단어의 임베딩 벡터값은 제멋대로 바뀝니다. (처음 돌렸을때는 Girl을 [0.1, 0.3, -0.4]에 임베딩했는데, 지금 다시 돌려보니 [0.5, -0.2, 0.3]에 임베딩하는 것처럼) 다만 벡터값들은 계속 바뀌어도 단어 들 간의 관계(수학적으로는 &lt;b&gt;각도&lt;/b&gt;로 표현됩니다)는 그대로 유지되는게 특징입니다.&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:788px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99E25A475C00EBD620&quot; filemime=&quot;image/jpeg&quot; filename=&quot;ex1.png&quot; height=&quot;376&quot; original=&quot;yes&quot; width=&quot;788&quot;/&gt;&lt;span class=&quot;cap1&quot; style=&quot;display:block;max-width:100%;width:788px;&quot;&gt;단어 임베딩을 학습할 때 마다 각 점의 위치는 매번 바뀝니다. 오직 유지되는 건 점들 간의 각도입니다.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;투명 필름에 위 그림이 그려져있다고 상상해보시면 쉽습니다. 오른쪽,&amp;nbsp;왼쪽 좌표계의&amp;nbsp;점(Girl, Boy)들 위치는 다릅니다. 하지만 Girl과 Boy 사이의 각도는 유지되므로, 오른쪽 좌표계를 살포시 들어서 잘 돌리고 움직이다 보면 왼쪽 좌표계랑 Girl과 Boy가 같은 위치에 가도록 맞출 수 있을 겁니다. (이 때 맞추는 기준점을 pivot point라고 합니다.) 이렇게 좌표계를 회전시키고 이동시키고 확대시키고&amp;nbsp;축소시키는&amp;nbsp;변환들을 모두 통틀어 선형 변환이라고 합니다. 즉 선형 변환을 통해서 서로 다른 단어 임베딩 결과를 합치시킬 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이는 꼭 같은 언어에 국한되지 않습니다. 양쪽 다 충분히 큰&amp;nbsp;(같은 도메인 내의) 텍스트를 학습한 경우 Girl-Boy와 같이 주요 개념들 사이의 각도는 언어에 관계없이 거의 유사하게 됩니다. 따라서 한 언어를 기준으로 잡고, 다른 언어의 단어 임베딩 결과를 선형 변환시킴으로써 두 단어의 임베딩을 같은 공간에 위치시킬 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이 방법의 한계는 pivot point로 삼을 단어 목록이 필요하다는 것입니다. 위의 예시에서는 Girl과 Girl을 일치시키면 됐지만, 서로 다른 언어 사이에서는 &lt;b&gt;Girl-여자애&lt;/b&gt;처럼 무엇을 일치시켜야하는지 미리 알고 있어야 올바른 선형 변환이 가능하기 때문이죠. 즉 단어 사전이 필요하다는 건 큰 약점 중의 하나가 되겠습니다.&lt;/p&gt;&lt;p&gt;그리고 실제로&amp;nbsp;두 언어 간의 단어 임베딩 결과가 선형 변환만으로 완벽하게 일치하기&amp;nbsp;어려운 경우가 많습니다. 따라서 선형변환만으로는 오차가 커질 수 있기에, 어느 정도 한계가 있는 방법이라고 할 수 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;더 좋은 방법은 애초에 두 언어의 임베딩을 따로 구한뒤 선형변환으로 힘들게 합치지 말고, 처음부터 두 언어를 같이 넣고 임베딩을 구하는 것이겠죠. 이런 아이디어를 바탕으로 다음과 같은 방법들이 제시됩니다.&lt;/p&gt;&lt;h2&gt;단어 사전은 없지만 병렬 코퍼스는 있다&lt;/h2&gt;&lt;div&gt;Vulić, I.와 Moens, M. F.는 Word2Vec Skip-gram Negative Sampling을 응용하여 단어 사전이 없이도, 문헌 단위로 정렬된 병렬 코퍼스만 가지고 이중 언어 사이의 단어 임베딩이 가능함을 보였습니다. 문헌 단위로 정렬되었다는 말의 의미는 S언어로 된 문헌 D1_s가 있고, 이 문헌이 T언어로 번역된 D1_t가 있는 것을 말합니다. 이렇게 짝을 이룬 문헌쌍이 무수히 많을 경우, 다음과 같이 간단한 방법으로 Bilingual Word Embedding이 가능합니다.&lt;/div&gt;&lt;ol style=&quot;list-style-type: decimal;&quot;&gt;&lt;li&gt;모든 문헌쌍에 대하여(i는 문헌쌍의 일련번호) 다음을 반복한다&lt;/li&gt;&lt;li&gt;S언어로 된 문헌 Di_s와 T언어로 된 문헌 Di_t의 모든 단어들을 가져온다.&lt;/li&gt;&lt;li&gt;2에서 가져온 단어들을 합쳐서 마구 뒤섞어 문헌 Di_u를 만든다. (즉 Di_u에는 Di_s, Di_t의 단어들이 무작위로 섞여 있을 겁니다.)&lt;/li&gt;&lt;li&gt;Di_u를 Word2vec SGNS의 입력으로 넣어 임베딩을 학습한다. (이때 context window 크기를 충분히 크게 주는게 포인트)&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;마구 뒤섞는게 조금 어이 없어 보이기도 하겠지만, 사실 이 방법은 굉장히 간단하면서도 현명한 해결책입니다. 이론적으로 두 언어 사이의 단어 임베딩을 학습하기 위해서는 S언어의 단어 Vs와 이에 대응하는 T언어의 단어 Vt가 같은 context에 들어가도록 해야합니다. 그렇지만 우리에게 주어진 것은 오직 S언어와 T언어 사이에 대응하는 문헌쌍만 있을뿐, 더 자세한 정보는 없는 상황이지요. 따라서 S언어의 문헌과 T언어의 문헌을 합쳐서 마구 섞고, context window를 충분히 크게 주면 단어 Vs에 대응하는 Vt가 context 안에 들어갈 확률이 높아집니다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;그리고 이런 문헌쌍의&amp;nbsp;수가 충분히 많다면 Word2Vec은 여기에서 단어들 간의 관계를 충분히 학습해낼 수 있습니다. 간단한 방법이지만, Bilingual Lexicon Induction(어휘사전 추론 문제)에서 평균 58%의 성능을 보이며 기존 방법들을 앞지른 기법입니다. 논문에서는 에스파냐어-영어, 이탈리아어-영어, 독일어-영어를 가지고 실험했는데, dimension은 200~300, context window는 16~48일때 좋은 성능을 보인것을 확인되었습니다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;병렬 코퍼스는 없지만 단어 사전은 있다&lt;/h2&gt;&lt;div&gt;이번에는 거꾸로 병렬 코퍼스는 없지만, 단어 사전은 가지고 있는 경우입니다. 사실 병렬 코퍼스보다는 단어 사전 쪽이 데이터를 구하기가 더 쉽습니다. Duong은 병렬 코퍼스는 없고&amp;nbsp;하나의&amp;nbsp;언어로만 된 코퍼스들만 가지고 있는 경우, 단어 사전을 활용하여 두 언어를 한 공간에 Word Embedding하는 기법을 제안했습니다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;txc-formula&quot; src=&quot;https://t1.daumcdn.net/cfile/tistory/99191C405C00F6C804&quot; historydata=&quot;%3Cflashrichtext%20version%3D%221%22%3E%0A%20%20%3Ctextformat%20font%3D%22Dotum%22%20size%3D%2216%22%20color%3D%222236962%22%20bold%3D%22false%22%20italic%3D%22false%22%20underline%3D%22false%22%20url%3D%22%22%20target%3D%22transparent%22%20align%3D%22left%22%20leftMargin%3D%2225%22%20rightMargin%3D%2225%22%20indent%3D%220%22%20leading%3D%220%22%20blockIndent%3D%220%22%20kerning%3D%22true%22%20letterSpacing%3D%220%22%20display%3D%22block%22%3E%28O%7E%20%3D%7E%20%5Csum%20_%7B%20i%20%7D%20%5Cleft%28%20%5Calpha%20%5Clog%20%7B%20%5Csigma%20%28v_%7B%20w_%7B%20i%20%7D%20%7D%5Ccdot%20h_%7B%20i%20%7D%29+%281-%5Calpha%20%29%5Clog%20%7B%20%5Csigma%20%28v_%7B%20%5Coverline%20%7B%20w_%7B%20i%20%7D%20%7D%20%20%7D%5Ccdot%20h_%7B%20i%20%7D%29+%5Csum%20_%7B%20j%20%7D%5E%7B%20p%20%7D%20_%7B%20E_%7B%20w_%7B%20j%20%7D%20%7D%5C%7E%20P_%7B%20n%20%7D%28w%29%20%7D%5Clog%20%7B%20%5Csigma%20%28-v_%7B%20w_%7B%20j%20%7D%20%7D%5Ccdot%20h_%7B%20i%20%7D%29%20%7D%20%20%7D%20%20%7D%20%20%5Cright%29%20%29%3C/textformat%3E%0A%3C/flashrichtext%3E%2C%0A14%2C%0A0xFFFFFF&quot; width=&quot;641&quot; height=&quot;70&quot;&gt;&lt;/p&gt;&lt;p&gt;복잡해보이는 수식이지만 α=1인 경우 Word2Vec의 Negative Sampling과 동일한 목적함수입니다. 가운데 추가된 (1-α)로 시작하는 항이 이 논문의 아이디어인데요, 말로 풀어쓰자면, 단어의 i의 context로 i 주변의 단어들뿐만 아니라, 어휘사전에서 i와 대응하는 외국어 어휘도 context로 고려하겠다는 겁니다. Vulic의 경우 외국어 어휘를 context에 포함하기 위하여 병렬 코퍼스를 합쳐서 마구 섞었다면, 여기서는 단어 사전에서 어휘를 찾아서 그걸 context에 넣어준 것이라고 할 수 있겠습니다.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;단 단어 사전을 이용할 때는&amp;nbsp;동음이의어, 유의어 등이 문제가 됩니다. 예를 들어 한국어 단어 사전의 경우 '눈'에 eye와 snow가 모두 들어가 있을 겁니다. 이 경우 한국어 단어 '눈'이 등장했을 경우 이에 대응하는 외국어 단어 무엇을 context에 넣어야할지 고민이 됩니다. 무책임한 해결책은 랜덤으로 아무 단어나 넣는 것이겠지만 (이 경우에도 어느 정도 성능이 나오긴 합니다), 더 좋은 것은 맥락에 알맞는 외국어 어휘를 고르는 것입니다. Duong은 EM 알고리즘의 아이디어를 활용하여, O를 최대화하는 외국어 단어를 고르고, 그 때의 오차를 줄이는 학습 과정을 추가함으로써 적절한 외국어 단어를 고를 수 있게 하였습니다.&lt;/p&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;여기에 추가적으로 in 공간의 벡터만 사용하는 것이 아니라, in-공간의 벡터와 out-공간의 벡터를 모두 활용하여 유사도를 계산하는 것으로 성능을 더욱 끌어올렸습니다. 최종적으로 BLI 실험 결과 에스파냐어-영어, 이탈리아어-영어, 독일어-영어 평균 76%의 성능을 기록하였습니다.&lt;/div&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;2016년에 나온 Duong의 기법이 Vulic의 기법을 앞질렀지만, 사실 두 방법은 어느 하나가 다른 방법보다 뛰어나고, 다른 하나는 뒤떨어졌다고 말할 수 있는 관계가 아닙니다. 오히려 가용한 데이터에 따라 둘 중 하나를 골라 사용할 수 있는&amp;nbsp;상보적인 관계에 놓여있다고 볼 수 있겠습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;나도 실험해볼거야!&lt;/h2&gt;&lt;p&gt;재미있어보이는 논문들이라 열심히 읽고 나서 실험해보고 싶어졌습니다. 다행히 저도 문헌 단위로 정렬된 병렬 코퍼스가 있어서, Vulic의 방법을 구현해보기로 했습니다. 단 아예 똑같이 구현하지는 않았고, 양쪽 언어의 문헌쌍을 합칠때 S언어와 T언어 각각의 context는 유지되도록 하고, 서로 다른 언어간 context window만 섞이도록 구현했습니다.&lt;/p&gt;&lt;p&gt;데이터는 라틴어 성경-한국어 성경이고, 정렬 단위는 완전 문헌은 아니고 장과 절입니다. 약 3만 쌍 단위를 형태소 분석, lemmatization하여 불용어 제거하고 입력 데이터로 사용했습니다.&lt;/p&gt;&lt;p&gt;Dimension = 200, windowSize = 4, crossWindowSize = 5, negativeSamples = 10로 설정하고&amp;nbsp;총 100 epochs(샘플 데이터가 워낙 작아서...)를 돌렸어요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;라틴어 단어의 경우 총 5536개, 한국어 단어의 경우 총 5630개가 나와 전체 Vocab의 Size는 11166개입니다. 다음은 몇개의 샘플을 입력으로 넣었을때 가장 유사한 단어(라틴어, 한국어 가릴것 없이) 상위 5개를 출력한 결과입니다.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;table class=&quot;std-table&quot;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;deus*&lt;/th&gt;&lt;th&gt;dico*&lt;/th&gt;&lt;th&gt;Abraham*&lt;/th&gt;&lt;th&gt;omnis*&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;하느님&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;말하다&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;아브라함&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;모든&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;dominus*&lt;/td&gt;&lt;td&gt;aio*&lt;/td&gt;&lt;td&gt;Isaac*&lt;/td&gt;&lt;td&gt;&lt;b&gt;모두&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;주님&lt;/td&gt;&lt;td&gt;&lt;b&gt;말씀하다&lt;/b&gt;&lt;/td&gt;&lt;td&gt;사악&lt;/td&gt;&lt;td&gt;universus*&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;주&lt;/td&gt;&lt;td&gt;ergo*&lt;/td&gt;&lt;td&gt;&lt;p&gt;사라&lt;/p&gt;&lt;/td&gt;&lt;td&gt;그리고&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;그분&lt;/td&gt;&lt;td&gt;loquor*&lt;/td&gt;&lt;td&gt;Lazarum*&lt;/td&gt;&lt;td&gt;오다&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table class=&quot;std-table&quot;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;임신하다&lt;/th&gt;&lt;th&gt;Cain*&lt;/th&gt;&lt;th&gt;nos*&lt;/th&gt;&lt;th&gt;nubo*&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;concipio*&lt;/td&gt;&lt;td&gt;&lt;b&gt;카인&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;우리&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;장가들다&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Liam*&lt;/td&gt;&lt;td&gt;Abel*&lt;/td&gt;&lt;td&gt;noster*&lt;/td&gt;&lt;td&gt;&lt;b&gt;시집가다&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;sterilis*&lt;/td&gt;&lt;td&gt;아벨&lt;/td&gt;&lt;td&gt;&lt;b&gt;저희&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;혼인하다&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Onan*&lt;/td&gt;&lt;td&gt;곱절&lt;/td&gt;&lt;td&gt;deus*&lt;/td&gt;&lt;td&gt;iunior*&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;pario*&lt;/b&gt;&lt;/td&gt;&lt;td&gt;septuplum*&lt;/td&gt;&lt;td&gt;하느님&lt;/td&gt;&lt;td&gt;Maala*&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;*표가 붙어있는게 라틴어 단어이고, 굵게 처리된 것이 올바른 번역입니다. 보시다시피 라틴어 단어와 한국어 단어가 결과에 섞여나오는데, 상위권에 라틴어에 대응하는 한국어 단어(혹은 그 반대)가 등장하는 것을 볼 수 있습니다. 정렬 단위가 문헌보다 좀더 작은 장/절 단위라서 성능이 잘 나오는것 같네요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;의외로 쉽게 라틴어-한국어 사이의 단어 임베딩 결과를 얻어냈는데, 이를 바탕으로 라틴어 사전의 표제어들을 보충할 수도 있을 것 같고, 다양한 방면에 써먹기 좋을것 같습니다.&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/623&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>그냥 공부</category>
<category>word2vec</category>
<category>기계번역</category>
<category>자연언어처리</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/623</guid>
<comments>https://bab2min.tistory.com/623#entry623comment</comments>
<pubDate>Fri, 30 Nov 2018 18:19:04 +0900</pubDate>
</item>
<item>
<title>한국어 고문헌 검색기 '어듸메' 개발기</title>
<link>https://bab2min.tistory.com/622</link>
<description>&lt;p&gt;예전에 국문학 전공하는 선배와 이야기를 나누다가, 어쩌다보니 한국어 역사자료를 검색하는게 굉장히 어렵다는 얘기를 듣게 된 적이 있습니다. 그래서 세종계획에서 구축한 역사자료 말뭉치를 구해서 이를 편하게 검색해주는 시스템을 만들어봐야겠다고 작년 이맘때쯤에 마음을 먹었습니다.&lt;/p&gt;&lt;p&gt;그런데 역사자료를 TEI 포맷으로 구축해놓았는데 생각보다 전산처리하기에 퀄리티가 많이 나쁘더라구요. &amp;lt;/p&amp;gt; 닫는 태그를 빼먹는다는지...&amp;nbsp;&amp;lt;&amp;gt; 기호를 〈〉 기호로 써놓았다던지... 열고 닫는 짝이 안 맞는 일은 예사고, 에러가 너무 많아 이를 코드상으로 수정해가면서 파일을 읽으려고 했으나 쉽지 않아서, 그 꿈은 접어두고 말았습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그런데 최근&amp;nbsp;&lt;a href=&quot;https://ithub.korean.go.kr/user/total/referenceManager.do&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://ithub.korean.go.kr/user/total/referenceManager.do&lt;/a&gt; 역사 자료 종합정비 결과가 공개되고 있다는 사실을 알아서 (최근 공개일이 17년 06월이었는데, 그걸 1년 넘게 모르고 있었다는건 함정) 다시 예전 계획을 부활시키게 되었습니다. 정비된 포맷은 XML인데 예전의 TEI 결과보다 훨씬 에러가 적어서 자동 처리하는데 용이했구요.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;십여일 간의 삽질 끝에 사이트를 개설했습니다. 존경하는 분이 친히 지어주신 이름은 '어듸메'로, 말 그대로 고전 문헌들 중에서 원하는 문구에 어디에 있는지 찾아준다는 뜻에서 저런 이름을 지었주셨다고 합니다.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://akorn.bab2min.pe.kr/&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://akorn.bab2min.pe.kr/ 어듸메&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99B876345BF5801721&quot; filemime=&quot;image/jpeg&quot; filename=&quot;p1.PNG&quot; height=&quot;569&quot; width=&quot;800&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;15세기부터 20세기까지 총 997개의 문헌이 현재 시스템에 수록되어 있습니다. 역사자료 정비작업이 더 진행되어 새로운 문헌들이 추가적으로 공개된다면 차차 추가해나갈 예정입니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/991D3F345BF5801820&quot; filemime=&quot;image/jpeg&quot; filename=&quot;p2.PNG&quot; height=&quot;445&quot; width=&quot;800&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;이 시스템을 위해서 자바스크립트로 옛 한글 입력기를 개발했습니다. Internet Explorer, Edge, Chrome에서 작동되는 것을 확인했고, 아쉽게도 무슨 수를 써도 Firefox에서는 작동시킬수가 없었습니다.&lt;/p&gt;&lt;p&gt;ㅿ는 Shift + ㅁ,&amp;nbsp;&lt;/p&gt;&lt;p&gt;ㆁ는 Shift + ㅇ,&amp;nbsp;&lt;/p&gt;&lt;p&gt;ㆆ는 Shift + ㅎ,&amp;nbsp;&lt;/p&gt;&lt;p&gt;ㆍ는 Shift + ㅏ를 눌러서 입력할 수 있구요, 기타 합용병서된 글자들은 각 자음이나 모음을 차례로 입력하여 조합할 수 있습니다.&lt;/p&gt;&lt;p&gt;입력된 글자는 유니코드 첫가끝 포맷으로 처리되므로, 이론상 모든 종류의 옛한글을 다 입력할 수 있습니다. (물론 입력만 가능할뿐 그렇게 입력된 글자가 검색에 잡히리라는 보장은 없습니다..)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p style=&quot;text-align: center; clear: none; float: none;&quot;&gt;&lt;span class=&quot;imageblock&quot; style=&quot;display:inline-block;width:800px;;height:auto;max-width:100%&quot;&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99BC8E345BF580181B&quot; filemime=&quot;image/jpeg&quot; filename=&quot;p3.PNG&quot; height=&quot;523&quot; width=&quot;800&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;최근 검색엔진 관련 수업을 들으면서 검색 이론을 공부중이었기 때문에 이를 적용해볼 수 있는 좋은 기회였습니다. tf-idf 가중치를 적용하고&amp;nbsp;Invert Indexing로 색인하여&amp;nbsp;기초적인 검색을 지원합니다. 아직 Boolean 연산자를 지원하지는 않지만, &quot;&quot;로 필수 일치 질의어를 지정해줄 수는 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;옛한글의 경우 마땅한 형태소 분석기가 개발되어 있지 않고, 문헌마다 표기법이 천차만별이었기 때문에 전체 문장을 색인어 단위로 tokenize하는것이 굉장히 까다로웠습니다. 띄어쓰기 단위로 자를수야 있지만, 아시다시피 교착어라는 한국어의 특징상 한 형태소에 수많은 형태소들이 붙어서 전체 단어를 이루기에, 공백 단위로 구분시 vocabulary 크기가 커지고, recall은 낮아지게 됩니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그래서 자동 tokenize를 위해서 Byte Pair Encoding 기법을 적용했습니다. 이는 자주 등장하는 글자쌍을 새로운 글자쌍으로 대체하는 작업을 반복하여 주어진 텍스트를 한정된 vocabulary의 크기로 분할하는 기법을 말합니다. 본 작업에서는 구글이 개발한 BPE 툴인 SentencePiece(&lt;a href=&quot;https://github.com/google/sentencepiece&quot; target=&quot;_blank&quot; class=&quot;tx-link&quot;&gt;https://github.com/google/sentencepiece&lt;/a&gt;)를 이용했습니다. 이를 이용해서 자주 등장하는 글자쌍을 포착하여 단어로 추출하고, 이런 단어 뒤에 등장하는 조사나 어미들을 어설프게나마 추출할 수 있었습니다. (그렇지만 역시 전반적인 성능은 현대 한국어의 형태소 분석기에 비할바가 못되죠.)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;그렇게하여 어찌어찌 시스템을 구축하긴 했습니다. 문헌마다 다른 표기법들을 정규화하는 작업이 난해하여 아직 엄두도 내지 못하고 있어서, 여전히 Recall이 낮습니다. 예를 들어&amp;nbsp;&lt;b&gt;ᄉᆞᄅᆞᆷ&lt;/b&gt;과 &lt;b&gt;사ᄅᆞᆷ&lt;/b&gt;, &lt;b&gt;사람&lt;/b&gt;은 사실 다 같은 단어이지만 현재는 다른 단어로 처리되어&amp;nbsp;ᄉᆞᄅᆞᆷ이라 입력하면 사람이 포함된 문헌을 검색하질 못합니다. 연철 표기법으로 글자의 형태가 바뀐경우도 검색이 잘 안되구요. 이 문제는 FastText 등의 Word Embedding 기법으로 형태와 의미가 유사한 단어를 뽑아 클러스터링하는 방식으로 극복할 예정에 있습니다.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;아직 부족한 시스템이지만, 중세나 근대 한국어 자료를 찾으시는 분들에게 도움이 되면 좋겠네요. 혹시라도 사용에 불편한 점이나 개선 아이디어가 있으시다면 언제든지 댓글이나 메일로 전달해주시면 감사하겠습니다!&lt;/p&gt;&lt;div style=&quot;text-align:left; padding-top:10px;clear:both&quot;&gt;
&lt;iframe src=&quot;//www.facebook.com/plugins/like.php?href=https://bab2min.tistory.com/622&amp;amp;layout=standard&amp;amp;show_faces=true&amp;amp;width=310&amp;amp;action=like&amp;amp;font=tahoma&amp;amp;colorscheme=light&amp;amp;height=65&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; style=&quot;border:none; overflow:hidden; width:310px; height:65px;&quot; allowTransparency=&quot;true&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
<category>잉여</category>
<category>검색</category>
<category>자연언어처리</category>
<category>한국어</category>
<author>적분 ∫2tdt=t²+c</author>
<guid>https://bab2min.tistory.com/622</guid>
<comments>https://bab2min.tistory.com/622#entry622comment</comments>
<pubDate>Thu, 22 Nov 2018 01:22:42 +0900</pubDate>
</item>
</channel>
</rss>