<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><rss xmlns:creativeCommons="http://backend.userland.com/creativeCommonsRssModule" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">

<channel>
	<title>Channy's Blog</title>
	<link>http://channy.creation.net</link>
	<language>en</language>
	<description>Insights for Web 2.0, Open Source and Open Standards</description>

<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/channy" /><feedburner:info uri="channy" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><creativeCommons:license>http://creativecommons.org/licenses/by-nc-sa/3.0/</creativeCommons:license><image><link>http://creativecommons.org/licenses/by-nc-sa/3.0/</link><url>http://creativecommons.org/images/public/somerights20.gif</url><title>Some Rights Reserved</title></image><feedburner:emailServiceId>channy</feedburner:emailServiceId><feedburner:feedburnerHostname>https://feedburner.google.com</feedburner:feedburnerHostname><feedburner:feedFlare href="https://add.my.yahoo.com/rss?url=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="http://us.i1.yimg.com/us.yimg.com/i/us/my/addtomyyahoo4.gif">Subscribe with My Yahoo!</feedburner:feedFlare><feedburner:feedFlare href="http://www.netvibes.com/subscribe.php?url=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="//www.netvibes.com/img/add2netvibes.gif">Subscribe with Netvibes</feedburner:feedFlare><feedburner:feedFlare href="http://fusion.google.com/add?feedurl=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="http://buttons.googlesyndication.com/fusion/add.gif">Subscribe with Google</feedburner:feedFlare><feedburner:feedFlare href="http://www.feedly.com/home#subscription/feed/http://feeds.feedburner.com/channy" src="http://michaeltunnell.com/images/projects/feedlyflare.jpg">Subscribe with Feedly</feedburner:feedFlare><feedburner:feedFlare href="http://www.hanrss.com/add_sub.qst?url=http%3A%2F%2Ffeeds.feedburner.com%2Fchanny" src="http://static.hanrss.com/images/add_to_hanrss2.gif">Subscribe with HanRSS</feedburner:feedFlare><feedburner:browserFriendly>Thanks for your reading.</feedburner:browserFriendly><item>
	<title>故 박부웅 장로님을 추모하며</title>
	<link>http://blog.creation.net/parkbuwoong-memorial</link>
	<description>&lt;p&gt;&lt;em&gt;이 글은 지난 3월 19일 소천하신 저의 장인 어른이신 故 박부웅 원로 장로님의 추모의 마음을 담았습니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;1996년 7월. 지금의 와이프와 교제를 허락받기 위해 여자 친구의 부모님을 찾았던 무더운 여름날&amp;#8230; &amp;#8220;밥이나 많이 먹고 가라&amp;#8221;고 하시며 안타까운 마음으로 물끄러미 나를 쳐다 보셨다.&lt;/p&gt;
&lt;p&gt;故 박부웅 장로님은 고작 7살 나이에 어머니를 여의고 아버지의 보살핌도 잘 받지 못하신 채, 고교 시절에 하나님에 대한 신앙을 통해 매번 닥치는 인생의 난관들을 극복하셨다. 그렇기 때문에 10살때 어머니를 잃은 나에게 같은 연민을 느끼시고, 당시 직장도 없는 대학원생에 불과한 나와 귀하게 키운 딸과의 결혼을 허락하셨다.&lt;/p&gt;
&lt;p&gt;형제 자매도 많고 다들 집을 떠나 유학을 하면서 생활해서 가족간의 교제가 부재했던 우리집에 비해 2남 1녀의 처가는 늘 만남이 넘쳤다. 나로서는 어린 시절 쉽게 받지 못했던 생일 케이크도 매년 받고, 시골에 정착한 날을 기념일 삼아 가족들을 늘 불러모으시고 손주들을 축복해 주시는 그런 어른이셨다. 가족간의 사랑이 늘 흐르는 안식처를 만드는 분이셨다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2111" src="http://blog.creation.net/data/tisotry/2019/03/28031259/2019-parkbuwoong-memorial-2-1024x768.jpg" alt="" width="1024" height="768" /&gt;&lt;/p&gt;
&lt;p&gt;1969년에 우리 장모님과 결혼하신 후, 온천중앙교회에 다니기 시작하셨으니 돌아가시기까지 만 50년을 한 교회를 주일학교 교사로, 교육부장으로 믿음의 인재들을 길러내시고, 91년 부터 장로로 교회를 섬기셨다. 1995년에 시골로 정착하시고도 20년이 넘는 기간을 매주 주일 성수를 하시면서, 주차 봉사로도 수고하시며 교인들의 귀감이 되셨다. 장례를 치르는 동안 많은 교인들이 추모 예배와 함께 장지까지 동행 해주실 정도로 존경을 받는 어른이셨다.&lt;/p&gt;
&lt;p&gt;최근 1년간 몸이 점차 경직되는 힘든 와병중에 계시면서도 늘 유쾌함을 잃지 않으셨고, 가족들에게도 병을 이기는 용기를 보여주셨다. 연초에 지독한 폐렴을 앓으시고 난 후 잘 견디셨지만, 얼마전 다시 폐렴으로 입원하시고 일주일을 보내시다 새벽에 잠자듯이 편안하게 하나님 나라로 떠나셨다.&lt;/p&gt;
&lt;p&gt;가장 가까운 가족을 떠나보낸다는 것은 참 마음 아픈일이다. 오로지 남아 있는 사람이 감당하는 것도 쉬운 일은 아니다. 장례가 끝나고 돌아온 후, 며칠 동안 어제까지 있었던 분이 오늘 존재하지 않는&amp;#8230; 그래서 마치 꿈을 꾸고 깬 듯한 슬픔이 우리 장모님과 와이프에게도 찾아왔다.&lt;/p&gt;
&lt;p&gt;하지만, 천국에서 다시 만날거라는 소망을 가진 기독교인들에게 남은 이의 삶은 여전히 선물이고, 슬픔을 이겨내고 열심히 살 수 있도록 하는 원동력이다. 장로님이 생전에 남겨주신 신앙의 유지(&lt;span class="st"&gt;有志&lt;/span&gt;)를 이어가면서&amp;#8230;&lt;/p&gt;
&lt;p&gt;아버님, 그동안 열심히 사셨습니다. 천국에서 다시 만납시다!&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=zCnjhW7RMPo:lNLqHknpXho:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 27 Mar 2019 17:21:49 +0000</pubDate>
	<comments>http://blog.creation.net/parkbuwoong-memorial#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>WWW 30주년 – 최초의 웹 사이트들을 만나자!</title>
	<link>http://channy.creation.net/blog/1220</link>
	<description>&lt;p&gt;오늘이 바로 WWW 30주년이 되는 날입니다.&lt;/p&gt;
&lt;p&gt;유럽 입자가속기 연구소인 CERN에서 각기 다른 운영 체제와 시스템을 가진 컴퓨터들 사이에 저장된 정보를 공유하기 위해 팀 버너스 리 (Tim Berners-Lee)는 서로 다른 컴퓨터에서 정보를 연결하기위한 통합 구조를 구상했으며 1989년 3월 12일 &amp;#8220;&lt;a href="https://cds.cern.ch/record/369245/files/dd-89-001.pdf"&gt;Information Management: A Proposal&lt;/a&gt;&amp;#8220;이라는 제안서를 작성했습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channy.creation.net/wp/data/channy/cern/DSCF0301.jpg" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;출처: &lt;a href="http://channy.creation.net/blog/483"&gt;CERN 방문 후기(1) &amp;#8211; Microsom&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;이것이 바로 오늘날 월드와이드웹의 시초입니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;어디에서나 컴퓨터에 저장된 모든 정보가 링크 될 수 있다고 상상해보세요. 모든 것을 다른 모든 것에 연결할 수있는 공간을 만들기 위해 컴퓨터로 프로그래밍 할 수 있다고 상상해 보세요.&lt;/p&gt;
&lt;p&gt;&amp;#8211; Tim Berners-Lee, World Wide Web 발명가&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;저도 학부 3학년인 1994년 처음으로 웹을 사용해 보면서, 국내에 처음 도입될 당시 웹 기술 커뮤니티인 &lt;a href="http://channy.creation.net/blog/969"&gt;WWW-KR&lt;/a&gt;에 참여하면서 새로운 기술과 그 사상에 매료되었습니다. 그래서, 96년에는 진로를 웹 개발자로 바꾸면서 인터넷 업계에 뛰어들었습니다. 웹 10주년이 되는 1999년에는 한참 바쁘게 일하느라 아무 생각이 없었는데, 웹 표준 및 Mozilla 커뮤니티에 활동을 하면서 &lt;span class="search-result-panel-list-title"&gt;&lt;a href="http://channy.creation.net/blog/680"&gt;WWW 20주년&lt;/a&gt;, &lt;/span&gt;&lt;span class="search-result-panel-list-title"&gt;&lt;a href="http://channy.creation.net/blog/959"&gt;25주년&lt;/a&gt;&lt;/span&gt; 그리고 &lt;a href="http://channy.creation.net/blog/1004"&gt;한국웹20주년&lt;/a&gt; 등 기념일을 조금씩 챙기기 시작했네요.&lt;/p&gt;
&lt;p&gt;웹 30주년을 맞이하여 CERN과 웹 재단에서 &lt;a href="https://web30.web.cern.ch/"&gt;기념 행사&lt;/a&gt;도 마련했네요.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channyblog.s3-ap-northeast-2.amazonaws.com/data/channy/2019/03/13084520/web@30_News-580x322.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;오늘은 몇 가지 기념 스크린샷을 추가해 보고자 합니다. 아래는  1993년 우리나라 최초의 웹 서버로 불리는 &lt;a href="https://web.archive.org/web/20010402024809/http://cair.kaist.ac.kr/"&gt;CAIR.KAIST.AC.KR&lt;/a&gt;의 웹 페이지입니다. 텍스트로만 되어 있어서 엄청 심플하지만, 한국 정보, KAIST 정보, 그리고 각종 링크 정보나 당시 자바, 윈도95 등 다양한 정보도 포함하고 있었네요.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1222" src="http://channy.creation.net/data/channy/2015/1993-kaist-cair-webpage.png" alt="" width="885" height="304" /&gt;아래 페이지는 1995년에 제가 처음 만든 홈페이지이면서, 학교 비공식 홈페이지인 &amp;#8216;&lt;a href="http://web.archive.org/web/19961230093308/http://hyowon.cc.pusan.ac.kr:8080/"&gt;효원인의 방&lt;/a&gt;&amp;#8216;였습니다. 당시 교내 인터넷 사용자 모임에 참여하면서  저의 유닉스 계정에 만든 서버입니다. 학교 공식 홈페이지가 만들어질때까지 몇 년간 학교 정보, 교내 시스템 활용 방법, 교내 홈페이지 목록 서비스 등 다양한 정보를 제공했습니다.&lt;/p&gt;
&lt;p&gt;나름 사진 디지털화 작업하고, 디자인 작업도 하고 해서 꽤 그래픽하게 만들었죠?&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1223" src="http://channy.creation.net/data/channy/2015/1995-pnu-hyowon-webpage.png" alt="" width="876" height="1093" /&gt;&lt;/p&gt;
&lt;p&gt;이쯤되면 최초의 홈페이지도 궁금하시죠. CERN에서 아예 옛날 기억을 떠올릴 수 있도록 &lt;a href="http://info.cern.ch/hypertext/WWW/TheProject.html"&gt;최초의 웹 사이트&lt;/a&gt;를 박물관 처럼 저장해두었습니다. 저도 웹코리아 커뮤니티의 웹마스터로서, &lt;a href="http://www-kr.org/"&gt;1999년 당시 모습&lt;/a&gt;을 그대로 박제해 두었습니다.&lt;/p&gt;
&lt;p&gt;그런데, 1991년 웹 사이트가 처음 나왔더라도 우리가 흔히 생각하는 그런 웹 페이지는 아니었습니다.&lt;/p&gt;
&lt;p&gt;사실 본격적인 그래픽 브라우저인 Mosaic과 Netsscape가 대중화 되기 전에는 주로 터미널에서 키보드로 웹 서핑을 했었답니다. (&lt;a href="https://lynx.browser.org/"&gt;Lynx&lt;/a&gt;라는 터미널 브라우저가 있었습니다.)  이를 체험해볼 수 있도록 &lt;a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html"&gt;터미널에서 웹 사이트 보기 시뮬레이터&lt;/a&gt;를 제공하고 있습니다. 여러분도 한번 해보시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1224" src="http://channy.creation.net/data/channy/2015/1989-cern-webpage-linemode.png" alt="" width="1740" height="1204" /&gt;&lt;/p&gt;
&lt;p&gt;여러분이 만든 첫 웹 사이트를 기억하시나요? 사실 과거 자료를 보존한다는 건 어려운 일이라 옛날 기억을 떠올릴 수 있게 만든 &lt;a href="https://archive.org/"&gt;archive.org&lt;/a&gt;라는 웹 사이트가 참 고맙기까지 합니다. 여러분도 추억속의 웹 페이지를 한번 떠 올려 보세요.&lt;/p&gt;
&lt;p&gt;그리고  &lt;a href="https://twitter.com/hashtag/www30?src=hash"&gt;#www30&lt;/a&gt;, &lt;a href="https://twitter.com/hashtag/MyWeb30?src=hash"&gt;#MyWeb30&lt;/a&gt; 등으로 한번 공유해 보시기 바랍니다!&lt;/p&gt;
&lt;p&gt;p.s 웹 30주년 기념 짤방~!&lt;/p&gt;
&lt;p&gt;&lt;img src="http://farm4.static.flickr.com/3467/3388056138_19aa749b6e.jpg?v=0" /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;10년 전 웹 20주년에 &lt;a href="http://channy.creation.net/blog/686"&gt;WebSci09 컨퍼런스&lt;/a&gt; 중 Tim Burners-Lee와 함께 내 포스터 발표 앞에서&lt;/small&gt;&lt;/center&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WQOpwxCRGmw:pBndbZlkBQY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 13 Mar 2019 00:40:27 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1220#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>넥슨 매각에 따른 국내 IT 산업의 영향 가상 예측</title>
	<link>http://channy.creation.net/blog/1213</link>
	<description>&lt;p&gt;넥슨의 김정주 회장이 지주회사인 &lt;a href="https://m.news.naver.com/read.nhn?mode=LSD&amp;mid=sec&amp;sid1=101&amp;oid=015&amp;aid=0004072071"&gt;NXC를 매각&lt;/a&gt;한다고 하네요. 예전에도 인수 합병 루머가 많았고, 아직 공식적인 발표가 없는 이상 섣부른 판단은 금물이겠습니다만… 최근의 넥슨의 상황을 보면 당연한 수순이 아닌가 싶네요. &lt;/p&gt;



&lt;p&gt;2018년 넥슨 연매출은 총 3조원 안팎으로 예상되는데, 이중 70%가 해외 매출입니다. 그 중에서도 중국  의존도가 매우 큽니다. 영업 이익이 1조원이 넘는데, 90% 가량이 텐센트가 중국에서 서비스하고 있는 네오플의 던전앤파이터에서 나옵니다. &lt;em&gt;(2017년 기준 넥슨코리아의 영업 이익은 700억 안팎인데, 네오플은 1조원 가량 됨).&lt;/em&gt; 텐센트가 중국 매출의 30% 정도를 로열티로 지급하고 있는 것을 고려하면, 던전의 중국 매출은 3조원이 넘는 거고, 텐센트 입장에서는 안정적인 수익원을 확보해야 할거에요. &lt;/p&gt;



&lt;div class="wp-block-image"&gt;&lt;img src="http://channy.creation.net/data/channy/public_html/data/channy/2015/5384_5777_3948.png" alt="" class="wp-image-1214" /&gt;던전앤파이터 (c) 네오플 홈페이지&lt;/div&gt;



&lt;p&gt;근데 이럴 수 밖에 없는게… 최근 몇 년 사이에 던전 매출이 너무 급격하게 늘어났구요. 그러다 보니, 넥슨 전체 매출 및 이익에서 중국 의존도가 엄청나게 심화되었습니다. 따라서, 텐센트가 넥슨에 인수의사를 제안했고, 그나마 네오플이 제일 가격이 좋을 때 김정주 회장이 결단한 것이 아닐까 예상됩니다. 텐센트는 이미 슈퍼셀, 라이엇게임즈를 계열사로 가지고 있고, 에픽게임즈의 대주주기도 합니다. 국내 PUBG의 지분도 10% 가량 가지고 있구요. 투자 여력은 아직도 많이 있는 것으로 압니다. &lt;/p&gt;



&lt;p&gt;아직 공식적인 발표가 나오기 까지 루머에 불과하고 해프닝이 될 가능성도 있겠습니다만… 이번 매각이 진짜 이루어진다면, 국내 IT 산업의 최대 인수 합병 사례가 되는 만큼 그 영향도 클 것으로 보여집니다. 몇 가지 섣부른 예측을 해봅니다.&lt;/p&gt;



&lt;ol&gt;&lt;li&gt;&lt;strong&gt;넥슨이 1위를 지키고 있는 국내 게임 업계 판도는 크게 변화가 없을 것입니다. &lt;/strong&gt;주주가 교체됐다고 넥슨의 사업 방향이 크게 바뀌지는 않을 것이니까요. 넥슨 저팬은 도쿄에, 코리아는 서울에, 네오플이 제주에 본사가 있는데&amp;#8230; 갑작스럽게 변화를 꽤하기도 힘든 구조라 직원들의 신상 변화는 크지 않을 것으로 예상됩니다. 다만, 넥슨 저팬 및 코리아의 사업이 장기적으로 좀 더 중국 지향화 된다면 실적이 크게 향샹될 가능성도 있지 않을까 싶네요. &lt;/li&gt;&lt;li&gt;&lt;strong&gt;해외 시장에서 국내 게임 업계의 위상도 여전할 것으로 예상됩니다.&lt;/strong&gt; 넷마블과 PUBG, 펄비어스 등이 미국, 일본, 유럽 시장에서 큰 성과를 거두고 있으니까요. 국내에서는 김택진 대표가 책임 경영을 하고 있기 때문에, 엔씨소프트가 좀 더 업계 큰 형님으로서 역할을 할 것으로 기대 됩니다. 이번 매각으로 게임 산업을 바라보는 시각과 규제 등에 긍정적인 영향을 미친다면 장기적으로는 국내 게임 업계에 도움이 될 것으로 보이네요.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;게임을 비롯한 신규 IT 산업에 큰 도움이 될 것입니다.&lt;/strong&gt; 김정주 회장 입장에서 넥슨을 10조원에 매각한다고 했을 때 사실상 넥슨의 10년치 이익을 땡겨 받는 건데, 오히려 이 거액의 자금이 국내 미래 IT 산업에 재투자 된다면 그게 더 유익하다고 생각합니다. 모 대기업처럼 10조원을 건물 짓는데 쓰는 거 보다는 장기적으로 많은 도움이 되겠죠. 이미 김정주 회장은 비트스탬프나 코빗 등 가상 화폐 거래소 등을 천억 이상 들여서 인수하고, 그외 다양한 산업의 유력 기업에도 개인적인 투자를 해왔습니다.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;국내 스타트업 투자 규모가 확대 될 수 있습니다. &lt;/strong&gt;국내에 이미 1세대 벤처 기업 창업자들이 투자자로서 큰 역할을 하고 있는데, 여기에 김정주 의장까지 가세한다면 다양한 큰 규모의 투자가 이뤄질 수 있을 것이라고 생각합니다. 대개 스타트업을 유니콘 기업을 키우는데 있어 그 다음 단계로 나아갈 때, 국내 VC업계의 투자 규모는 글로벌 기업에 못 미칩니다. 예를 들어, 쿠팡의 경우도 조 단위 투자는 해외에서 받았으니까요. 김정주 회장이 손정의에 버금가는 투자자가 된다면 국내 IT 산업의 미래는 더 밝지 않을까 생각되네요. &lt;/li&gt;&lt;/ol&gt;



&lt;p&gt;아무튼 새해 부터 다이나믹하네요~&lt;br /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=XKAXm_iqkbo:TJTIdQ0OGdw:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 03 Jan 2019 02:43:19 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1213#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>왜 기술 커뮤니티가 중요할까?</title>
	<link>http://channy.creation.net/blog/1204</link>
	<description>&lt;p&gt;벌써&amp;nbsp;연말이네요.&amp;nbsp;다들&amp;nbsp;한해를&amp;nbsp;마무리하고,&amp;nbsp;내년을&amp;nbsp;준비할&amp;nbsp;때죠.&amp;nbsp;저도&amp;nbsp;매년&amp;nbsp;이맘때면&amp;nbsp;올해를&amp;nbsp;회고하고,이력서도&amp;nbsp;정리합니다.&amp;nbsp;평가나&amp;nbsp;이직을&amp;nbsp;위해&amp;nbsp;하는&amp;nbsp;게&amp;nbsp;아니라,&amp;nbsp;올해&amp;nbsp;내가&amp;nbsp;이력서에&amp;nbsp;적을&amp;nbsp;수&amp;nbsp;있을&amp;nbsp;만큼&amp;nbsp;가치있고&amp;nbsp;의미&amp;nbsp;있는&amp;nbsp;게&amp;nbsp;무엇인지&amp;nbsp;정리해&amp;nbsp;둘&amp;nbsp;목적입니다.&amp;nbsp;&lt;em&gt;(&lt;/em&gt;&lt;em&gt;예전에&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;제가&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;매니저일&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;때&lt;/em&gt;&lt;em&gt;,&amp;nbsp;&lt;/em&gt;&lt;em&gt;연말이면&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;팀원들&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;이력서&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;리뷰도&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;해주고&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;그랬어요&lt;/em&gt;&lt;em&gt;.&amp;nbsp;&lt;/em&gt;&lt;em&gt;여러분도&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;이력서&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;정리&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;한번&lt;/em&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;em&gt;해보시길&lt;/em&gt;&lt;em&gt;~&amp;nbsp;&lt;/em&gt;&lt;em&gt;참고&lt;/em&gt;&lt;em&gt;.&amp;nbsp;&lt;a href="http://channy.creation.net/blog/1130"&gt;Channy의&amp;nbsp;개발자&amp;nbsp;경력&amp;nbsp;개발&amp;nbsp;관리조언&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;이번 주에도 연말 회고를&amp;nbsp;하면서&amp;nbsp;오늘&amp;nbsp;여러분께&amp;nbsp;이야기&amp;nbsp;하고&amp;nbsp;싶은&amp;nbsp;주제는&amp;nbsp;&lt;strong&gt;&amp;#8216;기술&amp;nbsp;커뮤니티(Tech Community)&amp;#8217;&lt;/strong&gt;에대한&amp;nbsp;것입니다.&amp;nbsp;일반적으로&amp;nbsp;개발자&amp;nbsp;커뮤니티에&amp;nbsp;대해&amp;nbsp;편견들이&amp;nbsp;많습니다.&amp;nbsp;외부에&amp;nbsp;유명해질려고,&amp;nbsp;혹은&amp;nbsp;실력이&amp;nbsp;없는&amp;nbsp;사람들이&amp;nbsp;그냥&amp;nbsp;주워듣는&amp;nbsp;용도로&amp;nbsp;행사에나&amp;nbsp;참여한다는&amp;nbsp;것이죠.&amp;nbsp;실제로&amp;nbsp;커뮤니티에서&amp;nbsp;유명한사람을&amp;nbsp;회사에&amp;nbsp;데려다&amp;nbsp;놨더니&amp;nbsp;일을&amp;nbsp;잘&amp;nbsp;못하더라는&amp;nbsp;소문도&amp;nbsp;꽤&amp;nbsp;많은&amp;nbsp;게&amp;nbsp;사실이구요.&amp;nbsp;무엇이든&amp;nbsp;케바케지만완전히&amp;nbsp;틀린&amp;nbsp;사실도&amp;nbsp;아닙니다.&lt;/p&gt;



&lt;img src="http://channy.creation.net/data/channy/2015/IMG_0392-940x459.jpg" alt="" class="wp-image-1205" /&gt;AWS한국사용자모임의 송년회 (2018.12.20)



&lt;p&gt;저도&amp;nbsp;대학생일&amp;nbsp;때&amp;nbsp;우연히&amp;nbsp;커뮤니티를&amp;nbsp;통해&amp;nbsp;IT에&amp;nbsp;입문해서, 20여년이&amp;nbsp;넘는&amp;nbsp;IT&amp;nbsp;경력&amp;nbsp;중에&amp;nbsp; 5년&amp;nbsp;마다&amp;nbsp;활동&amp;nbsp;커뮤니티를&amp;nbsp;바꿔&amp;nbsp;가면서&amp;nbsp;참여하고&amp;nbsp;있습니다.&amp;nbsp;그간&amp;nbsp;경험에&amp;nbsp;따르면,&amp;nbsp;모든&amp;nbsp;신흥&amp;nbsp;IT&amp;nbsp;기술은&amp;nbsp;항상&amp;nbsp;커뮤니티가&amp;nbsp;선행한다는&amp;nbsp;것입니다.&amp;nbsp;따라서,&amp;nbsp;초기에&amp;nbsp;커뮤니티에&amp;nbsp;진입하는&amp;nbsp;사람은&amp;nbsp;신&amp;nbsp;기술에&amp;nbsp;업무&amp;nbsp;경험이&amp;nbsp;전혀&amp;nbsp;없는&amp;nbsp;문외한인데다,&amp;nbsp;대개&amp;nbsp;기존&amp;nbsp;회사에서&amp;nbsp;일이&amp;nbsp;별로&amp;nbsp;없어&amp;nbsp;새&amp;nbsp;기술을&amp;nbsp;배우는데&amp;nbsp;여유가&amp;nbsp;있거나&amp;nbsp;혹인&amp;nbsp;진짜&amp;nbsp;백수인&amp;nbsp;경우가&amp;nbsp;태반입니다.&amp;nbsp;다만,&amp;nbsp;이런&amp;nbsp;사람들은&amp;nbsp;신&amp;nbsp;기술을&amp;nbsp;배우는데&amp;nbsp;주저하지&amp;nbsp;않고,&amp;nbsp;적극적으로&amp;nbsp;사람들을&amp;nbsp;만나서&amp;nbsp;배우는&amp;nbsp;데&amp;nbsp;꺼리낌이&amp;nbsp;없다는&amp;nbsp;점이&amp;nbsp;조금&amp;nbsp;다릅니다.&lt;/p&gt;



&lt;p&gt;제가&amp;nbsp;Daum에&amp;nbsp;근무할&amp;nbsp;때인&amp;nbsp;2012년에&amp;nbsp;&lt;a href="https://awskrug.github.io/index.html"&gt;AWS한국&amp;nbsp;사용자모임(AWSKRUG)&lt;/a&gt;이&amp;nbsp;처음&amp;nbsp;만들어졌습니다.&amp;nbsp;그&amp;nbsp;당시AWS를&amp;nbsp;쓰던&amp;nbsp;몇몇&amp;nbsp;스타트업&amp;nbsp;개발자들과&amp;nbsp;재미로&amp;nbsp;사용해보던&amp;nbsp;학생들이&amp;nbsp;일년에&amp;nbsp;한&amp;nbsp;두번&amp;nbsp;모여&amp;nbsp;세션을&amp;nbsp;진행하던&amp;nbsp;커뮤니티인데,&amp;nbsp;당시&amp;nbsp;저도&amp;nbsp;참여를&amp;nbsp;하면서,&amp;nbsp;모임&amp;nbsp;장소가&amp;nbsp;없어서&amp;nbsp;다음&amp;nbsp;오피스&amp;nbsp;회의실을&amp;nbsp;빌려주기도&amp;nbsp;했습니다.&amp;nbsp;제가&amp;nbsp;참여한&amp;nbsp;이유는&amp;nbsp;2009년&amp;nbsp;부터&amp;nbsp;2년간&amp;nbsp;박사과정&amp;nbsp;코스웍을&amp;nbsp;할&amp;nbsp;때,&amp;nbsp;처음으로&amp;nbsp;AWS를&amp;nbsp;기반으로&amp;nbsp;하둡클러스터를&amp;nbsp;만들었는데요.&amp;nbsp;다음에&amp;nbsp;근무할때는&amp;nbsp;풍성한(?)&amp;nbsp;물리&amp;nbsp;장비를&amp;nbsp;쓰다가,&amp;nbsp;가난한&amp;nbsp;랩에서&amp;nbsp;어쩔&amp;nbsp;수&amp;nbsp;없이&amp;nbsp;AWS를&amp;nbsp;쓸&amp;nbsp;수&amp;nbsp;밖에&amp;nbsp;없더라구요.&lt;/p&gt;



&lt;p&gt;2011년에&amp;nbsp;다음&amp;nbsp;복귀하면서&amp;nbsp;처음&amp;nbsp;했던&amp;nbsp;일이&amp;nbsp;바로&amp;nbsp;내부에 프라이빗&amp;nbsp;클라우드팜을&amp;nbsp;만들고,글로벌&amp;nbsp;서비스&amp;nbsp;만들던&amp;nbsp;개발팀에&amp;nbsp;&lt;a href="https://www.youtube.com/watch?v=FAsDEsVFGDU"&gt;AWS 활용을 지원&lt;/a&gt;하기도 했습니다. &amp;nbsp;덕분에 지금의 회사로 오게 된 인연이 되기도 했죠. 사실 좋은&amp;nbsp;회사에&amp;nbsp;다니고&amp;nbsp;있다&amp;nbsp;보면,&amp;nbsp;밖에서&amp;nbsp;어떤&amp;nbsp;일이&amp;nbsp;벌어지는지&amp;nbsp;잘&amp;nbsp;알&amp;nbsp;수가&amp;nbsp;없습니다.&amp;nbsp;대부분의&amp;nbsp;혁신은&amp;nbsp;외부에서&amp;nbsp;일어나고,&amp;nbsp;밖에&amp;nbsp;있는&amp;nbsp;사람을&amp;nbsp;안으로&amp;nbsp;데리고&amp;nbsp;오던지&amp;nbsp;아니면&amp;nbsp;나가서&amp;nbsp;배워야&amp;nbsp;알&amp;nbsp;수가&amp;nbsp;있습니다.&lt;/p&gt;



&lt;p&gt;6년이&amp;nbsp;지난&amp;nbsp;AWSKRUG는&amp;nbsp;페이스북,&amp;nbsp;슬랙,&amp;nbsp;밋업닷컴을 합해 총&amp;nbsp;2만명이&amp;nbsp;모여있는&amp;nbsp;거대&amp;nbsp;커뮤니티로&amp;nbsp;성장했습니다. &amp;nbsp;커뮤니티 활동면에서도&amp;nbsp;올해만&amp;nbsp;20개가&amp;nbsp;넘는&amp;nbsp;소모임에서&amp;nbsp;97번의&amp;nbsp;밋업이&amp;nbsp;열렸고,&amp;nbsp;연인원&amp;nbsp;3,600여명이&amp;nbsp;참여했습니다.&amp;nbsp;밋업마다&amp;nbsp;평균&amp;nbsp;2개의&amp;nbsp;발표가&amp;nbsp;있으니&amp;nbsp;진행한&amp;nbsp;세션만&amp;nbsp;&lt;a href="https://github.com/awskrug/meetups"&gt;200개&lt;/a&gt;가&amp;nbsp;넘고,&amp;nbsp;공식적인 AWS Summit이나&amp;nbsp;DevDay, Community Day등에서&amp;nbsp;진행한&amp;nbsp;품질&amp;nbsp;높은 세션&amp;nbsp;숫자도&amp;nbsp;40개에&amp;nbsp;달합니다.&amp;nbsp;지난&amp;nbsp;리인벤트에&amp;nbsp;참여했던&amp;nbsp;15명의&amp;nbsp;개발자들이&amp;nbsp;내년&amp;nbsp;초에&amp;nbsp;가장&amp;nbsp;관심이&amp;nbsp;높은&amp;nbsp;신규&amp;nbsp;서비스만&amp;nbsp;뽑아서 &lt;a href="https://pages.awscloud.com/aws-community-day-seoul-2019.html"&gt;AWS Community Day&lt;/a&gt;도&amp;nbsp;진행합니다.&amp;nbsp;각&amp;nbsp;소모임에는&amp;nbsp;수십&amp;nbsp;명의&amp;nbsp;개발자들이&amp;nbsp;운영진으로&amp;nbsp;참여하고&amp;nbsp;있고,&amp;nbsp;여기에&amp;nbsp;참여하는&amp;nbsp;사람들간의정보&amp;nbsp;교류는&amp;nbsp;우리&amp;nbsp;상상을&amp;nbsp;초월합니다.&lt;/p&gt;



&lt;p&gt;제가&amp;nbsp;개발자&amp;nbsp;커뮤니티를&amp;nbsp;중요시하는&amp;nbsp;것은&amp;nbsp;두&amp;nbsp;가지&amp;nbsp;이유에서입니다.&amp;nbsp;하나는&amp;nbsp;개발자의&amp;nbsp;후생&amp;nbsp;때문입니다.신기술은&amp;nbsp;개발자들의&amp;nbsp;새로운&amp;nbsp;일자리를&amp;nbsp;만드는데&amp;nbsp;굉장히&amp;nbsp;큰&amp;nbsp;영향을&amp;nbsp;끼치고,&amp;nbsp;커뮤니티에서&amp;nbsp;신기술을&amp;nbsp;장착한&amp;nbsp;사람들이&amp;nbsp;회사를&amp;nbsp;옮겨&amp;nbsp;다니면서&amp;nbsp;기술&amp;nbsp;전파를&amp;nbsp;하고&amp;nbsp;다른&amp;nbsp;사람에게&amp;nbsp;영향을&amp;nbsp;줍니다.&amp;nbsp;주기적으로&amp;nbsp;신기술이&amp;nbsp;IT&amp;nbsp;시장을&amp;nbsp;변혁하기&amp;nbsp;때문에,&amp;nbsp;그&amp;nbsp;씨앗(seed)은&amp;nbsp;회사가&amp;nbsp;아니라&amp;nbsp;커뮤니티일&amp;nbsp;수&amp;nbsp;밖에&amp;nbsp;없습니다.&amp;nbsp;지난&amp;nbsp;목요일에&amp;nbsp;AWSKRUG&amp;nbsp;송년회가&amp;nbsp;열렸는데,&amp;nbsp;끝나고&amp;nbsp;라이트닝&amp;nbsp;토크를&amp;nbsp;시작했더니&amp;nbsp;20명이&amp;nbsp;넘는&amp;nbsp;사람들이&amp;nbsp;AWS가&amp;nbsp;자기를&amp;nbsp;어떻게&amp;nbsp;바꿨는지&amp;nbsp;간증(?)을&amp;nbsp;하더군요.&amp;nbsp;대부분&amp;nbsp;자기&amp;nbsp;회사에서&amp;nbsp;AWS를&amp;nbsp;잘하는&amp;nbsp;사람&amp;nbsp;구인한다는&amp;nbsp;이야기를&amp;nbsp;포함해서요.&lt;/p&gt;



&lt;img src="http://channy.creation.net/data/channy/2015/IMG_0363-940x705.jpg" alt="" class="wp-image-1206" /&gt;AWSKRUG 송년회 라이트닝 토크 모습 (2018.12.20)



&lt;p&gt;두번째는&amp;nbsp;커뮤니티가&amp;nbsp;개발자를&amp;nbsp;돋보이게&amp;nbsp;해&amp;nbsp;줍니다.&amp;nbsp;일부의&amp;nbsp;부작용이&amp;nbsp;분명히&amp;nbsp;존재하지만,&amp;nbsp;자기&amp;nbsp;일의&amp;nbsp;열정과&amp;nbsp;가치를&amp;nbsp;만들어&amp;nbsp;주는&amp;nbsp;곳입니다.&amp;nbsp;대개&amp;nbsp;커뮤니티에서는&amp;nbsp;발표를&amp;nbsp;장려하고,&amp;nbsp;작은&amp;nbsp;것도&amp;nbsp;칭찬해주고&amp;nbsp;서로의&amp;nbsp;고민을&amp;nbsp;들어주고&amp;nbsp;격려해주거든요.&amp;nbsp;회사에서는&amp;nbsp;생존을&amp;nbsp;위해&amp;nbsp;기술을&amp;nbsp;장착해야&amp;nbsp;하고&amp;nbsp;평가를&amp;nbsp;받아야&amp;nbsp;하지만&amp;nbsp;커뮤니티는&amp;nbsp;그런&amp;nbsp;이해&amp;nbsp;관계가&amp;nbsp;전혀&amp;nbsp;없기&amp;nbsp;때문이죠.&amp;nbsp;일부는&amp;nbsp;유명한&amp;nbsp;사람이&amp;nbsp;되고,&amp;nbsp;다른&amp;nbsp;개발자에게&amp;nbsp;주는&amp;nbsp;영향력이&amp;nbsp;크게&amp;nbsp;올라갑니다.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;그만큼 개발자에게 커뮤니티는 필수 불가결한 것이구요. 혹시 여러분이 마음에 있는 커뮤니티가 있다면, 어떤 주제라도 상관없으니 내년에는 적극적으로 참여해 보셨으면 합니다. 다만, 커뮤니티는 기술의 변화에 따라 흥망성쇠를 거듭하는 생물과 같습니다. 지금 가장 인기있는 커뮤니티를 고른다면, 가장 열정적인 사람들을 만날 수 있겠죠.&lt;/p&gt;



&lt;p&gt;아직까지&amp;nbsp;&lt;a href="https://awskrug.github.io/"&gt;AWSKRUG&lt;/a&gt;에&amp;nbsp;가입하지&amp;nbsp;않으신&amp;nbsp;분들이라면&amp;nbsp;&lt;a href="https://www.facebook.com/groups/awskrug"&gt;페북에&amp;nbsp;가입&lt;/a&gt;을&amp;nbsp;해주시고,&amp;nbsp;어떤&amp;nbsp;일이&amp;nbsp;벌어지고&amp;nbsp;있는지&amp;nbsp;눈팅하셔도&amp;nbsp;좋을&amp;nbsp;것&amp;nbsp;같습니다. ^^&lt;/p&gt;



&lt;p&gt;즐거운&amp;nbsp;연말&amp;nbsp;연시&amp;nbsp;보내시구요.&amp;nbsp;새해에&amp;nbsp;새&amp;nbsp;마음으로&amp;nbsp;만나요~!&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=hVbAtZxXrok:WijZ6jmwK1E:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 26 Dec 2018 19:10:37 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1204#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>훌륭한 개발 문화의 이면(5) – 소통 비용의 절약: 서로 API로 말하자</title>
	<link>http://channy.creation.net/blog/1199</link>
	<description>&lt;p&gt;효율적인 개발과 운영을 위해서 개발팀 내에서 다양한 의사 소통 수단은 필수적입니다. 이메일, 메신저, 이슈트래커와 코드 리뷰 등 다양한 방법이 동원되죠. 우리가 만드는 소프트웨어 혹은 서비스간 소통도 매우 중요합니다. 대개 팀 내 혹은 팀간 서비스간 인터페이스(Interface)는 각양각색입니다. CSV나 엑셀 파일을 필요할 때 메일로 보내 준다거나, 주기적으로 XML 파일을 대량 다운로드 받게 하기도 합니다. 최근들어서는 JSON 방식의 REST API를 제공하는 경우가 늘고 있습니다.&lt;/p&gt;
&lt;p&gt;사실 API라고 하면 우리가 프로그램을 짤 때, (자바 API나 닷넷 API 처럼) 특정 라이브러리나 플랫폼의 원하는 기능을 호출하는 인터페이스를 말하는 것인데, 최근에는 데이터를 교환하거나 애플리케이션 내 구현 기능을 호출할 때 사용하고 있습니다. API를 통한 메시지 교환 방식은 이른바 서비스 지향 아키텍터(SOA)가 나온 시기로 거슬러 올라가지만, 본격적으로 활용 되기 시작한 때는 바로 웹 2.0이 태동 되던 2000년대 중반이라고 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 웹2.0, 오픈 API의 태동&lt;/strong&gt;&lt;br /&gt;
2005년 초반에 저는 Daum의 CTO와 함께 전사 기술 전략과 개발자 채용, 경력 관리 등을 새로 셋팅하는 작업을 하고 있었습니다. 당시 300여명이 넘는 개발자가 20여개가 넘는 개발팀에서 다양한 방식으로 협업을 하는데, 서비스간 소통 방식을 각양각색이었죠. 웹 서비스에서 어떻게 하면 확장 가능하고 효율적인 소통을 만들고, 이를 사내 뿐만 아니라 회사 밖 제3자 개발자에게 전달할 수 있을까 하는 게 주요 관심사였습니다.&lt;/p&gt;
&lt;p&gt;2004년 열린 &lt;a href="http://channy.creation.net/blog/133"&gt;오라일리 웹2.0 컨퍼런스&lt;/a&gt;에서는 닷컴버블 이후에 살아남은 인터넷 서비스 기업들이 어떻게 플랫폼 기업으로 살아 남았는지 알려주는 큰 계기가 되었습니다. 즉, 구글, 아마존, 이베이 같은 기업들은 자사의 서비스 인터페이스를 공개함으로서 플랫폼 영향력을 확대했다는 것입니다. 원래 소프트웨어 플랫폼 API는 폐쇄적인데 반해, 웹 서비스 기반 API는 공개되어 있다고 해서 이를 오픈 API라고 불렀습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://blog.creation.net/wp-content/uploads/1/dk14.jpg" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;당시 다음은 웹 검색 엔진을 구글의 유료 API 서비스를 차용하고 있었습니다. 즉, REST API로 검색어 질의를 보내면, 반환된 XML 기반 메시지를  파싱 후 제공하는 간단한 것이었지만, 연간 API 서비스 비용이 수억원에 달하고 있었습니다. 마찬가지로 아마존은 야후 같은 포털에 상품 및 리뷰 API를 제공해 주고, 사용자가 상품 구매 시 커미션을 주는 비지니스를 하고 있었고, 이베이는 경매 상품을 올리는 API를 제공하고 있었는데 무려 50%에 달하는 상품이 리스팅 API를 통해 이루어지고 있었습니다. 대량 판매자들에게 이베이에 상품을 올려주는 프로그램이 엄청나게 많이 만들어지고 있었죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 플랫폼 vs. 서비스&lt;/strong&gt;&lt;br /&gt;
제가 주목한 건 바로 오픈 API가 플랫폼 사업의 가능성을 보여준다는 사실이었습니다. Daum 사용자는 웹 브라우저와 모바일로 메일, 카페, 검색, 지도를 이용하지만, 이를 오픈 API로 제공하면 이를 활용하는 기업의 또 다른 사용자층까지 확보할 수 있습니다. 예를 들어, 다음 메일을 기업용 인트라넷을 만드는 회사에 API로 제공하면, 이들은 사내용 메일을 구축하려는 기업에게 판매를 할 수 있겠죠. 그러면, 개인용으로만 다음 메일을 사용하는 사람들이 회사에서도 사용하니까, 보이지 않는 사용자의 사용 시간도 대거 확보할 수 있겠죠.&lt;/p&gt;
&lt;p&gt;그러나, 이러한 전략은 회사에서 크게 힘을 발휘하지 못했습니다. 왜냐하면, 각 팀의 역량을 모드 일반 사용자에게 쏟고 있는데 외부 기업이나 사용자까지 지원할 여력이 부족하고, 제공하더라도 3자 서비스는 사용하는 다음 밖에 사용자가 다음 사용자냐 아니냐 하는 문제도 있으니까요. 요즘은 일반화한 API 유료 판매 모델이나 광고 등 수익화 방식 제휴 등도 명확하지 않았던 시기였습니다. &lt;i&gt;(왜 플랫폼을 위한 사내 문화가 중요한지 &lt;a href="http://blog.creation.net/515"&gt;플랫폼은 문화다!&lt;/a&gt;라는 글을 참고하세요.)&lt;/i&gt; &lt;/p&gt;
&lt;p&gt;2006년 3월 저는 웹 기술 커뮤니티 멤버들과 함께 &lt;a href="http://channy.creation.net/blog/293"&gt;NGWEB 2.0&lt;/a&gt; 행사 프로그램에 참여하고 있었는데, 그 행사에는 당시 아마존웹서비스(AWS)의 에반젤리스트였던 Jeff Barr가 방한해 &amp;#8220;&lt;a href="https://news.naver.com/main/read.nhn?mode=LSD&amp;mid=shm&amp;sid1=105&amp;oid=022&amp;aid=0000151446"&gt;AWS는 웹 2.0 핵심&lt;/a&gt;&amp;#8220;라는 주제로 기조 연설을 하였습니다. 당시만 해도 AWS는 오늘날 같은 클라우드 서비스가 아니라, &lt;a href="http://channy.creation.net/blog/465"&gt;상품 및 검색 API 등을 제공&lt;/a&gt;하고 있었습니다. 제프는 행사 전날 도착해서 기조 연설을 마치고 부랴부랴 귀국길에 올랐는데, 그 날이 AWS 클라우드의 사실상 최초 서비스인 Amazon S3가 출시한 날이었습니다. (왜 그렇게 빨리 갔는지 그 뒤에 알게 되었죠.)&lt;/p&gt;
&lt;p&gt;그 해, 저는 &lt;a href="http://channy.creation.net/blog/336"&gt;이베이 개발자 컨퍼런스&lt;/a&gt; 및 &lt;a href="http://channy.creation.net/blog/332"&gt;Where 2.0 컨퍼런스&lt;/a&gt; 그리고 Google Maps Developer Day에서 세상의 변화를 현장에서 목도 할 수 있었습니다. 여기서 보고 느낀 점이 새로운 일을 만드는데 큰 동기 부여가 되었습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ Daum DNA, 오픈 API 플랫폼을 만들다&lt;/strong&gt;&lt;br /&gt;
안타깝게도 제가 하려고 하는 일에 회사는 관심이 없었지만, 다행히 못하게 말리지는 않더군요. CTO를 설득해서 저의 사이드잡으로 하기로 하고, 우선 검색팀에 가서 막무가내로 몇 가지 API를 만들어달라고 했습니다. 당시는 다음소프트에서 운영하던 검색 서비스를 직접 개발 운영하는 내재화를 시작하고 있었을 뿐 아니라, 때마침 네이버 검색 API가 오픈했던 시기라서 잘 설득할 수 있었습니다. 그리고, 당시 오픈 ID에 관심이 있던 회원정보팀 팀장에게 부탁해서 API를 위한 XML기반 인증 수단(그 이후에 OAuth로 변경)도 만들었습니다. 이는 메일, 블로그, 카페처럼 다음 아이디 기반 서비스 API를 만들 수 있게 하기 위함이었습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" title="" src="http://channy.creation.net/wp/data/channy/2006/2006-daum-dna-screenshot.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;이래저래 십시일반 도움을 받아 2006년 10월&lt;a href="http://channy.creation.net/blog/359"&gt; 다음개발자네트워크(Daum DNA)를 오픈&lt;/a&gt;할 수 있게 되고, 1명의 팀원과 인턴을 받아서 작은팀을 꾸렸습니다. 그 후 여러분이 아시는 대로, API 매쉬업과 해커톤 같은 다양한 개발자 전도 활동과 기업간 API 제휴 사업을 확대할 수 있었습니다.&lt;/p&gt;
&lt;p&gt;7년이 지나 2013년 연말에는 30여개의 API가 공개되어 있었고, 매일 7천개의 API키에서 &lt;a href="http://biz.chosun.com/site/data/html_dir/2013/12/03/2013120301304.html"&gt;월간 3억건의 호출&lt;/a&gt;이 일어나고 있었습니다. 검색 및 지도를 비롯해서 다음 오픈 API는 국내에서 가장 앞선 API 서비스였다고 자부할 수 있었습니다.&lt;/p&gt;
&lt;p&gt;그 와중에서 오픈 API 플랫폼을 구축하고 운영하는 일은 매우 어려운 일이었습니다. 가장 애로점은 API 공개를 설득하는 일이었습니다. 앞에서 말씀 드린대로 없던 API를 만들고 이를 공개하게 하는 건 서비스 기획자들이 이해하기 힘들 일이죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 내부 API 활성화라는 부수 효과&lt;/strong&gt;&lt;br /&gt;
그런데, 생각지도 못한 일이 벌어졌습니다. 맨 처음 API를 만든 검색팀의 경우, 오픈 API로 만든 인터페이스를 내부 개발팀이 사용하고 싶다고 요청이 늘어난 것이죠. 블로그나 카페 검색 결과 같은 기능을 내부 여러 서비스에서 추가하려다 보니 필연적으로 편한 API를 활용하겠다는 것입니다. 그러다 보니, 검색팀에서 오픈 API외에도 다른 종류의 내부 API를 추가로 만들고 심지어 API 제공을 위한 서버팜을 구성하기에 이르렀습니다.&lt;/p&gt;
&lt;p&gt;실제 검색 API의 경우 오픈 API로 나가는 트래픽은 얼마 되지 않았는데, 내부 API 호출이 기하급수적으로 늘어나는 것이었습니다. 게다가 API를 만들어 공유하는데 소극적인 개발팀들이 좀 더 쉽게 협업을 하기 위해 API를 만들어 직접 내부 공개하고, 심지어 요청하지도 않았는데 Daum DNA에 공개해달라고 부탁하기도 했습니다. 외부 개발자를 위한 플랫폼을 만들겠다고 시작한 일인데, 오히려 사내의 활용도가 더 커지고 있었던 것이죠. 이를 통해 크게 배운 게 있습니다. 결국 내부에서 (개밥먹기 방식으로) 효율성이 인정되면, 결국 옳은 방향으로 가게 된다는 점입니다.&lt;/p&gt;
&lt;p&gt;이렇게 API간 직접 호출이 늘어나니 보이지 않은 부작용도 생겼습니다. 예를 들어, 여행 섹션 같이 새로 오픈 하는 섹션은 지도 API나 블로그 검색 API 등을 활용한 정보 페이지가 주요 기능이 되다 보니, 타 팀의 API의 직접 호출이 그 팀 API 서비스 트래픽 및 부하에 영향을 주는 것입니다. 그러다 보니, 타팀에서 오는 운영 부하에 드는 부담을 도대체 어디까지 질건가 하는게 큰 이슈가 되었죠. 공개된 API를 누가 쓰는지 얼마나 쓰는지 알아야 하는 문제도 발생하였습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1200" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/internal-api.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;당시 오픈 API 플랫폼은 외부 개발자 인증/키발급과 캐싱을 통해 어느 정도 부하를 처리하고 있었는데, 오로지 다음 내부 개발자를 위한 API 서비스 플랫폼이 필요하게 되었고 결국 저희팀에서 TF를 만들어 내부 API Hub를 만들게 되었습니다. 그 후 사내에 있었는지도 몰랐던 API들이 하나둘씩 등록하게 되고, 서로 서로 손쉽게 찾고 키 발급 받아 활용할 수 있게 되었습니다. 2013년 내부 API 호출 수는 일간 5억건에 달했습니다. 사실 99% 트래픽이 사내에서 소비되고, 1%만이 외부에서 사용하는 꼴인데, 이건 당시 에버노트나 넷플릭스의 API 사례에서도 마찬가지였습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ API 소통을 통한 마이크로서비스로의 전환&lt;/strong&gt;&lt;br /&gt;
이제는 API을 통한 소통이 일상화 되었습니다. 작은 스타트업도 자신들의 서비스를 API로 공개해 놓고 있으며, 많은 국내 IT 기업들도 API를 외부에 오픈해 놓고 있습니다. 누구나 이러한 흐름에 동의할 것입니다. 최근에 &lt;a href="http://channy.creation.net/articles/microservices-by-james_lewes-martin_fowler"&gt;마이크로서비스(Microservices) 아키텍처&lt;/a&gt;가 각광을 받고 있습니다. 즉, 독립적인 작은 서비스 단위로 기능을 쪼개고, API로 소통함으로서 개발 및 배포 속도를 높이는 기법입니다. 하지만, 아직까지 내부 팀간 협업을 REST API로 하고 있지 않다면, 좀 더 민첩한 개발 방식을 도입하는데 큰 장애가 될 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2015/microservice-netflix.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;당장 모든 기능을 API로 만들 필요는 없습니다. 우선 외부 팀에서 많이 요청하는 몇 가지 기능 부터 별도로 쪼갠 후, API로 만들어 제공하는 것이 좋습니다. 일단 누군가 API를 쓰기 시작하면, 이들 트래픽이 주 사용자에 영향을 미치지 않아야 합니다. 또한, 주기적인 피드백으로 기능 개선을 따로 할 수 있기 때문이죠. 마이크로서비스라는게 어려운게 아니고, 바로 &lt;a href="http://channy.creation.net/blog/1051"&gt;이렇게 시작&lt;/a&gt;하는 것입니다. 아직 여러분 회사는 API로 소통하지 않으세요? 지금 시작하세요!&lt;/p&gt;
&lt;p&gt;참고. 아래는 제가 발표한 Daum API 사례 및 마이크로서비스 아키텍처 구현 사례에 대한 발표 자료 및 영상입니다.&lt;br /&gt;
 &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;혹시 API 모범 사례에 대한 다양한 정보를 수집하시려면 아래 해외 컨퍼런스를 참고하시기 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://events.linuxfoundation.org/events/apistrat-2018/"&gt;API Strategy &amp;amp; Practice Conference 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://apiworld.co/"&gt;API World 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.apidays.co/"&gt;API Days 2018 Series &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gluecon.com/"&gt;GlueCon 2019 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;연재 목차&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1104"&gt;훌륭한 개발 문화의 이면(1) – 코딩 테스트인터뷰 제대로 하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1107"&gt;훌륭한 개발 문화의 이면(2) – 자율적 개발 환경을 선택하라!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1110"&gt;훌륭한 개발 문화의 이면(3) – 다른 팀 소스 코드를 볼 수 있는가?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1183"&gt;훌륭한 개발 문화의 이면(4) – 사내 라이브러리를 잘 관리하려면?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1199"&gt;훌륭한 개발 문화의 이면(5) – 소통 비용의 절약 &amp;#8211; 서로 API로 말하자&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;훌륭한 개발 문화의 이면(6) – 오픈 소스를 통한 외부 개발자 전략 구사하기&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=GUYTDkZENSQ:uCarCIxNuzs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 29 Jul 2018 23:30:46 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1199#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>트럼프는 어떻게 김정은을 만났나?</title>
	<link>http://channy.creation.net/blog/1194</link>
	<description>&lt;p&gt;70년 적대 관계 후 북미 정상의 역사적 첫 만남! ‬1년 전 도널드 트럼프 미국 대통령과 김정은 북한 국무위원장의 서로의 행보를 볼때, 일어나지 않을 정말 기적과 같은 일이 벌어졌습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1195" src="http://channy.creation.net/data/channy/2018/06/13150217/dprk-us-summit.jpg" alt="" width="960" height="540" /&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Donald Trump &amp;#8220;위대한 회담에서 놀랄만한 성공으로 의심 없이 좋은 관계 맺을 것이다.&amp;#8221; 김정은, &amp;#8220;우리 발목을 잡는 과거들이 때론 눈과 귀를 가렸으나 모든 것을 이겨내고 이 자리까지 왔다.&amp;#8221;‬&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;지난 미국 대선에서 한국에서도 정말 누구도 대통령이 될 거라 생각도 못했고 깜도 안된다고 생각했지만, 반전은 일어났고요. 북핵 고도화 상황에서 온탕과 냉탕을 오갔지만, 결국은 변화와 결과를 만들어 내고 있습니다. 무엇 보다 주목할 점은 회담 이후, 트럼프 대통령 한 시간 동안 전 세계 기자들과 혼자서 기자 회견을 했다는 점입니다. 김정은 위원장은 호텔에 가서 봤겠죠.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;span class="mce_SELRES_start"&gt;﻿&lt;/span&gt;&lt;/center&gt;쏟아지는 질문에 대해 막힘없이 다 막아 내면서 드러난 트럼프의 인식은 그동안 봐왔던 정치인들과 확연히 다른 것이었습니다. 이 날의 기록을 잠깐 남겨볼려고 하는데, 본 기자 회담을 살펴 보면서 어떻게 이런 일이 벌어질 수 있었는지 나름의 생각을 정리해 봤습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Bottom-up이 아닌 Top-down &lt;/strong&gt;&lt;br /&gt;
그동안 북미 협상은 사실상 (민주당 정권에서) 우선 순위에 밀려있거나, (공화당 정권에서) 북한에 대한 불신이 강한 상태에서, 북미 실무진 혹은 다자 회담에서만 서로 이야기하고 통큰 결단이 일어나지 못했죠. 역사적으로 닉슨-모택동(1972), 레이건-고르바초프(1986), 오바마-카스트로(2016) 등 공산 정권과의 관계 정상화는 탑-다운인데 진행 됐던데 반해, 그동안 북미 관계 접근 방식이 맞지 않았다고 생각합니다. 오히려 트럼프 대통령의 리더쉽 자체가 탑-다운을 지향하고, 자기를 돋보이게 하는 쇼맨쉽이 강하다는 측면에서 큰 도움이 되었네요. 기존 미국 대통령들이 명분과 주변 관계에 너무 신경썼었던 반면, 기존 정치권과 완전히 다른 트럼프의 성향이 드러난 결과입니다. 김정은 위원장에게도 국제 사회에 데뷰할 수 있게 하는 원동력이 됐을 겁니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 대북 관계도 돈의 문제로&lt;/strong&gt;&lt;br /&gt;
기자 회견 내내 트럼프는 대북 관계를 돈으로 계산했습니다. &amp;#8220;북한이 평창 올림픽에 참여함으로서 (티켓 판매에 도움을 줘) 오히려 성공을 거두었다&amp;#8230; 북한 핵사찰을 할 때 검증단이 들어가도 돈이 많이 든다&amp;#8230; 한미 군사 훈련시, 괌에서 비싼 장비를 6시간 동안 날아가 폭탄 투하 훈련을 하는 것도 돈이 많이 든다.&amp;#8221; 이는 한반도 문제만 아니라, 유럽에서 NATO 운영 비용에 대한 교역 불균형 문제, 중국과 캐나다의 무역 적자 이슈 등 트럼프의 국제 외교 이슈도 모든 문제를 경제적인 관점에서 바라보고 있다. 기존 미국 대통령들이 정치적인 명분과 관계에 집중하는 것과는 확연이 다른 점입니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. 인권 문제도 자국 이익 우선 &lt;/strong&gt;&lt;br /&gt;
북한 인권에 대해 이야기했느냐, 어떻게 해결할거냐는 질문에 대해 트럼프 대통령은 오히려 &amp;#8220;미군 유해 송환 합의&amp;#8221;에 방점을 두었습니다. 사실 북한 주민의 인권 문제 보다도 자국의 이익을 우선한 것이죠. 한번에 하나씩 해결하는 방법이기도 하지만, 미국 자국민의 인권을 먼저 생각한것입니다. 실질적인 북미 관계가 좋았던 1990년 &lt;a href="http://shindonga.donga.com/Print?cid=100656"&gt;미군 유해 발굴 사업&lt;/a&gt;을 시작하여 2007년까지 443구의 유해가 미국으로 송환됐고, 2007년 빌 리처드슨 미국 뉴멕시코 주지사 방문으로 송환된 6구를 마지막으로 유해 발굴은 중단됐습니다. &lt;/p&gt;
&lt;p&gt;과거에 미군 유해 송환시, 유해 발굴 비용(북한 주민 임금, 대지 활용 비용)등을 북한측에 제공하기도 했구요. 실제로, 즉각적인 진행이 되면 미군 유해 발굴단이 북한에 직접 들어가게 되므로 이러한 과정 중에도 평화 유지가 될 수 있는 좋은 합의라고 생각됩니다.&lt;/p&gt;
&lt;p&gt;‪트럼프 대통령은 지극히 이기적인 사람입니다. 북미 관계 개선을 자신의 정치적 성과로 만들 목적이 뚜렸합니다. 기자 회견 중에도 기존 정권과 달리 본인에게 우선 순위가 높다고 했으니까요. 하지만, 이런 상황이 오히려 한반도 평화에 도움을 주고 있는 역설적인 상황입니다. &lt;/p&gt;
&lt;p&gt;미국 보수적 공화당 정권의 이상한(?) 대통령 그리고 한국의 민주 정권, 그리고 이제 막 밖으로 나오려는 북한의 젊은 지도자&amp;#8230; 이러한 조합은 정말 하늘이 준 기회라고 생각이 됩니다. &lt;/p&gt;
&lt;p&gt;역사는 늘 바뀝니다. 어제의 적이 오늘의 친구가 되고, 모로가도 서울만 가면 되는거죠. (북한 로동신문에 게재된) 대표적인 매파인 존 볼턴과 김정은의 악수가 그런것을 보여줍니다. &lt;/p&gt;
&lt;p&gt;&lt;img src="http://channy.creation.net/data/channy/2018/06/13154213/bolton-kim-hands.png" alt="" width="640" height="381" class="alignnone size-full wp-image-1197" /&gt;&lt;/p&gt;
&lt;p&gt;이제 시작입니다! 이를 기점으로 진정한 평화가 오길 바랍니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Donald Trump, “누구나 전쟁을 일으킬 수 있지만, 오직 용기 있는 자만이 평화를 만들 수 있다. (Anyone can make war, but only the most courageous can make peace.)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;h3 class="collapseomatic " id="id5cc878460688c" tabindex="0" title="참고- 북미 공동 성명 한국어 전문 자세히보기"&gt;참고- 북미 공동 성명 한국어 전문 자세히보기&lt;/h3&gt;&lt;div id="target-id5cc878460688c" class="collapseomatic_content "&gt;&lt;br /&gt;
&lt;small&gt;&lt;br /&gt;
도널드 트럼프 미국 대통령과 김정은 북한 국무위원장은 역사적인 첫 정상회담을 2018년 6월 12일 개최했다.&lt;/small&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;트럼프 대통령과 김정은 위원장은 새로운 미국과 북한의 관계와 한반도의 평화를 위한 포괄적이고, 심도있고, 진심이 담긴 의견을 교환했다.&lt;/p&gt;
&lt;p&gt;트럼프 대통령은 북한에 체제 안정을 제공하기로 약속했고, 김정은 위원장은 한반도의 완전한 비핵화를 위한 확실한 약속을 재확인했다.&lt;/p&gt;
&lt;p&gt;새로운 미·북 관계가 한반도와 세계의 평화와 번영을 가져오는 것을 확신하며, 이러한 양측의 자신감은 한반도의 비핵화를 이룰 수 있기 때문에 트럼프 대통령과 김정은 위원장은 다음 내용에 합의한다.&lt;/p&gt;
&lt;p&gt;1. 미국과 조선인민민주주의공화국은 평화와 번영을 위한 양국 국민의 열망에 따라 새로운 미-조 관계를 수립할 것을 약속한다.&lt;/p&gt;
&lt;p&gt;2. 미국과 조선인민민주주의공화국은 한반도에 항구적이고 안정적인 평화 체제를 구축하기 위한 노력에 동참할 것이다.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;3. 조선인민민주주의공화국은 2018년 4월 27일 ‘판문점 선언’을 재확인하고 한반도의 완전한 비핵화를 위해 노력할 것을 약속한다.&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;4. 미국과 조선인민민주주의공화국은 이미 확인된 전쟁 포로 유골의 즉각적인 송환을 포함해 전쟁포로와 실종자의 유해 복구를 약속한다.&lt;/p&gt;
&lt;p&gt;역사상 첫 미·북 정상회담은 두 나라의 수십년간 지속된 적대적인 관계를 청산하고 새로운 미래를 여는 역사적인 행사였다. 트럼프 대통령과 김정은 위원장은 공동 협약의 조항을 완전하고 신속하게 이행하기로 약속한다. 이후 미국과 조선인민민주주의공화국은 마이크 폼페이오 미 국무장관이 진행하는 고위급 실무 회담을 최대한 빨리 추진해 미·북 정상회담의 결과를 실행에 옮길 것이다.&lt;/p&gt;
&lt;p&gt;도널드 트럼프 대통령과 김정은 북한 국무위원장은 새로운 미-조 관계 형성과 더불어, 한반도뿐 아니라 전 세계의 평화·번영·안보를 위해 협력하기로 약속했다.&lt;/p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;img src="http://channy.creation.net/data/channy/2018/06/13154048/trump-kim-sign.jpg" alt="" width="1024" height="443" class="alignnone size-full wp-image-1196" /&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=isYFhCbhIiI:YP-CWcVe4XU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 13 Jun 2018 06:05:09 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1194#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>[후기] 밥잘사주는 예쁜 누나, 지금 만나러 갑니다!</title>
	<link>http://blog.creation.net/sonyejin-pretty-meet-you</link>
	<description>&lt;p&gt;1983년 9월 3일 엄마가 돌아가셨다.&lt;/p&gt;
&lt;p&gt;이미 35년이 흘렀지만 그 날은 잊혀지지 않는다. 내 나이 고작 열살&amp;#8230; 딴에 그 극한의 슬픔을 알았는지, 집 마루에 누워 울다가 지쳐 어렴풋이 쳐다본 하늘은 더럽게도 푸르렀다. 장례가 끝난 후, 몇 달 동안 꿈에 엄마가 집에 돌아 올 정도로 그리웠다. 그렇게 잊혀졌던 나의 아픔과 기억을 소환하고 치유한 영화를 만났다.&lt;/p&gt;
&lt;p&gt;얼마전 개봉했던 &lt;a href="https://search.naver.com/search.naver?query=%EC%A7%80%EA%B8%88+%EB%A7%8C%EB%82%98%EB%9F%AC+%EA%B0%91%EB%8B%88%EB%8B%A4"&gt;&amp;#8216;&lt;strong&gt;지금 만나러 갑니다&amp;#8217;&lt;/strong&gt;&lt;/a&gt;. 배우 소지섭(정우진 역)과 손예진(임수아 역) 뿐만 아니라 엄마를 기다리는 아역 김지환 군(정지호 역)이 출연한 영화다. 이 영화는 장마가 시작되면 다시 돌아오겠다는 약속을 남기고 죽은 엄마가 기억을 잃은 채 부자에게 모습을 나타내면서 시작된다. 그리고, 장마가 끝나면 이들은 다시 이별을 고하게 된다.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;small&gt;영화 &amp;#8216;지금 만나러 갑니다&amp;#8217; 엄마 펭귄의 여행 오프닝&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;영화속 대부분의 이야기는 엄마 아빠의 첫사랑과 결혼, 출산 그리고 때 이른 이별에 이르는 필연적 과정을 담고 있지만&amp;#8230; 내가 정말 공감했던 부분은 바로 극중 수아가 아들 지호에게 다시 떠나기 전에 이별 연습을 하던 장면이다. 아들이 엄마 없이도 꿋꿋이 살아갈 수 있도록 계란 후라이를 하는 방법, 빨래 말리는 방법, 청소하는 방법, 머리 감는 방법 등을 가르쳐 준다. 그리고, 매년 아들의 성년이 될때까지 아들 생일을 위해 미리 축하 카드를 써서 전달하는 아빠 친구에게 부탁도 한다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2104" src="http://blog.creation.net/data/tisotry/2018/05/25050711/soyejin-pretty-meet-you-1-1024x513.png" alt="" width="1024" height="513" /&gt;&lt;br /&gt;
&lt;small&gt;수아와 아들 지호와 이별하는 장면&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;과거 엄마와 갑작스런 이별을 했던 나에게 이 장면들 속에 있는 엄마와 아들의 교감과 과정에서 크게 힐링이 되었다. 엄마와 지호가 다시 이별하는 장면에서 &amp;#8216;지호가 없는 세상에서는 백년을 살아도 행복하지 않았을 거야. 엄마는 구름 나라에 가서 지호를 계속 보고 있을께. 멋진 어른이 되야돼&amp;#8217;라는 대사에서 너무 가슴뭉클했다. 천국에 계신 엄마가 보시기에 나도 멋진 어른이 되어 있을까? (이 장면은 두고 두고 다시 볼 수 있게 휴대폰에 클립을 저장해 두었다. 돌아가신 엄마 생전 육성 녹음과 함께&amp;#8230;)&lt;/p&gt;
&lt;p&gt;수많은 영화나 드라마를 보면서 자신의 인생과 경험에 감정을 이입하게 하는 절절한 연기를 만난다는 건 쉽지 않다. 그래서, 이 작품을 통해 이러한 경험을 해주게 한 수아역의 손예진이라는 배우를 다시 보게 되었다. 멜로퀸이라는 수식어가 가능하게 했던 20대의 풋풋한 사랑 이야기, 그리고 그 이후의 다양한 연기 변신을 거듭했지만 공감 연기가 빛을 발하는 그녀의 최근 작품에 매료되었다.&lt;/p&gt;
&lt;p&gt;얼마전 종영한 JTBC 드라마 &amp;#8216;&lt;strong&gt;&lt;a href="https://search.naver.com/search.naver?query=밥+잘+사주는+예쁜+누나"&gt;밥 잘 사주는 예쁜 누나&lt;/a&gt;&lt;/strong&gt;&amp;#8216;도 마찬가지다. 드라마에서 윤진아역을 맡은 손예진은 연하의 친구 동생인 서준희(정해인 분)과 달달한 사랑에 빠지지만 현실 연애의 장벽과 가족의 반대, 사회적인 압박 등을 이겨나가는 30대 여성의 삶을 연기했다.&lt;/p&gt;
&lt;p&gt;우연히 이 드라마를 보게 되었는데 실제로 우리 부부도 연상연하 커플이다 보니 내가 실제 겪은 연애 과정에서의 고민과 아픔이 고스란히 드러나 보였다. 이전에도 연상연하 커플일 다루는 드라마는 많았지만, 대부분 호기심과 케미에 집중했던 반면 이 드라마는 정말 일어나는 일이여서 공감할 수 있었다.&lt;/p&gt;
&lt;p&gt;연상연하 커플의 경우 3살 차이라고 해도 사회 경력이 짧게는 3년 길게는 6년 정도 차이가 나기 때문에, 여자는 (집안의 반대, 연애 후 결혼까지의 과정, 이후 경제적 측면의) 위험 감수를 해야 하고, 남자는 그러한 여자를 안심시켜 주어야 하는 끈질기고 순애보적인 사랑이 있어야만 어느 정도 이뤄진다. 그래서, 내가 결혼했던 20년전에도 우리 나라 전체 결혼 커플의 4.5%만이 연상연하였다. (물론 최근에는 그 비율이 15%까지 늘었다고 한다.)&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2102" src="http://blog.creation.net/data/tisotry/2018/05/25050708/soyejin-pretty-meet-you-2-1024x506.png" alt="" width="1024" height="506" /&gt;&lt;br /&gt;
&lt;small&gt;드라마 &amp;#8216;밥 잘 사주는 예쁜 누나&amp;#8217; 사랑 고백 장면&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;극 중 둘의 사랑이 무르익어가던 6회와 7회에서 준희가 전화로 &amp;#8216;사랑해&amp;#8217;라는 고백을 해도, 아무 말을 할 수 없었던 진아. 그리고 그런 진아에게 &amp;#8216;사랑한다는 데 왜 말이 없어?&amp;#8217;라고 애타하는 마음이 충분히 이해가 갔다. 그리고, 준희의 아낌없이 쏟아내는 사랑에 결국 음성 녹음이지만 &amp;#8216;고마워 나를 사랑해줘서&amp;#8230; 많이 배우고 있어. 준희야 사랑해 아주 많이, 아주 오래 오래 사랑할께&amp;#8217;라는 장면에서 다시 한번 공감이 갔다. (우리 와이프도 나에게 사랑한다는 말을 그렇게 아꼈었다. 하하&amp;#8230;)&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;그 이후에 극 전개가 좀 답답하고 캐릭터간의 감정들이 대치되는 면이 있었으나, 현실 연애에서 늘 있을 법한 배려와 오해가 교차하는 부분이라 충분히 이해할 수 있었다. 다행히 우리 부부는 3년의 연애 끝에 부모님들이 허락을 해주어서 행복한 결혼에 골인할 수 있었다. 딸이 나이가 차고, 아들이 그렇게 죽고 못사는데 자식이기는 장사는 없다. 극 중 진아와 준희도 서로를 못잊고 마지막에 제주도에서 다시 만나 해피엔딩으로 끝났는데, 아무리 속물 캐릭터 엄마도 그쯤되면 허락할 수 밖에 없을거다&amp;#8230; 하하!&lt;/p&gt;
&lt;p&gt;안판석 감독님 드라마 특성상 유독 롱테이크가 많고, 대사 하나하나가 실제로 나의 경험에 또 다시 감정이입을 할 수 있는 멜로 드라마다 보니,  인생 드라마가 되었다. (&amp;#8216;지금 만나러 갑니다&amp;#8217; 처럼 드라마속 몇 가지 공감 장면은 클립을 만들어 OST와 함께 폰에 저장해뒀다.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;내 인생에서 없어서는 안될 두 여자. 나의 어머니 그리고 와이프&amp;#8230;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;그녀들과의 아픈 기억, 잊지 못할 경험을 다시 꺼내고 힐링해 주었던 좋은 영화와 드라마였다. 아마 손예진 배우님을 만난다면 꼭 아래 사진에 싸인을 한번 받아두고 싶다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2103" src="http://blog.creation.net/data/tisotry/2018/05/25050709/soyejin-pretty-meet-you-3-1024x547.png" alt="" width="1024" height="547" /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=NCvmVyqSktc:zUbNdH_Wm40:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 24 May 2018 20:12:34 +0000</pubDate>
	<comments>http://blog.creation.net/sonyejin-pretty-meet-you#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>개발자 비급(祕笈) – 1. 연봉은 실력의 결과가 아니다</title>
	<link>http://channy.creation.net/blog/1186</link>
	<description>&lt;p&gt;&lt;em&gt;회사에서 개발자가 프로그래밍 실력만으로 대우 받으면 좋겠지만, 세상이 그렇게 원하는대로 돌아가지는 않습니다. 개발자들의 연봉(몸값), 경력 관리, 그리고 협업 등에 대한 경험담을 시리즈로 풀어보려고 합니다. 요즘은 이렇게 쓰겠다고 맘을 먹어야 그나마 시작할 수 있네요.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;연말이면 평가 시즌이 돌아오고 CTO와 함께 수 많은 개발자들의 연봉을 결정했던 적이 있습니다. 대개 팀장이 1차 평가를 하고, 상위 임원이 2차 평가를 하게 됩니다. 진리의 케바케(?)라는 말이 있듯이, 대부분의 회사가 같은 연차의 개발자라도 연봉은 천차만별입니다.&lt;/p&gt;
&lt;p&gt;그러다 보니, 능력에 걸맞는 업무 성과가 나왔는지를 판단해 개인별 연봉 인상률을 결정하는 건 여간 까다로운게 아닙니다. 특별히 예외적 성과나 정말 고려해야 하는 경우를 제외하고, 전체 개발자의 80%는 늘 같은 결과를 얻습니다.&lt;/p&gt;
&lt;p&gt;그런데도, 많은 개발자들이 연봉 협상에 들어가면 내가 올해 올린 성과로 협상의 여지가 있을거라고 믿는 착각을 합니다. 사실상 연봉 협상이 아니라 &amp;#8216;연봉 결정&amp;#8217;이 되어 있는 상황인데도 말입니다. 역량 평가나 업무 성과라는 것은 나의 실력과 업무 성과가 내 몸값을 결정하는 게 아니라면 도대체 무엇일까요?&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter wp-image-1187 size-full" src="http://channy.creation.net/data/channy/2018/05/02065739/dev-story-1-1.jpg" alt="" width="600" height="330" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 연봉은 내가 다닌 회사의 궤적일 뿐&lt;/strong&gt;&lt;br /&gt;
여러분이 받는 연봉이라는 건 그냥 다녔던 회사의 종류와 특성에 따른 궤적에 불과합니다. 대기업에 입사해서 초봉을 높은 수준으로 받아, 스타트업에 한번 가서 대박을 치고, 컨설팅 업계로 왔다면 5년만에 억대 연봉도 가능합니다. 다만, 병특으로 작은 기업에서 2-3년 보내면 동년배보다 경력이 높은데도, 졸업 후 들어온 공채보다 연봉이 낮을 수도 있습니다.&lt;/p&gt;
&lt;p&gt;실제 Daum에 다녔던 어떤 개발자는 대학교 2학년때 병특으로 입사해서 3년을 일한 후, 휴직하고 복학 및 졸업을 해서 2년후 회사에 다시 온 경우가 있었습니다. 회사가 성장하다 보니 신입 사원들의 연봉 수준은 매년 올라가고, 이 친구는 휴직 후 복직이다 보니 연봉은 휴직 전 연봉으로 책정되는 불상사가 생긴겁니다. 그렇다고 HR 규정상 엄청나게 올려줄 수는 없으니 연차 대비 연봉 수준은 시작부터 낮게 되는 것이죠.&lt;/p&gt;
&lt;p&gt;이런 경력 패턴으로 몇 군데 회사를 옮기다 보면, 같은 회사에서 비슷한 경력과 실력을 가졌는데도 연봉 차이가 많이 나는 현상이 발생합니다. 그래서, 저 사람은 실력도 없는데 왜 저렇게 연봉을 많이 받는 건가라는 의문과 뒷담화가 난무하기도 합니다. 회사에서는 사람을 뽑을 때 연봉이란 게 일단 뽑고 나면, 최대한 맞춰주기 때문이죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 돈이냐? 행복이냐?&lt;/strong&gt;&lt;br /&gt;
자! 그러면 몸값을 올리는 방법은 무엇일까요? 처음 부터 대기업에 가서 하기 싫은 일 하면서 꾸역꾸역 일하다가 연봉 높게 올리는 경력 개발을 하는게 맞느냐? 아니면 연봉을 낮추어 보람찬 일을 하더라도 만족하면서 살거냐? 정말 돈 안되는 직업을 때려치우고 &lt;a href="http://moneyman.kr/archives/5379"&gt;돈되는 새로운 직업으로 갈거냐&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;한 가지 중요한 사실은 김창준님이 쓰신 &amp;#8220;&lt;a href="http://agile.egloos.com/5783372"&gt;몸 값 안 올리기&lt;/a&gt;&amp;#8220;라는 글에서 보듯, &amp;#8220;다수의 경력 개발 연구에서 연봉이 많이 오르면 행복해 지는 경향은 있으나, 그렇게 크지 않으며 그 효과도 오래가지 않는다는 점&amp;#8221;입니다. &amp;#8220;오직 연봉 인상을 위해 이직을 하다보면, 그 외의 요소 (가령, 기업 문화)를 무시하는 경향이 있어, 그 후 만족도가 떨어질 수 있기도&amp;#8221; 합니다. 오히려, 업무에서 행복하기 위한 주관적인 요소가 많고 그렇게 일하는 방법을 찾으라는 조언입니다.&lt;/p&gt;
&lt;p&gt;저도 창준님 이야기에 동의합니다. 20년 넘게 회사를 다니면서, 내가 하는 일에서 행복을 얻고, 가급적 그럴 수 있는 회사로 이직을 결정했으니까요. 하지만, 인생이 나 혼자 사는 것도 아니고, 가족이 생기면 돈이 절대적인 건 아니지만 무시할 수 없는 요소가 됩니다.&lt;/p&gt;
&lt;p&gt;그러다 보니, 시간이 흘러 제가 했던 결정에 아쉬운 것도 있더라고요. 이를 기초로 연봉과 몸값에 대한 몇 가지 제가 얻은 몇 가지 경험적인 팁을 공유합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 스타트업도 고액 연봉을&amp;#8230; 대신 회사에 투자 기회를&lt;/strong&gt;&lt;br /&gt;
제 첫 직장은 스타트업이었습니다. 3명으로 시작한 회사에 직원으로 입사했는데, 열심히 하다 보니 기회를 얻어 회사의 지분을 얻게 되었고 주도적인 역할을 하게되었습니다. 직원들 연봉을 결정하는 자리에 올랐지만, 제 연봉 만큼은 동결을 하거나 낮게 유지했습니다. 그게 회사의 비용 구조에도 도움이 되고 궁극적으로 회사에 도움이 된다고 생각했거든요.&lt;/p&gt;
&lt;p&gt;하지만, 이직을 해야 하는 상황이 생기자 제 기본급이 너무 낮았던 것을 알게되었습니다. 언제나 그렇듯 초기 상수는 불변입니다. 요즘은 스타트업들도 신입 연봉 많이 준다고 합니다. 개발자를 뽑기가 어려우니까요. 그러나, 안정적인 연봉을 주는 대기업에 있는 개발자가 스타트업으로 옮기려면 연봉을 낮춰야 합니다.&lt;/p&gt;
&lt;p&gt;가끔 후배 스타트업 CTO를 만나면 비슷한 고민을 듣게 됩니다. 제가 주는 조언은 일단 CTO 자신 부터 시작해서 개발자들 연봉을 업계 수준 이상으로 책정을 하라고 합니다. 다만, 그렇게 받은 연봉을 다른 방법으로 회사에 재투자하도록 하는 것입니다. 예를 들어, 자기 주식을 매년 매입을 하는 등 합법적으로 회사로 다시 돌아오게 하는 방법은 많습니다.&lt;/p&gt;
&lt;p&gt;대개 스타트업으로 입사할 때, (대박을 친다는 가정하에) 주식을 얼마나 주냐에 관심이 많지 내가 얼마를 투자할 수 있느냐를 물어보는 사람은 극히 드뭅니다. 자기 돈을 직접 투자를 못하는 회사라면, 대박칠 가능성도 낮겠죠. 여기에 두 가지 이점이 있는데요. 우선 개발자들이 스스로 회사에 대한 오너쉽이 생기게 됩니다. 스타트업은 사람이 전부라 성공 가능성이 훨씬 높겠죠. 이 회사가 나의 현재 뿐만 아니라 미래도 보살피고 있구나라는 생각도 듭니다. 만약의 경우, 퇴사를 하는 경우라도 기본급에 대한 보장이 되고 있으니까요.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 대기업에서는 신규 서비스를 만드는 팀을 찾아라!&lt;/strong&gt;&lt;br /&gt;
어느 정도 규모가 있는 대기업에 이직할 때는 전통적인 개발팀 보다는 신규 사업을 시작하는 팀을 찾아 지원해야 합니다. 누구나 신규 사업에 대한 사내 이직은 꺼리고, 외부에서 뽑으려면 어쩔 수 없이 연봉을 높여야 하는 경우가 태반입니다. 따라서, 이직 시 원하는 대로 연봉을 부를 수 있는 자격(?)이 주어질 수 있습니다.&lt;/p&gt;
&lt;p&gt;많은 개발자들이 오해하고 있는 것 중 하나가 대기업에서 이미 알려진 주요 서비스를 핵심 부서라고 생각하는 것입니다. 예를 들어, 네이버는 검색팀, Daum은 한메일 및 카페팀 등등. 그러나, 이들 팀들은 너무 오래되어서 여러 사람이 거쳐간 레거시 코드가 너무 많습니다. 결국 유지 보수 업무에 투입이 되거나, 뭔가 새로운 걸 펼치기에 너무 많은 기술적 부채들이 존재할 가능성이 큽니다.&lt;/p&gt;
&lt;p&gt;물론 주니어 개발자라면, 이런 팀에서 배울 수 있는 점도 많을 것입니다. 그러나, 결국 대형 서비스를 하는 업체는 필연적으로 대용량 트래픽을 다루게 되는데, 그런 노하우는 어떻게든 습득하거나 사내 플랫폼으로 해결 가능합니다. 새로 생긴 팀에 입사하면, 개발 자유도도 훨씬 높고 의욕적인 사람도 많아 업무 만족도도 높습니다.&lt;/p&gt;
&lt;p&gt;저도 두번째 직장이었던 Daum의 첫 팀은 전사 빌링개발팀이었습니다. 다음에 한번 이야기할 때가 있겠지만, 시행 착오가 있었고 꽤 성공적이지 못했습니다. 그 후로, 회사에 없던 새로운 일을 만들고 직접 새 팀을 꾸렸습니다. 새로운 팀은 위험도많지만,기회도 많구요. 큰 기업에서는 안정적인 실험도 가능합니다.&lt;/p&gt;
&lt;p&gt;따라서, 대기업에서 어떤 신규 서비스가 만들어지고 있는지, 소셜 미디어나 인맥을 통해 어떤 부서에서 사람이 많이 필요로 하고 있는지 추적하고 있는 건 매우 중요하구요. 자신의 몸값을 한 단계 업그레이드하기 위한 선택에 도움이 될 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 국내 뿐만 아니라 해외에도 눈을 돌려야&amp;#8230;&lt;/strong&gt;&lt;br /&gt;
개발자들은 기본급에만 집착하는 경우가 많은데, 인센티브나 주식 보상 등에도 꼭 신경을 써야 합니다. 지금 다니는 글로벌 기업의 연봉 체계는 국내 기업과 완전히 다릅니다. 입사 시, 정해지는 레벨에 따라 구간이 설정된 기본급과 별도로 최초 1-2년에는 &lt;a href="https://ko.wikipedia.org/wiki/%EC%82%AC%EC%9D%B4%EB%8B%9D_%EB%B3%B4%EB%84%88%EC%8A%A4"&gt;사인온 보너스&lt;/a&gt;라는 정기 인센티브를 지급함으로서 이직에 따른 연봉 상승 효과를 극대화해줍니다.&lt;/p&gt;
&lt;p&gt;3-4년차에는 &lt;a href="http://www.workingus.com/forums/topic/restricted-stock-unit-rsu%EA%B0%80-%EB%AD%94%EA%B0%80%EC%9A%94/"&gt;RSU(Restricted Stock Unit)&lt;/a&gt;라는 상장된 주식을 사인온 보너스 만큼 줍니다. 즉, 그때 바로 팔수 있는 주식입니다. 만약 주식이 그대로라면 3년차가 되면 연봉이 떨어집니다. 주식이 떨어지면 그 격차는 너무 커서 자동적으로 이직 사유가 되겠죠. 다만, 그 반대의 경우라면 연봉은 지속적으로 인상되는 효과를 얻습니다. 이런 체계에서는 회사가 잘 안될 경우, 자연적인 정리 해고가 가능해지는 효율이 높은 방식입니다.대신 국내 기업은 그냥 매년 연봉 인상률 몇 %거나 팀이나 업무 성과에 따른 인센티브를 받는 정도입니다.&lt;/p&gt;
&lt;p&gt;따라서, 국내 기업에 있는 개발자들이 유망한 글로벌 IT 대기업으로 이직하는 경우가 많고, 저 또한 권장하는 바입니다. 최근 미국 증시에 상위 기업 대부분이 IT 기업이고 이들의 주식 가격 상승률은 최근 몇년간 지속적으로 상승했습니다. IT 기술이 모든 산업에서 중심이 되고 있어, 아마 그 추세는 향후에도 변하지 않을 겁니다.&lt;/p&gt;
&lt;p&gt;상장된 국내 IT 기업에서도 스톡 옵션을 통해 구성원들의 업무 의지를 높히는 바람직한 회사들이 있습니다. RSU와는 다르지만, 스톡옵션을 통한 이익도 중요하기 때문에 스톡옵션을 주는 회사를 선택해도 좋겠습니다. 기본급 그리고 인센티브, 주식 가치를 통한 금융 자산은 모두 보상에 해당되고요. 근로소득 원천징수 영수증을 끊으면 다 포함되어 합산됩니다. 따라서, 이직할 회사에 협상 시 증빙 자료로 유리한 고지를 점하게 되는 거죠.&lt;/p&gt;
&lt;p&gt;마지막으로, 저는 22년 경력에서 이제 세번째 회사에 다니고 있습니다. 한 회사에서 오래 다니면서 업무 성과를 올리고, 이를 몸값에 반영하는 노력도 중요합니다. 일을 통해서 만족감과 행복을 얻는 것은 물론 중요하구요. 때에 따라 주기적으로 나름의 위기가 찾아오고, 그걸 여러 가지 방법으로 극복해야만 합니다.&lt;/p&gt;
&lt;p&gt;너무 회사를 자주 옮기는 것도 좋지는 않지만, 한 회사에서 너무 오래 있는 것보다는 3-5년 주기적으로 새로운 일과 회사를 찾아 가는 것도 필요합니다. 이 때, 몸값을 올림으로서 얻는 (일시적인) 행복감을 조금이나마 더 얻을 수 있도록 제 경험담이 도움이 되었으면 좋겠네요.&lt;/p&gt;
&lt;p&gt;물론 이 글의 내용은 개인적이고, 주관적 의견이라 여러분의 생각이 다르시다면, 댓글을 남겨주세요.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 목차&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1186"&gt;개발자 비급(祕笈) &amp;#8211; 1. 연봉은 실력의 결과가 아니다&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;개발자 비급(祕笈) &amp;#8211; 2. 나의 실패한 프로젝트 답사기&lt;/li&gt;
&lt;li&gt;개발자 비급(祕笈) &amp;#8211; 3. 코드 구루들의 특징&lt;/li&gt;
&lt;li&gt;개발자 비급(祕笈) &amp;#8211; 4. 업계 연예인이 되는 방법&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=AON0sgk2eGQ:8-n7PRC5AAQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 01 May 2018 21:58:26 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1186#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>훌륭한 개발 문화의 이면(4) – 사내 라이브러리를 잘 관리하려면?</title>
	<link>http://channy.creation.net/blog/1183</link>
	<description>&lt;p&gt;&lt;em&gt;(지난번 연재 3편을 끝내고 너무 오래 쉬었네요. 최근에 예전 회사 OB 모임을 갔다가 이런저런 이야기를 해보면서, 필요한 분이 있으실 것 같아 다시 연재를 시작합니다. 연초에 시간을 많이 투자한 AWS Summit도 끝나서 잠깐의 여유를 찾았네요.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;현업을 하다보면 개발자에게 숙명같은 일들이 있습니다. 반복되는 일을 자동화해야 하거나, 다른 팀과 소통을 하면서 필요한 인터페이스를 구성하거나, 아니면 팀 내에 재사용 가능한 뭔가를 만들어야 하는 때이죠.&lt;/p&gt;
&lt;p&gt;이 세 가지는 어떤 식으로 만나게 되고, 이를 위해 개발자 혹은 팀은 다양한 비급(祕笈)을 담은 라이브러리나 프레임워크를 만듭니다. 대부분 오픈 소스로 가져다 쓰는 경우도 많지만, 딱 맞는 기능만 있으면 좋겠는데 너무 무겁거나 사내 기술 표준이나 (인증, 지불, 통계, 모니터링 같은) 내부 시스템과 연동을 위해 인-하우스 솔루션으로 만드는 경우도 많죠.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1184" src="http://channy.creation.net/data/channy/2018/04/29151411/library-framework.png" alt="" width="600" height="300" /&gt;&lt;/p&gt;
&lt;p&gt;물론 &lt;a href="https://blog.outsider.ne.kr/924"&gt;사내 프레임워크 만들지 말자.&lt;/a&gt; &lt;small&gt;(Outsider&amp;#8217;s Dev Story)&lt;/small&gt;라던가 &lt;a href="https://sangheon.com/2012/09/26/2521"&gt;자신만의 개인 라이브러리 또는 프레임웤이 필요할까&lt;/a&gt; &lt;small&gt;(Sangheon&amp;#8217;s Archive)&lt;/small&gt;라는 의견도 있습니다만 어느 정도 규모가 되는 회사에 좋은 팀이 꾸려지면, 사실 사내 라이브러리는 필연적으로 만들어집니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 라이브러리는 생산성에 큰 영향을 끼친다&lt;/strong&gt;&lt;br /&gt;
자신의 노하우를 가진 라이브러리 몇 개쯤은 가지고 있어야 신규 개발할 때 편하듯, 회사에서도 팀 생산성을 위해 필요해집니다. 먼저 이런일을 누가 할거냐 하느게 개발팀의 숙제죠. 많은 개발자가 게을러 터져서 자동화나 최적화를 잘 했으면 하는데 꼭 그렇지만도 않더라구요. 어떤 사람은 성실하게(?) 코딩하는 분도 계시고, 새로운 것만 좋은 개발자도 있게 마련이죠.&lt;/p&gt;
&lt;p&gt;그래서 대개 한 두 사람에게 이런 일이 가게 마련입니다. 아니, 그런 사람들이 이런 일을 벌리죠. &amp;#8220;얼마 전 캐싱 이슈가 있었는데, 어제 시간이 남아서 간단하게 만들어 봤어요.&amp;#8221; 하면서 팀 회의에 짜잔 하면서 내놓는 천재 개발자입니다. 사실 써보니 또 좋아요&amp;#8230; 그러면 계속 그 사람들에게 동기 부여가 되고, 전체적인 팀 생산성은 높아집니다. 따라서, 꼭 팀에 이런 분들이 한두분 있어야 하구요. 자신도 그렇게 되도록 노력하는게 꼭 필요합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 팀을 넘어서 회사로&lt;/strong&gt;&lt;br /&gt;
그런데, 이게 회사 차원으로 옮겨가면 이야기가 좀 달라집니다. 많은 개발팀들이 개별적으로 만든 라이브러리나 프레임워크 혹은 전사로 쓸 수 있는 서비스를 팀내에서만 아니라 다른 팀도 쓰면 좋겠다고 홍보를 하는 것이죠. 사내 세미나를 열기도 하고, 개발 본부 내에서 공유도 합니다.&lt;/p&gt;
&lt;p&gt;예를 들어, 대용량 트래픽을 감당해야 하는 팀은 DB Pool의 한계를 극복하기 위해 특정 SQL을 캐싱해주는 시스템을 만들기도 하고, 프론트엔드 개발자들은 우리 회사 가이드에 맞는 UI 라이브러리를 그리고 쇼핑 개발팀의 경우 이벤트를 많이 하니까 이벤트 개설 부터 추첨 및 팔로우업까지 하는 시스템을 만들어 배포하기도 하구요. 이러니 사내에서 입소문이 나서 알음알음 다른 팀이 가져다 쓰게 됩니다.&lt;/p&gt;
&lt;p&gt;제가 Daum 입사 초기에 몇 년간 기술 스탭을 하면서 &amp;#8220;우리 팀이 이런거 개발했다. 근데 이팀 저팀이 쓴다. 전사적으로 쓰면 좋겠다&amp;#8221; 이런식의 요청을 많이 받았습니다. 즉, 전사 공통 라이브러리가 됐으면 하는 바램이 있는 거죠. 하지만, 역할과 의무를 논하면 이야기가 달라집니다. 우리 팀은 다른 팀 요구 사항을 받고 처리하는 자원은 부족하니, 사내 공통 플랫폼팀에서 맡아줬으면 하는 겁니다.&lt;/p&gt;
&lt;p&gt;하지만, 사내 공통 플랫폼을 맡아서 하는 팀도 하는 일이 없는게 아니잖아요. 이미 그런식으로 올라와서 관리하는 라이브러리와 도구가 한 두개가 아닌데다 어떤 건 레거시처럼 거의 신경도 못쓰고 굴러가는 것도 있을 정도로 리소스가 부족합니다. 게다가, 사내 전체적인 수요에 의해 결정된 것도 아니고 궁극적으로 남이 만든 코드를 유지 보수를 하라는 건데 누가 좋아하겠습니까? 그래서 대개 그런 건 거절을 하게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 콘트롤 타워 부재의 문제점&lt;/strong&gt;&lt;br /&gt;
이렇게 되면 크게 두 가지 현상이 발생합니다. 여전히 공개에 적극적인 팀은 그냥 스스로 쓰겠다는 팀에 맘대로 배포를 합니다. 버전을 올려 배포를 하긴 하지만, 소스채로 배포를 해주고 팀마다 커스트마이징을 하니 버전이 몇 십개가 생깁니다. 다른 팀 피드백을 받아 고치면 자기들에게도 도움이 되니 적극적이지만, 팀 마다 여러 벌이 생기면 이마지도 쉽지 않게됩니다. 시간이 흘러 사내에서 너무 많이 쓰고 있어서 이제 전사적으로 관리를 해보겠다고 치면 세상에&amp;#8230; 팀마다 다 다른 버전을 들고 있습니다.&lt;/p&gt;
&lt;p&gt;더 큰 문제는 무작위 배포한 버그로 인한 장애가 전사에 영향을 주는 경우입니다. 어떤 팀에서 난 장애가 아직 다른 팀에서는 잠복하고 있는 경우도 있구요. 결국 전사 기술 스탭이 나서야 하는 상황이 생깁니다. 개별 팀에서 생긴 문제를 인지하고, 해결법에 대해 전사 공유, 그리고 각 팀이 해결했는지 코드 검증 등등 배보다 배꼽이 더 큰 경우가 생기게 돼죠.&lt;/p&gt;
&lt;p&gt;그나마 공개에 적극적이지 않은 팀은 그냥 자기네들만 쓰면 상관이 없습니다. 그런데, 공개를 요구하면서도 자기들은 직접 하기 싫은 팀은 오히려 뒷말을 하는 경우도 있습니다. 도대체 기술 스탭은 뭐하는 거냐? 공통 플랫폼팀은 그런거 하라고 있는거 아니냐? 결국 기술 스탭이 또 나서야 하는 상황이 생깁니다. 설득을 하든 강하게 거절을 하든&amp;#8230; 피곤합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 사내 공통 플랫폼 레포지터리 구축&lt;/strong&gt;&lt;br /&gt;
결국 제가 발견한 해법은 이런 겁니다. 사내에 오픈 소스 레포지터리를 만드는 것이죠. 개별 팀이 공개를 목적으로 만든 사내 라이브러리, 프레임워크 모두 올리게 합니다. 그리고, 그냥 오픈 소스 작동 원리에 맡깁니다. 잘되는 라이브러리는 잘될거고 안되는 건 안되겠죠.&lt;/p&gt;
&lt;p&gt;당시 오픈 소스 개발 원리를 회사에 주입(?) 시키는 건 쉬운 일이 아니었습니다. 제가 일하던 시절이 12년 전이라 그 흔한 Pull Request 개념도 없었습니다. 이미 Github이 대세이고, 대부분 개발자들이 오픈 소스 원리로 개발을 하는게 뭔지 많이 알고 있기 때문에 요즘에는 훨씬 쉬울겁니다.&lt;/p&gt;
&lt;p&gt;결과적으로 몇 가지 이점이 있습니다. 막상 코드를 공개하게 되면, 초기 개발팀에서 개별 배포하는 것 보다 좀 더 친절하게 매뉴얼이나 주석에 좀 더 신경을 쓰게 됩니다. 오픈 소스 레포지터리가 가진 기본 기능들, 즉 코드 리비전 관리, 버전 관리, 설치 방법, API Docs 문서 기능 등의 부담이 들지만, 누구나 쉽게 코드 개선 제안을 하고 버그가 발견되면 쉽게 고쳐지는 이점이 있습니다. 기술 스탭의 입장에서 선호도와 사용량을 보고 전사 관리해야 하는 것과 아닌 것을 판별할 수 있습니다. 설득에 도움도 됩니다.&lt;/p&gt;
&lt;p&gt;사내 공통 플랫폼팀이 관리하던 라이브러리나 시스템이 우선 공개 대상이 됐구요. 개별 팀이 만들어 공개하고 싶은 것도 자유롭게 공개하도록 했습니다. 그러니, 정리될 건 정리되고 시간이 흘러 더 이상 유지 보수가 안되는 쉽게 알 수 있게 되었습니다. 무엇보다 새로 들어온 개발자들이 사내 공통 플랫폼에 맞추어 선택해 사용할 수 있는게 많구나! 좋은 회사 왔구나 하는 심리적인 안정감도 큰 도움이 되었습니다. 그 전에는 궁금한 점이 있으면 누구에게 물어봐라 위키 찾아봐라 정도가 대부분이었거든요.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 사내 개발자 경력 개발의 이점&lt;/strong&gt;&lt;br /&gt;
12년 전에 이게 잘됐냐 물으신다면 저는 만족하지는 못했습니다. 오히려 잘 안됐다고 보는게 맞겠죠. 그렇지만, 이런 시도는 진짜 중요하구요. 이를 계기로 공통 플랫폼 개발하는 팀이 하는 일이 투명화 되고, 유지 리소스 산정에 도움이 되었습니다. 대개 서비스팀에 소속된 개발자들이 공통 플랫폼팀으로 팀 이직을 하는 경우도 많아졌습니다. 여기 오면 전사 관점에서 눈을 키울 수 있거든요.&lt;/p&gt;
&lt;p&gt;제가 꼭 하고 싶었는데 못했던 것이 서비스팀에서 몇년을 일하면 무조건 공통 플랫폼팀에서 1년을 일하도록 강제하는 개발자 HR제도입니다.&lt;/p&gt;
&lt;p&gt;대개 서비스 개발팀장들은 잘 하는 사람을 놓기 싫어하고, 그러다 보니 경력 개발 상 하나의 도메인에 집중하게 됩니다. 공통 개발팀에 오면, 전사의 서비스팀 관계자와 두루 소통을 하게 되어 시야도 넓어질 뿐 아니라 다른 팀으로 옮겨 가는 기회도 더 포착할 수 있거든요. 실제로 그런 경우도 많이 봤구요. (개발팀장들의 반대로 못했던게 끝내 아쉽습니다.)&lt;/p&gt;
&lt;p&gt;이렇게 누가 뭘하는지 서로 잘 알게되면 사내 개발 문화를 바꾸는데도 도움이 됩니다. 문제를 해결하고 전사 생산성을 높이는 사람이 좀 더 주목을 받을 수 있구요. 자연적으로 이런 정보를 전달하거나 받고자 하는 요구가 늘어나고 사내 개발자 콘퍼런스나 세미나가 활성화 됩니다. 좋은 개발자를 뽑는 거 다른 거 없습니다. 안에서 잘하면 자동적으로 입소문이 나고, 이를 외부에 잘 공유하면 누구나 잘 알게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 오픈 소스 기반 개발 환경은 필수&lt;/strong&gt;&lt;br /&gt;
이를 위해서는 회사 내에 오픈 소스 소프트웨어 사용 빈도를 높이는 것이 매우 중요합니다. Daum의 서비스는 리눅스, 자바 기반의 오픈 소스 플랫폼(톰캣, 스프링 등)을 주로 활용하고 개발 환경도 이클립스와 메이븐 등이고 개발 지원 서버들도 서브버전, Git, JIRA, 컨플루언스 위키, Sonar, Jenkins 등을 활용하고 있습니다. 개발자들이 오픈 소스를 모르면 개발하기 힘든 상황이었던 겁니다.&lt;/p&gt;
&lt;p&gt;그러다 보니, 오픈 소스를 바로 다운로드 받아 사용할 수 있는 미러(Mirror) 서버가 필요했고, 2002년부터 사내 FTP 미러 서버가 제공되었다고 한다. 제가 이를 2007년에 외부에 &lt;a href="http://ftp.daum.net"&gt;ftp.daum.net&lt;/a&gt;이라는 이름으로 공개하고 아파치, 이클립스, 우분투, 센트OS 등의 공식 미러로 지정되었습니다. 당시 17TB의 스토리지에 일간 피크타임 기준 600MB의 트래픽을 제공해서 당시 국내 웬만한 개발자들은 한번쯤 이용했습니다.&lt;/p&gt;
&lt;p&gt;물론 사용하는 거랑 소스 공헌하는 거는 완전히 다른 이야기입니다. 톰캣 가져다 커스트마이징해서 쓰다가 업스트림에 반영을 못해서 2.0으로 버전이 올라가도 1.0을 그대로 써야 하는 문제도 있었구요. 이런 문제를 해결하려면, 사내에서 부터 오픈 소스 개발 프로세스를 체득하고 실습해 볼 수 있는 내부 플랫폼은 필수입니다. &lt;em&gt;더 자세한 이야기는 &lt;a href="http://channy.creation.net/blog/569"&gt;기업 내 오픈 소스 개발 방식 도입記&lt;/a&gt;를 참고하시기 바랍니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;이제 회사가 성장함에 따라 몇 개의 백엔드/프론트엔드 라이브러리 정도가 아나라 수십개의 모바일 SDK, 수백개의 API 등 그 숫자는 갯수는 기하급수적으로 늘어났습니다. 이를 정리할 또 다른 방법이 필요했습니다. 다음 편에서는 어떻게 사내 API 플랫폼을 성장 시켰고, 개발 문화를 바꾸었는지 살펴보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 목차&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1104"&gt;훌륭한 개발 문화의 이면(1) – 코딩 테스트인터뷰 제대로 하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1107"&gt;훌륭한 개발 문화의 이면(2) – 자율적 개발 환경을 선택하라!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1110"&gt;훌륭한 개발 문화의 이면(3) – 다른 팀 소스 코드를 볼 수 있는가?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/1183"&gt;훌륭한 개발 문화의 이면(4) – 사내 라이브러리를 잘 관리하려면?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;훌륭한 개발 문화의 이면(5) – 소통 비용의 절약 &amp;#8211; 서로 API로 말하자&lt;/li&gt;
&lt;li&gt;훌륭한 개발 문화의 이면(6) – 오픈 소스를 통한 외부 개발자 전략 구사하기&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=SW7ZgB9Pz0k:v7OM8zntQxY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 29 Apr 2018 06:19:39 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1183#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>블록체인 (비트코인) 열풍 속 현실적 조언</title>
	<link>http://channy.creation.net/blog/1175</link>
	<description>&lt;p&gt;&lt;em&gt;&lt;a href="http://channy.creation.net/blog/1145"&gt;인공 지능(AI) 시대의 현실적 조언&lt;/a&gt;에서 처럼 저는 블록체인(Blockchain)이나 암호화폐(Cryptocurrency) 전문가가 아닙니다. 다만, 이 글은 IT 기술로 인해 세상이 바뀌기 시작할 때 그 주변에서 일어나는 패턴을 몇번 경험해 본 결과, 각 분야에 있는 분들이 현실적으로 택할 수 있는 이야기를 간단하게 풀어 보려고 합니다. 이 내용을 진지하게 받아들일지 말지는 전적으로 여러분의 선택에 달려 있습니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;일반 투자자&lt;/strong&gt;&lt;br /&gt;
올해 블록체인 기반의 주요 암호화폐들이 폭등을 거듭하였습니다. 많은 분들이 미리 투자할 걸 하는 생각하셨을 겁니다. 하지만, (저를 포함하여) 여러분이 암호화폐에 아직 투자를 안하셨다면, 정말 잘 하신 겁니다. 소위 도박장에 한번 발을 들이면 쉽게 빠져 나오기도 어렵거니와 생업과 일상 생활에도 엄청난 지장을 주니까요. &lt;em&gt;(심지어 &lt;a href="http://www.hankookilbo.com/v/4057fcf820e44cedafc28db2d5488b13"&gt;목숨&lt;/a&gt;까지&amp;#8230;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;개인적으로 몇 년전 부터 블록체인 학습차 코인 채굴 과정이나 소량 코인 매매에 참여한적이 있습니다. 큰돈 벌었냐구요? 대부분 조금 오르면 팔고해서 큰 수익이 나지 않았습니다. 실제로 암호화폐에 관심있어 투자하셨던 분들도 사고 파는 과정에서 큰 수익을 내지 못했고, 변동성이 너무 커서 관심 끄고 그냥 투자차원에서 묻어두는 것도 힘들다고 하시더라구요. (최근 급등장 바로 앞에 진입하신 분이나 진득하니 오랜 기간 보유하고 있었던 분들은 대박이겠지만, 세상은 원래 불공평하고 일부는 항상 운이 엄청 좋죠.)&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1178" src="http://channy.creation.net/data/channy/2018/01/02060110/bitcoin-growth-2017.png" alt="" width="1340" height="668" /&gt;&lt;/p&gt;
&lt;p&gt;블록체인이 엄청 대단한 거처럼 포장되지만 IT 기술 거품은 항상 존재했고, 언젠가는 꺼집니다. 20년전 낙관론을 펴던 인터넷 닷컴 버블도 크게 붕괴했고, 지금은 소수 거대 사업자에 의해 시장이 움직이고 있습니다. 과거 블록체인과 유사한 MP3 공유 사이트, P2P 파일 공유 같은 탈중앙화 시도 역시 음지에서 양화로 바뀌는데 대부분 실패했습니다. 모든 기술 현상에는 역작용이 있으며, 블록체인도 그런 과도기를 거칠 것으로 예상됩니다. 누구에게는 선구자로서 부를 함께 얻을 수 있겠지만, 대다수에게는 큰 피해로 남을 가능성이 큽니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. Steemit #kr-coin &lt;a href="https://steemit.com/trending/kr-coin"&gt;대세글&lt;/a&gt;이나 &lt;a href="https://steemit.com/hot/kr-coin"&gt;인기글 목록&lt;/a&gt; (투자 정보)&lt;br /&gt;
2. 머니맨님의 &lt;a href="http://moneyman.kr/archives/tag/%ec%bd%94%ec%9d%b8"&gt;코인 투자 칼럼&lt;/a&gt; (긍정적)&lt;br /&gt;
3. 반달가면님의 &lt;a href="http://bahndal.egloos.com/tag/%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8"&gt;비트코인 칼럼&lt;/a&gt; (부정적)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;일반 개발자&lt;/strong&gt;&lt;br /&gt;
블록체인 기술은 개발자에게 친숙한 기술입니다. 분산 환경에서 각 노드에 분산된 정보를 공유해서 데이터 정합성을 유지하는 기본 개념을 가지지만, 이것을 인터넷 스케일로 가져가면 여러가지 네트워크나 스토리지 등 물리적 한계에 직면할 수 밖에 없습니다.&lt;/p&gt;
&lt;p&gt;거래량이 늘어나면서 체결에 많은 시간이 걸리고, 원장 크기도 기하급수적으로 늘어나서 탈 중앙화 개념 자체가 무력화 될 수 있습니다. 2017년 12월말 현재 비트코인의 총 원장 크기는 약 &lt;a href="https://blockchain.info/ko/charts/blocks-size?timespan=all"&gt;150GB&lt;/a&gt;입니다. 이제 개인 PC에 블록 데이터를 /bitcoin/blocks 폴더에 다운받는다는 건 이제 거의 불가능한 상태입니다. ㅠㅠ&lt;/p&gt;
&lt;p&gt;그럼에도 불구하고 새로운 기술에 대한 감을 익히는 것은 매우 중요합니다. 개발자라면 신규 기술의 개념을 살펴보고, 간단한 테스트를 하는 건 장려합니다. 하지만, 인공지능 기술 만큼은 아닙니다. 만약 새해 학습 계획에 우선 순위를 둔다면, AI/ML 그 다음에 블록체인을 공부 하세요.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. 박재현님의 이더리움프로그래밍 &lt;a href="http://wisefree.tistory.com/477?category=697903"&gt;http://wisefree.tistory.com&lt;/a&gt;&lt;br /&gt;
2. 이더리움 한국어 백서 &lt;a href="https://github.com/ethereum/wiki/wiki/%5BKorean%5D-White-Paper"&gt;https://github.com/ethereum/wiki/White-Paper&lt;/a&gt;&lt;br /&gt;
3. Awesome Blockchain &lt;a href="https://github.com/openblockchains/awesome-blockchains"&gt;https://github.com/openblockchains/awesome-blockchains&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;스타트업&lt;/strong&gt;&lt;br /&gt;
새해에도 블록체인 스타트업 투자는 계속 될것입니다. 사실 스타트업이 꿈에다 투자를 하는 거고, 이미 되든 안되는 바이럴에는 성공했으니까요. &lt;em&gt;(비트코인 덕에 이제 옆집 개도 블록체인이 뭔지 알게 됐어요.)&lt;/em&gt; 현재는 코인 거래소나 가능성 높은 &lt;a href="https://steemit.com/ico/@shiningmoon1969/ico"&gt;ICO(Initial Coin Offering)&lt;/a&gt;에 투자가 몰리는 것 같지만, 상위 코인거래소와 주요 암호화폐의 성패는 어느정도 드러났고, 투자 거품까지 끼고 있으니 언젠가는 (조만간) 정리될 것 같네요.&lt;/p&gt;
&lt;p&gt;여전히 스타트업에게 블록체인 기술만이 해결 가능한 문제가 무엇인가?라는 질문에 답을 찾는게 급선무입니다. 많은 분들이 분산화된 거래 원장, 사용자 인증, 크라우드 펀딩 다양한 유즈케이스를 이야기하지만 블록체인이 아니라도 해결 가능합니다. 과거 Web 2.0시대에도 분산화된 인증 기술(예: OpenID 등) 관련 스타트업이 많이 나와서 기존 인증을 대체하려고 했지만 일부 M&amp;amp;A한 경우를 제외하고는 모두 망했습니다. 그러나, 새로운 시도는 언제나 옳고 성공하지 못하더라도 뛰어드는게 스타트업이죠. 도전하실 분은 뛰어드세요~&lt;/p&gt;
&lt;p&gt;제가 가끔 5년 &lt;a href="http://blog.creation.net/452"&gt;웹 분산-집중 주기설&lt;/a&gt;을 주창해곤 했는데, 2015년 은 &amp;#8216;블록체인&amp;#8217;이네요. 그동안 주요 IT 기업에 의한 집중화된 플랫폼 비즈니스의 효용성이 분산 기술 보다도 훨씬 IT 산업 성장에 영향을 주었습니다만, 분산 기술이 뜰 때가 다음 성장을 예고하는 것처럼 보입니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1176 aligncenter" src="http://channy.creation.net/data/channy/2018/01/02052620/blockchain-2015.png" alt="" width="500" height="250" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. 표철민님의 &lt;a href="https://charlespyo.com/2017/11/16/%EB%B8%94%EB%A1%9D%EC%B2%B4%EC%9D%B8%EC%9D%80-%ED%98%84%EC%9E%AC-%EC%96%B4%EB%94%94%EC%AF%A4-%EC%99%80%EC%9E%88%EB%82%98/"&gt;블록체인은 현재 어디쯤 와있나?&lt;/a&gt; (스크롤 압박 및 말빨 현혹(?) 주의)&lt;br /&gt;
2. 박재현님 ZDNet 칼럼 &lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20170904160310"&gt;블록체인 기반 플랫폼 비즈니스를 이해하자&lt;/a&gt; 및 &lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20171201145305"&gt;블록체인 도입시 고려 사항&lt;/a&gt; (냉정하고 객관적인 조언)&lt;br /&gt;
3. Tony Kim님 &lt;a href="https://brunch.co.kr/@blockchainstory/2"&gt;블록체인에서 가능한 재미난 비즈니스 모델&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;그 밖에 분들&amp;#8230;&lt;/strong&gt;&lt;br /&gt;
그 밖에 분들은 이번 블록체인 흐름에는 그냥 비껴가는 것도 좋습니다. 세상의 모든 변화에 동조하고 관심가지고, 휩쓸릴 필요는 없습니다. 괜히 관심을 가지다가, 맨 앞의 일반 투자자로 들어서고 정신과 금전 모두 피폐해지는 것을 경험하느니 내가 하고 있는 소중한 일에 더 힘을 쏟고, 사랑하는 주변 사람들에게 관심을 가지는 게 더 효과적인 일이니까요.&lt;/p&gt;
&lt;p&gt;다만 국내 코인 거래소들의 경우, 통신 사업자 달랑 끊고 금융 거래에 준하는 &lt;em&gt;(불법도 탈법도 아닌)&lt;/em&gt; 사업을 하고 있는데, 정부와의 한판 대결이 올해 주요 관전 포인트입니다. 2000년대 초반 온라인 음악 스트리밍이랑 소규모 지불 결제(PG) 사업을 할 때, 음반 사업자와 저작권(법) 이슈로 합법화로 가는데 지리한 싸움에 직접 참여 한 적이 있습니다. PG 사업의 경우, 카드깡에 시달리면서 전자금융법 개정까지 불법 업체처럼 여겨졌었죠.&lt;/p&gt;
&lt;p&gt;당시 음반 사업자나 카드사처럼 민간 금융 기관 같은 기득권자가 들고 일어나야 하는데, 지금은 정부가 대리전을 하고 있는 양상입니다. 그럴수록 공신력은 더 높아가고, 정부가 버블을 자초하는게 우려됩니다. 음악 스트리밍과 지불 결제 사업은 그래도 뭔가 움직이는 재화가 있는데, 암호화폐란건 실체 없이 다단계 방식에 도박 같은 묻지마 투자를 하는 느낌입니다.&lt;/p&gt;
&lt;p&gt;암호화폐를 가장한 블록체인 붐도 언젠가 잠잠해 질 겁니다. 하지만, 그때는 블록체인이 필요 없는 것이 아니라 대안이 되는 서비스가 성장하는 시대가 되겠죠.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;p.s. 거품이 꺼지는 징조는 사람들이 특정 자산에 미치면서, 정산 나간 일을 시작할 때라고 합니다. 대표적인 경우가, &lt;a href="https://jesuscoin.network/"&gt;Jejus Coin&lt;/a&gt;인데, 예수님의 재림을 위해 만든 암호화폐라고&amp;#8230; 죄사함도 받을 수 있고, 기존 대형 교회가 가진 재산을 넘는 시가 총액을 목표로 한다고 하네요. 정말 말세(末世)네요.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;아무튼 &lt;a href="http://www.businessinsider.com/infographic-shows-how-close-we-could-be-to-an-economic-bubble-collapse-2017-9"&gt;세상의 모든 게 버블&lt;/a&gt;인 시대에 살고 있어요 ㅠㅠ&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone size-full wp-image-1179" src="http://channy.creation.net/data/channy/2018/01/02061509/everything-bubble.png" alt="" width="812" height="2560" /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=9S1wi7wC4_I:R3FbOO_ksZg:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 01 Jan 2018 21:18:46 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1175#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>아이폰 10주년 아이폰X(10) 공개</title>
	<link>http://channy.creation.net/blog/1169</link>
	<description>&lt;p&gt;아이폰 10주년을 맞아 애플이 &lt;a href="https://www.apple.com/kr/iphone-x/"&gt;아이폰X(10)&lt;/a&gt;을 공개했습니다. 그동안 아이폰 출시에 대한 많은 비밀주의가 있었는데, 이번 만큼은 외부에 너무 많이 공개 되었죠. &lt;em&gt;(아는 사람은 다 아는 그래서 전혀 새롭진 않았습니다.) &lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;홈버튼은 역사속으로&lt;/strong&gt; &amp;#8211; 요즘 안드로이드 최신 버전에는 홈버튼이 없죠. 아이폰의 트레이드 마크였지만 이제 역사속으로 없애고, 아래에서 위로 쓸면 됩니다. 밀어서 잠금 해제가 없어지고, 눌러서 잠금 해제도 이제 사라졌습니다. 따라서, 지문 인식을 통한 TouchID도 사라짐&amp;#8230;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;인공지능 Face ID 향상&lt;/strong&gt; &amp;#8211; 얼굴을 통한 인식 기능에 신경을 많이 쓴 것 같습니다. 헤어스타일이나 화장을 해도 자기 얼굴을 인식할 수 있는데, 이를 위해 앞에 카메라 뿐만 아니라 각종 감지 센서, 적외선, 조명 등이 동원됐고, A11칩에 신경망 엔진(딥러닝)까지 집어 넣었군요. 이모지 캐릭터로 얼굴 감정 변화를 넣을 수 있다는 재미 요소도 넣었지만&amp;#8230; 너무 하이 스펙인 것 같아요.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;고성능 OLED 디스플레이&lt;/strong&gt; &amp;#8211; 화면은 5.8인치, 해상도는 2436 x 1125, 458ppi 기반 OLED 디스플레이를 탑재했습니다. 아이폰 7에 비해 거의 2배입니다. 자! 모바일 앱 개발자들과 디자이너들은 또 한번 디자인 리소스 업데이트를 해야 하고, 사용자들은 더 많은 저장공간과 데이터 사용량이 필요하겠네요. 점점 더 무거워지고 더 비싸지고&amp;#8230;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;가격은 999달러 부터&lt;/strong&gt; &amp;#8211; 예약 판매는 10월 27일, 출시는 11월 3일 입니다. (한국은 아무래도 연말이나 내년 초?) 64GB와 256GB 용량으로 999달러 부터 판매를 합니다. 1차 출시 예약을 빨리 해도 추수감사절 시즌에나 받을 수 있을 것 같네요. 요걸 노린듯&amp;#8230;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;놀라운 건 이게 아니라, 아이폰X 출시와 함께 &lt;a href="https://www.apple.com/kr/iphone-8/"&gt;아이폰 8&lt;/a&gt;이라는 어중간한 제품을 같이 출시했다는 겁니다. 아이폰 8에 대해 열심히 설명을 했지만, &lt;em&gt;(죄송 기억이 안납니다)&lt;/em&gt; 아이폰X에 다 묻혀버렸습니다. 이번달에 출시하는데, 아이폰 7을 대체할 만큼 좋은지는 잘 모르겠습니다. &lt;em&gt;(애플 워치3, 애플 TV 4K도 공개했습니다만&amp;#8230; 죄송 역시 기억이 안납니다.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;결론&amp;#8230; 저는 올해 1월 1일에 산 아이폰 7 그대로 쓸 겁니다 ㅎㅎ&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1170 size-full" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/2017-iphone10-1.png" alt="" width="1875" height="930" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;제품 사이트&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;아이폰 X : &lt;a href="https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.apple.com%2Fkr%2Fiphone-x%2F&amp;h=ATMtdHE8D1k9mZSeWoB9-AW-3yycTzjzVzjAa801MEdDd6f71F-fdEl8hwu42uttNzSUBusvSyDRZRXtjZSvCB1V77HNHYTxuuGyKvMcl67weAHvMeXWBf1hXMKgpM3gEUsvi8-AgsDE7skkyGGZEmHw2htEb0WQIPKBe0V-uiD7zRIQHXmAdzofEyOyeyMpoW4WnJMI5K7e0VNce2GS8xuipsPZficxTFcRalOMeSOBSeAjPUiuhuoTrP8pj4V7YS2gSDeqmnnxzmCpUShZV3nqiFJ2v_aPmmpjuD59jG549w" target="_blank" rel="noopener"&gt;https://www.apple.com/kr/iphone-x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 8 : &lt;a href="https://www.apple.com/kr/iphone-8/" target="_blank" rel="noopener"&gt;https://www.apple.com/kr/iphone-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;동영상&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이벤트 전체 영상 &amp;#8211; &lt;a href="https://www.apple.com/apple-events/september-2017"&gt;https://www.apple.com/apple-events/september-2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 X &amp;#8211; &lt;a href="https://www.youtube.com/watch?v=mW6hFttt_KE"&gt;https://www.youtube.com/watch?v=mW6hFttt_KE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 X (조니 아이브) &amp;#8211; &lt;a href="https://www.youtube.com/watch?v=mW6hFttt_KE"&gt;https://www.youtube.com/watch?v=mW6hFttt_KE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;아이폰 8과 8 플러스 &amp;#8211; &lt;a href="https://www.youtube.com/watch?v=UL3K5QJKOLg"&gt;https://www.youtube.com/watch?v=UL3K5QJKOLg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;p.s. &lt;a href="http://channy.creation.net/blog/381"&gt;2007년&lt;/a&gt; 아이폰이 출시 되었으니 벌써 10년이 지났군요. 10년이면 강산이 변한다는데, 정말 많은 게 바뀌었습니다. 한국 출시가 계속 늦어져서, 우리 나라 사람들은 &lt;a href="http://blog.creation.net/430"&gt;2010년&lt;/a&gt;이 되어서야 쓸 수 있게 되었고, 안드로이드가 나오고, 전 세계적인 거대한 모바일 환경 변화가 있었습니다. (Thanks, Jobs!)&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=YKm7a7PnPE4:xJuVloHFdow:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 12 Sep 2017 21:35:02 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1169#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>어른들의 소통 방식</title>
	<link>http://channy.creation.net/blog/1167</link>
	<description>&lt;p&gt;김상조 공정위원장이 &amp;#8216;스티브 잡스&amp;#8217;를 비유로 들어 이해진 의장의 경영 능력에 대해 비판한 것은 적절하지 못했습니다. 국내 IT 산업에 대한 무지를 그대로 드러낸 것이고, &lt;a href="http://[인터뷰] 김상조 “이해진에 잡스 얘기 해주고 싶었다”"&gt;&amp;#8220;당시 이 얘기를 할까 말까 하다가 안 했다&amp;#8221;&lt;/a&gt;라는 이야기를 언론에 한 것은 더욱 그렇지요. 당사자에게 개인적으로 하면 되지, 언론에 그 이야기를 왜 하시나요? &lt;em&gt;(9월 5일자 국민일보 인터뷰 기사)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;하지만, 비유가 적절하지 못했을 뿐, 김 위원장의 생각에 대한 맥락에 대해서는 다른 기사를 통해서는 이해할만 합니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;-지난달 14일 공정위를 직접 방문한 이 전 의장과 10분 가량 환담을 나눴는데.&lt;/p&gt;
&lt;p&gt;&amp;#8220;10분 가량 이야기를 나누면서 이 전 의장에 대해 우리나라 신(新) 산업을 일으킨 ‘개척자’라는 존경심을 갖게 됐다. 다만 개척자로서 이 전 의장이 우리 사회에서 보다 ‘영속성’을 지니기 위해서는 조금 더 고민이 깊어져야 한다는 느낌을 받았다. 친인척을 경영에 개입시키지 않고 지분을 최소한도로 줄이는 등 그 동안 사회적으로 지탄을 받아 온 재벌 총수와 다른 모습을 보여준 것만으로 미래까지 담보할 순 없다. 지금까지 이 전 의장이 보여준 모습은 우리가 개혁해야 할 ‘과거의 구태’로부터 벗어난 것일 뿐이다. 우리사회가 지향해야 할 새로운 기업의 모습을 아직 보여주진 못하고 있다.&amp;#8221;&lt;/p&gt;
&lt;p&gt;-네이버가 국내 검색 시장을 사실상 독점하는 것에 대한 우려의 목소리가 적지 않다.&lt;/p&gt;
&lt;p&gt;&amp;#8220;이 같은 우려에 대해 이 전 의장이 스스로 사회에 메시지를 전달할 필요가 있다. 이것을 하고 있지 않기 때문에 경쟁당국이 법으로 집행해야 한다는 필요성을 느끼게 된 단계까지 온 것이다. 이 전 의장이 지금까지 자신이 달성한 부분에 대해 우리 사회가 어떻게 평가하고 있는지를 폭넓게 청취하고, 사회에 메시지를 던져야 한다. 너무 늦어지면 법적 ‘태클’을 받을 수밖에 없다. 규모가 큰 기업의 리더가 해야 하는 중요한 역할은 시장 개척이나 신기술 개발이 아니다. 이는 전문경영인에게 위임할 수 있다. 진짜 리더의 역할은 사회 구성원들이 어떤 요구를 하고 있는가를 파악하고, 사회와 맞춰가는 것이다. 이재용 삼성전자 부회장이 실패한 게 바로 이 지점이다.&amp;#8221; &lt;a href="http://www.hankookilbo.com/v/9f0557149db146978e5da34e1184c7a2"&gt;[한국일보] 김상조 “4대 그룹 개혁, 12월이 데드라인” &lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;본 인터뷰에서는 네이버 뿐만 아니라, 삼성과 현대차의 지배 구조와 총수들에 대해서도 비판을 마다하지 않았습니다. 공정위원장 자리에서 주요 대기업의 이슈에 대해 직접적인 이야기하는 것이 부적절할 수 있습니다.&lt;/p&gt;
&lt;p&gt;하지만, 대기업들이 일반적인 국민 정서를 잘 이해하지 못하고, 총수의 지배 구조를 방어하기에만 급급하고, 기업 총수는 사회적 책임을 방기하고 있다면, 공정위원장으로서 시정하도록 조언을 해주는 것도 필요합니다. 삼성전자, 네이버를 포함 우리 나라 대다수 대기업의 주요 주주가 바로 국민 연금입니다. 일반적인 국민들이 느끼는 감수성에 맞출 필요가 있지요.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1168 size-full" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/naver-kimsangjo-jwlee.jpg" alt="" width="600" height="615" /&gt;&lt;/p&gt;
&lt;p&gt;다음 창업자인 이재웅님이 김상조 위원장의 발언에 &amp;#8216;부적절하다&amp;#8217;고 표현 하신 것은 수긍합니다. 기업가를 대하는 정부 관료의 자세는 아니니까요. 만약 이런 충고를 할 수는 있지만, 하더라도 개인적으로 하는 게 맞습니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;제 글이 언론에 인용될 줄 몰랐습니다. 오만이라는 표현은 부적절했습니다. 김상조위원장의 표현도 부적절했습니다만 제 표현도 부적절했습니다. 수정합니다.&lt;br /&gt;
&amp;#8212;&amp;#8212;&lt;br /&gt;
할 말이 많습니다만 딱 한 마디만 하겠습니다,&lt;br /&gt;
김상조 위원장이 지금까지 얼마나 대단한 일을 했고, 앞으로 얼마나 대단한 일을 할지는 모르겠지만, 아무것도 없이 맨몸으로 정부 도움 하나도 없이 한국과 일본 최고의 인터넷 기업을 일으킨 기업가를 이렇게 평가하는 것은 부적절합니다.&lt;br /&gt;
동료기업가로서 화가 납니다. (출처: &lt;a href="https://www.facebook.com/soventure/posts/10155749113883833"&gt;이재웅 페이스북&lt;/a&gt;)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;하지만, 이를 설명하는데 있어 맥락 없이 (나중에 수정하셨지만) &amp;#8216;오만하다&amp;#8217;고 표현하신 것은 적절하지 못했습니다. 뉴스로 쓰라고 미끼를 던진 것 뿐이고, &lt;a href="http://news.chosun.com/site/data/html_dir/2017/09/10/2017091002084.html"&gt;언론이 받아 쓸 수 밖에 없는 종류&lt;/a&gt;의 발언이었죠.&lt;/p&gt;
&lt;p&gt;그 뒤로 페북에 쓴 사견이 기사화 될 것을 몰랐다고 해명하시면서 &lt;a href="https://www.facebook.com/soventure/posts/10155754495243833"&gt;맥락을 다시 설명하셨는데&lt;/a&gt;, 제가 알기로 재웅님의 페북 발언 인용 기사가 나온 건 한두번도 아닙니다. 나는 이재웅님이 혁신 기업가로서 하신 일을 존경하고, 제 삶에서도 그 분의 경영자로서 하신 판단에 영향을 많이 받아 왔습니다. 하물며 내 개인에게도 그럴진데 우리 나라 2대 포털 사이트를 만든 창업자의 발언의 크기가 어느 정도인지 스스로 모르셔서 하는 이야기입니다.&lt;em&gt; (그때 마다 내 이야기가 기사화 될지 몰랐다고 이야기하실 건가요?)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;김상조 위원장 말처럼 이해진 의장이야 아예 입을 닫고 산다고 치고, 이재웅님처럼 그동안 사회적 발언을 해 오셨다면 그게 사견을 이야기하는 공간이라도 신중하게 이야기를 하고, 어떤 이야기를 할 때는 맥락을 충분히 설명해야 합니다.&lt;/p&gt;
&lt;p&gt;저도 페북에 한마디 한마디 할 때는 1만명 가까이 되는 팔로워 여러분들의 감수성을 고려하고 단어 하나 하나 고르고 바꾸고 퇴고합니다. (비공개로 이야기한 것도 공익이든 아니든 기사화하는 게 기자들입니다. 기자들을 믿어서는 안됩니다.)&lt;/p&gt;
&lt;p&gt;페이스북이나 트위터를 우리나라에서는 소셜 네트워크 서비스(SNS)라고 하지만, 해외에서는 소셜 미디어라고 합니다. 그만큼 공적 공간이죠. &lt;em&gt;(친구들끼리 잡담은 1:1로 메신저 하거나, 아예 사람을 지정해서 비공개로 하면 됩니다.)&lt;/em&gt; 아직까지 많은 어른들이 이러한 파급력을 모르거나, 알더라도 순진하게 생각하는 것 같습니다. 이제 언론만 미디어가 아닙니다. 나의 말이 어떻게 퍼질지 모르는 세상입니다. 어제도 어떤 시인 한분께서 예술적 감성으로 한 진담반 농담반의 특급 호텔의 숙박 요구에 대해 한바탕 홍역을 치르신것으로 압니다.&lt;/p&gt;
&lt;p&gt;어른들 뿐만 아니라 아이들도 소셜 미디어의 파급의 표적이 됩니다. 득이 될 수도 실이 되기도 하는 양날의 검이죠. 사람들과의 원활한 소통을 가능하게 도와주지만 그 반대의 영향을 가져올 수 있기 때문에 지혜롭게 사용할 줄 알아야 합니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=HVImOlxC3YM:tR7Q88MX_uo:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 10 Sep 2017 23:00:49 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1167#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>네이버 창업자의 대기업 동일인 지정에 대한 시각</title>
	<link>http://blog.creation.net/naver-kftc-issue</link>
	<description>&lt;p&gt;&amp;#8220;좋은 법은 선량한 사람에게는 해가 없다.&amp;#8221;&lt;/p&gt;
&lt;p&gt;일정 규모 이상의 기업 집단에 대한 순환 출자와 총수에 의한 사익 편취를 막기 위해 만든 대기업 규제가 네이버와 이해진 전 의장에게 얼마나 큰 피해를 주는 지 모르겠다. 오히려 기업의 사회적 책임을 더 한다는 측면에서, 공정위의 판단에 동의한다. (카카오나 넥슨의 경우, 대기업 지정에 따른 창업자에 대한 동일인 지정에 대해 이슈를 제기하지 않았다.)&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-full wp-image-2100" src="http://blog.creation.net/data/tisotry/2017/09/11132853/naver-kftc-issue.jpg" alt="" width="600" height="400" /&gt;&lt;/p&gt;
&lt;p&gt;또한, 전체적인 맥락에서 이 전 의장이 실효적인 의사 결정과 지배를 하고 있다는 공정위의 판단 역시 합리적이다. 단순히 현재 이 전 의장의 지분율이 낮고, 경영에 직접 관여하지 않고, 글로벌 시장에서 안 좋은 평판을 준다는 이유는 너무 아마추어적인 접근 같다.&lt;/p&gt;
&lt;div class="text_exposed_show"&gt;
&lt;p&gt;누가봐도 그가 지금까지 네이버을 실질적으로 지배해왔으며, 본인의 이사회 의장 사퇴와 경영진 변화가 대기업 집단 지정을 앞두고 전격적으로 이뤄졌다. 여전히 네이버에서 사내 이사로서, 글로벌 투자 담당으로서 영향력이 상당하고, 이번 동일인 선정 이슈와 관련해 (행정 소송 고려 등) 전사적으로 여론전을 펴는 것만 봐도 알 수 있다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;공정위는 &amp;#8220;이 전 의장이 최근 미래에셋대우와 맺은 자사주 교환도 총수 판단의 근거가 됐다. 공정위는 이 전 의장은 당시 교환으로 1.71%의 우호 지분을 확보했으며, 향후에도 10.9%에 달하는 잔여 자사주의 추가 활용 가능성을 배제할 수 없다고 판단했다.&amp;#8221;  &lt;a href="http://biz.khan.co.kr/khan_art_view.html?artid=201709031200001&amp;code=920100"&gt;[경향신문] 공정위 “네이버 총수는 이해진” 결론···‘재벌’로 규제 시작&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;일반인들은 잘 모르겠지만, 네이버는 상장 후 지금까지 거의 매년 2천억 이상의 자사주를 계속 사 들여왔다. 네이버 순이익이 연간 4천-5천억 정도임을 감안하면 거의 반이다. 그로 인해 네이버 전체 지분율에서 자사주 비율은 2008년 3%에 불과했으나, 2016년 말에는 12.6%가 되었다. 이해진 의장을 제외하면 가장 높은 비율이다. 물론 자사주 취득이 주주 가치를 높인다는 이유로 이뤄졌고 의결권을 가지고 있지는 않지만, 결과적으로 주가를 높여 적대적 M&amp;amp;A을 막고, 우호 지분 확보로 경영권을 방어할 수 있는 효과적인 수단이 되었다.&lt;/p&gt;
&lt;p&gt;국내 대표 IT기업이면서도, 글로벌과의 규모의 경쟁에서 힘들다고 늘 앓는 소리를 해 온 네이버가 순이익의 반을 직접 투자가 아닌 자사주 취득에 썼다는 점은 눈 여겨 볼 대목이다. 2007년 부터 10년간 자사주 매입에 쓴 돈만 1조 7천억이 넘는다.&lt;/p&gt;
&lt;p&gt;그럼에도 불구하고 나는 그동안 창업자 본인의 사업 철학과 비전을 흔들리지 않고 실현하고, 안정적인 경영권을 유지 하기 위해 네이버가 올바른 결정을 해왔다고 생각한다. 하지만, 그 결과로 인해 (공정위가) 책임을 질 위치에 있다고 판단했다면, 책임을 지는 것이 정도가 아닐까?&lt;/p&gt;
&lt;p&gt;하고 싶은 데로 다 하고, 막상 책임질 상황이 되면 발을 빼는 게 재벌 총수들이 하는 행태 아니던가. 오히려 모범을 보여 이런 규제가 네이버에게 정말 불필요 했구나라고 인식시키며, 변화를 이끌어내는 것이 업계 리더로서 올바른 자세일 것이다. 네이버에게 낡은 규제였음을 스스로 증명하면 된다.&lt;/p&gt;
&lt;/div&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=axpT2gF9STk:x1f137OM2II:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 04 Sep 2017 23:00:52 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>클라우드에 딱 맞는 MXNet의 5가지 딥러닝 학습 기능</title>
	<link>http://blog.creation.net/mxnet-deep-learning-features-aws-cloud</link>
	<description>&lt;p&gt;&lt;a href="https://aws.amazon.com/mxnet/" target="_blank" rel="noopener noreferrer"&gt;Apache MXNet&lt;/a&gt;  (인큐베이팅 프로젝트)는 최첨단 딥러닝(Deep Learning) 학습 모델 제작을 지원하는 확장 성이 뛰어난 오픈 소스 프레임 워크입니다. 이를 통해 CNN (Convolutional Neural Network), LSTM (Long Term Memory Network) 등을 만들 수 있고, Python, Scala, R 및 Julia를 포함한 다양한 언어를 지원합니다.&lt;/p&gt;
&lt;p&gt;이 글에서는 MXNet이 AWS 클라우드 개발자 친화적인 프레임워크로서 자리 매김하는 몇 가지 독특한 기능을 소개합니다.  Python에서 MXNet을 사용하여 신경망 코딩을 하는 분을 위한 한 장짜리 기능 요약집도 하단에 있으니, 많이 참고해 보시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#1  코드 몇 줄로 다중 GPU 학습 지원&lt;/strong&gt;&lt;br /&gt;
다중 GPU 기반 학습 실행 기능은 MXNet 아키텍처의 핵심 부분입니다. 모델을 학습시키려는 장치 목록을 전달하면 됩니다. 기본적으로 MXNet은 데이터 병렬 처리를 사용하여 여러 GPU에서 작업 부하를 분할합니다. 예를 들어, GPU가 3 개인 경우 각 모델은 전체 모델 사본을 받고 각 데이터 배치(Batch)의 1/3로 나눠 학습을 진행합니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;import mxnet as mx 
# Single GPU
module = mx.module.Module(context=mx.gpu(0))
# Train on multiple GPUs
module = mx.module.Module(context=[mx.gpu(i) for i in range(N)], ...)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img class="aligncenter" src="https://cdn-images-1.medium.com/max/1600/1*rfow_hmfd9AVCYiFyoXimA.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;MXNet은 다중 GPU 혹은 다중 서버 기반 학습에서 가장 뛰어난 효율을 보이고 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#2 다중 서버 기반 학습 가능&lt;/strong&gt;&lt;br /&gt;
MXNet은  다중 서버에서 여러 GPU에 대한 학습 또한 간소화하도록 설계한 분산형 딥러닝 학습 프레임 워크입니다. 서버 클러스터 전체에서 학습을 하려면, 모든 컴퓨터에 MXNet을 설치하고 SSH를 통해 서로 통신 할 수 있는지 확인한 다음 서버 IP가 포함 된 파일을 만들어야 합니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;$ cat hosts 
192.30.0.172 
192.30.0.171
$ python ../../tools/launch.py -n 2 --launcher ssh -H hosts python train_mnist.py --network lenet --kv-store dist_sync&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MXNet은 키-밸류 스토어를 사용하여 서버 간의 그라디언트와 파라미터를 동기화할 수 있습다. 이를 통해 분산 학습을 수행 할 수 있으며, 본 기능은 &lt;tt&gt;USE_DIST_KVSTORE = 1을&lt;/tt&gt; 사용하여 MXNet을 새로 컴파일하면 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#3 데이터는 Amazon S3!&lt;/strong&gt;&lt;br /&gt;
MXNet에서 데이터 반복자(iterators)는 Python iterator 객체와 비슷합니다. 단, 해당 레이블과 함께 &amp;#8220;n&amp;#8221;개의 학습 예제가 포함 된 &lt;a href="https://github.com/dmlc/mxnet/blob/master/python/mxnet/io.py" target="_blank" rel="noopener noreferrer"&gt;DataBatch&lt;/a&gt; 객체로 데이터 배치(batch)를 반환한다는 점이 다릅니다. MXNet에는 NDArray 및 CSV와 같은 공통 데이터 유형에 대해 미리 작성된 효율적인 데이터 반복자를 가지고 있습니다. 또한, HDFS와 같은 분산 파일 시스템에서 효율적인 I/O를 위해 바이너리 형식을 사용하기도 합니다. mx.io.DataIter 클래스를 확장하여 사용자 정의 데이터 반복기를 만들 수 있습니다. 이 기능을 구현하는 방법에 대한 자세한 내용은 &lt;a href="http://mxnet.io/tutorials/basic/data.html#custom-iterator" target="_blank" rel="noopener noreferrer"&gt;기본 튜토리얼&lt;/a&gt;을 참조하시면 됩니다 .&lt;/p&gt;
&lt;p&gt;특히, Amazon S3 (Amazon Simple Storage Service)는 대량의 데이터를 매우 저렴한 비용으로 저장해야 하는 고객에게 유용합니다. MXNet에서는 데이터를 디스크에 직접 다운로드 할 필요 없이 RecordIO, ImageRecordIO, CSV 또는 NDArray 형식의 Amazon S3에 저장된 데이터를 참조하는 반복자를 만들 수 있습니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;data_iter = mx.io.ImageRecordIter(     
     path_imgrec="s3://bucket-name/training-data/caltech_train.rec",
     data_shape=(3, 227, 227),
     batch_size=4,
     resize=256)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;# 4 신경망 시각화 기능 &lt;/strong&gt;&lt;br /&gt;
MXNet에서는 신경망 아키텍처를 시각화 할 수 있도록 &lt;a href="http://www.graphviz.org/" target="_blank" rel="noopener noreferrer"&gt;Graphviz&lt;/a&gt;와 통합되어 있습니다. 네트워크 시각화를 생성하려면, &lt;tt&gt;node_atters&lt;/tt&gt; 속성으로 정의한 대로 네트워크의 모양과 함께 네트워크의 마지막 레이어를 참조하는 심볼을 사용 합니다. 아래 예제는 &lt;a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener noreferrer"&gt;LeNet&lt;/a&gt; 표준 CNN 을 시각화하는 방법을 보여줍니다 .&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;mx.viz.plot_network(symbol=lenet, shape=shape)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_1.gif" target="_blank" rel="noopener noreferrer"&gt;&lt;img class="alignnone wp-image-1281 size-full" src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_1_thumb.gif" alt="" width="140" height="836" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;자세한 코드 및 구현 지침은이 &lt;a href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" target="_blank" rel="noopener noreferrer"&gt;자습서를&lt;/a&gt; 참조하십시오 .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#5 프로파일 러 지원&lt;/strong&gt;&lt;br /&gt;
MXNet에는 &lt;tt&gt;USE_PROFILER = 1&lt;/tt&gt; 플래그를 통해 사용 가능한 내장 프로파일러가 있습니다 . 이를 통해, 네트워크(심볼 수준) 실행 시간을 계층 별로 분류할 수 있습니다. 이 기능은 일반적인 프로파일링 도구인  &lt;em&gt;nvprof&lt;/em&gt;  및  &lt;em&gt;gprof을 &lt;/em&gt; 보완하며, 함수, 커널, 또는 학습 수준에서, 운영자 수준에서 처리할 수 있게 합니다. 환경 변수를 사용하여 전체 Python 프로그램에 대해 &lt;a href="http://mxnet.io/how_to/env_var.html#control-the-profiler" target="_blank" rel="noopener noreferrer"&gt;활성화&lt;/a&gt; 할 수 있습니다 . 또는 아래와 같이 프로그램의 하위 집합에 코드를 통합하여 활성화 할 수 있습니다.&lt;/p&gt;
&lt;pre class=" language-code"&gt;&lt;code class=" language-code"&gt;mx.profiler.profiler_set_config(mode='all', filename='output.json')     
mx.profiler.profiler_set_state('run')      
# Code to be profiled goes here...      
mx.profiler.profiler_set_state('stop')&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;프로파일링 출력을 Chrome과 같은 웹 브라우저에 로드하고 다음과 같이 브라우저의 추적 ( Chrome 브라우저에서 &lt;tt&gt;chrome://tracing)&lt;/tt&gt;으로 이동하여 프로필을 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1266 size-full" src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_2.gif" alt="" width="800" height="436" /&gt;&lt;/p&gt;
&lt;p&gt;위 스크린 샷은 프로파일링 도구를 사용하여 MXNet에 &lt;a href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" target="_blank" rel="noopener noreferrer"&gt;구현&lt;/a&gt; 된 원래의 LeNet 아키텍처로 MNIST 데이터 세트를 학습하는 프로파일을 보여줍니다 .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One more thing: MXNet CheetSheet&lt;/strong&gt;&lt;br /&gt;
이제 MXNet의 고유한 기능을 기반으로 신경망 학습을 시작하는데 아래 &lt;a href="https://s3.amazonaws.com/aws-bigdata-blog/artifacts/apache_mxnet/apache-mxnet-cheat.pdf" target="_blank" rel="noopener noreferrer"&gt;치트 시트&lt;/a&gt;가 도움이 될 것입니다. 여기에는 CNN, RNN/LSTM, 선형 회귀 및 로지스틱 회귀에 대한 몇 가지 일반적인 아키텍처가 포함되어 있습니다. 이를 사용하여, 데이터 반복자 및 Amazon S3 반복기를 작성하고 체크 포인트를 구현하며 모델 파일을 저장하는 방법에 대한 간단한 코드가 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://s3.amazonaws.com/aws-bigdata-blog/artifacts/apache_mxnet/apache-mxnet-cheat.pdf" target="_blank" rel="noopener noreferrer"&gt;&lt;img class="wp-image-1283 size-full" src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/28/mxnet_cheat_sheet_1_thumb-1.gif" alt="Apache MXNet 치트 시트" width="1000" height="829" border="1" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p class="wp-caption-text"&gt;&lt;em&gt;확대하려면 클릭하십시오.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;MXNet 커뮤니티는 &lt;a href="http://gluon.mxnet.io/" target="_blank" rel="noopener noreferrer"&gt;Gluon&lt;/a&gt; 이라는 동적인 사용하기 쉬운 명령형 인터페이스를 지원하기 시작했고, MXNet으로 딥러닝을 시작하려면 &lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api" target="_blank" rel="noopener noreferrer"&gt;튜토리얼을&lt;/a&gt; 참조하십시오 .&lt;/p&gt;
&lt;p&gt;&lt;em&gt;이 글은 Sunil Mallya이 쓴 &lt;a href="https://aws.amazon.com/ko/blogs/ai/exploiting-the-unique-features-of-the-apache-mxnet-deep-learning-framework-with-a-cheat-sheet/#more-1264"&gt;Exploiting the Unique Features of the Apache MXNet Deep Learning Framework with a Cheat Sheet&lt;/a&gt;의 한국어 번역입니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;필수 참고 자료 &lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://channy.creation.net/blog/all-about-mxnet"&gt;Apache MXNet의 모든 것 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://facebook.com/groups/mxnetkr/"&gt;MXNet 한국 사용자 모임&lt;/a&gt; (페이스북 그룹) 가입!&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=Fsivd57p9FY:mvCLHb0Au4g:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 10 Aug 2017 18:56:55 +0000</pubDate>
	<comments>http://blog.creation.net/mxnet-deep-learning-features-aws-cloud#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>클라우드 네이티브 컴퓨팅 – Adrian Cockcroft</title>
	<link>http://channy.creation.net/blog/1165</link>
	<description>&lt;p&gt;&lt;strong&gt;Written by Adrian Cockcroft &lt;a href="http://channy.creation.net/feed#Disclaimer"&gt;* Disclaimer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1600/1*nar6VC_mKyF-INRdV2ZAwQ.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;@adrianco의 사진 &amp;#8211; 무지개에 의한 굴절의 클라우드 네이티브 컴퓨팅.&lt;/small&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p id="dd05" class="graf graf--p graf-after--h3"&gt;Amazon Web Services는 &lt;a class="markup--anchor markup--p-anchor" href="https://www.cncf.io/announcement/2017/08/09/amazon-web-services-joins-cloud-native-computing-foundation-platinum-member/" target="_blank" rel="nofollow noopener"&gt;Cloud Native Computing Foundation에 가입했습니다.&lt;/a&gt;  저는 앞으로 AWS를 대표하여 CNCF 이사회 멤버로 참여하고, AWS 오픈 소스 팀의 &lt;a class="markup--anchor markup--p-anchor" href="https://twitter.com/arungupta" target="_blank" rel="nofollow noopener"&gt;Arun Gupta&lt;/a&gt; 와 함께 CNCF 프로젝트 및 워킹 그룹과 기술 협력을 진행할 예정입니다. 이러한 활동이 무엇을 의미하는지 잘 설명하기 위해, 저는 &amp;#8220;클라우드 네이티브&amp;#8221;가 의미하는 바를 설명해 드리고 싶습니다.&lt;/p&gt;
&lt;p id="331b" class="graf graf--p graf-after--p"&gt;저는 2009 년에 Netflix에서 일하고 있었습니다. 당시 엔지니어링 팀은 AWS로 마이그레이션  해야하는 새로운 애플리케이션 아키텍처 패턴을 파악하고 있었습니다.  팀 내에서는 eBay, Yahoo 및 Google에서 근무한 다양한 사람들이 클라우드에서 대규모 배포를 자동화하는 방법을 적용 중이었고, &lt;a class="markup--anchor markup--p-anchor" href="http://queue.acm.org/detail.cfm?id=1142065" target="_blank" rel="nofollow noopener"&gt;Werner Vogels&lt;/a&gt; 와 AWS로부터 새로운 아이디어를 배웠습니다 . 결과적으로 신규 아키텍처에는 클라우드 네이티브(Cloud Native)라는 기본 가정이 생겼습니다. 2010 년에 우리는 클라우드 마이그레이션에 관해 공개적으로 이야기하기 시작했고, 2012 년에는 &lt;a class="markup--anchor markup--p-anchor" href="http://netflix.github.io/" target="_blank" rel="nofollow noopener"&gt;NetflixOSS&lt;/a&gt; 로 &lt;a class="markup--anchor markup--p-anchor" href="http://netflix.github.io/" target="_blank" rel="nofollow noopener"&gt;통칭&lt;/a&gt; 되는 일련의 오픈 소스 프로젝트를 통해 전혀 새로운 플랫폼을 구축할 수 있었습니다.&lt;/p&gt;
&lt;p id="8865" class="graf graf--p graf-after--p"&gt;우리가 이러한 &amp;#8216;클라우드 네이티브&amp;#8217; 패턴을 모두 만들지는 않았지만, 함께 모아서 하나의 아키텍처로 모으고, 대규모로 구현하고, &lt;a class="markup--anchor markup--p-anchor" href="https://www.slideshare.net/adrianco/yowworkshop-131203193626phpapp01-1" target="_blank" rel="nofollow noopener"&gt;공개적으로 이야기&lt;/a&gt; 하고, 코드를 공유하는 공유하였습니다.&lt;/p&gt;
&lt;p id="0288" class="graf graf--p graf-after--p"&gt;클라우드 네이티브 아키텍처는 주문형(On-deman) 제공, 글로벌 배포, 탄력성 높은 클라우드 서비스를 최대한 활용합니다. 이를 통해 개발자 생산성, 비즈니스 민첩성, 확장성,  가용성, 활용도 및 비용 절감 효과를 크게 높일 수 있습니다.&lt;/p&gt;
&lt;p id="2409" class="graf graf--p graf-after--p"&gt;온-디멘드 방식은 사람들이 클라우드로 이동하는 가장 중요한 이유인 경우가 많지만, 기존 응용 프로그램 배포 시간을 단축 시키는 것은 아닙니다.  하지만, 클라우드 네이티브에서는 자원에 대한 임시(ephemeral ) 및 불변(immutable) 배포을 할 수 있습니다 . 자원 확보에 수주가 걸리는 구형 배포 모델에서는 불필요한 추가 용량을 미리 주문하고, 사용 후에도 돌려 주기를 꺼려하는 문제를 만들어냅니다. 대신,  클라우드 네이티브 패턴은 임시로 인스턴스를 받아오고, 콘테이너를 빌드하고, 필요한 만큼 많은 동일한 복제본을 배포하고, 사용 후에는 그냥 반납하고, 코드가 변경 될 때마다 새로운 이미지를 만들어 다시 반복할 수 있습니다. NetflixOSS는 맞춤형 Amazon Machine Images (AMI)를 만들어 이러한 개념을 적용하였습니다.&lt;/p&gt;
&lt;p id="3b1f" class="graf graf--p graf-after--p"&gt;기존에 물리적인 다수 데이터 센터에  애플리케이션을 배포하는 것은 상대적으로 드물고 구현하기 복잡합니다. 하지만, 클라우드 네이티브 아키텍처는 다중 가용 영역(Zone) 및 다중 리전(Region) 배포가 기본입니다. 이러한 배포 모델을 효과적으로 작업하려면, 개발자는 분산 시스템 개념을 잘 이해하고 있어야합니다.  그래서, 넷플릭스에서 개발자를 뽑을 때 &amp;#8220;&lt;a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="nofollow noopener"&gt;CAP 정리&lt;/a&gt;&amp;#8220;는 주요 인터뷰 질문 중에 하나였습니다.  기술의 엄청난 향상에도 불구하고, 빛의 속도는 일정하므로 네트워크 대기 시간, 특히 글로벌 다중 리전 대기 시간은 항상 제약이 될 것입니다.&lt;/p&gt;
&lt;p id="0e38" class="graf graf--p graf-after--p"&gt;클라우드 네이티브 아키텍처는 높은 확장성을 가집니다. 2010년 &lt;a class="markup--anchor markup--p-anchor" href="https://www.slideshare.net/adrianco/netflix-on-cloud-combined-slides-for-dev-and-ops" target="_blank" rel="nofollow noopener"&gt;Netflix의 AWS 사용에 대해 처음 발표&lt;/a&gt; 했을 때, 당시 수 천 개의 AWS 인스턴스에서 프론트 엔드 애플리케이션을 실행하면서 미국 내 약 1,600만 고객을 지원했습니다.  최근 Netflix는 AWS로 완벽하게 마이그레이션하였고, 현재 1 억 명이 넘는 글로벌 고객을 보유하고 있으며, &lt;a class="markup--anchor markup--p-anchor" href="https://aws.amazon.com/solutions/case-studies/netflix/" target="_blank" rel="nofollow noopener"&gt;100,000 개가 넘는 인스턴스를 실행&lt;/a&gt;하고 있습니다. 구현된 세부 사항은 몇 년에 걸쳐 변경되었지만, 근본적인 아키텍처 패턴은 동일합니다.&lt;/p&gt;
&lt;p id="10a9" class="graf graf--p graf-after--p"&gt;시간이 지남에 따라 클라우드 네이티브 아키텍처의 구성 요소는 실험적이고, 경쟁력 있는 구현을 통해 정의 된 외부 서비스로 이동하고 있습니다. 데이터베이스, 데이터 사이언스 파이프 라인, 콘테이너 스케줄러 및 모니터링 도구가 진화 하는 것을 목격했습니다.  이 부분에서 바로 CNCF가 이를 모아서 협력하는 역할을 합니다. CNCF &lt;a class="markup--anchor markup--p-anchor" href="https://github.com/cncf/toc" target="_blank" rel="nofollow noopener"&gt;기술 감독 위원회&lt;/a&gt;(&lt;a class="markup--anchor markup--p-anchor" href="https://github.com/cncf/toc" target="_blank" rel="nofollow noopener"&gt;Technical Oversight Committee)&lt;/a&gt;는 어느 클라우드 기반 프로젝트가 실험 단계에서 경쟁력 있는 단계로 넘어갈 때, 이를 검토하고, 인큐베이션 여부를 결정합니다. 너무 빠르게 움직여서 조금 혼란스러운 클라우드 기술 변화를 추적하려는 고객에게 CNCF가 승인하는 프로젝트 브랜드는 이를 판단하는데 조금이나마 도움을 줄 것입니다.  이들은 하나의 통합된 클라우드 네이티브 아키텍처가 아니라 느슨하게 연결된 모음으로서, &lt;span id="result_box" class="short_text" lang="ko" tabindex="-1"&gt;CNCF 회원이나 프로젝트 사용자에게 특정 프로젝트에 대한 편향적인 지지를 하지 않습니다.&lt;/span&gt;&lt;/p&gt;
&lt;p id="4625" class="graf graf--p graf-after--p"&gt;CNCF는 현재 10 개의 프로젝트를 진행하고 있으며 콘테이너 오케스트레이션을위한 Kubernetes, 모니터링을 위한 Prometheus, 애플리케이션 흐름 모니터링을 위한 Open Tracing, 로깅을 위한 Fluentd, 서비스 혼합를 위한 Linkerd, 원격 프로 시저 호출을 위한 gRPC, 서비스 디스커버리를 위한 CoreDNS, Containerd 콘테이너 런타임에 대한 Rkt, 콘테이너 네이티브 네트워킹에 대해서는 CNI 등이 있습니다.&lt;/p&gt;
&lt;p id="1b37" class="graf graf--p graf-after--p"&gt;AWS의 관점에서 여러 CNCF 프로젝트와 워킹 그룹에 관심을 가지고 있습니다. 특히, AWS는 &lt;a class="markup--anchor markup--p-anchor" href="http://containerd.io/" target="_blank" rel="nofollow noopener"&gt;Containerd&lt;/a&gt; 프로젝트 &lt;a class="markup--anchor markup--p-anchor" href="https://www.docker.com/docker-news-and-press/docker-extracts-and-donates-containerd-its-core-container-runtime-accelerate" target="_blank" rel="nofollow noopener"&gt;시작 멤버&lt;/a&gt; 였습니다. Containerd 커뮤니티를 통해 고객이 더 나은 경험을 할 수 있도록 어떻게 도와 줄 수 있는지 많은 아이디어를 가지고 있습니다. 또한, 곧 출시 될 Amazon ECS &lt;a class="markup--anchor markup--p-anchor" href="https://github.com/aaithal/amazon-ecs-agent/blob/cec1ece6d619aaafe4b485cb4b74e1aafa786428/proposals/eni.md" target="_blank" rel="nofollow noopener"&gt;Task Networking 기능&lt;/a&gt;은 &lt;a class="markup--anchor markup--p-anchor" href="https://github.com/containernetworking" target="_blank" rel="nofollow noopener"&gt;CNI&lt;/a&gt; 플러그인으로 작성되었으며, CNI가 AWS의 모든 콘테이너 기반 네트워킹의 기반이 될 것으로 기대합니다. 또한, 최근 &lt;a class="markup--anchor markup--p-anchor" href="https://www.cncf.io/blog/2017/06/28/survey-shows-kubernetes-leading-orchestration-platform/" target="_blank" rel="nofollow noopener"&gt;CNCF 조사&lt;/a&gt;에 따르면 응답자의 63 %가 Amazon EC2에서 Kubernetes를 호스팅 한다고 합니다. Arun Gupta는 AWS에서 Kubernetes 활용을 위한  &lt;a class="markup--anchor markup--p-anchor" href="http://kubernetes-aws.io/" target="_blank" rel="nofollow noopener"&gt;다양한 옵션&lt;/a&gt;과 &lt;a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/"&gt;Kops 기반 활용 방법&lt;/a&gt;에 대한 블로그를 쓰고 있습니다.&lt;/p&gt;
&lt;p id="61c5" class="graf graf--p graf-after--p"&gt;제가 맡고 있는 AWS의 오픈 소스팀의 사명은 오픈 소스 프로젝트, 커뮤니티 및 재단과 협력하고 AWS 엔지니어링에서 더 많은 오픈 소스 공헌을 유도하고 장려하는 데 도움을 주는 것입니다. AWS는 이미 CNCF를 운영하고 있는 The Linux Foundation의 멤버이며, 현대적인 분산 시스템에 최적화된 &lt;a class="markup--anchor markup--p-anchor" href="https://www.cncf.io/about/charter/" target="_blank" rel="nofollow noopener"&gt;새로운 컴퓨팅 패러다임&lt;/a&gt;을 발명하고 추진하기 위한 업계의 동료들과 함께 일하기를 기대 합니다.&lt;/p&gt;
&lt;p id="2757" class="graf graf--p graf-after--p graf--trailing"&gt;AWS에서 오픈 소스 활동에 대한 최신 정보를 얻으시려면, &lt;a class="markup--anchor markup--p-anchor" href="http://twitter.com/awsopen" target="_blank" rel="nofollow noopener"&gt;@AWSOpen&lt;/a&gt;을 팔로우 해 주시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;br /&gt;
&lt;em id="Disclaimer"&gt;본 글은 AWS의 클라우드 전략 담당 부사장인 Adrian Cockcroft의 허락하에 &lt;a href="https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf"&gt;Cloud Native Computing&lt;/a&gt;의 한국어 편집본입니다. 본 글은 Adrian Cockcroft의 개인적인 의견이며, 본 블로그의 의견이 아닙니다. 한국어 번역 기사인 본 글에 대한 인용 및 사용 허가는 오직 Adrian Cockcroft의 영문 원글에 효력이 있습니다. 본 글을 인용하실 때에는 이러한 주의 사항을 반드시 추가하시기 바랍니다. (Disclamar: This article is a Korean translation of &lt;a href="https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf"&gt;Cloud Native Computing&lt;/a&gt; written by Adrian Cockcroft. It is not opinion of this blog and this translation may contain some errors or incorrect expressions. Please only refer to the original article that was published in English for accuracy.)&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class="instagram-media"&gt;
&lt;div&gt;
&lt;div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://www.instagram.com/p/BJpl3SYjxSi/" target="_blank" rel="noopener"&gt;Rainbow over the cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Channy Yun(@channyun)님의 공유 게시물님, 2016 8월 28 오전 3:58 PDT&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=U_gS0ZtzK3M:RfNiGDJc05s:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 09 Aug 2017 16:34:35 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1165#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>영화 “택시운전사”에 나오지 않은 네가지 이야기</title>
	<link>http://channy.creation.net/blog/1159</link>
	<description>&lt;p&gt;주말에 영화 &amp;#8220;택시운전사&amp;#8221;를 보신 분들 많으시죠?&lt;/p&gt;
&lt;p&gt;우리 나라의 근대사의 가장 아픈 역사 중 하나인 5.18 민주화 운동에 대해 직접 잠입 취재를 한 외국인 기자  위르겐 힌츠페터와 그와 함께 운전을 해서 광주를 다녀온 기사이자 통역인 김사복씨에 대한 실화를 기반으로 만들어졌습니다.&lt;/p&gt;
&lt;p&gt;영화는 실화를 각색한 것으로 광주 밖의 일반 시민의 입장에서 참상을 겪으면서, 직접 현장에 뛰어드는 택시 운전사에 초점을 맞추고 있지만, 더욱 주목할 만한 사실은 외국인으로 비밀 잠입해서 취재를 한 힌츠페터에 있습니다. 이 글에서는 영화에서 나오지 않는 몇 가지 역사적 사실을 살펴 볼께요. (스포일러는 없음)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 광주로 향한 사람은 힌츠페터 뿐이 아니었다.&lt;/strong&gt;&lt;br /&gt;
독일 공영방송 ARD의 기자인 힌츠페터는 광주를 취재하기 위해 19일 나리타에서 서울로 들어왔다. 소개 받은 기사 김사복 외에도 녹음 기사 헤닝 루모어도 함께 있었다. 20일 그들은 광주까지 텅빈 고속도로를 달려 와서 검문소에서 군인들에게 길이 엇갈린 동료를 찾으러 광주로 들어가야 한다고 했다. 시위대에 희생당할 수 있으므로 구출해야 한다고 이야기를 만들었다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1162 aligncenter" src="http://channy.creation.net/data/channy/2017/08/06204907/tax-driver-3.jpg" alt="" width="80%" /&gt;&lt;br /&gt;
는 당시 광주에 있었던 외국인은 국제사면위원회 소속의 두 외국인 젊은이와 인터뷰를 하고, 그들이 목격한 끔찍한 일에 대해 알게 되었다. 군인들이 폭력을 쓰고 학대하면서 시위대를 진압했다는 것으로 상상을 초월하는 규모의 잔혹함이 처음 며칠 동안 일어났다고 전했다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 광주에서 나올 때 검문은 삼엄하진 않았다.&lt;/strong&gt;&lt;br /&gt;
21일 힌츠페터는 취재 자료를 외부에 공개하기 위해 광주에서 나오면서 두 번 검문 받았는데, 촬영 필름을 차 안에 숨겨두었고 대개 군인은 그 안에 총기가 있는지 살폈지, 촬영 자료를 발견하지 못했다. 서울에 도착한, 힌츠페터 일행은 필름 10개를 두 개로 나눠 5개는 허리춤에 넣고, 5개는 쿠키 상자에 넣는 방법으로 나누어 압수 위험에 대비했다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1161 aligncenter" src="http://channy.creation.net/data/channy/2017/08/06204906/tax-driver-4.jpg" alt="" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;그는 일본에 도착 한 후, 바로 필름을 독일로 보냈다. 독일 제1공영방송은 참상을 담은 그의 컬러 필름을 21일 TV 뉴스로 내보냈고, 많은 외신들이 이 기사를 통해 광주의 참상을 알게되었다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. 힌츠페터는 출국 후 곧장 한국으로 돌아왔다!&lt;/strong&gt;&lt;br /&gt;
힌츠페터는 일본에 3시간만 머물고 다시 한국으로 귀국했다. 귀국 후 그는 김영삼 자택앞에서 취재를 했으나 거부당하고, 23일 광주로 다시 들어갔다. 이미 그때는 계엄군과 시민군 사이의 총격전으로 많은 사람이 죽고 다친 상태였다.&lt;/p&gt;
&lt;p&gt;그의 두번째 취재 영상은 생각보다 평온했던 시민들의 일상을 담았고, 당시 계엄군 측이 언론에 흘린 &amp;#8216;폭도가 점령해 아비규환이 된 시내 상황&amp;#8217;같은 주장을 정면으로 반박하는 확실한 증거가 되었다.  그의 취재와 영상 자료가 없었다면 위 주장대로 사실이 날조되었을 가능성이 컸다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. 그의 필름은 23년 만에 한국에 공식 공개됐다!&lt;/strong&gt;&lt;br /&gt;
힌츠페터가 광주에서 찍은 영상은 1980년 9월 독일에서 《기로에 선 한국》이라는 다큐멘터리로 제작되었고, 독일에서 유학중이던 한국인 신부들이 들여와 국내 각처에서 비밀 상영되었다. 23년만인 2003년 5월 18일 KBS 1TV 「일요스페셜」&amp;#8217;80년 5월, 푸른 눈의 목격자&amp;#8217;편에서 처음 공개됐다.&lt;/p&gt;
&lt;p&gt;아래는 전편 영상!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/center&gt;1986년 11월에는 광화문 네거리에서 시위를 취재하던 도중 사복 경찰에게 집단 구타를 당해 목뼈와 척추가 부러지는 중상을 입고 귀국한 그는 한때 생명이 위태로울 때 국립 5.18 묘역에 묻히고 싶다는 의지를 피력했다. 2005년 송건호 언론상을 수상했으며, 2016년 1월 25일 세상을 떠난 후, 생전에 남긴 모발과 손톱 등이 그해 5.18 기념식에서 구묘역 입구에 안치되었다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://newsimg.sedaily.com/2017/08/02/1OJLELFKR2_1.jpg" alt="" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;역시 역사는 행동하는 사람에 의해 바뀐다는 점!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;더 읽어볼 글&lt;/strong&gt;&lt;br /&gt;
&amp;#8211; &lt;a href="https://ko.wikipedia.org/wiki/%EC%9C%84%EB%A5%B4%EA%B2%90_%ED%9E%8C%EC%B8%A0%ED%8E%98%ED%84%B0"&gt;위르겐 힌츠페터 위키 백과&lt;/a&gt;&lt;br /&gt;
&amp;#8211; &lt;a href="https://namu.wiki/w/%EC%9C%84%EB%A5%B4%EA%B2%90%20%ED%9E%8C%EC%B8%A0%ED%8E%98%ED%84%B0"&gt;위르겐 힌츠페터 나무위키&lt;/a&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=xaXuvtZzSFM:kf5jKtjgUEI:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sun, 06 Aug 2017 15:00:07 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1159#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>시애틀에 있는 세 가지 아마존(Amazon) 명소</title>
	<link>http://channy.creation.net/blog/1157</link>
	<description>&lt;p&gt;시애틀(Seattle)하면 생각 나는 것이 많죠! &amp;#8216;시애틀의 잘 못 이루는 밤, 스페이스니들, 파이크 수산 시장, 스타벅스 1호점&amp;#8217; 등등&amp;#8230; 그 중에서도 시애틀을 대표하는 기업 중 하나인 아마존(Amazon)이 있습니다. 재미있는 건 오피스가 거의 다운 타운에 있는데, 최근에 관광객들에게도 인상적인 몇 가지 볼거리가 늘어나기 시작했습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-full wp-image-1158 aligncenter" src="http://channy.creation.net/data/channy/2017/08/04082113/seattle-amazon.jpg" alt="" width="90%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.amazon.com/b?node=16008589011"&gt;Amazon Go &lt;/a&gt;스토어 &amp;#8211; &lt;small&gt;2131 7th Ave, Seattle&lt;/small&gt;&lt;/strong&gt;&lt;br /&gt;
컴퓨터 비전, 딥러닝, 센서 융합 같은 기술을 통해 만든 판매원이 없는 매장입니다. 현재 직원들을 대상으로 베타 테스트를 하고 있는데, 들어가 봤는데 실제로 잘 동작하더라구요. 과일팩 하나 사 가지고 나왔습니다.  아직 들어가지는 못하지만, 밖에서 사진 찍는 건 가능합니다.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/b?node=16008589011"&gt;홈페이지&lt;/a&gt;에서 언제 일반인에게 오픈 하는지 알림 메일을 받으실 수도 있습니다. 아마 여러분이 오실 때 쯤 되면 일반 공개 되면 좋겠네요.&lt;/p&gt;
&lt;p&gt;Update. 2018년 1월 22일 부터 시애틀 상점에 대한 일반인 서비스가 시작되었습니다!&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;편리한 오프라인 고객 경험을 만들어낸 Amazon Go의 핵심인 Just Walk Out 기술에 대해서는 아마존이 상세하게 공개하지 않겠지만, 몇몇 관련 기사에서 약간의 힌트를 얻을 수 있다. &lt;a href="https://www.recode.net/2016/12/5/13842892/amazon-go-grocery-store-no-lines-cashier-paying"&gt;recode는&lt;/a&gt; 2016년 기사에서 아마존의 특허를 인용하며 센서를 통해 수집된 고객 데이터를 바탕으로 AI와 컴퓨터 비전 기술을 통해 매장 고객이 어떤 물건을 집어 들었는지 파악한다고 한다. &lt;a href="https://www.geekwire.com/2016/amazon-go-works-technology-behind-online-retailers-groundbreaking-new-grocery-store/"&gt;GeekWire는&lt;/a&gt; Amazon Go에 RFID 기술은 적용되지 않았음을 확인했으며, &lt;a href="https://www.digitalpulse.pwc.com.au/amazon-go-strategy-retail-grocery/?utm_source=pwcchair&amp;utm_campaign=shared-content&amp;utm_medium=referral"&gt;PwC도&lt;/a&gt; 블루투스 비콘 등 Amazon Go에 적용된 기술을 분석하는 아티클을 공개했다. &lt;a href="https://techcrunch.com/2018/01/21/inside-amazons-surveillance-powered-no-checkout-convenience-store/"&gt;TechCrunch의 최신 기사에 따르면&lt;/a&gt; Amazon Go 천장에는 수많은 RGB 카메라가 달려 있어 고객과 상품의 움직임을 추적하며, 선반에도 저울이 달려 있어 무게 변화를 미세하게 측정한다고 한다. 무인 결제 시스템을 악용한 절도를 걱정했던 &lt;a href="https://www.nytimes.com/2018/01/21/technology/inside-amazon-go-a-store-of-the-future.html"&gt;뉴욕타임즈 기자는&lt;/a&gt; (미리 아마존의 허락을 맡고) 자신이 직접 바닐라 탄산수를 숨겨서 몰래 나왔지만 결제가 됐다는 에피소드를 전하기도 했다.  &amp;#8211; 출처: &lt;a href="http://techneedle.com/archives/33910"&gt;http://techneedle.com/archives/33910&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Amazon Sphere  &lt;small&gt;2117 7th Ave, Seattle, WA&lt;/small&gt;&lt;/strong&gt;&lt;br /&gt;
Amazon Go 바로 옆에는 Spheres라는 3개의 대형 유리 원형 조형물이 만들어지고 있습니다. 직원, 시민, 관광객들이 도심에서 자연을 느낄 수 있도록 거대 열대 우림을 조성하는 프로젝트입니다.&lt;/p&gt;
&lt;p&gt;이를 위해 시애틀 근교 온실에서 2015년 부터 4만종의 다양한 식물을 키우고 있습니다. 얼마 전부터 식물을 옮기는 작업을 하고 있고, 2018년에 오픈되면 시애틀의 새로운 명소가 될듯합니다. &lt;a href="https://www.youtube.com/channel/UCzE5rz2KHTFYAkmMksUpPLA/videos"&gt;Amazon News 유튜브 채널&lt;/a&gt;을 보시면 관련 동영상들이 계속 업데이트 되고 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;스페어 바로 옆 건물은 아마존 미팅 센터가 있습니다. 이 건물은 일반인들도 쉴 수 있는 공간을 제공하구요. 재미있는 건 매일 바나나를 가져 갈 수 있도록 놔 둔다는 점인데요. 누구나 하나씩 가져갈 수 있어요.^^&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.amazon.com/b?node=13270229011"&gt;Amazon Book Store&lt;/a&gt; &amp;#8211; &lt;small&gt;4601 26th Ave NE, Seattle&lt;/small&gt;&lt;/strong&gt;&lt;br /&gt;
이미 2015년에 오픈해서 운영 중인 오프라인 서점입니다. 시내에서 북쪽으로 워싱턴주립대(UW) 근처에 유니버시티 빌리지라는 곳에 있습니다. UW 캠퍼스가 이쁘니까 구경하러 갔다가 가면 좋습니다. 주로 아마존에서 별점 높은 책들이 전시되어 있고, 킨들 같은 디지털 기기들이 전시되어 있는 곳입니다. 이미 많은 분들이 다녀갔고, &lt;a href="https://estimastory.com/2016/03/12/amazonbooks/"&gt;임정욱님&lt;/a&gt;, &lt;a href="https://brunch.co.kr/@lifidea/8"&gt;김진영님&lt;/a&gt; 등 다양한 후기가 있으니 한번 읽어 보시길&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;아마존 북 스토어는 시애틀 뿐만 아니라 뉴욕, 샌디에고, 보스톤 근교 등 &lt;a href="https://www.amazon.com/b?node=13270229011"&gt;8개&lt;/a&gt;가 있는데, 캘리포니아를 중심으로 더 늘어날 예정입니다.&lt;/p&gt;
&lt;p&gt;시애틀을 방문해 보신다면, 한번 방문해 보신다면 어떨까요?&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=FefYCYLyQ9M:q-VBEliGlLU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 03 Aug 2017 23:44:26 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1157#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 기반 추천 오픈 소스 딥러닝 프로젝트 모음</title>
	<link>http://blog.creation.net/apache-mxnet-deep-learning-project</link>
	<description>&lt;p&gt;&lt;a href="http://mxnet.io/"&gt;Apache MXNet&lt;/a&gt; 은 일반 개발자가 손쉽게 딥러닝(Deep Learning) 모델을 구축, 학습 및 실행하는 데 도움을 주는 오픈 소스 라이브러리입니다. &lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;이전 시리즈&lt;/a&gt;에서 MXNet API 및 주요 기능, 활용 방법에 대해 소개했습니다.&lt;/p&gt;
&lt;p&gt;이 글에서는 MXNet을 다양한 유스 케이스에 적용하는 특징적인 오픈 소스 프로젝트를 소개합니다. (참고로 &lt;a href="http://mxnet.io/model_zoo/"&gt;MXNet Model Zoo&lt;/a&gt;에는 다양한 주요 딥러닝 학습 모델 사례가 있으니, 먼저 살펴 보시기 바랍니다!)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#1 — 이미지 객체 인식&lt;/strong&gt;&lt;br /&gt;
이 프로젝트는 하나의 이미지에서 여러개의 객체를 탐지하는  것으로 &lt;a href="https://github.com/zhreshold/mxnet-ssd"&gt;mxnet-ssd&lt;/a&gt; (&lt;a href="https://arxiv.org/abs/1512.02325"&gt;논문 링크&lt;/a&gt;)라는 프로젝트를 개량한 것으로 MXNet의 특징이라고 할 수 있는, 멀티 GPU에서 성능을 향상 시킨 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*69-LDvP5UV3z1kaG9XgWQQ.png" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/precedenceguo/mx-rcnn"&gt;&lt;strong&gt;precedenceguo/mx-rcnn&lt;/strong&gt;&lt;br /&gt;
&lt;/a&gt;&lt;em&gt;mx-rcnn &amp;#8211; Faster R-CNN, an MXNet implementation with distributed implementation and data parallelization&lt;br /&gt;
&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 아래 연구 결과를 기반으로 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ross Girshick: &lt;a href="https://arxiv.org/abs/1504.08083"&gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun: &lt;a href="https://arxiv.org/abs/1506.01497"&gt;Faster R-CNN: Towards real-time object detection with region proposal networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;#2 — 스마트폰용 이미지 분석 프로젝트&lt;/strong&gt;&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*oc9YknlZp6rWnf7DCXL9tQ.gif" alt="" width="200" align="right" hspace="10" /&gt; &lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 입문 마지막 가이드&lt;/a&gt;에서 살펴본 대로,  &lt;a href="https://arxiv.org/abs/1602.07261"&gt;Inception v3&lt;/a&gt; 을 사용하면, 모바일 기기에서도 실시간으로 이미지 분석이 가능합니다.&lt;/p&gt;
&lt;p&gt;아래 프로젝트는 안드로이드 및 iOS에ㅓ 사용할 수 있는 이미지 인식 프로젝트입니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dneprDroid/ImageRecognizer-iOS"&gt;&lt;strong&gt;dneprDroid/ImageRecognizer-iOS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dneprDroid/ImageRecognizer-Android"&gt;&lt;strong&gt;dneprDroid/ImageRecognizer-Android&lt;/strong&gt;&lt;/a&gt;&lt;em&gt;&lt;br /&gt;
&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#3 — 얼굴 인식 및 안면 감지 기능&lt;/strong&gt;&lt;br /&gt;
이 프로젝트는 &lt;a href="https://aws.amazon.com/fr/rekognition/"&gt;Amazon Rekognition&lt;/a&gt;의 얼굴 인식과 유사한 기능을 제공합니다. 좀 더 자세한 구현을 하고 싶은 경우, 좋은 출발점이 될 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image aligncenter" src="https://cdn-images-1.medium.com/max/1455/1*vvCWxc4Z1IOzrT_-u2IATA.png" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tornadomeet/mxnet-face"&gt;&lt;strong&gt;tornadomeet/mxnet-face&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 아래 연구 결과를 기반으로 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wu X, He R, Sun Z. &lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Lightened CNN for Deep Face Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rudd E, Günther M, Boult T. MOON: &lt;a href="https://arxiv.org/abs/1603.07027"&gt;A Mixed Objective Optimization Network for the Recognition of Facial Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jiang H, Learned-Miller E. &lt;a href="https://arxiv.org/abs/1606.03473"&gt;Face detection with the faster R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;#4— 자동차 번호판 인식하기 &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 81 % 정확도로 MacBook Pro에서 초당 9 매의 번호판 인식을 수행할 수 있습니다. 약간의 노력을 더 한다면 다른 문자 인식 사용 사례에 적용할 수 있습니다 &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f642.png" alt="&#x1f642;" class="wp-smiley" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/szad670401/end-to-end-for-chinese-plate-recognition"&gt;&lt;strong&gt;szad670401/end-to-end-for-chinese-plate-recognition&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#5 — Sockeye : 기계 번역 프로젝트&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sockeye 프로젝트는 MXNet에 기반한 신경망 기계 번역(Neural Machine Translation)을 위한 시퀀스-시퀀스(sequence-to-sequence) 프레임 워크입니다.  AWS에서 개발하고 있으며, 더 자세한 것은 &lt;a href="https://aws.amazon.com/ko/blogs/korea/train-neural-machine-translation-models-with-sockeye/"&gt;MXNet 기반 Sockeye를 통한 기계 번역 학습 해보기&lt;/a&gt;를 참고하시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/awslabs/sockeye"&gt;&lt;strong&gt;awslabs/sockeye&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;sockeye &amp;#8211; Sequence-to-sequence framework with a focus on Neural Machine Translation based on MXNet&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AWS  기반 배포 방법&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;지금까지 다른 Python 애플리케이션과 마찬가지로 Amazon EC2 인스턴스에서 MXNet 코드를 실행했습니다. AWS에서 애플리케이션을 실행할 수 있는 대체 방법(콘테이너 및 서버리스)이 있으면, 이는 MXNet에 적용할 수 있겠죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;#6— Amazon ECS와 코드 도구를 통한 MXNet API 지속적 배포 방식&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;이 프로젝트는 &lt;a href="https://aws.amazon.com/fr/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; 템플릿을 사용하여 MXNet 모델 또는 애플리케이션 코드의 변경 사항을 파이프 라인을 통해 배포, 구성 및 조율하는 자동화 된 워크 플로우를 생성할 수 있습니다. &lt;a href="https://aws.amazon.com/codepipeline/"&gt;CodePipeline&lt;/a&gt;과 &lt;a href="https://aws.amazon.com/codebuild/"&gt;CodeBuild&lt;/a&gt;를 사용하여 지속적 전달(CD) 방식이 가능하고,  몇 분 만에 사용자가 사용할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/awslabs/ecs-mxnet-example"&gt;&lt;strong&gt;awslabs/ecs-mxnet-example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/fr/blogs/ai/deploy-deep-learning-models-on-amazon-ecs/"&gt;Deploy Deep Learning Models on Amazon ECS | AWS AI Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;#7 —  MXNet Lambda 함수로 배포 하기&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AWS Lambda를 통해 MXNet을 사용해 미리 학습된 모델을 통해 이미지 인식 등을 해 볼 수 있는 프로젝트입니다. Serverless Application Model (SAM) 템플릿을 통해 서버리스 API 엔드포인트도 자동으로 구현합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/awslabs/mxnet-lambda"&gt;&lt;strong&gt;awslabs/mxnet-lambda&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/fr/blogs/compute/seamlessly-scale-predictions-with-aws-lambda-and-mxnet/"&gt;Seamlessly Scale Predictions with AWS Lambda and MXNet | AWS Compute Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;지금까지 다양한 MXNet 기반의 추천 오픈 소스 프로젝트를 살펴 보았습니다. 혹시 더 추천해 주실만한 프로젝트가 있으면 알려주세요!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=xI70O930_EE:tsPOh-lNd5M:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 02 Aug 2017 17:21:48 +0000</pubDate>
	<comments>http://blog.creation.net/apache-mxnet-deep-learning-project#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>Apache MXNet에 대한 모든 것!</title>
	<link>http://channy.creation.net/blog/1155</link>
	<description>&lt;p&gt;아마존의 CTO인 Werner Vogels 박사는 &lt;a href="http://www.allthingsdistributed.com/2016/11/mxnet-default-framework-deep-learning-aws.html"&gt;MXNet – Deep Learning Framework of Choice at AWS&lt;/a&gt;라는 글에서 확장 능력, 개발 속도, 이동성 등의 다양한 요인을 비추어 볼 때, MXNet이 가장 좋은 인공 지능 애플리케이션 개발을 위한 딥러닝 프레임웍이라고 판단하고, 이를 기반한 딥러닝 서비스 개발 지원 및 오픈 소스 지원에 대한 의지를 피력한 바 있습니다.&lt;/p&gt;
&lt;p&gt;이 글은 다양한 오픈 소스 딥러닝 프레임웍 중에 아마존이 선택한 Apache MXNet에 관한 다양한 한국어 자료들을 모아서 제공하는 것을 목적으로 합니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://blog.creation.net/data/tisotry/2017/07/31141306/apache-mxnet.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Apache MxNet은 개발자들에게 친숙한 심볼릭(Symbolic)과 명령형(imperative) 프로그래밍의 혼합 방식을 지원할 뿐만 아니라 CPU와 GPU 연산을 지원하고, 특히 GPU 클러스터에 최적화된 엔진을 사용해서 성능이 뛰어납니다.&lt;/p&gt;
&lt;p&gt;또한, 실무적으로 많이 사용하는 Python, C++, R, Scala, Julia, Matlab, and JavaScript을 지원하고, 모바일 기기 부터 서버까지 다양한 디바이스를 지원하여 산업계에서 응용하기에 매우 적합한 딥러닝 프레임워크입니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apache MXNet 입문 가이드&lt;/strong&gt;&lt;br /&gt;
이 시리즈는 AWS 테크 에반젤리스트인 Julien Simon이 연재한 &lt;a href="https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab?source=user_profile---------17-----------"&gt;MXNet 관련 글 모음&lt;/a&gt;의 번역 편집본으로 최근 각광 받고 있는 Deep Learning 라이브러리인 &lt;a href="http://mxnet.io/"&gt;Apache MXnet&lt;/a&gt;을 개괄적으로 설명하려고 합니다.&lt;/p&gt;
&lt;p&gt;이 글은 간단한 코드를 이해하는 개발자라면 기계 학습과 인공 지능을 잘 알지 못하는 분이라도 쉽게 따라올 수 있도록 했습니다. 너무 겁먹지 않으셔도 됩니다.&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;동영상&lt;/strong&gt;&lt;br /&gt;
&lt;a href="http://hunkim.github.io/ml/"&gt;모두를 위한 딥러닝&lt;/a&gt; 강의로 유명한 홍콩과기대 김성훈 교수와 MXNet 코드 개발자인 Xingjian Shi가 함께 MXNet의 장점과 함께 간단한 딥러닝 학습 문제를 데모로 보여 드립니다. (&lt;a href="https://www.slideshare.net/awskorea/2-mx-net"&gt;슬라이드&lt;/a&gt;)  모두를 위한 딥러닝을 청취하신 분들이라면, Lab 강의에 대한 &lt;a href="https://github.com/hunkim/DeepLearningZeroToAll/tree/master/mxnet"&gt;MXNet 소스 코드&lt;/a&gt;를 참고하셔도 됩니다!&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Apache MXNet에 대해 간단하게 소개하고, AWS에서 Deep Learning AMI을 이용하여 Amazon EC2 인스턴스에서 MXNet을 구동하고, 테스트하는 방법을 살펴 봅니다. 또한, 분산 딥러닝 클러스터 생성 템플릿으로 멀티 GPU에서 구동하는 방법도 살펴 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;사용자 모임&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apache MXNet에 대한 관심이 늘어나고 있고, 배우려는 분들과 질문/답변을 할 수 있도록 페이스북에 그룹을 만들었습니다. 관심 있는 분들 참여해 주시길&amp;#8230;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/groups/mxnetkr/"&gt;https://www.facebook.com/groups/mxnetkr/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;더 자세한  정보&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/mxnet"&gt;AWS &amp;#8211; Apache MXNet 소개 페이지&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/blogs/korea/aws-deep-learning-framework-mxnet/"&gt;AWS로 딥 러닝을 위한 프레임워크 MxNet 활용하기 | AWS 블로그&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/blogs/korea/excited-about-mxnet-joining-apache/" rel="bookmark"&gt;MXNet, Apache 재단 오픈 소스 프로젝트 참여 | AWS 블로그&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/ko/blogs/korea/train-neural-machine-translation-models-with-sockeye/" rel="bookmark"&gt;MXNet 기반 Sockeye를 통한 기계 번역 학습 해보기 | AWS 블로그&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.popit.kr/mxnet%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%EB%A5%98-%EC%95%B1-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0/"&gt;MXNet을 활용한 이미지 분류 앱 개발하기 | Popit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;더 자세한 것은 AWS 블로그 &lt;a href="https://aws.amazon.com/ko/blogs/korea/category/mxnet/"&gt;MXNet 카테고리&lt;/a&gt;를 참고하셔도 됩니다.&lt;/p&gt;
&lt;p&gt;앞으로 이 글에는 &lt;a href="https://chatbotslife.com/training-mxnet-part-1-mnist-6f0dc4210c62"&gt;MXNet 기반 모델 학습 시리즈&lt;/a&gt; 한국어 번역 및 &lt;a href="https://aws.amazon.com/ko/blogs/ai/"&gt;Amazon AI 블로그&lt;/a&gt;의 MXNet 관련 글 모음 등 다양한 정보를 소개할 예정입니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WyM00RfUUZU:FsM0kCPx1pU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 31 Jul 2017 23:04:31 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1155#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기</title>
	<link>http://blog.creation.net/mxnet-part-6-realtime-object-detection</link>
	<description>&lt;p&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;5편&lt;/a&gt;에서는 이미지의 객체 검출을 위해 유명한 세 가지 학습 모델을 사용하여, 간단한 MXNet 소스 코드를 사용하여 몇 가지 이미지를 테스트해보았습니다.&lt;/p&gt;
&lt;p&gt;우리가 배운 것 중 하나는 각 모델이 서로 다른 메모리 요구 사항이 있다는 것입니다. 가장 메모리를 적게 먹는 것은 Inception v3으로 &amp;#8220;43MB&amp;#8221;만 사용입니다. 정말 이게 잘 될까? 예를 들어, &amp;#8220;라스베리파이(Raspberry Pi)&amp;#8221; 같은 작은 디바이스에서 실행할 수 있을까라는 질문을 해볼 수 있을 텐데요. 이번 글에서 함께 알아 보시죠!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mxnet.io/get_started/raspbian_setup.html"&gt;공식 가이드&lt;/a&gt;가 있지만, 몇 가지 단계가 빠진 것을 발견해서 아래 내용을 따라가면 최신 Raspbian을 실행하는 Raspberry Pi 3에서 제대로 작동합니다.&lt;/p&gt;
&lt;pre&gt;$ uname -a
 Linux raspberrypi 4.4.50-v7+ #970 SMP Mon Feb 20 19:18:29 GMT 2017 armv7l GNU/Linux&lt;/pre&gt;
&lt;p&gt;맨 먼저 필요한 라이브러리를 추가합니다.&lt;/p&gt;
&lt;pre&gt;$ sudo apt-get update
$ sudo apt-get -y install git cmake build-essential g++-4.8 c++-4.8 liblapack* libblas* libopencv* python-opencv libssl-dev screen&lt;/pre&gt;
&lt;p&gt;그런 다음 MXNet 저장소를 복제하고, 최신 버전을 가져옵니다. 마지막 단계를 놓치지 마세요.&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/dmlc/mxnet.git --recursive
$ cd mxnet
# List tags: v0.9.3a is the latest at the time of writing
$ git tag -l
$ git checkout tags/v0.9.3a&lt;/pre&gt;
&lt;p&gt;MXNet은 Amazon S3에서 데이터를 로드하고 저장할 수 있으므로 나중에 이 기능을 사용하면 편리 할 것입니다. MXNet은 또한 HDFS를 지원하지만 Hadoop을 로컬에 설치해야 하므로 좀 힘들겠죠?&lt;/p&gt;
&lt;p&gt;make를 실행할 수 있지만 Pi의 제한된 처리 능력을 감안할 때, 빌드는 약간 시간이 걸릴 것입니다. SSH 세션이 만료되면 문제가 있을 수 있으니 &lt;a href="https://www.gnu.org/software/screen/"&gt;Screen&lt;/a&gt;을 사용하면 좋습니다.&lt;/p&gt;
&lt;p&gt;약간 속도를 높이기 위해 2 코어(4개 중)에서 병렬 실행을 실행할 수 있습니다. 더 많은 코어를 사용하면, 응답을 안할 수가 있으니 꼭 2개만 사용하시기 바랍니다.&lt;/p&gt;
&lt;pre&gt;$ export USE_S3=1
$ screen make -j2&lt;/pre&gt;
&lt;p&gt;이것은 약 1 시간 정도 걸립니다. 마지막 단계는 라이브러리와 Python 바인딩을 설치하는 것입니다.&lt;/p&gt;
&lt;pre&gt;$ cd python
 $ sudo python setup.py install
 $ python
 Python 2.7.9 (default, Sep 17 2016, 20:26:04)
 [GCC 4.9.2] on linux2
 Type "help", "copyright", "credits" or "license" for more information.
 &amp;gt;&amp;gt;&amp;gt; import mxnet as mx
 &amp;gt;&amp;gt;&amp;gt; mx.__version__
 '0.9.3a'&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;모델 로딩하기&lt;/strong&gt;&lt;br /&gt;
일단 모델 파일을 Pi에 복사 한 후에는 실제 파일을 로드 할 수 있어야합니다. 5편에서 쓴 똑같은 코드를 재사용할 수 있습니다. Pi는 약 580MB의 여유 메모리가 있는 CLI 모드를 지원합니다. 모든 데이터는 32GB SD 카드에 저장됩니다. 이제 VGG16를 로딩해봅시다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; vgg16,categories = init("vgg16")
 terminate called after throwing an instance of 'std::bad_alloc'
 what(): std::bad_alloc&lt;/pre&gt;
&lt;p&gt;앗! VGG16이 너무 커서 메모리에 맞지 않네요. ResNet-152를 사용해 봅시다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; resnet152,categories = init("resnet-152")
 Loaded in 11056.10 milliseconds

&amp;gt;&amp;gt; print predict("kreator.jpg",resnet152,categories,5)
 Predicted in 7.98 milliseconds
 [(0.87835813, 'n04296562 stage'), (0.045634001, 'n03759954 microphone, mike'), (0.035906471, 'n03272010 electric guitar'), (0.021166906, 'n04286575 spotlight, spot'), (0.0054096784, 'n02676566 acoustic guitar')]&lt;/pre&gt;
&lt;p&gt;ResNet-152는 약 10 초 만에 성공적으로 로딩되고, 10 밀리초 이내에 예측이 가능합니다. 이제 Inception v3으로 넘어 가 보죠.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; inceptionv3,categories = init("Inception-BN")
 Loaded in 2137.62 milliseconds

&amp;gt;&amp;gt; print predict("kreator.jpg",resnet152,categories,5)
 Predicted in 2.35 milliseconds
 [(0.4685601, 'n04296562 stage'), (0.40474886, 'n03272010 electric guitar'), (0.073685646, 'n04456115 torch'), (0.011639798, 'n03250847 drumstick'), (0.011014056, 'n02676566 acoustic guitar')]&lt;/pre&gt;
&lt;p&gt;Pi와 같은 제한된 장치에서는 모델 차이가 훨씬 더 분명하게 보입니다! Inception v3은 훨씬 빠른 속도로 로딩하고, 몇 밀리 초 안에 예측이 가능합니다. 모델이 로딩 될 경우에도 실제 응용 프로그램을 실행하는 충분한 RAM이 PI에 남아있어야 한다는 점은 확인하기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pi 카메라를 사용하여 이미지 캡처하기&lt;/strong&gt;&lt;br /&gt;
Raspberry Pi에 추가 할 수 있는 제일 좋은 디바이스는 카메라 모듈입니다. 진짜 간단합니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; inceptionv3,categories = init("Inception-BN")

&amp;gt;&amp;gt;&amp;gt; import picamera
&amp;gt;&amp;gt;&amp;gt; camera = picamera.PiCamera()
&amp;gt;&amp;gt;&amp;gt; filename = '/home/pi/cap.jpg'

&amp;gt;&amp;gt;&amp;gt; print predict(filename, inceptionv3, categories, 5)&lt;/pre&gt;
&lt;p&gt;아래는 간단한 예입니다.&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*ZRHWR2Bzb-S0mccRVTQcwQ.jpeg" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;Predicted in 12.90 milliseconds
 [(0.95071173, 'n04074963 remote control, remote'), (0.013508897, 'n04372370 switch, electric switch, electrical switch'), (0.013224524, 'n03602883 joystick'), (0.00399205, 'n04009552 projector'), (0.0036674738, 'n03777754 modem')]&lt;/pre&gt;
&lt;p&gt;잘 동작하죠?  이제 Amazon AI 서비스를 추가 해보면 어떨까요.  얼마 전에 만든 Python 스크립트 (&lt;a href="https://medium.com/@julsimon/a-hands-on-look-at-the-amazon-rekognition-api-e30e19e7d88b"&gt;문서&lt;/a&gt;, &lt;a href="https://github.com/juliensimon/aws/tree/master/rekognition"&gt;코드&lt;/a&gt;)를 사용하여 &lt;a href="https://aws.amazon.com/rekognition/"&gt;Amazon Rekognition&lt;/a&gt;을 통해 같은 사진을 실행해 볼 수 있습니다.&lt;/p&gt;
&lt;pre&gt;$ ./rekognitionDetect.py jsimon-public cap.jpg copy
 Label Remote Control, confidence: 94.7508468628&lt;/pre&gt;
&lt;p&gt;인식된 레이블을  음성으로 전달해 본다면 어떨까요? &lt;a href="https://medium.com/@julsimon/amazon-polly-hello-world-literally-812de2c620f4"&gt;Amazon Polly를 추가&lt;/a&gt;하는 것도 쉽습니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Amazon Rekognition 및 Amazon Polly는 AWS가 제공하는 딥러닝(Deep Learning) 기술을 기반으로 만들어진 완전 관리(Fully-managed) 서비스입니다.  개발자라면 누구나 쉽게 사용 가능하며 학습 모델이나 인프라에 대해 걱정할 필요가 없고, 단지 API를 호출만 하면 됩니다. 다양한 이미지 인식 기능과 많은 언어의 음성 합성 기능을 제공합니다.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;MXnet 기반의 Inception v3 모델을 사용하여 실시간 개체 감지를 수행하고, Amazon Polly에서 본 내용을 설명하는 Raspberry Pi의 비디오를 한번 살펴 보세요!&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;지금까지 MXNet에 대한 API와 이를 사용한 이미지 인식을 위해 많은 것을 살펴 보았습니다.  이미지내 객체 감지를 위해 합성곱 신경망(Convolutional Neural Networks)에 초점을 맞추었지만, MXNet에는 훨씬 더 많은 학습 모델이 있고, 추가로 새로운 시리즈를 통해 배울 수 있을 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;이 시리즈는 AWS 테크 에반젤리스트인 Julien Simon이 연재한 &lt;a href="https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab?source=user_profile---------17-----------"&gt;MXNet 관련 글 모음&lt;/a&gt;의 번역 편집본으로 최근 각광 받고 있는 Deep Learning 라이브러리인 &lt;a href="http://mxnet.io/"&gt;Apache MXnet&lt;/a&gt;  을 개괄적으로 설명하려고 합니다.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=X2Ug4qdnPKI:uYWVGLOR4Q4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sat, 29 Jul 2017 18:43:29 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기</title>
	<link>http://blog.creation.net/mxnet-part-5-vgc16-resnet152</link>
	<description>&lt;p&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;4편&lt;/a&gt;에서는 사전 학습된 Inception v3 모델을 사용하여, 이미지 내 분류를 검색하는 것이 얼마나 쉬운지 확인했습니다. 이 글에서는 두 개의 유명한 &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;합성곱 신경망&lt;/a&gt;(Convolutional neural network, CNN)인 VGG19와 ResNet-152을 사용해 보고, Inception v3 모델과 비교해 보겠습니다.&lt;/p&gt;
&lt;p&gt;CNN이라는 어려운 단어가 나왔지만, 과정은 크게 다르지 않습니다. 레이어를 더 많이 늘린 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://cdn-images-1.medium.com/max/1455/1*V_lnH58ZtxQyxXCqHYt6DQ.png" alt="" /&gt;&lt;br /&gt;
&lt;small&gt;Architecture of a CNN (Source: Nvidia)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VGG16&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2014 년에 발표된 VGG16은 &lt;strong&gt;16 개의 레이어&lt;/strong&gt;로 구성된 모델입니다 (&lt;a href="https://arxiv.org/abs/1409.1556"&gt;연구 논문&lt;/a&gt;). 객체 분류에서 7.4 %의 오류율을 달성하여 &lt;a href="http://image-net.org/challenges/LSVRC/2014/"&gt;2014 년 ImageNet Challenge&lt;/a&gt;에서 우승했습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ResNet-152&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2015 년에 발표된 ResNet-152는 &lt;strong&gt;152 개의 레이어&lt;/strong&gt; (&lt;a href="https://arxiv.org/abs/1512.03385"&gt;연구 논문&lt;/a&gt;)로 구성된 모델입니다. 이미지 내 객체 탐지에 대한 오류율 3.57 %를 달성함으로써 &lt;a href="http://image-net.org/challenges/LSVRC/2015/"&gt;2015 ImageNet Challenge&lt;/a&gt;에서 우승했습니다.  이건 진짜 놀라운 결과인데, 인간이 대체적으로 5 %의 오류율을 가지는데 (100개를 보면 5개를 긴가민가하게 생각하는) 사람 보다 더 똑똑하다고 볼 수 있겠죠.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 다운로드 하기&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;MXNet zoo를 다시 방문 할 시간입니다! Inception v3와 마찬가지로 모델 정의와 매개 변수를 다운로드해야합니다. 세 모델 모두 동일한 카테고리에 대해 교육을 받았으므로 synset.txt 파일을 다시 사용할 수 있습니다.&lt;/p&gt;
&lt;pre&gt;$ wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json
$ wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params
$ wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json
$ wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;모델 로딩하기&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;세 가지 모델은 224 X 224의 전형적인 이미지 크기의 ImageNet 데이터 셋을 기반으로 학습하기 때문에 우리가 이전에 사용한 코드를 재사용 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;우리가 바꾸어야하는 것은 모델 이름입니다 : loadModel()과 init() 함수에 매개 변수를 추가합니다.&lt;/p&gt;
&lt;pre&gt;def loadModel(modelname):
        sym, arg_params, aux_params = mx.model.load_checkpoint(modelname, 0)
        mod = mx.mod.Module(symbol=sym)
        mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])
        mod.set_params(arg_params, aux_params)
        return mod

def init(modelname):
        model = loadModel(modelname)
        cats = loadCategories()
        return model, cats&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;모델별 예측 비교하기&lt;/strong&gt;&lt;br /&gt;
이전에 사용한 샘플 이미지를 기반으로 이번 두 가지 모델을 비교해 보겠습니다.&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*KcUFMWb7KKTvOd_fgrXydA.jpeg" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;*** VGG16
[(0.58786136, 'n03272010 electric guitar'), (0.29260877, 'n04296562 stage'), (0.013744719, 'n04487394 trombone'), (0.013494448, 'n04141076 sax, saxophone'), (0.00988709, 'n02231487 walking stick, walkingstick, stick insect')]&lt;/pre&gt;
&lt;p&gt;상위 2개 카테고리는 잘 맞는데, 나머지 3개는 잘못되었습니다. 마이크 받침대의 수직 모양에서 혼란스러워하네요.&lt;/p&gt;
&lt;pre&gt;*** ResNet-152
[(0.91063803, 'n04296562 stage'), (0.039011702, 'n03272010 electric guitar'), (0.031426914, 'n03759954 microphone, mike'), (0.011822623, 'n04286575 spotlight, spot'), (0.0020199812, 'n02676566 acoustic guitar')]&lt;/pre&gt;
&lt;p&gt;상위 카테고리에서 매우 높습니다. 나머지 4 개 모두 의미가 있습니다.&lt;/p&gt;
&lt;pre&gt;*** Inception v3
[(0.58039135, 'n03272010 electric guitar'), (0.27168664, 'n04296562 stage'), (0.090769522, 'n04456115 torch'), (0.023762707, 'n04286575 spotlight, spot'), (0.0081428187, 'n03250847 drumstick')]&lt;/pre&gt;
&lt;p&gt;상위 2 개 카테고리의 VGG16과 매우 유사한 결과. 나머지 세 개는 역시 잘 모르겠네요.&lt;/p&gt;
&lt;p&gt;다른 사진으로 해볼까요?&lt;br /&gt;
&lt;img src="https://cdn-images-1.medium.com/max/1455/1*03jvGuldTuyRbYCFwwRP7A.jpeg" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;*** VGG16
[(0.96909302, 'n04536866 violin, fiddle'), (0.026661994, 'n02992211 cello, violoncello'), (0.0017284016, 'n02879718 bow'), (0.00056815811, 'n04517823 vacuum, vacuum cleaner'), (0.00024804732, 'n04090263 rifle')]

*** ResNet-152
[(0.96826887, 'n04536866 violin, fiddle'), (0.028052919, 'n02992211 cello, violoncello'), (0.0008367821, 'n02676566 acoustic guitar'), (0.00070532493, 'n02787622 banjo'), (0.00039021231, 'n02879718 bow')]

*** Inception v3
[(0.82023674, 'n04536866 violin, fiddle'), (0.15483995, 'n02992211 cello, violoncello'), (0.0044540241, 'n02676566 acoustic guitar'), (0.0020963412, 'n02879718 bow'), (0.0015099624, 'n03447721 gong, tam-tam')]&lt;/pre&gt;
&lt;p&gt;세 가지 모델 모두 상위 카테고리에서 매우 높은 점수를 받았습니다. 바이올린의 모양이 신경망에 대해 매우 모호한 패턴이라고 가정 할 수 있습니다.&lt;/p&gt;
&lt;p&gt;몇 가지 샘플에서 사용 여부를 결정해서는 안됩니다. 사전 학습된 모델을 찾고 있다면 반드시 학습 데이터 셋을 잘 살펴보고, 자신의 데이터에 대한 테스트를 실행하고 사용할지 여부를 정해야합니다!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;기술적 성능 비교&lt;/strong&gt;&lt;br /&gt;
위의 연구 논문에서 많은 모델의 성능에 대한 벤치 마크 결과를 보실 수 있습니다. 개발자의 경우, 가장 중요한 두 가지 요인은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;모델에 필요한 메모리 양은 얼마인가?&lt;/li&gt;
&lt;li&gt;얼마나 빨리 예측 가능한가?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;첫 번째 질문에 답하기 위해 우리는 매개 변수 파일의 크기를 보고 추측을 할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VGG16: 528MB (about 140 million parameters)&lt;/li&gt;
&lt;li&gt;ResNet-152: 230MB (about 60 million parameters)&lt;/li&gt;
&lt;li&gt;Inception v3: 43MB (about 25 million parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;보시다시피 더 적은 매개 변수로 더 많은 레이어로 심층 네트워크를 사용하는 것입니다. 이것은 교육 시간 단축(네트워크가 매개 변수를 덜 학습하므로)과 메모리 사용량 감소라는 두 가지 이점이 있습니다.&lt;/p&gt;
&lt;p&gt;두 번째 질문은 조금 복잡해서 배치 크기와 같은 다양한 매개 변수에 따라 달라집니다. 예측 호출 및 실행 시간에 중점을 두고 예제를 다시 실행 해 봅시다.&lt;/p&gt;
&lt;pre&gt;t1 = time.time()
model.forward(Batch([array]))
t2 = time.time()
t = 1000*(t2-t1)
print("Predicted in %2.2f millisecond" % t)&lt;/pre&gt;
&lt;p&gt;결과는 다음과 같습니다. (몇 번 호출한 뒤 평균값입니다.)&lt;/p&gt;
&lt;pre&gt;*** VGG16
Predicted in 0.30 millisecond
*** ResNet-152
Predicted in 0.90 millisecond
*** Inception v3
Predicted in 0.40 millisecond&lt;/pre&gt;
&lt;p&gt;자! 이제 요약을 해보죠. ResNet-152는 세 가지 네트워크 중에서 가장 우수한 정확도를 가졌지만, 2-3 배 더 느립니다.&lt;br /&gt;
VGG16은 레이어 수가 적기 때문에 가장 빠릅니다. 그러나, 높은 메모리 사용과 최악의 정확도를 가지고 있습니다.&lt;/p&gt;
&lt;p&gt;Inception v3은 더 빠른 정확성과 가장 보수적인 메모리 사용을 제공하는 동시에 거의 빠릅니다. 이 마지막 점은 실시간 분석과 같은 제한된 환경에서는 좋은 후보가 됩니다. 이에 대한 자세한 부분은 마지막 6편에서 다뤄 보겠습니다.&lt;/p&gt;
&lt;p&gt;다음 글:  &lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;코드 전체 보기&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=5nzXCYgNenI:evKyMVihp-A:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Fri, 28 Jul 2017 15:21:35 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)</title>
	<link>http://blog.creation.net/mxnet-part-4-inception-v3</link>
	<description>&lt;p&gt;이전 글에서는 처음으로 신경망을 구축하고 학습하는 방법을 배웠습니다. 이제 좀 더 실질적인 문제 해결을 할 수 있는 사례를 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;우선 최근에 사용되는 딥러닝 학습 모델은 매우 복잡하다는 사실을 알고 계셔야 합니다. 수 백개의 레이어가 있으며 막대한 양의 데이터를 학습하는 데 며칠이 걸릴수 있으며, 이러한 모델을 만들고 조정하는 데는 많은 전문 지식이 필요합니다.&lt;/p&gt;
&lt;p&gt;매우 다행인 것인 이러한 모델을 사용하는 것은 생각 보다 간단하며 몇 줄의 소스 코드만 가지고 할 수 있습니다. 이 글에서는 &lt;a class="markup--anchor markup--p-anchor" href="https://arxiv.org/abs/1512.00567" target="_blank" rel="nofollow noopener"&gt;Inception v3&lt;/a&gt;이라는 이미지 분류를 위해 미리 학습된 모델을 살펴 볼 것입니다.&lt;/p&gt;
&lt;p&gt;2015년 12월에 나온 Inception v3은 &lt;a class="markup--anchor markup--p-anchor" href="https://arxiv.org/abs/1409.4842" target="_blank" rel="nofollow noopener"&gt;GoogleNet&lt;/a&gt; 모델(&lt;a class="markup--anchor markup--p-anchor" href="http://image-net.org/challenges/LSVRC/2014/" target="_blank" rel="nofollow noopener"&gt;2014 ImageNet Challenge&lt;/a&gt;에서 우승 한 모델)을 발전 시킨 것입니다. 연구 논문의 세부 사항에 대해서는 언급하지 않겠지만, 결론적으로 Inception v3는 당시에 사용 가능한 최고의 학습 모델보다 15-25% 정확도가 높으며, 연산에 있어 6배 저렴하고 최소 20% 미만의 매개 변수를 사용합니다 (즉, 모델 사용에 필요한 RAM 사용량이 적습니다.)&lt;/p&gt;
&lt;p&gt;딥러닝을 인기있는 모델로 끌어 올린 대단한 물건인데, 이를 MXNet으로 한번 작동시켜 봅시다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MXNet Zoo 학습 모델&lt;/strong&gt;&lt;br /&gt;
&lt;a class="markup--anchor markup--p-anchor" href="http://mxnet.io/model_zoo/" target="_blank" rel="nofollow noopener"&gt;Model Zoo&lt;/a&gt;는 MXNet에서 손쉽게 사용할 수 있도록 미리 학습된 모델 모음입니다. 모델 정의, 모델 매개 변수 (즉, 뉴런 가중치) 및 사용 가이드 등으로 구성되어 있습니다.&lt;/p&gt;
&lt;p&gt;여기서 ImageNet에 해당 하는 모델 정의와 매개 변수 파일을 다운로드하세요. (파일 이름을 변경해야 할 수도 있음). 첫 번째 파일을 열면 모든 레이어의 정의가 표시되어 있습니다. 두 번째 파일은 바이너리 파일입니다.&lt;/p&gt;
&lt;pre&gt;$ wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json
$ wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params
$ mv Inception-BN-0126.params Inception-BN-0000.params&lt;/pre&gt;
&lt;p&gt;이 모델은 ImageNet 데이터 세트에서 학습되었으므로 해당 이미지 카테고리 (1000개)도 다운로드해야 합니다.&lt;/p&gt;
&lt;pre&gt;$ wget http://data.dmlc.ml/models/imagenet/synset.txt

$ wc -l synset.txt
 1000 synset.txt

$ head -5 synset.txt
 n01440764 tench, Tinca tinca
 n01443537 goldfish, Carassius auratus
 n01484850 great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
 n01491361 tiger shark, Galeocerdo cuvieri
 n01494475 hammerhead, hammerhead shark&lt;/pre&gt;
&lt;p&gt;다운로드 다 받으셨나요? 그러면 이제 모델을 가져와서 작업을 시작해 봅시다!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 로딩하기&lt;br /&gt;
&lt;/strong&gt;우리가 해야 할 일을 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;1. 저장 상태에서 모델 로딩하기 : MXNet에서는 이것을 체크 포인트라고 부릅니다.  우리는 입력 Symbol과 모델 매개 변수를 반환 받습니다.&lt;/p&gt;
&lt;pre&gt;import mxnet as mx
sym, arg_params, aux_params = mx.model.load_checkpoint('Inception-BN', 0)&lt;/pre&gt;
&lt;p&gt;2. 새로운 모듈을 생성하고 그것을 입력 심볼로 할당합니다. 모델을 어디에서 실행할지를 나타내는 컨텍스트 매개 변수를 지정할 수도 있습니다. 기본값은 cpu (0)이지만 gpu (0)를 사용하여 GPU에서 실행할 수도 있습니다.&lt;/p&gt;
&lt;pre&gt;mod = mx.mod.Module(symbol=sym)&lt;/pre&gt;
&lt;p&gt;3. 입력 심볼에 입력 데이터를 바인딩합니다. 이름은 네트워크의 입력 레이어에 있는 이름이기 때문에 &amp;#8216;data&amp;#8217;라고 합시다. (JSON 파일의 처음 몇 줄을보십시오). &amp;#8216;data&amp;#8217;의 크기를 1 x 3 x 224 x 224로 정의합니다. 당황하지 마세요 &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f609.png" alt="&#x1f609;" class="wp-smiley" /&gt; &amp;#8216;224 x 224&amp;#8217;는 이미지 해상도로 모델 학습 방법입니다. &amp;#8216;3&amp;#8217;은 채널 수입니다: 빨강, 초록, 파랑​​(순서대로). &amp;#8216;1&amp;#8217;은 배치 크기입니다. 한 번에 하나의 이미지를 예측합니다.&lt;/p&gt;
&lt;pre&gt;mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])&lt;/pre&gt;
&lt;p&gt;4. 모델 매개 변수를 설정합니다.&lt;/p&gt;
&lt;pre&gt;mod.set_params(arg_params, aux_params)&lt;/pre&gt;
&lt;p&gt;이게 전부입니다. 코드 네 줄! ㅋㅋ 이제 몇 가지 데이터를 밀어 넣고 무슨 일이 일어나는지 살펴 보겠습니다. (데이터 먼저 준비하구요.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;데이터 준비하기&lt;br /&gt;
&lt;/strong&gt;사실 개발자라면 1970년대 부터 하던 일인데, 데이터베이스 부터 기계 학습, 딥러닝에 이르기까지 빠지지 않는 게 데이터 준비네요. 좀 귀찮은 일이긴 하지만 꼭 필요하니 시작해 봅시다.&lt;/p&gt;
&lt;p&gt;이 모델은 하나의 224 x 224 이미지의 빨강, 녹색 및 파란 채널을 유지하는 4차원 NDArray를 사용합니다. 우리는 인기있는 OpenCV 라이브러리를 사용하여 입력 이미지로 부터 데이터를 추출해서 NDArray에 넣을 것입니다. OpenCV를 설치하지 않은 경우 &amp;#8220;pip install opencv-python&amp;#8221;을 실행하면 대부분의 경우 충분합니다. &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f642.png" alt="&#x1f642;" class="wp-smiley" /&gt;&lt;/p&gt;
&lt;p&gt;단계는 다음과 같습니다.&lt;/p&gt;
&lt;p&gt;1. 이미지 읽기 : BGR 순서 (파란색, 녹색 및 빨간색)의 세 채널을 사용하여 (이미지 높이, 이미지 너비, 3) 모양의 수적으로 배열을 반환합니다.&lt;/p&gt;
&lt;pre&gt;img = cv2.imread(filename)&lt;/pre&gt;
&lt;p&gt;2. 이미지를 RGB로 변환합니다.&lt;/p&gt;
&lt;pre&gt;img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)&lt;/pre&gt;
&lt;p&gt;3. 이미지를 224 x 224 크기로 조정합니다.&lt;/p&gt;
&lt;pre&gt;img = cv2.resize(img, (224, 224,))&lt;/pre&gt;
&lt;p&gt;4. 배열을 (이미지 높이, 이미지 너비, 3)에서 (3, 이미지 높이, 이미지 너비)로 바꿉니다.&lt;/p&gt;
&lt;pre&gt;img = np.swapaxes(img, 0, 2)
img = np.swapaxes(img, 1, 2)&lt;/pre&gt;
&lt;p&gt;5. 네 번째 차원을 추가하고 NDArray를 정의합니다.&lt;/p&gt;
&lt;pre&gt;img = img[np.newaxis, :]
array = mx.nd.array(img)
&amp;gt;&amp;gt;&amp;gt; print array.shape
 (1L, 3L, 224L, 224L)&lt;/pre&gt;
&lt;p&gt;자! 이제 실제 해볼까요? 아래에 이미지 파일이 하나 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*sPdrfGtDd_6RQfYvD5qcyg.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;Input picture 448&amp;#215;336 (Source: metaltraveller.com)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;이미지 처리가 끝나면 그림 크기가 조정되어 array[0]에 저장된 RGB 채널로 분할됩니다 (아래 그림을 생성하는 데 사용 된 코드가&lt;br /&gt;
&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*yqdl78KIugYepzJ4-lMY8g.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;array[0][0] : 224&amp;#215;224 red channel&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*sitDwoAzPDLrav0dXQrcbA.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;array[0][1] : 224&amp;#215;224 green channel&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image" src="https://cdn-images-1.medium.com/max/1455/1*h1RyEPvd2fqIgd2jkfES-w.jpeg" /&gt;&lt;br /&gt;
&lt;small&gt;array[0][2] : 224&amp;#215;224 blue channel&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;일괄 처리 크기가 1보다 크면, array[1]에 두 번째 이미지, array[2]에 세 번째 이미지 등이 나눠집니다. 이제 예측을 해볼까요?&lt;br /&gt;
&lt;strong&gt;예측하기&lt;/strong&gt;&lt;br /&gt;
파트 3에서 Module 객체에서 데이터를 일괄적으로 모델에 공급해야 한다는 것을 기억하실 거에요. 일반적으로 데이터 반복기(Iterator)를 사용하는 것이 일반적인 방법입니다. (특히, NDArrayIter 객체 사용).&lt;/p&gt;
&lt;p&gt;여기서 이미지 &lt;strong&gt;하나만&lt;/strong&gt; 예측하고 싶습니다. 데이터 반복기를 사용할 수는 있겠지만 너무 오버하는 것이 될 수 있습니다. 대신 데이터 속성이 참조될 때 입력 NDArray를 반환하여, 가짜 반복기 역할을 할 Batch라는 이름의 튜플(tuple)을 만들 것입니다.&lt;/p&gt;
&lt;pre&gt;from collections import namedtuple
Batch = namedtuple('Batch', ['data'])&lt;/pre&gt;
&lt;p&gt;이제 &amp;#8220;Batch&amp;#8221;를 모델에 전달하고 예측할 수 있습니다.&lt;/p&gt;
&lt;pre&gt;mod.forward(Batch([array]))&lt;/pre&gt;
&lt;p&gt;모델은 1000 개 카테고리에 해당하는 &lt;strong&gt;1000개의 확률&lt;/strong&gt;을 갖는 NDArray를 출력합니다. 일괄 처리 크기가 1이므로 한 줄 밖에 없습니다.&lt;/p&gt;
&lt;pre&gt;prob = mod.get_outputs()[0].asnumpy()

&amp;gt;&amp;gt;&amp;gt; prob.shape
 (1, 1000)&lt;/pre&gt;
&lt;p&gt;이것을 squeeze()를 사용하여 배열로 바꾼 후, argsort()를 사용하여 &lt;strong&gt;내림차순&lt;/strong&gt;으로 정렬된 두 번째 배열 &lt;strong&gt;인덱스&lt;/strong&gt;를 만듭니다.&lt;/p&gt;
&lt;pre&gt;prob = np.squeeze(prob)

&amp;gt;&amp;gt;&amp;gt; prob.shape
(1000,)
&amp;gt;&amp;gt; prob
[ 4.14978594e-08 1.31608676e-05 2.51907986e-05 2.24045834e-05
2.30327873e-06 3.40798979e-05 7.41563645e-06 3.04062659e-08 etc.

sortedprob = np.argsort(prob)[::-1]

&amp;gt;&amp;gt; sortedprob.shape
(1000,)&lt;/pre&gt;
&lt;p&gt;현재 모델에 따르면, 사진의 가장 가능성이 큰 카테고리는 #546이며 확률은 58%입니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt; sortedprob
[546 819 862 818 542 402 650 420 983 632 733 644 513 875 776 917 795
etc.
&amp;gt;&amp;gt; prob[546]
0.58039135&lt;/pre&gt;
&lt;p&gt;이 카테고리의 이름을 찾아 보겠습니다. synset.txt 파일을 사용하여 카테고리 목록을 작성하고, #546를 목록에서 찾을 수 있습니다.&lt;/p&gt;
&lt;pre&gt;synsetfile = open('synset.txt', 'r')
categorylist = []
for line in synsetfile:
categorylist.append(line.rstrip())

&amp;gt;&amp;gt;&amp;gt; categorylist[546]
'n03272010 electric guitar'&lt;/pre&gt;
&lt;p&gt;어떠세요? 전기 기타(electric quitar)라고 나오죠? 두번째를 볼까요?&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; prob[819]
0.27168664
&amp;gt;&amp;gt;&amp;gt; categorylist[819]
'n04296562 stage&lt;/pre&gt;
&lt;p&gt;무대(stage)라고 나옵니다. 어떠세요? 그냥 됩니다. 우리는 이제 이미지 내의 분류를 위해 사전 학습된 모델을 사용하는 방법을 사용해 보았습니다. 코드는 고작 4줄이었고 주로 했던 일은 데이터 준비였습니다.&lt;/p&gt;
&lt;p&gt;맨 아래 전체 코드가 있습니다. 다음에는 이미지 분석을 위해 좀 더 잘 학습된 모델인 VGG16과 ResNet-152를 살펴 보도록 하겠습습니다. 잘 따라 와 주세요~&lt;/p&gt;
&lt;p&gt;다음 글: &lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) – NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) – Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) – Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) – 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) – VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) – Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;코드 전체 보기&lt;/strong&gt;&lt;br /&gt;
&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=eKx26a-yUeI:ZpdSVZJGlT4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Fri, 28 Jul 2017 14:28:58 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>친정인 ‘카카오뱅크’ 오픈 소식에 계좌 개설!</title>
	<link>http://channy.creation.net/blog/1152</link>
	<description>&lt;p&gt;&lt;a href="http://www.kakaobank.com/"&gt;카카오 뱅크&lt;/a&gt; 오픈 소식에 오전에 사람들이 좀 몰려서 일시 장애가 있었던 듯 싶으나 은행계 시스템이란게 초기 계좌 개설 같은 DB 쓰기 용량이 한계가 있으니, 그건 어느 정도 이해하고… 일반인들의 경우도 약간의 노이즈 마케팅이 될 수 있으니 오케이! (기사에 따르면, &lt;a href="http://naver.me/xQ3llS2W"&gt;6시간만에 6만 계좌 개설&lt;/a&gt;됐음. 케이뱅크의 경우, 15시간 동안 1만 5천계좌에 비하면 엄청난 관심이네요.)&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channy.creation.net/data/channy/2017/07/27155709/kakao-bank-account.jpg" alt="" width="80%" /&gt;&lt;/p&gt;
&lt;p&gt;1. 계좌 개설 방법은 비교적 간단(?)하지만, 중간에 타행 계좌에 1원 입금해 주고, 계좌 이체한 사람 이름 확인하는 과정에서 멘붕. 갑자기 타행 계좌 조회해야 하는 불상사가 생겨서 당황했는데, 필수 조건이기 때문에 타행 조회 꼭 준비하셔야 합니다.&lt;/p&gt;
&lt;p&gt;2. Bad gateway 오류가 계속 나는데도 불구하고 무시 전략으로 롤백을 믿고 강행한 덕에 계좌 개설, 체크 카드 신청 및 카톡 무료 이모티콘 다운로드까지 일사 천리 10분안에 진행 완료. 계좌번호 처음이 삼삼하게 3333으로 시작하는데, 계좌 번호도 자기가 선택할 수 있었으면 좋겠네요.&lt;/p&gt;
&lt;p&gt;3. 중간에 오류가 나면 전화나 카톡 고객 센터를 연결해 주는 친절한 메시지는 나쁘지 않은 듯. 웹 사이트에는 오로지 고객 센터 도움말과 상담 기능만 있으며, 모든 업무 처리는 앱으로만 가능해서 PC를 버리는 선택과 집중은 개발 리소스가 적을텐데 좋은 전략입니다.&lt;/p&gt;
&lt;p&gt;구, 다음 출신들이 카카오뱅크에 가서 고생을 많이 했는데 서비스 오픈 하느라 고생많으셨을듯. 이게 일반 웹 서비스가 아니라, 레거시 은행 기간망이랑 연동하면서 만들어야 하는 거라 어려움이 있었을 텐데요. 유닉스가 아니라 리눅스 계열 X86으로 바꾸고, 오픈 소스를 쓰면서 만드는 첫번째 실험이라 앞으로 잘되길 바랍니다.&lt;/p&gt;
&lt;p&gt;개인적으로 공인 인증서 (발급/인증) 및 OTP  같은 복잡한 절차 없이 은행 업무를 볼 수 있다는 것만으로도 충분히 도움이 될 것 같네요. 앞으로 꼭 성공하길!~&lt;/p&gt;
&lt;div class="intro_main"&gt;
&lt;h3 class="tit_main"&gt;같지만 다른 은행 카카오뱅크&lt;/h3&gt;
&lt;ul class="list_store"&gt;
&lt;li&gt;&lt;a class="link_store" href="https://www.kakaobank.com/download/android?utm_source=channyblog&amp;utm_medium=channyblog&amp;utm_campaign=channyblog" target="_blank" rel="noopener"&gt;Google Play 다운로드&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="link_store" href="https://www.kakaobank.com/download/ios?utm_source=channyblog&amp;utm_medium=channyblog&amp;utm_campaign=channyblog" target="_blank" rel="noopener"&gt;App Store 다운로드&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WQZ7pbbxJr8:9wt_7j-_cPs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 27 Jul 2017 07:05:15 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1152#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (3) – Module API</title>
	<link>http://blog.creation.net/mxnet-part-3-module-api</link>
	<description>&lt;p&gt;2편에서는 Symbols를 사용하여 NDArrays에 저장된 데이터를 처리하는 연산 그래프를 정의하는 방법을 설명했습니다. 이 글에서는 Symbol 및 NDArrays에서 배운 것을 사용하여 일부 데이터를 준비하고 신경망을 구성합니다. 그런 다음 &lt;a href="http://mxnet.io/api/python/module.html"&gt;Module API&lt;/a&gt;를 사용하여 신경망 기반 데이터를 학습하고 결과를 예측해보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;데이터셋 정의&lt;/strong&gt;&lt;br /&gt;
(가상) 데이터 세트는 1000개의 데이터 샘플로 구성합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;각 샘플에는 100개의 &lt;strong&gt;기능(Feature)&lt;/strong&gt;이 있습니다.&lt;/li&gt;
&lt;li&gt;각 기능은 &lt;strong&gt;0에서 1 사이의 float 값&lt;/strong&gt;으로 표현됩니다.&lt;/li&gt;
&lt;li&gt;샘플은 &lt;strong&gt;10개의 카테고리&lt;/strong&gt;로 나뉩니다. 네트워크의 목적은 주어진 샘플에 대한 올바른 카테고리를 예측하는 것입니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;학습(Training)&lt;/strong&gt;을 위해 800개의 샘플을 사용하고 &lt;strong&gt;검증(Validation)&lt;/strong&gt;을 위해 200개의 샘플을 사용할 것입니다.&lt;/li&gt;
&lt;li&gt;학습 및 검증을 위해 &lt;strong&gt;배치(Batch) 크기 10&lt;/strong&gt;을 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;아래는 데이터 정의를 위한 간단한 MXNet 코드입니다.&lt;/p&gt;
&lt;pre id="df70" name="df70" class="graf graf--pre graf-after--li"&gt;import mxnet as mx
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

sample_count = 1000
train_count = 800
valid_count = sample_count - train_count

feature_count = 100
category_count = 10
batch=10&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;데이터셋 생성&lt;/strong&gt;&lt;br /&gt;
균등 분포를 사용하여 1000 개의 샘플을 생성합시다. 이를 &amp;#8216;X&amp;#8217;라는 이름의 NDArray에 저장합니다. (즉, &lt;strong&gt;1000라인, 100 컬럼&lt;/strong&gt;)&lt;/p&gt;
&lt;pre id="ece8" name="ece8" class="graf graf--pre graf-after--p"&gt;X = mx.nd.uniform(low=0, high=1, shape=(sample_count,feature_count))

&amp;gt;&amp;gt;&amp;gt; X.shape
(1000L, 100L)
&amp;gt;&amp;gt;&amp;gt; X.asnumpy()
array([[ 0.70029777,  0.28444085,  0.46263582, ...,  0.73365158,
         0.99670047,  0.5961988 ],
       [ 0.34659418,  0.82824177,  0.72929877, ...,  0.56012964,
         0.32261589,  0.35627609],
       [ 0.10939316,  0.02995235,  0.97597599, ...,  0.20194994,
         0.9266268 ,  0.25102937],
       ...,
       [ 0.69691515,  0.52568913,  0.21130568, ...,  0.42498392,
         0.80869114,  0.23635457],
       [ 0.3562004 ,  0.5794751 ,  0.38135922, ...,  0.6336484 ,
         0.26392782,  0.30010447],
       [ 0.40369365,  0.89351988,  0.88817406, ...,  0.13799617,
         0.40905532,  0.05180593]], dtype=float32)&lt;/pre&gt;
&lt;p&gt;각 1000개의 샘플에 대한 카테고리는 0-9 범위의 정수로 표시됩니다. 랜덤하게 생성되어 &amp;#8216;Y&amp;#8217;라는 NDArray에 저장됩니다.&lt;/p&gt;
&lt;pre id="60cf" name="60cf" class="graf graf--pre graf-after--p"&gt;Y = mx.nd.empty((sample_count,))
for i in range(0,sample_count-1):
  Y[i] = np.random.randint(0,category_count)

&amp;gt;&amp;gt;&amp;gt; Y.shape
(1000L,)
&amp;gt;&amp;gt;&amp;gt; Y[0:10].asnumpy()
array([ 3.,  3.,  1.,  9.,  4.,  7.,  3.,  5.,  2.,  2.], dtype=float32)&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;데이터셋 나누기&lt;/strong&gt;&lt;br /&gt;
다음 단계로 학습과 검증을 위해 데이터셋을 80/20으로 나눠야합니다. 이를 위해 NDArray.crop 함수를 사용합니다. 여기서 데이터 세트는 완전 무작위이므로 학습을 위해 상위 80%를 사용하고, 유효성을 검사하기 위해 하위 20%를 사용합니다. 실제 정식으로 할 때는 순차적으로 생성 된 데이터에 잠재적인 편향을 피하기 위해 데이터셋을 먼저 뒤섞어 놓아야 할 것입니다.&lt;/p&gt;
&lt;pre id="6487" name="6487" class="graf graf--pre graf-after--p"&gt;X_train = mx.nd.crop(X, begin=(0,0), end=(train_count,feature_count-1))

X_valid = mx.nd.crop(X, begin=(train_count,0), end=(sample_count,feature_count-1))

Y_train = Y[0:train_count]

Y_valid = Y[train_count:sample_count]&lt;/pre&gt;
&lt;p&gt;자, 이제 데이터 준비가 끝났습니다. 다음 단계로 넘어가 볼까요?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;신경망 바인딩&lt;/strong&gt;&lt;br /&gt;
우리가 만든 네트워크는 매우 간단합니다. 각 레이어를 살펴 보겠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;입력 레이어는 &amp;#8216;data&amp;#8217;라는 심볼로 표현됩니다. 나중에 실제 입력 데이터에 바인딩 할 것입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="70f8" name="70f8" class="graf graf--pre graf-after--li"&gt;data = mx.sym.Variable('data')&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fc1에서 첫 번째 숨겨진 레이어는 &lt;strong&gt;64개 연결된 신경(Neurons)&lt;/strong&gt;으로 구성됩니다. 즉, 입력 레이어의 각 기능은 64개의 모든 신경에 연결됩니다. 이를 위해 Symbol.FullyConnected 함수를 사용합니다.이 함수는 수동으로 각 연결을 만드는 것보다 훨씬 편리합니다!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="5582" name="5582" class="graf graf--pre graf-after--li"&gt;fc1 = mx.sym.FullyConnected(data, name='fc1', num_hidden=64)&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fc1의 각 결과 출력은 &lt;a href="https://en.wikipedia.org/wiki/Activation_function"&gt;활성화 함수&lt;/a&gt;를 통해 수행합니다. 활성화 함수란 Sigmod처럼 어디에 속하는지 분류하기 위해 일정 값을 두고 그 값을 넘어야 성공 혹은 참으로 분류하는 함수 입니다. 여기서 우리는 &lt;a class="markup--anchor markup--li-anchor" href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" target="_blank" rel="nofollow noopener"&gt;Rectified Linear Unit&lt;/a&gt; , 일명 &amp;#8216;ReLU&amp;#8217;를 사용합니다.  (역자주: ReLU는 신경망 학습에서 매우 중요한 성능을 높이는 판단 기준을 제안한 것으로, 출력이 0보다 작을 때는 0을 사용하고, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법입니다. 음수에 대해서는 값이 바뀌지만, 양수에 대해서는 값을 바꾸지 않습니다.)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="7f79" name="7f79" class="graf graf--pre graf-after--li"&gt;relu1 = mx.sym.Activation(fc1, name='relu1', act_type="relu")&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;fc2에서 두 번째 숨겨진 레이어는 연결된 10개의 뉴런으로 구성되며 10개의 카테고리에 매핑됩니다. 각 뉴런은 임의의 부동 소수점 값을 출력합니다. 10개의 값 중 가장 큰 값은 데이터 샘플의 가장 가능성 있는 카테고리를 나타냅니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="6cfc" name="6cfc" class="graf graf--pre graf-after--li"&gt;fc2 = mx.sym.FullyConnected(relu1, name='fc2', num_hidden=category_count)&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;출력 레이어는 fc2 레이어에서 10개의 값에 Softmax 함수를 적용합니다.이 값은 0과 1사이의 10 개의 값으로 변환되어 1을 가산합니다. 각 값은 각 카테고리의 예상 확률을 나타내며 가장 가능성 있는 큰 카테고리를 가리키게 됩니다. (역자주: Softmax 함수는 자연수 N개의 값에서, n 번째 값의 중요도를 찾는 함수로서 각각의 값의 편차를 확대시켜 큰 값은 상대적으로 더 크게, 작은 값은 상대적으로 더 작게 만든 다음 정규화 시키는 함수이다.)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="e2c5" name="e2c5" class="graf graf--pre graf-after--li"&gt;out = mx.sym.SoftmaxOutput(fc2, name='softmax')
mod = mx.mod.Module(out)&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;데이터 반복자(Iterator) 만들기&lt;/strong&gt;&lt;br /&gt;
Part 1에서는 한 번에 학습되지 않은 하나의 샘플을 가진 신경망를 살펴 보았습니다. 성능 관점에서 볼 때 매우 비효율적이죠. 대신에 고정 된 수의 샘플인 배치(Batch)를 사용합니다.&lt;/p&gt;
&lt;p&gt;이러한 배치를 네트워크에 전달하려면 NDArrayIter 함수를 사용하여 반복기(Interna)를 만들어야합니다. 그 파라미터는 학습 데이터, 카테고리 (MXNet에서는 라벨(Label)이라고 부름) 및 배치 크기입니다.&lt;/p&gt;
&lt;p&gt;아래 처럼, 한 번에 10개의 샘플과 10개의 레이블로 데이터셋을 반복 할 수 있습니다. 그런 다음 reset() 함수를 호출하여 반복기 상태를 원래로 복원합니다.&lt;/p&gt;
&lt;pre id="9cbc" name="9cbc" class="graf graf--pre graf-after--p"&gt;train_iter = mx.io.NDArrayIter(data=X_train,label=Y_train,batch_size=batch)
&amp;gt;&amp;gt;&amp;gt; for batch in train_iter:
...   print batch.data
...   print batch.label
...
[&amp;lt;NDArray 10x99 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10x99 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10x99 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
[&amp;lt;NDArray 10 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
&lt;em class="markup--em markup--pre-em"&gt;&amp;lt;edited for brevity&amp;gt;
&amp;gt;&amp;gt;&amp;gt; &lt;/em&gt;train_iter.reset()&lt;/pre&gt;
&lt;p&gt;우리가 만든 데이터셋을 통한 네트워크는 이제 학습 준비가 끝났습니다!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 학습하기&lt;/strong&gt;&lt;br /&gt;
먼저 입력 심볼을 실제 데이터셋 (샘플 및 레이블)에 바인딩합니다. 반복기를 통해 쉽게 할 수 있겠죠?&lt;/p&gt;
&lt;pre id="dbdb" name="dbdb" class="graf graf--pre graf-after--p"&gt;mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)&lt;/pre&gt;
&lt;p&gt;다음으로, 네트워크에서 뉴런 가중치를 초기화 해 봅시다. 이것은 실제로 매우 중요한 단계입니다. &amp;#8220;올바른&amp;#8221;기술로 초기화하면 네트워크가 훨씬 빨리 학습하는 데 도움이 됩니다. Xavier 초기화(이것을 만든 Xavier Glorot (&lt;a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="nofollow noopener"&gt; PDF&lt;/a&gt;)의 이름을 따서 명명)는 이러한 기술 중 하나입니다.&lt;/p&gt;
&lt;pre id="7dc8" name="7dc8" class="graf graf--pre graf-after--p"&gt;# Allowed, but not efficient
mod.init_params()
# Much better
mod.init_params(initializer=mx.init.Xavier(magnitude=2.))&lt;/pre&gt;
&lt;p&gt;다음으로 최적화 매개 변수를 정의해야 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;우리는 기계 학습 및 딥러닝 애플리케이션에 오랫동안 사용해온 &lt;a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"&gt;Stochastic Gradient Descent 알고리즘&lt;/a&gt; (일명 SGD)을 사용하고 있습니다.&lt;/li&gt;
&lt;li&gt;우리는 학습 속도를 SGD의 일반적 값인 0.1로 설정하고 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre id="fa49" name="fa49" class="graf graf--pre graf-after--li"&gt;mod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 0.1), ))&lt;/pre&gt;
&lt;p&gt;이제 신경망에 대한 학습을 시작합니다. 50번의 학습 단계(Epochs)를 반복합니다. 즉, 전체 데이터셋이 네트워크를 통해 50회 (10개 샘플 배치 처리)로 진행합니다.&lt;/p&gt;
&lt;pre id="3abf" name="3abf" class="graf graf--pre graf-after--p"&gt;mod.fit(train_iter, num_epoch=50)
INFO:root:Epoch[0] Train-accuracy=0.097500
INFO:root:Epoch[0] Time cost=0.085
INFO:root:Epoch[1] Train-accuracy=0.122500
INFO:root:Epoch[1] Time cost=0.074
INFO:root:Epoch[2] Train-accuracy=0.153750
INFO:root:Epoch[2] Time cost=0.087
INFO:root:Epoch[3] Train-accuracy=0.162500
INFO:root:Epoch[3] Time cost=0.082
INFO:root:Epoch[4] Train-accuracy=0.192500
INFO:root:Epoch[4] Time cost=0.094
INFO:root:Epoch[5] Train-accuracy=0.210000
INFO:root:Epoch[5] Time cost=0.108
INFO:root:Epoch[6] Train-accuracy=0.222500
INFO:root:Epoch[6] Time cost=0.104
INFO:root:Epoch[7] Train-accuracy=0.243750
INFO:root:Epoch[7] Time cost=0.110
INFO:root:Epoch[8] Train-accuracy=0.263750
INFO:root:Epoch[8] Time cost=0.101
INFO:root:Epoch[9] Train-accuracy=0.286250
INFO:root:Epoch[9] Time cost=0.097
INFO:root:Epoch[10] Train-accuracy=0.306250
INFO:root:Epoch[10] Time cost=0.100
...
INFO:root:Epoch[20] Train-accuracy=0.507500
...
INFO:root:Epoch[30] Train-accuracy=0.718750
...
INFO:root:Epoch[40] Train-accuracy=0.923750
...
INFO:root:Epoch[50] Train-accuracy=0.998750
INFO:root:Epoch[50] Time cost=0.077&lt;/pre&gt;
&lt;p&gt;훈련의 정확도는 빠르게 상승하여 50 단계 후에 99+%에 도달합니다. 우리가 만든 네트워크에서 학습 세트를 잘 수행을 했으며, 아주 좋은 결과를 보입니다. 이제 유효성 검사를 수행해 보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모델 유효성 검사하기&lt;/strong&gt;&lt;br /&gt;
이제 우리는 네트워크에 새로운 데이터 샘플, 즉 학습에 사용하지 않은 데이터 20%를 통해 유효성 검사를 해보겠습니다. 유효성 검사 샘플과 라벨을 사용하여 반복기를 먼저 만듭니다.&lt;/p&gt;
&lt;pre id="7809" name="7809" class="graf graf--pre graf-after--p"&gt;pred_iter = mx.io.NDArrayIter(data=X_valid,label=Y_valid, batch_size=batch)&lt;/pre&gt;
&lt;p&gt;그 다음으로 &lt;code&gt;Module.iter_predict()&lt;/code&gt; 함수를 사용하여 네트워크를 통해 20% 샘플을 실행합니다. 이제 예상 라벨과 실제 라벨을 비교합니다. 유효성 점수와 검증 정확도를 추적하는데, 이는 네트워크가 유효성 검증셋의 수행 결과가 잘 되었는지 알아 볼 수 있습니다.&lt;/p&gt;
&lt;pre id="4d78" name="4d78" class="graf graf--pre graf-after--p"&gt;pred_count = valid_count
correct_preds = total_correct_preds = 0

for preds, i_batch, batch in mod.iter_predict(pred_iter):
    label = batch.label[0].asnumpy().astype(int)
    pred_label = preds[0].asnumpy().argmax(axis=1)
    correct_preds = np.sum(pred_label==label)
    total_correct_preds = total_correct_preds + correct_preds

print('Validation accuracy: %2.2f' % (1.0*total_correct_preds/pred_count))&lt;/pre&gt;
&lt;p&gt;뭔가 막 잘되고 있는 것 같죠? &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f642.png" alt="&#x1f642;" class="wp-smiley" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;iter_predict()&lt;/code&gt;는 다음 값을 반환합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;i_batch&lt;/strong&gt; : Batch 번호&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;batch&lt;/strong&gt; : NDArrays 배열로 현재 배치를 저장하는 단일 NDArray가 있습니다. 현재 배치에서 10개의 데이터 샘플의 레이블을 찾는 데 사용합니다. 레이블을 numpy 배열 (10개 요소)에 저장합니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;preds&lt;/strong&gt; : NDArrays의 배열로 현재 배치에 대해 예측된 레이블을 저장하는 단일 NDArray가 있습니다. 각 샘플에 대해 10개의 범주 (10&amp;#215;10 매트릭스)에 대한 확률을 예측합니다. 따라서, &lt;code&gt;argmax()&lt;/code&gt; 함수를 사용하여 가장 높은 값, 즉 가장 가능성이 큰 카테고리의 인덱스를 찾습니다. 특히, pred_label은 현재 배치의 각 데이터 샘플에 대한 예측 카테고리를 보유하는 10 요소 배열입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;그런 다음 &lt;code&gt;Numpy.sum()&lt;/code&gt;을 사용하여 label과 pred_label의 동일한 값의 수를 비교합니다.&lt;/p&gt;
&lt;p&gt;마지막으로 검증 정확도를 계산하여 표시합니다.&lt;/p&gt;
&lt;pre id="de32" name="de32" class="graf graf--pre graf-after--p"&gt;Validation accuracy: 0.09&lt;/pre&gt;
&lt;p&gt;음&amp;#8230; 9%라니? 이것은 정말로 나쁜 결과네요! 하지만, 데이터 세트가 더 무작위였다면 결과는 더 좋을 것입니다.&lt;/p&gt;
&lt;p&gt;결론은 실제로 신경망 학습으로 많은 것을 배울 수 있지만, 우리의 결과처럼 데이터가 무의미하다면 아무 것도 예측할 수 없게됩니다. 쓰레기 입력, 쓰레기 출력(Garbage in, garbage out)이 있을 뿐이죠!&lt;/p&gt;
&lt;p&gt;하지만, MXNet으로 신경망 학습을 하는 아주 기본적인 소스 코드를 배울 수 있었습니다. 자신의 실제 데이터에 사용하려면 시간을 투자하십시오. 이것이 배우는 가장 빠른 방법입니다. 다음 글 부터는 실제 데이터를 활용하여 학습을 해보겠습니다.&lt;/p&gt;
&lt;p&gt;다음 글: &lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) &amp;#8211; 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) &amp;#8211; NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) &amp;#8211; Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) &amp;#8211; Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) &amp;#8211; 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) &amp;#8211; VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) &amp;#8211; Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;코드 전체 보기&lt;/strong&gt;&lt;br /&gt;
&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=yua0XOaxmEw:EvSZsbgBjig:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 26 Jul 2017 15:20:37 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (2) – Symbol API</title>
	<link>http://blog.creation.net/mxnet-part-2-symbol-api</link>
	<description>&lt;p&gt;&lt;a href="http://blog.creation.net/mxnet-api-part-1-ndarrays"&gt;1편&lt;/a&gt;에서는 몇 가지 MXNet 기본 사항을 살펴보고, NDArray API에 대해 알아보았습니다 (요약하면, NDArrays는 데이터, 매개 변수 등을 저장하는 장소입니다.) 이제 MXNet이 계산 단계를 정의하는 방법을 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;계산 단계? 코드를 말하는 건가요?&lt;/strong&gt;&lt;br /&gt;
좋은 질문이네요. 우리 모두가 &amp;#8220;프로그램 = 데이터 구조 + 코드&amp;#8221;라고 배웠으니까요. NDArrays가 데이터 구조라면 이제 코드를 추가하면 됩니다.&lt;/p&gt;
&lt;p&gt;일반적으로 모든 계산 단계를 명시적으로 정의하고 데이터에서 순차적으로 실행해야 합니다. 이를 &amp;#8220;&lt;a href="https://ko.wikipedia.org/wiki/%EB%AA%85%EB%A0%B9%ED%98%95_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D"&gt;명령형 프로그래밍(Imperatitive Programming)&lt;/a&gt;&amp;#8220;이라고 하며 Fortran, Pascal, C, C ++ 등이 작동하는 방식입니다.&lt;/p&gt;
&lt;p&gt;그러나, 신경망(Neural Nework)은 본질적으로 병렬 컴퓨팅을 지향합니다. 주어진 레이어 내부에서 모든 산출물을 동시에 계산할 수 있어야 합니다. 독립적인 레이어도 병렬로 실행될 수 있습니다. 따라서, 좋은 성능을 얻으려면 멀티 스레딩이나 비슷한 것을 사용하여 병렬 처리를 구현해야 합니다. 보통 어떻게 작동할지 가늠이 되실 텐데요. 코드를 잘 짰다하더라도 데이터 크기나 네트워크 레이아웃이 계속 바뀌면 얼마나 재사용성이 클 수가 없겠죠.&lt;/p&gt;
&lt;p&gt;다행히 대안이 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image aligncenter" src="https://cdn-images-1.medium.com/max/1600/1*V0ykrxGre3rGMet1I-eLVA.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Dataflow_programming"&gt;&lt;strong&gt;데이터 흐름 프로그래밍&lt;/strong&gt;&lt;/a&gt; (Dataflow programming)&lt;/p&gt;
&lt;p&gt;&amp;#8220;데이터 흐름 프로그래밍&amp;#8221;은 데이터를 그래프로 통해 흐르는 병렬 계산을 정의하는 방법입니다. 그래프는 동작 순서, 즉 순차적으로 실행할지 또는 병렬로 실행 가능할지 여부를 결정합니다. 각 동작은 블랙 박스입니다. 실제 동작을 지정하지 않고 입력과 출력만 정의합니다.&lt;/p&gt;
&lt;p&gt;이것은 컴퓨팅에 대한 횡설수설(?)로 들릴지 모르지만, 사실 이러한 모델이 신경망을 정의하는 데 정확히 필요합니다. 입력 데이터를 &amp;#8220;레이어&amp;#8221;라고 부르는 동작 순서에 따라 흐리게 하는데, 이들 레이어는 병렬적으로 실행하는 많은 명령을 가집니다.&lt;/p&gt;
&lt;p&gt;이야기는 그만하고 실제 사례를 살펴보죠. E를 (A * B) + (C * D)로 정의하는 방법입니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image aligncenter" src="https://cdn-images-1.medium.com/max/1600/1*h0M4n_9FPyriCwT-LjE0HQ.png" /&gt;&lt;/p&gt;
&lt;pre id="16ce" name="16ce" class="graf graf--pre graf-after--p"&gt;E = (A * B) + (C * D)&lt;/pre&gt;
&lt;p&gt;여기에서 A, B, C, D가 무엇인지는 큰 관계가 없습니다. 단지 &lt;strong&gt;기호(Symbol)&lt;/strong&gt;일 뿐입니다.&lt;/p&gt;
&lt;p&gt;입력 값 (정수, 벡터, 행렬 등)에 관계없이 위의 그래프는 &amp;#8220;+&amp;#8221; 및 &amp;#8220;*&amp;#8221; 연산을 정의한 경우, 출력 값을 계산하는 방식을 알려줍니다.&lt;/p&gt;
&lt;p&gt;이 그래프는 (A * B)와 (C * D)가 동시에 &lt;strong&gt;병렬로 계산&lt;/strong&gt;할 수 있다는 사실을 알 수 있습니다. 물론, MXNet은 이러한 정보를 최적화 목적으로 사용합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Symbol API&lt;/strong&gt;&lt;br /&gt;
이제 이를 왜 기호라고 부르는 지 자세히 알아보고자, 위의 예제를 코딩 할 수 있는지 살펴 보겠습니다.&lt;/p&gt;
&lt;pre id="16ce" name="16ce" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; import mxnet as mx
&amp;gt;&amp;gt;&amp;gt; a = mx.symbol.Variable('A')
&amp;gt;&amp;gt;&amp;gt; b = mx.symbol.Variable('B')
&amp;gt;&amp;gt;&amp;gt; c = mx.symbol.Variable('C')
&amp;gt;&amp;gt;&amp;gt; d = mx.symbol.Variable('D')
&amp;gt;&amp;gt;&amp;gt; e = (a*b)+(c*d)&lt;/pre&gt;
&lt;p&gt;어떤가요? 이러한 방식으로 a, b, c, d가 무엇인지 모른 채로 결과를 e에 지정할 수 있습니다.  좀 더 살펴 볼까요?&lt;/p&gt;
&lt;pre id="1cdf" name="1cdf" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; (a,b,c,d)
(&amp;lt;Symbol A&amp;gt;, &amp;lt;Symbol B&amp;gt;, &amp;lt;Symbol C&amp;gt;, &amp;lt;Symbol D&amp;gt;)
&amp;gt;&amp;gt;&amp;gt; e
&amp;lt;Symbol _plus1&amp;gt;
&amp;gt;&amp;gt;&amp;gt; type(e)
&amp;lt;class 'mxnet.symbol.Symbol'&amp;gt;&lt;/pre&gt;
&lt;p&gt;a, b, c, d는 명시적으로 선언 한 기호입니다. 근데 e는 다르죠. 기호이긴 하지만 &amp;#8216;+&amp;#8217;연산의 결과입니다. e에 대해 더 자세히 알아 보죠.&lt;/p&gt;
&lt;pre id="d1be" name="d1be" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; e.list_arguments()
['A', 'B', 'C', 'D']
&amp;gt;&amp;gt;&amp;gt; e.list_outputs()
['_plus1_output']
&amp;gt;&amp;gt;&amp;gt; e.get_internals().list_outputs()
['A', 'B', '_mul0_output', 'C', 'D', '_mul1_output', '_plus1_output']&lt;/pre&gt;
&lt;p&gt;위의 코드를 통해 알 수 있는 것은&amp;#8230;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e는 변수 a, b, c, d에 의존하며,&lt;/li&gt;
&lt;li&gt;e를 계산하는 연산은 이들의 합계이며,&lt;/li&gt;
&lt;li&gt;e는 실제로 (a * b) + (c * d)입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;우리는 물론 &amp;#8216;+&amp;#8217;및 &amp;#8216;*&amp;#8217;기호 보다 훨씬 많은 것을 할 수 있습니다. NDArrays와 마찬가지로 다양한 연산을 정의할 수 있구요. 자세한 것은 API 세부 정보를 살펴 보세요.&lt;/p&gt;
&lt;p&gt;이제 연산 단계를 정의하는 방법을 배웠으니, 실제 데이터에 적용하는 방법을 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NDArrays 와 Symbols 바인딩 하기&lt;br /&gt;
&lt;/strong&gt;NDArrays에 저장된 데이터에 기호(Symbols)로 정의한 연산 단계를 적용하려면 &amp;#8216;&lt;strong&gt;바인딩&lt;/strong&gt;(Binding)&amp;#8217;이라고 하는 연산, 즉 그래프의 각 입력 변수에 NDArray를 할당해야합니다.&lt;/p&gt;
&lt;p&gt;앞에서 본 사례를 계속 해서 &amp;#8216;A&amp;#8217;를 1, B를 2, C를 3으로, D를 4로 설정합니다. 그런 다음 하나의 정수를 포함하는 4 개의 NDArrays를 생성합니다.&lt;/p&gt;
&lt;pre id="edd7" name="edd7" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; a_data = mx.nd.array([1], dtype=np.int32)
&amp;gt;&amp;gt;&amp;gt; b_data = mx.nd.array([2], dtype=np.int32)
&amp;gt;&amp;gt;&amp;gt; c_data = mx.nd.array([3], dtype=np.int32)
&amp;gt;&amp;gt;&amp;gt; d_data = mx.nd.array([4], dtype=np.int32&lt;/pre&gt;
&lt;p&gt;다음으로 각 NDArray를 해당 Symbol에 바인딩합니다. 연산을 실행을 수행할 컨텍스트 (CPU 또는 GPU)를 선택해야 한다는 점 알아두세요.&lt;/p&gt;
&lt;pre id="f457" name="f457" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; executor=e.bind(mx.cpu(), {'A':a_data, 'B':b_data, 'C':c_data, 'D':d_data})
&amp;gt;&amp;gt;&amp;gt; executor
&amp;lt;mxnet.executor.Executor object at 0x10da6ec90&amp;gt;&lt;/pre&gt;
&lt;p&gt;c, d는 명시적으로 선언 한 기호입니다. 근데 e는 다르죠. 기호이긴 하지만 &amp;#8216;+&amp;#8217;연산의 결과입니다. e에 대해 더 자세히 알아 보죠.&lt;/p&gt;
&lt;p&gt;이제 결과를 얻기 위해 입력 데이터가 그래프를 따라 진행시키기 위해 forward () 함수를 사용할 차례입니다. 그래프에 여러 개의 출력이 있을 수 있으므로, NDArrays 배열을 반환합니다. 여기서 우리는 (1 * 2) + (3 * 4)와 동등한 값인 &amp;#8217;14&amp;#8217;값을 갖는 단일 출력을 갖습니다.&lt;/p&gt;
&lt;pre id="bf01" name="bf01" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; e_data = executor.forward()
&amp;gt;&amp;gt;&amp;gt; e_data
[&amp;lt;NDArray 1 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; e_data[0]
&amp;lt;NDArray 1 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;
&amp;gt;&amp;gt;&amp;gt; e_data[0].asnumpy()
array([14], dtype=int32)&lt;/pre&gt;
&lt;p&gt;0과 1 사이의 무작위 부동 소수점(random floats)으로 채워지는 4개의 1000 x 1000 행렬에 같은 그래프를 적용 해 봅시다. 우리가 할일은 새로운 입력 데이터를 정의하는 것 뿐이구요. 바인딩과 계산 방식은 동일합니다.&lt;/p&gt;
&lt;pre id="9e05" name="9e05" class="graf graf--pre graf-after--p"&gt;&amp;gt;&amp;gt;&amp;gt; a_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))
&amp;gt;&amp;gt;&amp;gt; b_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))
&amp;gt;&amp;gt;&amp;gt; c_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))
&amp;gt;&amp;gt;&amp;gt; d_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))&lt;/pre&gt;
&lt;pre id="3156" name="3156" class="graf graf--pre graf-after--pre"&gt;&amp;gt;&amp;gt;&amp;gt; executor=e.bind(mx.cpu(), {'A':a_data, 'B':b_data, 'C':c_data, 'D':d_data})
&amp;gt;&amp;gt;&amp;gt; e_data = executor.forward()&lt;/pre&gt;
&lt;pre id="02e0" name="02e0" class="graf graf--pre graf-after--pre"&gt;&amp;gt;&amp;gt;&amp;gt; e_data
[&amp;lt;NDArray 1000x1000 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; e_data[0]
&amp;lt;NDArray 1000x1000 &lt;a class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" href="http://twitter.com/cpu" target="_blank" rel="nofollow noopener"&gt;@cpu&lt;/a&gt;(0)&amp;gt;
&amp;gt;&amp;gt;&amp;gt; e_data[0].asnumpy()
array([[ 0.89252722,  0.46442914,  0.44864511, ...,  0.08874825,
         0.83029556,  1.15613985],
       [ 0.10265817,  0.22077513,  0.36850023, ...,  0.36564362,
         0.98767519,  0.57575727],
       [ 0.24852338,  0.6468209 ,  0.25207704, ...,  1.48333383,
         0.1183901 ,  0.70523977],
       ...,
       [ 0.85037285,  0.21420079,  1.21267629, ...,  0.35427764,
         0.43418071,  1.12958288],
       [ 0.14908466,  0.03095067,  0.19960476, ...,  1.13549757,
         0.22000578,  0.16202438],
       [ 0.47174677,  0.19318949,  0.05837669, ...,  0.06060726,
         1.01848066,  0.48173574]], dtype=float32)&lt;/pre&gt;
&lt;p&gt;멋지죠? 이러한 데이터와 연산 상호 간의 명확한 분리를 통해 우리는 &lt;strong&gt;두 가지 장점&lt;/strong&gt;을 모두 얻을 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;데이터는 우리가 잘 알고있는 &lt;strong&gt;명령형&lt;/strong&gt; 프로그래밍 모델을 사용하여 가져와 준비합니다. 우리는이 과정에서 외부 라이브러리를 사용할 수도 있습니다. (옛날 코드도 상관 없어요!)&lt;/li&gt;
&lt;li&gt;연산은 &lt;strong&gt;심볼릭&lt;/strong&gt; 프로그래밍 모델을 사용하여 수행됩니다. 이는 MXNet에서 코드와 데이터를 분리 할뿐만 아니라 그래프 최적화된 병렬 연산을 가능하게 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;오늘은 여기까지 살펴 보고, 다음에는 신경망에서 모델 학습을 하기 전에 마지막으로 Module API를 살펴볼 것입니다.&lt;/p&gt;
&lt;p&gt;다음 글: &lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) &amp;#8211; Module API&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) &amp;#8211; NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) &amp;#8211; Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) &amp;#8211; Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) &amp;#8211; 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) &amp;#8211; VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) &amp;#8211; Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=WQOKR2-YXwI:oDie8j_1_ro:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 26 Jul 2017 03:24:43 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>네이버 DISCO앱에 바라는 7가지</title>
	<link>http://channy.creation.net/blog/1148</link>
	<description>&lt;p&gt;순전히 20년지기 친구의 소개로 네이버가 만든 자칭(?) 인공 지능 주제별 뉴스 큐레이션 서비스인 DISCO를 사용해 보기 시작했습니다.&lt;/p&gt;
&lt;p&gt;(2달 전에 나온 &amp;#8211; &lt;a href="https://blog.naver.com/clova_ai/221013361020?"&gt;취향저격 콘텐츠 추천 앱 DISCO 출시!!&lt;/a&gt; 상태지만) 아직 서비스 초기라서 그런지 내부 직원들을 비롯해서 몇몇 콘텐츠 큐레이터들이 초기 테스트를 해 보면서 개선을 하고 있는 것 같습니다. 계속 쓸지 여부는 차치하고라도 신규 서비스 써 보는 것은 늘 즐거운 경험이라 간단하게 리뷰 남겨 봅니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;앱 설치 후 로그인은 현재로서 &amp;#8216;네이버 아이디&amp;#8217;로만 가능합니다. 우선 (그리 어려운 일도 아닌데) 페이스북, 트위터, 카카오 등의 소셜 로그인이 가능해야 합니다. 심지어 사내 앱인 라인 로그인도 안된다는 건 약간 이상한 일입니다. 네이버 사용자가 많다는 건 알겠지만, 뉴스 큐레이션 서비스에 절대적인 가치인 소셜 바이럴 효과를 감안하면 기존의 소셜 네트워크 인맥을 지렛대 삼아, 타 소셜망에 쉽게 공유가 가능해야 더 빠른 확산이 가능할 것입니다. 과거 Daum에서도 다음 로그인만 고집하다가 망한 앱들이 한둘이 아니었구요. 현재로서 네이버 아이디로는 소셜 바이럴에 할 수 있는게 아무것도 없습니다.&lt;/li&gt;
&lt;li&gt;여차저차 로그인을 하면, 맨 처음 주제별로 취향을 물어봅니다. 제가 못 찾았는지 입력할 수는 없고, 몇 개를 고르도록 하는 데 빠르게 취향을 찾을 수 있도록 해 주는 것도 좋지만 검색 창도 하나 두었으면 합니다. (끝나고 메인 화면에 가면 키워드 검색이 있긴 합니다.) 그냥 남들 취향에 맞추어야 할 느낌이 나네요. 하나의 키워드를 입력하면, 관심사 클러스터가 보이는게 좋을 듯 합니다.&lt;/li&gt;
&lt;li&gt;메인 화면 메뉴의 &amp;#8216;홈&amp;#8217;과 &amp;#8216;인기&amp;#8217;는 가장 많이 눌러 보는 버튼이 될텐데, 비슷한 취향 또는 뜨는 글이나 주제별 뉴스 링크 중심이 아닌 이상하게도 사람 중심의 페이스북 타임라인 처럼 보입니다. 그래서 초기 사용자들이 이게 페이스북과 뭐가 달라?라는 의문을 품을 것 같습니다. 물론 그 옆으로 주제별 키워드를 누르면 뉴스 링크 중심으로 &amp;#8216;몇 명이 소개한 글&amp;#8217; 형태로 목록이 나타나기는 합니다. 하지만, 그것도 &amp;#8216;전체&amp;#8217;나 &amp;#8216;인기순&amp;#8217; 등으로 정렬을 해야 보입니다. 각 메뉴에서 일관성 있게 뉴스 링크 중심의 클러스터링을 뚜렷하게 하고, 각 뉴스 링크에 대한 반응을 보는 것으로 앱의 성격을 뚜렷하게 하는 게 좋지 않을까요?&lt;/li&gt;
&lt;li&gt;&amp;#8216;좋아&amp;#8217;/&amp;#8217;싫어&amp;#8217; 어감 역시 페이스북의 그것과 비슷해서 사람들에게 또 혼란을 주는 것 같습니다. 마치 &amp;#8216;싫어&amp;#8217;를 누르면 안될 것 같은&amp;#8230; 어찌보면 페이스북 타임 라인에서 글 숨기기 같은 기능이기 때문에 오히려 &amp;#8216;추천&amp;#8217;/&amp;#8217;숨기기&amp;#8217; 등으로 바꾼다면, 오해 없이 명확하게 사용할 수 있을 듯 하네요. 앱 서비스팀에서 &amp;#8216;싫어&amp;#8217; 기능이 오해하지 말고 쓰라는 공지를 할 정도면 오해하는 사용자를 뭐라할게 아니라 직관적이지 않은 네이밍이 문제입니다.&lt;/li&gt;
&lt;li&gt;초기 서비스의 성패는 참여하는 사용자의 질과 커뮤니티에 좌우합니다. 지금은 내부 직원들 위주로 사용하고 있는 것 같고, 일부 IT 오피니언 리더들이 들어와 있는 그야말로 초기인데요. 좀 더 다양한 주제의 뉴스 생산자가 참여하도록 이에 대한 반대 급부도 제공하는 알뜰 살뜰한 기획력이 부족해 보입니다. 네이버가 이런 방면에 일가견이 있는 것으로 아는데, 조금 의외네요. 뉴스 큐레이션 서비스는 무조건 공유만 하는 헤비 링커 보다는 뉴스 생산자가 더 중요하다는 사실을 깨달아야 합니다. 인터넷 역사를 통틀어 오피니언 리더는 생산자이지 큐레이터는 아니었습니다. 큐레이터의 동기는 어뷰징 밖에 없습니다. 대신 생산자는 공유의 동기가 있습니다. 따라서, 생산자에게 자신의 블로그, 티스토리, 네이버 블로그, 브런치, 페이스북이 아닌 디스코를 써야 하는 동기 부여를 명확하게 해야 합니다.&lt;/li&gt;
&lt;li&gt;사람들이 디스크 외부로 공유를 하게 될 경우, &lt;a href="https://disco.me/articles/c034b4f04"&gt;아웃 링크된 웹 페이지&lt;/a&gt;에서 읽기와 링크 클릭 및 앱 다운로드 외에 할 수 있는 게 거의 없습니다. 적어도 링크처럼 보이는 &amp;#8216;키워드&amp;#8217;와 &amp;#8216;아이디&amp;#8217;를 클릭했을 때 최소한의 타임라인 정도만 보이더라도 앱을 설치하기 전에 어떤 앱인지 알고 설치할 동기를 부여해 줄 것입니다. 로그인한 사용자에게만 보여야 할 &amp;#8216;좋아&amp;#8217;/&amp;#8217;싫어&amp;#8217;가 링크 밑에 크게 박혀 있는 것 때문에 (뭐 이런 비인간적인 앱이 있나?)라고 앱 설치도 하기 전에 오해할 소지도 다분합니다. 저 같으면 싫어요 받을 것 같은 앱 설치 안할 듯. ㅎㅎ&lt;/li&gt;
&lt;li&gt;마지막으로 외부에 보이는 Disco 웹 사이트의 &lt;a href="http://disco.me/robots.txt"&gt;robots.txt&lt;/a&gt;도 Twitterbot에만 허가하고, 모든 검색 엔진이 불허되어 있는 것도 교정해야 할 사항 입니다. 구글은 그렇다 치고 네이버에서도 검색이 안되네요 ㅠㅠ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;우선 인공 지능 기반 뉴스 큐레이션 앱이라는 신선한 도전과 함께 만드는 사람이 사용자 피드백을 통한 개선을 지속적으로 세부적으로 하는 모습이 보이는 것 같아 만족스럽습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone wp-image-1149 size-full" src="http://channy.creation.net/data/channy/public_html/data/channy/2015/2017-07-26-naver-disco.png" alt="" width="1600" height="800" /&gt;&lt;/p&gt;
&lt;p&gt;다만, 유사한 성공 사례인 &lt;a href="http://biz.chosun.com/site/data/html_dir/2017/05/26/2017052601649.html"&gt;중국의 터우탸오(今日頭條)&lt;/a&gt;의 사례 처럼 거인의 어깨에 올라타서 최대한 지렛대 삼는 게 필요합니다. 기존 소셜 네트워크의 활동으로 사용자 취향 파악이 쉬울텐데 네이버 로그인으로 시작한 것은 의아한 대목입니다.&lt;/p&gt;
&lt;p&gt;초기 사용자의 관건인 뉴스 생산자의 참여 동기 부여가 필요하고, 앱 내에서 기획자가 의도한 부분을 사용자가 지속적으로 오해하고 있는 게 있다면 그건 빠르게 바꾸어야 합니다.&lt;/p&gt;
&lt;p&gt;네이버와 카카오 등 국내 주요 서비스 업체에서 어떤 형태든 다양한 시도와 도전이 이루어진다는 것은 좋은 일입니다. IT 시장에 긍정적인 시그널을 주고, 전반적인 다양한 변화가 가능하도록 마중물이 되기도 하니까요.&lt;/p&gt;
&lt;p&gt;카카오가 만든 브런치(Brunch)는 &lt;a href="https://brunch.co.kr/@channy/"&gt;글 하나만 쓰고 말았지만&lt;/a&gt;, 네이버가 만든 DISCO앱은 어떨까요?&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=s-4wR-_E_Eg:zDo95uINObY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 25 Jul 2017 16:23:41 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1148#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>MXNet 시작하기 (1) – NDArrays API</title>
	<link>http://blog.creation.net/mxnet-part-1-ndarrays-api</link>
	<description>&lt;p&gt;&lt;em&gt;이 시리즈는 AWS 테크 에반젤리스트인 Julien Simon이 연재한 &lt;a href="https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab?source=user_profile---------17-----------"&gt;MXNet 관련 글 모음&lt;/a&gt;의 번역 편집본으로 최근 각광 받고 있는 Deep Learning 라이브러리인 &lt;a href="http://mxnet.io/"&gt;Apache MXnet&lt;/a&gt;  을 개괄적으로 설명하려고 합니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;이 글은 간단한 코드를 이해하는 개발자라면 기계 학습과 인공 지능을 잘 알지 못하는 분이라도 쉽게 따라올 수 있도록 했습니다. 너무 겁먹지 않으셔도 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="size-medium wp-image-1141 aligncenter" src="http://channy.creation.net/data/channy/2017/05/20025115/mxnet-ndarray-1-580x326.png" alt="" width="580" height="326" /&gt;&lt;/p&gt;
&lt;p&gt;우선 몇 차례에 걸쳐 Apache MXNet의 주요 기능과 관련된 API를 예제 코드 위주로 살펴 보겠습니다.&lt;/p&gt;
&lt;p&gt;MXNet의 이론적 근거와 아키텍처에 대해 더 자세히 알고 싶다면 &amp;#8220;&lt;a href="https://arxiv.org/abs/1512.01274"&gt;MXNet : 이기종 분산 시스템을위한 유연하고 효율적인 기계 학습 라이브러리&lt;/a&gt;&amp;#8220;라는 논문를 읽어 보시길 추천합니다. 하지만, 어려운 논문을 살펴 보는 건 이 시리즈의 목적이 아니고, 개발자를 위해 기존 개념을 다루면서도 코드를 직접 사용하여 보다 이해하기 쉽게 ​​접근 할 예정입니다.&lt;/p&gt;
&lt;p&gt;여러분은 천천히 따라오면서 필요한 만큼의 최소한의 수학과 어려운 용어를 조금씩 익힐 수 있습니다. 저도 여러분과 같이 아직 전문가가 아니며, 우리가 만드는 애플리케이션에 인공 지능 기능을  어떻게 추가 할 수 있는지에 대해서만 집중하려고 합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MXNet 설치 및 실행하기&lt;/strong&gt;&lt;br /&gt;
우선 먼저 MXNet을 설치하십시오. &lt;a href="http://mxnet.io/get_started/index.html"&gt;여기&lt;/a&gt;에서 공식 가이드를 찾을 수 있지만, 여기에서 간단히 설치할 수 있는 방법을 알려드립니다.&lt;/p&gt;
&lt;p&gt;MXNet의 주요 기능 중 하나는 CPU와 GPU에서 동일하게 실행될 수 있다는 것입니다. 즉, 컴퓨터에 NVIDIA GPU (MacBook과 동일)가 없더라도 나중에 AWS에서 제공하는 GPU 기반 가상 클라우드 인스턴스에서 사용 가능한 MXNet 코드를 작성하고 실행할 수 있습니다.&lt;/p&gt;
&lt;p&gt;여러분 PC에 GPU가 있으면 &lt;a href="https://developer.nvidia.com/cuda-toolkit"&gt;CUDA&lt;/a&gt; 및 &lt;a href="https://developer.nvidia.com/cudnn"&gt;cuDNN&lt;/a&gt;툴킷을 설치해야합니다. 그런데, 이 도구를 설치하는 것이 초보자 입장에서 매우 까다롭고 MXNet 바이너리와 Nvidea 도구 사이의 비 호환성으로 인해 설정이 깨져서 작업하기가 어려울 수 있습니다.&lt;/p&gt;
&lt;p&gt;이러한 이유로 MXNet 웹 사이트에서 제공되는 Docker 이미지 (CPU 환경 용, GPU 환경 용) (&lt;a href="https://github.com/NVIDIA/nvidia-docker"&gt;nvidia-docker&lt;/a&gt;가 필요함)를 사용하는 것이 좋습니다. 이 이미지에는 필요한 모든 것을 사전 설치하여, 몇 분 안에 시작할 수 있습니다.&lt;/p&gt;
&lt;pre&gt;sudo -H pip install mxnet --upgrade
 python
 &amp;gt;&amp;gt;&amp;gt; import mxnet as mx
 &amp;gt;&amp;gt;&amp;gt; mx.__version__
 '0.9.3a3'&lt;/pre&gt;
&lt;p&gt;Docker 이미지는 &amp;#8216;pip&amp;#8217;를 통해 사용할 수 있는 Python 패키지보다 최신 버전 인 것 같습니다.&lt;/p&gt;
&lt;pre&gt;docker run -it  mxnet/python
 root@88a5fe9c8def:/# python
 &amp;gt;&amp;gt;&amp;gt; import mxnet as mx
 &amp;gt;&amp;gt;&amp;gt; mx.__version__
 '0.9.5'&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;AWS에서 MXNet 실행하기&lt;br /&gt;
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AWS는 Linux 및 Ubuntu에서 사용할 수있는 Deep Learning AMI(&lt;a href="https://aws.amazon.com/marketplace/pp/B01M0AXXQB"&gt;Amazon Liunx Deep Learning AMI&lt;/a&gt; 및 &lt;a href="https://aws.amazon.com/marketplace/pp/B06VSPXKDX"&gt;Ubuntu Deep Learnin AMI&lt;/a&gt;)를 제공합니다. 본 AMI에는 모든 Nvidia 도구와 그 밖의 많은 딥러닝 프레임 워크 (MXNet 포함)가 사전 설치되어 제공됩니다.&lt;/p&gt;
&lt;pre&gt;====================================================================
 __|  __|_  )
 _|  (     /   Deep Learning AMI for Amazon Linux
 ___|\___|___|
 ====================================================================

[ec2-user@ip-172-31-42-173 ~]$ nvidia-smi -L
 GPU 0: GRID K520 (UUID: GPU-d470337d-b59b-ca2a-fe6d-718f0faf2153)

[ec2-user@ip-172-31-42-173 ~]$ python
 &amp;gt;&amp;gt;&amp;gt; import mxnet as mx
 &amp;gt;&amp;gt;&amp;gt; mx.__version__
 '0.9.3'&lt;/pre&gt;
&lt;p&gt;본 AMI는 일반 Amazon EC2 인스턴스 또는 GPU 인스턴스에서 실행할 수 있습니다. 컴퓨터에 Nvidia GPU가없는 경우 나중에 네트워크 교육을 시작할 때 유용 할 수 있습니다. 가장 저렴한 옵션은 g2.2xlarge 인스턴스를 시간당 0.65 달러로 사용하는 것입니다.&lt;/p&gt;
&lt;p&gt;우선은 공부를 위한 것이니 만큼 당분간 CPU 만 있으면됩니다! 이제 시작해 보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NDArrays가 중요한 이유&lt;/strong&gt;&lt;br /&gt;
MXNet API에서 살펴볼 첫 번째 부분은 NDArrays입니다.  NDArray는 동일한 유형 및 크기 항목 (32 비트 부동 소수점, 32 비트 정수 등)을 포함하는 n차원 배열입니다. 이 배열이 중요한 이유는  딥러닝 학습 및 실행에 필요한 많은 수학 연산을 통한 데이터 저장을 할 수 있는 다차원 배열이기 때문입니다. 입력 데이터, 가중치 및 출력 데이터는 벡터 및 행렬에 저장되므로 이러한 종류의 데이터 구조를 사용해야 합니다.&lt;/p&gt;
&lt;p&gt;간단한 예를 들어 봅시다. 아래 이미지는 손으로 쓴 &amp;#8216;8&amp;#8217;이라는 이미지를  18&amp;#215;18 픽셀로 나누었습니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="https://cdn-images-1.medium.com/max/1600/1*D6pp5hTfUl8FmzYwJ3N8LQ.png" /&gt;&lt;br /&gt;
&lt;small&gt;숫자 8, 18&amp;#215;18 픽셀&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;이 이미지는 18&amp;#215;18 매트릭스와 동일한 이미지로서 각 셀은 해당 픽셀의 그레이 스케일 값을 저장합니다. 흰색은 &amp;#8216;0&amp;#8217;, 검정은 &amp;#8216;255&amp;#8217;, 그레이 스케일은 &amp;#8216;254&amp;#8217;입니다. 이 행렬 표현은 신경망을 통해 0에서 9까지 숫자를 범주화하여 학습을 진행합니다.&lt;/p&gt;
&lt;p&gt;&lt;img class="progressiveMedia-image js-progressiveMedia-image aligncenter" src="https://cdn-images-1.medium.com/max/1600/1*Ct7GN4a5gqONNUFV_qMu_A.png" /&gt;&lt;br /&gt;
&lt;small&gt;그레이 스케일 값을 유지하는 18&amp;#215;18 매트릭스&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;이제 그레이 스케일 이미지 대신 컬러 이미지를 사용한다고 상상해봅시다. 각 이미지는 각 색상에 하나씩 3 개의 행렬을 사용하기 때문에 입력 데이터는 이제 약간 더 복잡해졌습니다.&lt;/p&gt;
&lt;p&gt;한 단계 더 들어가서, 자동차 자율 주행을 위해 실시간 이미지 인식을 하고 있다고 가정 해보죠. 카메라에 찍히는 사물 데이터를 인지하기 위해 초당 30 프레임의 1000 x 1000 픽셀 RGB 이미지를 사용하고 있습니다. 매초마다 90 개의 1000 x 1000 매트릭스 (30 프레임 x 3 색)를 처리해야 합니다. 각 픽셀을 32 비트 값으로 표현하면 90 x 1000 x 1000 x 4 바이트, 그 이상 또는 이하의 343 메가 바이트가됩니다. 여러 대의 카메라를 가지고 있다면 더 복잡해 집니다.&lt;/p&gt;
&lt;p&gt;이러한 정도의 데이터 크기가 신경망을 통해 전달하는 사이즈입니다. 최대 성능 (즉, 최소 대기 시간)을 위해 GPU는 이미지를 하나씩 처리하지 않고 일괄 적으로 처리합니다. 배치(Batch) 크기를 8로 설정하면 신경망은 입력 데이터를 1000 x 1000 x 24의 단위(chunk)로 처리합니다. 즉 3 개의 색상으로 8 1000 x 1000 이미지를 보유하는 3 차원 배열을 처리합니다.&lt;/p&gt;
&lt;p&gt;요점:  NDArrays는 신경망 학습에 있어서 기본이고, 모든 데이터를 저장하는 방법이라 첫번째로 꼭 알아두셔야 합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NDArray API 사용해 보기&lt;br /&gt;
&lt;/strong&gt;&lt;span id="result_box" class="" lang="ko" tabindex="-1"&gt;이제 실제로 NDArray가 어떻게 작동하는지 살펴 보겠습니다. 여러분이 Python 기반 &lt;a href="http://www.numpy.org/"&gt;numpy&lt;/a&gt; 라이브러리를 사용해 보셨다면, NDArrays는 이와 매우 유사하며 &lt;a href="http://mxnet.io/api/python/ndarray.html"&gt;NDArray API&lt;/a&gt;의 대부분을 바로 이해하실 수 있습니다.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;하지만 걱정하지 마세요! 기초부터 시작하겠습니다. &lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/1f642.png" alt="&#x1f642;" class="wp-smiley" /&gt;&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; a = mx.nd.array([[1,2,3], [4,5,6]])
 &amp;gt;&amp;gt;&amp;gt; a.size
 6
 &amp;gt;&amp;gt;&amp;gt; a.shape
 (2L, 3L)
 &amp;gt;&amp;gt;&amp;gt; a.dtype
 &amp;lt;type 'numpy.float32'&amp;gt;&lt;/pre&gt;
&lt;p&gt;기본적으로 NDArray에는 32 비트 부동 소수점이 있지만 사용자 정의 할 수 있습니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
 &amp;gt;&amp;gt;&amp;gt; b = mx.nd.array([[1,2,3], [2,3,4]], dtype=np.int32)
 &amp;gt;&amp;gt;&amp;gt; b.dtype&lt;/pre&gt;
&lt;p&gt;NDArray 출력도 간단합니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; b.asnumpy()
 array([[1, 2, 3],
 [2, 3, 4]], dtype=int32)&lt;/pre&gt;
&lt;p&gt;원하는 수학 연산자를 모두 사용할 수 있습니다. 행렬 곱셈을 시도해 봅시다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; a = mx.nd.array([[1,2,3], [4,5,6]])
 &amp;gt;&amp;gt;&amp;gt; b = a*a
 &amp;gt;&amp;gt;&amp;gt; b.asnumpy()
 array([[  1.,   4.,   9.],
 [ 16.,  25.,  36.]], dtype=float32)&lt;/pre&gt;
&lt;p&gt;행렬 곱셈 (dot product)은 다음과 같습니다.&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; a = mx.nd.array([[1,2,3], [4,5,6]])
 &amp;gt;&amp;gt;&amp;gt; a.shape
 (2L, 3L)
 &amp;gt;&amp;gt;&amp;gt; a.asnumpy()
 array([[ 1.,  2.,  3.],
 [ 4.,  5.,  6.]], dtype=float32)

&amp;gt;&amp;gt;&amp;gt; b = a.T
 &amp;gt;&amp;gt;&amp;gt; b.shape
 (3L, 2L)
 &amp;gt;&amp;gt;&amp;gt; b.asnumpy()
 array([[ 1.,  4.],
 [ 2.,  5.],
 [ 3.,  6.]], dtype=float32)

&amp;gt;&amp;gt;&amp;gt; c = mx.nd.dot(a,b)
 &amp;gt;&amp;gt;&amp;gt; c.shape
 (2L, 2L)
 &amp;gt;&amp;gt;&amp;gt; c.asnumpy()
 array([[ 14.,  32.],
 [ 32.,  77.]], dtype=float32)&lt;/pre&gt;
&lt;p&gt;조금 더 복잡한 것을 시도해 봅시다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;균일 분포로 1000&amp;#215;1000 매트릭스를 초기화하고, GPU #0에 저장합니다 (여기서 g2 인스턴스를 사용합니다.)&lt;/li&gt;
&lt;li&gt;GPU #0에서도 일반 분포 (평균 1과 표준 편차 2)를 갖는 또 다른 1000&amp;#215;1000 매트릭스를 초기화합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; c = mx.nd.uniform(low=0, high=1, shape=(1000,1000), ctx="gpu(0)")
 &amp;gt;&amp;gt;&amp;gt; d = mx.nd.normal(loc=1, scale=2, shape=(1000,1000), ctx="gpu(0)")
 &amp;gt;&amp;gt;&amp;gt; e = mx.nd.dot(c,d)&lt;/pre&gt;
&lt;p&gt;MXNet은 CPU와 GPU에서 동일하게 실행될 수 있습니다. 실례로 &amp;#8220;gpu(0)&amp;#8221;을 &amp;#8220;cpu(0)&amp;#8221;로 바꾸면 행렬 곱셈이 CPU에서 실행됩니다.&lt;/p&gt;
&lt;p&gt;정말 간단하게 NDArrays를 알아보았습니다. 신경망 구축을 위한 상위 기능(FullyConnected 등)은 나중에 실제 신경망을 알아볼 때 더 배워 보도록 하겠습니다. 다음 글에서는 데이터 흐름을 정의 할 수있는 Symbol API를 살펴볼 것입니다.&lt;/p&gt;
&lt;p&gt;다음 글: &lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) &amp;#8211; Symbol API&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;연재 순서&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="postList"&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-1-ndarrays-api"&gt;MXNet 시작하기 (1) &amp;#8211; NDArrays API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-2-symbol-api"&gt;MXNet 시작하기 (2) &amp;#8211; Symbol API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-3-module-api"&gt;MXNet 시작하기 (3) &amp;#8211; Module API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-4-inception-v3"&gt;MXNet 시작하기 (4) &amp;#8211; 이미지 분류를 위한 학습 모델 사용하기 (Inception v3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152"&gt;MXNet 시작하기 (5) &amp;#8211; VGG16 및 ResNet-152 학습 모델 사용하기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.creation.net/mxnet-part-6-realtime-object-detection"&gt;MXNet 시작하기 (6) &amp;#8211; Raspberry Pi에서 실시간 객체 분석 하기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class="alignright" src="http://blog.creation.net/data/tisotry/2017/08/01043846/with-julien-simon-aws-tech-evangelist.jpg" alt="" width="140" /&gt; &lt;small&gt;&lt;em&gt;Julien Simon은 파리에서 개발자들의 클라우드 활용을 돕는 AWS Principal Technical Evangelist로 일하고 있습니다. 선마이크로시스템에서 임베디드 개발자로 시작하여, 6천5백만명이 사용하는 Viadeo라는 스타트업을 비롯 Aldebaran Robotics, Criteo, Pixmania 등의 회사에서 CTO 역할을 수행하였습니다. 본 블로그에서는 Julien이 최근 기고하는 &lt;a href="https://medium.com/@julsimon/"&gt;MXNet 관련 블로그 글&lt;/a&gt;을 한국어로 소개해 드릴 예정입니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=QCI6qgjC9M4:uaiQk7FJR0s:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 25 Jul 2017 08:42:44 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>인공 지능(AI) 시대의 현실적 조언</title>
	<link>http://channy.creation.net/blog/1145</link>
	<description>&lt;p&gt;&lt;em&gt;우선 저는 인공 지능 전문가(?)가 아닙니다. 다만, 이 글은 IT 기술로 인해 세상이 바뀌기 시작할 때 그 주변에서 일어나는 패턴을 몇번 경험해 본 결과, 각 분야에 있는 분들이 현실적으로 택할 수 있는 이야기를 간단하게 풀어 보려고 합니다. 이 내용을 진지하게 받아들일지 말지는 전적으로 여러분의 선택에 달려 있습니다.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;일반 개발자&lt;/strong&gt;&lt;br /&gt;
개발자의 숙명처럼 새로운 기술이 나오니, 당연히 알아보고 배워야 합니다. &amp;#8216;늦었다고 생각할 때가 가장 늦었다&amp;#8217;라는 말이 있죠? 바로 그렇습니다. 지금 흥행 단어(Buzzword)로 뜨고 있는 기술은 이미 4-5년전에 완성되고, 세상을 바꿀 준비가 되어 있었을 확률이 높습니다. 따라서, 지금은 많이 늦었습니다.&lt;/p&gt;
&lt;p&gt;하지만, 실제 세상을 바꾸는 IT 기술은 진입 장벽이 높고, 특정 기업이 모든 노하우를 차지하는 경우는 드뭅니다. 오히려 누구나 접근 가능(easy to access)하면서 많은 사람이 참여함으로서 기술이 더 똑똑해 지고 쉬워지는 경향이 있죠. (사과 판매 회사는 예외로 두죠.)&lt;/p&gt;
&lt;p&gt;따라서, 최대한 새로운 기술의 흐름을 찾고 테스트해보고 공부해보려는 노력을 경주해야 합니다. 지금 보다 더 쉬워질 때까지 기다리면 그때는 정말 늦는 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. 김성훈 교수의 모두를 위한 딥러닝 &lt;a href="http://hunkim.github.io/ml/"&gt;http://hunkim.github.io/ml/&lt;/a&gt;&lt;br /&gt;
2. 골빈해커 텐서플로 코딩 튜토리얼 &lt;a href="https://github.com/golbin/TensorFlow-Tutorials"&gt;https://github.com/golbin/TensorFlow-Tutorials&lt;/a&gt;&lt;br /&gt;
3. 텐서플로 커뮤니티 인기 글 &lt;a href="http://fbsight.com/c/TF-KR/l/top"&gt;http://fbsight.com/c/TF-KR/l/top&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;스타트업&lt;/strong&gt;&lt;br /&gt;
AI 관련 사업 모델과 스타트업이 계속 나타날 것입니다. 이미 많은 벤처캐피털들이 AI 분야 투자를 진행하고 있으니, 조만간 유니콘에 버금가는 투자나 거품 이야기도 나올 것입니다. (특히, 한국에 많을 것으로 예상되는) 무늬만 인공 지능이나 기존 솔루션 업체나 데이터 분석하던 업체가 AI로 탈바꿈하는 경우도 나오겠죠.&lt;/p&gt;
&lt;p&gt;하지만, 스타트업은 하나의 문제를 정의하고 해결해야 합니다. 이미 대형 IT 기업들이 뛰어든 뻔한 아이디어 보다는 의료, 농업, 제조 등 산업 현장에서 일어나는 문제를 해결하는 버티컬 스타트업이 성공(인수) 확률이 높습니다. 제가 스타트업을 만나면 늘 하는 이야기지만, 일반 사용자가 서비스를 이해할 수 있고 유용하게 쓸 수 있는 기능을 만들어 피드백을 받아 개선해 나가면서 알파 유저나 기업 시장으로 가는 것이 좋습니다. 그리고, 현재의 법 테두리 안에서는 사업성이 모호한 오히려 불법에 가까운 회색 지대에 있는 아이템을 권장합니다.&lt;/p&gt;
&lt;p&gt;그런 문제를 푸는데 있어서 곁가지 인공 지능 기술은 그냥 클라우드 업체들이나 AI 기반 API 서비스 회사의 것을 그대로 사용하는 게 좋습니다. 이른바 텍스트 분석, 머신 러닝, 컴퓨터 비전, 딥러닝에 해당하는 생각할 수 있는 모든 바퀴는 새로 만들 필요는 없습니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. Vertical AI Startups: Solving Industry-specific Problems &lt;a href="http://www.bradfordcross.com/blog/2017/6/13/vertical-ai-startups-solving-industry-specific-problems-by-combining-ai-and-subject-matter-expertise"&gt;http://www.bradfordcross.com/blog/2017/6/13/vertical-ai-startups-solving-industry-specific-problems-by-combining-ai-and-subject-matter-expertise&lt;/a&gt;&lt;br /&gt;
2. Algorithmia의 AI 마이크로 서비스 &lt;a href="https://algorithmia.com"&gt;https://algorithmia.com&lt;/a&gt;&lt;br /&gt;
3. Amazon AI 서비스 &lt;a href="https://aws.amazon.com/ko/amazon-ai/"&gt;https://aws.amazon.com/ko/amazon-ai/&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;참고: Algorithmia에 가입하실 때, promo code에 &amp;#8216;reinvent16&amp;#8217;을 넣으면 50만 크레딧을 부여 받아 테스트 가능합니다. 예를 들어, 흑백 사진 한 장을 컬러로 바꾸는데 약 28 크레딧 정도가 필요합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;인터넷 기업&lt;/strong&gt;&lt;br /&gt;
그나마 연구 개발 능력이 있고, 기술 내재화를 중요시 하는 대형 IT 기업에 근무하면서 글로벌 빅 플레이어의 기술 팔로우 혹은 제품 및 서비스 카피캣 일을 하는 경우가 (지금도 많고 앞으로 더) 많아질 겁니다. 개발자라면 좋은 연봉 받으며 좋은 경험은 될 것입니다. 하지만, 세상을 바꿀 수 있는 가능성은 무척 낮고, 그 가운데 스트레스만 많아질 수 있습니다.&lt;/p&gt;
&lt;p&gt;기술과 조직 개편으로 점철되는 강제적 변화(?) 따라가기 보다는 오히려 미래에 중요해 질 마이너 기술을 찾아보세요. 현재 회자되지 않거나 누구도 관심을 가지지 않는 것이 오히려 기업의 변화를 바꿀 수 있습니다. 하지만, 그렇게 되기까지 생존 능력과 인내도 필요하죠.&lt;/p&gt;
&lt;p&gt;앞으로 인공 지능은 서비스化 될 것입니다. 따라서, 거기에 매달리기 보다는 도구로서 사용하고 새로운 마이너 기술에 역량을 쏟을 때 입니다. AI시대에는 AI를 버려야 합니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;u&gt;[현실적 가이드]&lt;/u&gt;&lt;br /&gt;
1. 2030년 미래에 대한 8가지 예측 &lt;a href="https://www.weforum.org/agenda/2016/11/8-predictions-for-the-world-in-2030/"&gt;https://www.weforum.org/agenda/2016/11/8-predictions-for-the-world-in-2030/ &lt;/a&gt;&lt;br /&gt;
2. 2030년에 정상 궤도에 오를 21가지 기술 &lt;a href="http://www.businessinsider.com/21-technology-tipping-points-we-will-reach-by-2030-2015-11"&gt;http://www.businessinsider.com/21-technology-tipping-points-we-will-reach-by-2030-2015-11 &lt;/a&gt;&lt;br /&gt;
3. 2030년까지 이끌 10대 미래 기술은? &lt;a href="http://www.econovill.com/news/articleView.html?idxno=307972"&gt;http://www.econovill.com/news/articleView.html?idxno=307972&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;엔터프라이즈&lt;/strong&gt;&lt;br /&gt;
한마디로 엔터프라이즈 기업에서 기술 내재화는 미신 혹은 신화에 가깝습니다. 그냥 스타트업이 하는 속도로 따라가세요!&lt;/p&gt;
&lt;p&gt;&amp;#8212;&lt;/p&gt;
&lt;p&gt;인공 지능 붐도 언젠가 잠잠해 질 겁니다. 하지만, 그때는 AI가 필요 없는 것이 아니라 필수가 되는 시대가 되겠죠.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" src="http://channy.creation.net/wp/data/channy/2017/artificial-intelligence-2228610__480.jpg" alt="" /&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=YJW-JqBoZbE:8b3afGPCCvw:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 04 Jul 2017 22:30:02 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1145#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>사람 냄새 나는 대통령의 탄생</title>
	<link>http://channy.creation.net/blog/1136</link>
	<description>&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2017/munjaein-president-1.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;드디어 문재인 대통령이 당선되었습니다.&lt;/p&gt;
&lt;p&gt;과거 많은 사람들이 품위있고 고상하고 위엄있는 대통령을 바라지만, 그 실상은 권위적이고 불통에다 편을 갈라치는 소수가 지배하는 권력이었다는 점을 목도했습니다.&lt;/p&gt;
&lt;p&gt;다시 한번 깨닫습니다. 소탈하고 탈권위주의 시대의 대통령이 이 시대에 부합하는 진정한 대통령상이라는 사실을&amp;#8230; 오늘 다시 한번 그런 시대를 맞게 되어 기쁩니다.&lt;/p&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2017/munjaein-president-2.png" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://blog.creation.net/358"&gt;8년전 슬픈 로그아웃&lt;/a&gt;을 했지만 인터넷 소통 문화의 발전과 이를 통한 직접 민주주의에 도전했던 소시민 대통령의 친구분으로서, 정치를 하기 싫었지만 그 친구 때문에 역사의 장에 다시 나오게 되신 대통령으로서 새로운 역사를 쓰시길 간절히 바래봅니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;나에게 있어서도 대외 활동, 특히 정부와 관련 지어 일하는데 있어 그분의 영향이 적지 않았다. 대통령은 진대제 정통부 장관을 지명했고 이에 발맞추어 소프트웨어 진흥원에도 첫 민선 고현진 원장이 취임하면서 공개 SW와 웹 표준에 대한 정부 내 지원을 시작했었기 때문이다. &lt;a href="http://blog.creation.net/358"&gt;웹2.0 대통령의 로그아웃&lt;/a&gt; 중&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;개인적으로 지난 9년간 과거 정부의 어떠한 자문 요청에도 응하지 않았지만, 앞으로 &lt;a href="http://blog.creation.net/536"&gt;IT실험실에서 글로벌로&lt;/a&gt;의 변화를 위해 새로운 민주 정부에서 도움이 필요하다면 언제든 기꺼이 도울 생각입니다.&lt;/p&gt;
&lt;p&gt;꼭 성공하시기 바랍니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=0kCVKcDbgMY:xPV7zYorWMA:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Wed, 10 May 2017 00:25:05 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1136#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>[ZDNet 칼럼] 서버리스 인공지능 시대를 열다</title>
	<link>http://blog.creation.net/zdnet-serverless-ai</link>
	<description>&lt;p&gt;아마존웹서비스(AWS)는 기존 서버 기반의 애플리케이션을 함수 단위로 쪼개서 실행할 수 있는 AWS 람다(Lambda) 서비스를 2014년에 출시하면서, 서버리스(Serverless) 아키텍처 시대를 열었다. 초기에는 AWS 자원의 변경이 일어나는 이벤트에 따라 간단한 동작을 수행하려는 요구 사항에 맞추어 출발했다. 하지만, 애플리케이션을 배포 및 운영해야 할 서버의 존재가 없어짐에 따라, 다양한 아이디어가 쏟아져 나왔다 아마존API 게이트웨이 서비스와 AWS가 원래 (서버 관리 필요 없이) 제공하던 아마존 S3, 다이나모DB 같은 빌딩 블록을 이용하여 서버리스 애플리케이션 모델을 구성할 수 있게 되었다.&lt;/p&gt;
&lt;p&gt;AWS는 클라우드 기반 기계 학습과 딥 러닝 기술을 통해 다양한 사용 사례 및 요건을 해결할 수 있는 AI 서비스 제품군을 제공하고 있다. 앞서 살펴 본, 아마존 머신러닝, 딥러닝 AMI 및 딥러닝 클러스터 구성 템플릿, 클라우드에 최적화된 MXNet 같은 오픈소스 딥러닝 엔진 등이 해당된다.&lt;/p&gt;
&lt;p&gt;AWS는 이러한 플랫폼을 이용한 인공지능에 대한 맞춤형 솔루션이 필요하지 않지만, 자신의 애플리케이션에 인공지능 기능을 활용하고자 하는 개발자라면 누구나 사용할 수 있도록 완전 관리형(Full Managed) 서비스를 제공한다. 개발자는 아마존 AI 서비스를 통해 자연어 이해(NLU), 자동 음성 인식(ASR), 비주얼 검색 및 이미지 인식, 텍스트 음성 변환(TTS), 기계 학습(ML) 기술을 이용할 수 있다. 이를 통해 개발자들은 API 호출로 스마트한 앱을 빠르게 개발하고 배포하여, 사용자 경험 향상과 비즈니스 가치를 실현할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 서버리스 AI 서비스를 위한 3종 세트&lt;/strong&gt;​&lt;/p&gt;
&lt;p&gt;아마존 렉스(Lex)는 음성 및 텍스트를 사용해 대화형 인터페이스를 모든 애플리케이션에 구현하는 서비스다. 아마존 렉스는 음성을 텍스트로 변환하는 자동 음성 인식(ASR)과 텍스트의 의도를 이해하는 자연어 처리(NLU) 등과 같은 첨단 딥 러닝 기능을 함께 제공한다. 이를 통해 사용자 경험을 증진하고 생생한 대화형 인터페이스를 제공하는 애플리케이션을 구축할 수 있다. 현재 아마존 알렉사에 탑재되는 것과 동일한 딥 러닝 기술을 사용한 아마존 렉스는정교한 자연어 대화봇(챗봇)을 빠르고 쉽게 개발할 수 있으며, AWS 람다와 아마존 다이나모DB 등 서버리스 빌딩 블록을 통해 개발된 애플리케이션을 페이스북 채팅에도 바로 적용할 수 있다.&lt;/p&gt;
&lt;p&gt;미국 오하이오주 공공 의료 서비스인 오하이오헬스(OhioHealth)는 &amp;#8220;아마존 렉스를 활용하여 환자에게 더 나은 정보를 적시에 제공하고 있다. 이 혁신적인 애플리케이션은 고객에게 향상된 환경을 제공하는 데 큰 도움이 되고 있다&amp;#8221;고 밝혔다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/04/03/yong2_2IbjEeOgptenMx.jpg" alt="오하이오헬스에서 개발한 아마존 렉스와 기반 서버리스 의료 상담 시스템(출처: AWS 홈페이지)" width="550px" height="230px" /&gt;&lt;br /&gt;
오하이오헬스에서 개발한 아마존 렉스와 기반 서버리스 의료 상담 시스템(출처: AWS 홈페이지)&lt;/div&gt;
&lt;p&gt;아마존 리코그니션(Rekognition)은 애플리케이션에 이미지 분석을 쉽게 추가할 수 있는 서비스다. 이미지 내 피사체, 장면, 얼굴을 분석하거나, 얼굴을 검색하거나 비교할 수 있다. 매일 수십억 개가 업로드되는 프라임 포토(Prime Photos)의 이미지들을 분석하기 위해 아마존의 컴퓨터 비전 과학자들이 개발한 이 서비스는 성능이 검증되었을 뿐만 아니라 확장성까지 뛰어난 딥 러닝 기술을 기반으로 하고 있다. 이 서비스는 심층 신경망 모델을 사용하여 이미지 속의 수많은 객체와 장면을 탐지하고 태깅하며, 개발자는 아마존에서 새로운 인식 라벨과 안면 인식 기능을 지속적으로 업데이트 받을 수 있다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/04/03/yong2_ubKM3pAQyprx03.jpg" alt="아마존 리코그니션 및 폴리를 통한 이미지 분석 및 읽어주기 앱 사용 예제(출처: AWS 홈페이지)" width="550px" height="325px" /&gt;&lt;br /&gt;
아마존 리코그니션 및 폴리를 통한 이미지 분석 및 읽어주기 앱 사용 예제(출처: AWS 홈페이지)&lt;/div&gt;
&lt;p&gt;아마존 폴리(Polly)는 텍스트를 생생한 음성으로 변환하는 서비스로서 고급 딥러닝 기술을 사용하여 실제 사람 목소리처럼 음성을 합성한다. 24개 언어로 47가지의 실제 음성이 포함되어 있어서 여러 국가에 따라 원하는 음성을 선택하여 음성 지원 애플리케이션을 개발할 수 있다. 라이선스 규정도 간단하여 폴리의 음성 오디오를 캐싱 및 저장하여 오프라인에서 재생하거나 재배포하는 것도 가능하다. 예를 들어, ‘허클베리핀의 모험’이라는 책을 MP3 포맷의 오디오 북으로 변환할 수 있으며, 그 비용은 2.4달러 정도에 불과하다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 서버리스 인공지능은 진화 중&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;서버리스 AI는 클라우드 애플리케이션에만 국한되지 않고 고성능 연산을 위한 플랫폼에서도 적용할 수 있다. 예를 들어, 병렬 처리를 통해 모델 선택 및 하이퍼 매개 변수 최적화를 가속화하기 위해 AWS 람다를 사용할 수 있다. AWS 람다 서비스는 분산된 병렬 처리가 가능하여, 실시간 데이터 처리와 광범위한 데이터 분석을 위한 매우 다양한 서비스로 입증되었다.&lt;/p&gt;
&lt;p&gt;예를 들어, 맬웨어 분석 및 침입 탐지 서비스를 제공하는 보안 업체인 파이어아이는 수십 억 건의 데이터를 AWS 람다로 분석하고 있다. 대개 대용량 하둡(Hadoop) 클러스터가 필요한 &lt;a href="https://aws.amazon.com/ko/blogs/big-data/building-scalable-and-responsive-big-data-interfaces-with-aws-lambda/"&gt;맵/리듀스(Map/Reduce) 작업을 AWS 람다를 활용하여 구현&lt;/a&gt;하였다. AWS 람다 함수 호출을 계단식으로 호출함으로써 아마존 S3에서 데이터를 저장 처리하고, 다시 호출하는 방식을 통해 진행하였다. 기존에 아마존 EC2에서 엘라스틱서치(Elastic Search)를 사용하던 것을 AWS 람다로 옮긴 이후 똑같은 성능에도 비용을 80 % 가량 줄였다. 이는 서버리스 환경에서 대용량 데이터 처리를 수행 할 수 있는 좋은 사례이다.​&lt;/p&gt;
&lt;p&gt;AWS 람다를 MXNet과 함께 사용하여 머신 러닝 및 딥러닝에서 간편성과 유연성을 제공할 수 있는 &lt;a href="https://aws.amazon.com/ko/blogs/compute/ad-hoc-big-data-processing-made-simple-with-serverless-mapreduce/"&gt;실험 사례&lt;/a&gt;도 있다. 여기에서는 이미지넷(ImageNet) 우승 모델인 18 계층의 심층 신경망 네트워크를 통해 이미지 라벨을 예측하는 레지듀얼네트워크(Residual Network)을 이용하여 샘플 응용 프로그램을 만들었다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/04/03/yong2_KdYS2RudoT86k5.jpg" alt="Residual Network를 통한 손글씨 인식 예측 모델(출처: AWS 홈페이지)" width="550px" height="364px" /&gt;&lt;br /&gt;
Residual Network를 통한 손글씨 인식 예측 모델(출처: AWS 홈페이지)&lt;/div&gt;
&lt;p&gt;AWS 람다에서 MXNet 라이브러리를 직접 컴파일하고 구축한 후, 4,495 개의 UCI Optdigits 훈련 데이터 세트 및 다른 1,125 개의 예제를 사용한 결과, EC2 인스턴스보다 5배 정도 속도가 빨라졌다. (오류율은 0.04로 같으면서도 AWS 람다 방식이 47초, 컴퓨팅 최적화된 C3.large인스턴스가 242초가 걸렸다.)&lt;/p&gt;
&lt;p&gt;또한, 이를 글로벌 수준에서 처리하기 위한 확장성을 고려한 벤치마킹에서도 평균 지연 시간이 1.18 초로서 매우 낮았다. 라이브러리 및 코드 샘플은 &lt;a href="https://github.com/awslabs/mxnet-lambda"&gt;mxnet-Lambda&lt;/a&gt; GitHub 저장소에서 살펴 볼 수 있다.&lt;/p&gt;
&lt;p&gt;최근 인공지능 연구는 CPU/GPU 등 대용량 컴퓨팅 자원, 빠른 딥러닝 엔진 선택, 확장성 높은 클러스터 구성 등으로 인해 더욱 민첩한 결과를 얻기를 원한다. 프로세스가 더 빨라질수록 같은 시간에 모델에서 훈련할 수 있는 데이터가 많아진다. 시스템 작동이 빨라질수록 단위 시간당 더 많은 복잡성을 모델링 할 수 있다. 이를 통해 개발자는 더 빠르게 모델 정확성과 성능, 변경 사항, 최적화를 이룰 수 있어 행복하게 연구할 수 있다. 서버리스 AI로의 진화는 이러한 모든 변화에 대한 최고의 선택지가 될 수 있다.&lt;/p&gt;
&lt;p&gt;앞서 언급한 대로 아마존 AI 플랫폼은 다양한 스펙트럼의 요구 사항을 가진 인공지능 연구자나 개발자에게 다양한 선택 옵션을 제공한다. 클라우드 기반 인공지능 연구는 물리 서버 관리, 구매, 클러스터 운영에 대한 물리적 혹은 시간적 제약을 없애고, 데이터 크기, 처리 시간, 지속적 모델 개발, 훈련에 필요한 균형 있는 대안을 제공하는 방향으로 발전하고 있다.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://m.zdnet.co.kr/column_view.asp?artice_id=20170403093357#imadnews"&gt;​원문 링크&lt;/a&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=FA-z3Wyu07M:mw3OY1N6s-Y:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 20 Apr 2017 05:17:22 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>[ZDNet 칼럼] 대용량 인공지능 플랫폼을 개발자들에게</title>
	<link>http://blog.creation.net/zdnet-large-scale-ai-plaform-developers</link>
	<description>&lt;p&gt;아마존은 사업 초기부터 인공 지능에 투자해 왔다. 아마존닷컴의 초창기 홈페이지를 보면 ‘Eyes &amp;amp; Editors’라는 기능이 있었는데, 이는 좋아하는 저자의 신규 서적에 대해 자동 검색 및 알림을 해 주는 에이전트 기반 서적 추천 엔진이다. 이미 2006년에 이러한 사용자 리뷰 및 행동 기반 추천을 통해 총 판매액의 35%가 추천 시스템에서 발생했다고 한다.&lt;/p&gt;
&lt;p&gt;최근에는 머신 러닝 및 딥러닝 기법을 물류센터에 도입하기도 했다. 사용자가 물건을 온라인 장바구니에 담기만 해도 주문자의 위치, 상품의 위치와 포장 및 운송 경로를 자동으로 예측하여, &amp;#8216;고객이 주문 전에 배송 계획 예측’하는 시스템을 운용하고 있다. 매 주간 총 500억회 이상 기계 학습을 기반한 예측을 하고 있다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/03/14/yong2_WMp1fcmEnHWkR3.jpg" alt="아마존닷컴 초창기 첫화면(출처: Internet Archive)" width="550px" height="541px" /&gt;&lt;br /&gt;
아마존닷컴 초창기 첫화면(출처: Internet Archive)&lt;/div&gt;
&lt;p&gt;이러한 예측을 기반으로 전 세계 아마존의 물류 센터 중 13 곳에는 시범적으로 &lt;a href="http://www.cnet.co.kr/view/123538" target="_blank" rel="noopener"&gt;키바(KIVA)라는 무인 로봇을 도입&lt;/a&gt;했다. 이 로봇은 배송 물품을 자동으로 계산하고 운반해서 포장하는 직원 앞에 순차적으로 놓아 준다. 그 결과 기존 1시간 이상 걸리던 물류 순환 속도를 15분으로 단축하고, 재고 공간 50% 향상, 운영 비용 20% 개선의 효과를 거두었다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/03/14/yong2_jrD17eNVCgr8VS.jpg" alt="아마존 창고를 책임지는 로봇 짐꾼 '키바'(왼쪽, 출처: CNet Korea), 아마존 물류창고의 AI 분석용 공개 데이터(오른쪽, 출처: AWS 홈페이지)" width="550px" height="182px" /&gt;&lt;/div&gt;
&lt;div class="img"&gt;아마존 창고를 책임지는 로봇 짐꾼 &amp;#8216;키바'(왼쪽, 출처: CNet Korea), 아마존 물류창고의 AI 분석용 공개 데이터(오른쪽, 출처: AWS 홈페이지)&lt;/div&gt;
&lt;p&gt;재미있는 점은 아마존 물류 센터에 상품을 보관하는 선반에는 크고 작은 다양한 물건이 무작위로 놓여져 있다. 사람이 직접 배송 물품을 포장하기 전에, 로봇으로 인해 예측된 물품이 옮겨지게 되는데, 이 때 물품 재고 및 내역 파악을 위해 컴퓨터 비전 기술과 함께 딥러닝을 통한 이미지 모델링 분석을 통해 상품의 배열 방식이 바뀌거나 이동하는 등 다양한 외부 요인에 상관 없이 재고 파악을 할 수 있다. 딥러닝 연구자를 위해 아마존 S3 공공 데이터에 선반 속 재고 상품 이미지 세트를 &lt;a href="https://aws.amazon.com/ko/public-datasets/amazon-bin-images/" target="_blank" rel="noopener"&gt;무료로 공개&lt;/a&gt;하기도 했다.&lt;/p&gt;
&lt;p&gt;아마존은 최근에 &lt;a href="https://www.amazon.com/b?node=16008589011" target="_blank" rel="noopener"&gt;&amp;#8216;아마존 고&amp;#8217;&lt;/a&gt;라는 새로운 형태의 무인 결제 오프라인 상점을 선보이기도 했다. 줄을 서서 기다릴 필요가 없는 ‘저스트 워크아웃(Just Walk Out)’이라는 기술을 통해 모바일 앱을 사용하여 상점에 입장, 원하는 제품을 선택하면 바로 가상 장바구니에 담기고, 상점을 나설 때 자동으로 결제가 되는 것이다. 상점내 각종 센서를 통해 컴퓨터 비전, 센서 퓨전 및 딥러닝과 같은 자율 주행 차량에 사용되는 것과 동일한 유형의 기술이 활용된다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;개발자를&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;위한&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;머신&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;러닝&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;서비스&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;출시&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;이러한 내부적 기술적 토대를 기반으로 AWS는 2015년 4월 ‘아마존 머신러닝’ 서비스 공개 이후, AWS 클라우드를 사용하는 고객들의 요구에 맞게끔 다양한 인공 지능을 위한 플랫폼 옵션을 공개해왔다. AWS는 대규모 자원을 가지고 있거나 투자 여력이 있는 회사만이 할 수 있는 인프라나 플랫폼을, 누구나 이용할 수 있도록 하게 하는 목표를 갖고 있다. 인공 지능 분야도 예외가 아니다.&lt;/p&gt;
&lt;p&gt;아마존 머신러닝 서비스는 기계 학습 전문 지식은 별로 없더라도 도메인 지식(혹은 대용량 데이터)을 보유한 개발자들이 사용할 수 있는 예측 분석을 제공하고 있다. 이는 아마존에서 내부적으로 사용하는 것과 동일한 기술로서, 모든 사용자가 AWS에서 바로 사용할 수 있다. 실제 많은 AWS 고객이 각종 위조 탐지, 쇼핑 분석 등에 이를 널리 활용하고 있다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/03/14/yong2_QrTWG5E1we7HvT.jpg" alt="아마존 머신러닝 콘솔을 통한 모델 훈련 및 예측(출처:AWS홈페이지)" width="550px" height="325px" /&gt;&lt;br /&gt;
아마존 머신러닝 콘솔을 통한 모델 훈련 및 예측(출처:AWS홈페이지)&lt;/div&gt;
&lt;p&gt;예를 들어, Hudl사는 스포츠 경기 데이터 및 비디오 분석 및 예측을 통해 코치와 운동선수가 경기를 준비하는데 도움을 받고 있다. Upserve는 식당 관리 시스템을 제공하는데, 아마존 머신러닝을 활용하고 있다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“아마존 머신 러닝을 통해 저녁 시간대에 레스토랑에 방문할 전체 고객 수를 예측할 수 있게 되었다. 그 결과 레스토랑은 저녁 시간대 직원 배치를 효과적으로 계획해 서빙 할 수 있다고 Upserve의 브라이트 풀턴, 인프라 엔지니어링 담당이사는 전한다.&amp;#8221;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;뿐만 아니라, 스타트업인 AdiMap은 광고 데이터 분석을 통해 그 결과를 예측하고 있으며, 크라우드소싱 사기 탐지 플랫폼인 Fraud.net은 복잡성을 줄이고 새롭게 등장하는 사기 패턴을 이해하기 위해 아마존 머신러닝을 사용하고 있다. AWS는 작년 AWS 리인벤트 행사에서 애플리케이션 개발자도 막대한 투자를 요하는 인공 지능 기능을 API 형태로 활용할 수 있는 새로운 AI 서비스를 출시했다. 또한, 지금은 AI 서비스의 초기이며, 좀 더 전문적인 결과를 얻으려는 고객의 피드백에 따라 아마존 AI 플랫폼에서는 알고리즘 튜닝과 모델 트레이닝이라는 두 가지 축을 따라 유연한 플랫폼을 확장하고 있다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;확장성&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;높은&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;고성능&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;딥러닝&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;플랫폼&lt;/b&gt;&lt;b&gt; &lt;/b&gt;&lt;b&gt;활용&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;​일부 인공 지능 연구자 또는 데이터 과학자들은 대량 데이터를 가지고 있을 뿐 아니라 직접 데이터 모델링 및알고리즘을 만들어 튜닝 하는 데 필요한 기술을 얻고자 한다. 이러한 경우 대부분 한정된 물리적 장비와 MXNet 및 텐서플로(Tensorflow)와 같은 딥러닝 프로그래밍 프레임워크을 사용하게 된다.&lt;/p&gt;
&lt;p&gt;딥러닝 연구에 필요한 확장성은 클라우드를 통해 해결할 수 있다. 신규 &lt;a href="https://aws.amazon.com/ko/ec2/instance-types/p2/" target="_blank" rel="noopener"&gt;P2 인스턴스&lt;/a&gt;는 192GB의 GPU 메모리, 4만 개의 병렬 처리 코어, 70테라플롭스의 단정밀도 부동 소수점 성능 및 23테라플롭스의 배정밀도 부동 소수점 성능을 갖춘 최대 16개의 엔비디아 K80 GPU, 64개의 vCPU 및 732GiB의 호스트 메모리를 제공한다. 최대 16개의 GPU까지 GPU다이렉트 (피어 투 피어 GPU 통신) 기능을 제공하므로 여러 개의 GPU가 단일 호스트 내에서 함께 작동할 수 있다. 또한, 최대 20GB의 밴드위스 기반으로 스케일 아웃 방식으로 클러스터를 구성할 수 있다.&lt;/p&gt;
&lt;p&gt;MXNet, 텐서플로, 토치(Torch), 카페(Caffe), 세라노(Theano) 및 쿠다(CUDA) 드라이버가 사전 설치되어있는 &lt;a href="https://aws.amazon.com/ko/blogs/korea/distributed-deep-learning-made-easy/" target="_blank" rel="noopener"&gt;딥러닝 이미지(AMI)와 AWS 클라우드포메이션(CloudFormation&lt;/a&gt;)을 통해 슈퍼 컴퓨터급 딥러닝 클러스터를 원클릭으로 생성하고 삭제할 수 있다. 비용적인 측면에서도 효과적이다. 빠르게 성장하는 GPU 성능과 하드웨어 출시로 곧장 낡은 기종이 되어 버릴 뿐만 아니라 관리상으로도 애로가 많은 하이엔드 물리 장비를 직접 구매하지 않더라도, 클라우드를 통해 필요할 때마다(저렴한 스팟 인스턴스 등을 활용하여) 비용 효율적인 클러스터를 구성 및 운영할 수 있다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2017/03/14/yong2_WRt8IrcIzeul9N.jpg" alt="GPU확장에 따라 선형 확장되는 MXNet의 처리량 및 속도 벤치마크 결과(출처: AWS블로그)" width="550px" height="322px" /&gt;&lt;/div&gt;
&lt;div class="img"&gt;GPU확장에 따라 선형 확장되는 MXNet의 처리량 및 속도 벤치마크 결과(출처: AWS블로그)&lt;/div&gt;
&lt;p&gt;특히, 아마존은 클라우드에 최적화된 GPU 확장성에 뛰어난 MXNet을 주력 엔진으로 선택했다. MXNet은 정교한 맞춤형 인공 지능 시스템을 구축 할 수 있도록 지원하는 오픈소스 딥러닝 엔진으로, 다양한 기능 및 확장성을 가지고 있다.&lt;/p&gt;
&lt;p&gt;예를 들어, 인기있는 이미지 인식 네트워크인 Resnet의 경우, MXNet을 통해 다른 엔진에 비해 2 배 높은 처리량을 제공해, 50%의 시간 동안 동일한 모델을 트레이닝 할 수 있다. 벤치 마킹 결과에 따르면, MXNet은 다른 엔진과 달리 수백 개의 GPU에 대한 선형 확장성을 보여 주어 클라우드에 적합하다.&lt;/p&gt;
&lt;p&gt;아마존에서는 MXNet 커뮤니티와 협력하고 있으며, 얼마전 &lt;a href="https://aws.amazon.com/ko/blogs/korea/excited-about-mxnet-joining-apache/" target="_blank" rel="noopener"&gt;아파치 재단의 인큐베이팅 프로젝트&lt;/a&gt;로 승인 받아 많은 개발자들의 참여를 독려하고 있는 중이다.&lt;/p&gt;
&lt;p&gt;이번 글에서는 아마존에서의 인공 지능 활용 사례와 이를 통해 축적된 인공 지능 기술을 맞춤형 솔루션을 원하는 연구자와 데이터 과학자들에게 어떻게 제공하고 있는지 알아보았다. 다음에는 스마트 애플리케이션을 개발하려는 일반 개발자들에게 서버리스(Serverless) AI 서비스를 구현하는 방법을 소개하고자 한다.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://m.zdnet.co.kr/column_view.asp?artice_id=20170314121011#imadnews"&gt;원문 링크&lt;/a&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=SBkpO2UG3dY:SvmzG0_0R7o:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 14 Mar 2017 05:11:24 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>[ZDNet 칼럼] 모두를 위한 아마존의 3가지 AI기술 계층</title>
	<link>http://blog.creation.net/zdnet-column-amazon-ai-three-pillars</link>
	<description>&lt;p&gt;인공지능이 버즈워드(Buzz word)로 뜨면서, 100명에게 &amp;#8216;인공지능&amp;#8217;의 정의를 묻는다면 적어도 100개 이상의 답변을 얻을 것이다. 한 가지 주목할 점은 인공지능 기술과 서비스를 주요 거대 IT기업이 주도해 나가는 양상을 보이고 있다는 것이다. 그 중에서도 아마존의 인공지능 분야에 대한 혁신 움직임은 단연 두드러진다. 본 칼럼에서도 작년 &amp;#8216;&lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20160322181122" target="_blank" rel="noopener"&gt;클라우드가 선사한 인공지능 기술의 자유와 기회&lt;/a&gt;&amp;#8216;, &amp;#8216;&lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20160720105047" target="_blank" rel="noopener"&gt;개발자들을 위한 인공지능 기술시대&lt;/a&gt;&amp;#8216;, &amp;#8216;&lt;a href="http://www.zdnet.co.kr/column/column_view.asp?artice_id=20161209121829" target="_blank" rel="noopener"&gt;클라우드, 현실 세계에 스며들다&lt;/a&gt;&amp;#8216; 등의 글을 통해 관련 내용을 다루었다.&lt;/p&gt;
&lt;p&gt;본 칼럼에서는 연초를 맞아 3회에 걸쳐 아마존이 바라보는 인공지능 기술에 대해 소개함으로써, 향후 기술 변화에 단초를 제시해 보고자 한다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■ 인공지능의 진입 장벽을 낮추다&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;딥러닝(Deep Learning)으로 대변되는 인공지능 기술 시스템 구축과 관련된 프로젝트는 대용량 컴퓨팅을 요구한다. 그 동안 기업 IT 워크로드의 대부분이 기존 데이터센터 환경에서 클라우드로 옮겨가고 있는 추세다. 이에 발맞춰 인공지능에서 다루는 데이터 크기가 커지고 모델 트레이닝이 잦아짐에 따라, 아마존웹서비스(AWS)는 고객의 수요에 따라 CPU 및 스토리지, 그리고 GPU 같은 컴퓨팅 자원을 활용할 수 있는 자유를 제공하고 있다.&lt;/p&gt;
&lt;p&gt;자율 주행을 위한 컴퓨터 비전 시스템부터 미국 식품의약국(FDA)이 승인하는 의료이미지 처리, 넷플릭스 동영상 추천, 핀터레스트의 이미지 검색 등은 AWS 클라우드 시스템을 기반으로 한다. 아마존닷컴의 경우, 추천 엔진으로부터, 머신 러닝에 기반한 주문 배송 예측을 통한 물류 센터의 로봇 활용에 이르기까지 고객의 구매 사이클에 걸쳐 시간을 단축하는 혁신을 이루고 있다. .&lt;/p&gt;
&lt;div class="img"&gt;&lt;img class="aligncenter" src="http://image.zdnet.co.kr/2017/02/16/yong2_xKacFCTERCYXDX.jpg" alt="Amazon Go 동영상(출처: Amazon.com 홈페이지)" width="550px" height="241px" /&gt;&lt;/div&gt;
&lt;div class="img"&gt;Amazon Go 동영상(출처: Amazon.com 홈페이지)&lt;/div&gt;
&lt;p&gt;아울러 아마존 에코와 같은 음성 인식 기반의 스마트 홈 기기와 음성 서비스 지원 기기 확대를 통한 새로운 경험을 현실로 제공하고, 최근에는 딥러닝 기술과 컴퓨터 비전을 통한 계산대가 없는 오프라인 가게인 &lt;a href="http://www.amazon.com/go"&gt;아마존고(Go)&lt;/a&gt;를 선보이기도 했다. 이처럼 AWS는 아마존 및 고객 모두에게 인공지능의 핵심 서비스를 제공하고 있다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■ 아마존 AI의 3계층 기술 스택 옵션&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;AWS가 클라우드를 만든 기본적인 이유는 돈이 많든 적든, 기업의 규모가 크든 작든 상관없이 클라우드를 통해 누구나 동등한 성공 기회를 얻도록 하기 위함이다. 인공지능도 동일하다. AWS는 가능한 많은 개발자들이 직접 인공지능 기반 서비스를 만들 수 있도록, 아마존의 수천 명의 엔지니어가 얻은 전문 지식을 서비스화 시켜 클라우드 이점을 결합함으로써, 기술 수준에 따라 다양한 기술 스택 중 하나를 선택할 수 있게 해주고 있다.&lt;/p&gt;
&lt;p&gt;아마존 인공지능(Amazon AI) 기술 스택은 AWS 인프라와 상위 애플리케이션 서비스를 포함해 아래와 같은 3개의 주요 계층으로 나뉜다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img class="aligncenter" src="http://image.zdnet.co.kr/2017/02/16/yong2_I4MTmmglTsZPZk.jpg" alt="아마존 AI 서비스 스택 (출처: AWS AI 블로그)" width="550px" height="226px" /&gt;&lt;/div&gt;
&lt;div class="img"&gt;아마존 AI 서비스 스택 (출처: AWS AI 블로그)&lt;/div&gt;
&lt;p&gt;먼저, AI 모델을 구현하는 데 필요한 기술 능력을 보유하고 있지 않거나 데이터가 부족한 개발자를 위해 최고 수준에서 인공지능 서비스를 제공한다. 자신의 애플리케이션에 인공지능을 활용해 보고자 하는 개발자를 대상으로 한다. 최근 AWS는 이미지 인식을 통한 사물 및 얼굴 분석 서비스인 &lt;a href="https://aws.amazon.com/ko/rekognition" target="_blank" rel="noopener"&gt;아마존 레코그니션(Amazon Rekognition)&lt;/a&gt;, 텍스트에서 음성 합성을 위한 &lt;a href="https://aws.amazon.com/ko/polly" target="_blank" rel="noopener"&gt;아마존 폴리(Amazon Polly)&lt;/a&gt; 및 대화식 채팅 봇 구축을 위한 자동 음성 인식 및 자연 처리 서비스인 &lt;a href="https://aws.amazon.com/ko/lex" target="_blank" rel="noopener"&gt;아마존 렉스(Amazon Lex)&lt;/a&gt;를 공개했다.&lt;/p&gt;
&lt;p&gt;한 단계 아래에는 기존 데이터를 보유한 고객이 맞춤형 모델을 구축하고자하는 경우, 직접 AI 데이터 트레이닝 및 모델 구축, 관리의 어려움을 제거하는 플랫폼을 제공한다. 맞춤형 선형 모델에 대한 예측을 위한 &lt;a href="https://aws.amazon.com/ko/machine-learning" target="_blank" rel="noopener"&gt;아마존머신러닝(Amazon Machine Learning)&lt;/a&gt; 은 데이터 기반 머신 러닝 일괄 처리 및 실시간 처리 기능을 제공한다. 또한, &lt;a href="https://aws.amazon.com/ko/emr" target="_blank" rel="noopener"&gt;아마존 EMR&lt;/a&gt;(스파크 및 스파크 ML 지원)을 통해 데이터를 처리하는 것도 가능하다. 마지막으로 정교한 인공지능 분석 시스템을 구축하려는 학자 및 데이터 과학자를 위한 오픈소스 AI 프레임워크인 아파치 MXNet, 텐서플로우, Caffe, Theano, Torch 및 CNTK와 같은 엔진을 손쉽게 사용할 수 있는 &lt;a href="http://bit.ly/deepami" target="_blank" rel="noopener"&gt;딥러닝용 이미지&lt;/a&gt;를 제공한다. 이를 기반으로 아마존 EC2(Elastic Compute Cloud)의 새로운 인스턴스 타입에는 맞춤형 고성능 인텔 프로세서 및 FPGA(Field-Programmable Gate Array)를 활용할 수 있게 됐다. 특히, 아마존 EC2의 신규 GPU 인스턴스 타입인 &lt;a href="https://aws.amazon.com/ko/ec2/instance-types/p2/" target="_blank" rel="noopener"&gt;P2&lt;/a&gt;를 통해 수백 개의 GPU를 활용할 수 있게 되었다. 또한 &lt;a href="http://mxnet.io/" target="_blank" rel="noopener"&gt;아파치 MXNet&lt;/a&gt;은 클라우드를 통한 GPU 성능의 선형 확장성과 성능에 탁월성을 보여, 이를 기본 딥러닝 엔진으로 아마존에서 직접 투자하고 있다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■ 서버리스(Serverless) AI 시대가 온다&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;AWS는 애플리케이션을 함수 단위로 쪼개서 실행 할 수 있는 &lt;a href="https://aws.amazon.com/ko/lambda" target="_blank" rel="noopener"&gt;AWS 람다(Lambda)&lt;/a&gt; 서비스를 출시하면서, 서버리스 아키텍처 시대를 열었다. 애플리케이션을 배포 및 운영해야 할 서버의 존재가 없어짐에 따라, 2년이 넘는 동안 다양한 애플리케이션 아이디어가 쏟아져 나왔다. 원래 AWS 자원의 변경이 일어나는 이벤트에 따라 간단한 동작을 수행하려는 기능에서 출발해서, API 게이트웨이 서비스와 AWS가 원래(서버 관리 필요 없이) 제공하던 아마존 S3, 아마존 DynamoDB 같은 빌딩블록을 이용하여 &lt;a href="https://github.com/awslabs/serverless-Application-model" target="_blank" rel="noopener"&gt;서버리스 애플리케이션 모델&lt;/a&gt;을 구성할 수 있게 되었다.&lt;/p&gt;
&lt;p&gt;앞서 언급한 아마존 레코그니션, 폴리, 렉스 같은 인공지능 서비스는 AWS람다와 연동 및 API 결합을 통해서 손쉽게 서버리스 애플리케이션의 빌딩 블록으로 활용 가능하다.&lt;/p&gt;
&lt;p&gt;예를 들어, 아래 AWS 서비스 아키텍처는 라즈베리 파이와 웹캠을 활용하여 문 앞에 손님이 우리 가족이거나 친구인지를 확인해주는 간단한 초인종 시스템이다. 이미지를 Amazon S3에 업로드 하고 이를 얼굴 인식 서비스를 통해 확인 한 후, 목소리로 안내를 해주는 시스템이다. 이를 데이터베이스에 담거나 모바일 앱으로 전달하는데 필요한 모든 빌딩 블록은 AWS 클라우드의 서버 관리가 전혀 필요 없는 관리형 서비스 들이다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img class="aligncenter" src="http://image.zdnet.co.kr/2017/02/16/yong2_m1WyDVbthVQ3AL.jpg" alt="IoT 기술을 활용한 AWS 기반 얼굴 인식 서버리스 초인종 시스템 (출처: DZone)" width="550px" height="352px" /&gt;&lt;/div&gt;
&lt;div class="img"&gt;IoT 기술을 활용한 AWS 기반 얼굴 인식 서버리스 초인종 시스템 (출처: DZone)&lt;/div&gt;
&lt;p&gt;즉, 이러한 서버리스 아키텍처는 향후 인공지능 기반 스마트 애플리케이션을 개발 및 배포할 때 서버 관리 부담을 줄여주고 더 빠르고 민첩하게 기능을 추가할 수 있도록 할 것이다.&lt;/p&gt;
&lt;p&gt;최근에는 한 걸음 더 나아가 람다 함수를 병렬 컴퓨팅 및 인공지능 학습에도 활용해 높은 성능을 보여준 사례도 나오고 있다. 다음 편에서는 AWS가 제공하는 다양한 옵션을 통해 밑으로부터 어떻게 모두를 위한 인공지능 플랫폼을 활용할 수 있는 지와 마지막으로 서버리스 AI 시대에서 개발자가 좀 더 민첩하고 빠르게 인공지능을 결합한 스마트 애플리케이션을 어떻게 개발 및 배포할 수 있는지에 대해 자세히 소개하고자 한다.&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=_Vj1RlBtcZg:UUwrcdMXDvg:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 16 Feb 2017 05:09:17 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>차니의 IT 이야기 #2- 개발자 경력 관리 조언</title>
	<link>http://channy.creation.net/blog/1130</link>
	<description>&lt;p&gt;지난 주에 한 IT 업체의 개발자와의 만남의 시간을 가졌습니다. 대개는 기술 주제를 가지고 강연을 진행했지만, 연말이고 해서 개발자 경력 관리에 대해 그 동안 가지고 있던 몇 가지 생각을 이야기해 주었습니다. 어찌 보면 자기 자랑일수도 있고 해서 부끄럽기도 하지만, 그래도 경험을 공유하는 것이 도움이 되지 않을까 하는 생각에서 진행을 했습니다.&lt;/p&gt;
&lt;p&gt;다녀와서 보내 준 회고 및 피드백 결과를 보니 다행히 많은 분들이 개발자 경력에 대해 다시 생각해 보게 되어서 좋았다는 의견을 많이 보내주셨더라구요. 지난 번에 &lt;a href="http://channy.creation.net/blog/1077"&gt;스타트업 창업자를 위한 조언&lt;/a&gt; 이후에 약간 용기를 내어(?) 그날 했던 이야기를 정리해서 올려 봅니다. &lt;/p&gt;
&lt;p&gt;  &lt;/p&gt;
&lt;div&gt; &lt;strong&gt; &lt;a href="http://www.slideshare.net/Channy/channy-story-2-developer-career-path" title="차니의 IT 이야기 #2- 개발자 경력 관리 조언" target="_blank"&gt;차니의 IT 이야기 #2- 개발자 경력 관리 조언&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a target="_blank" href="http://www.slideshare.net/Channy"&gt;Channy Yun&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;사실 자신의 경력 관리는 어느 순간 하는 게 아니라 지속적인 과정을 통해 진행해야 합니다. 온라인 시대에 맞게 가급적 자신이 하는 일을 많은 분들에게 공유하고, 자신이 배우는 것은 외부에 공유하는 습관을 가지면 좋겠습니다. 나이가 들어도 직접 자신이 손으로 할 수 있는 일을 지속하는 것도 중요합니다. 특히, 남들이 좋은 회사로 이직하고, 직급이나 직책이 올라가는 것에 일희일비 하지 않고 자신의 페이스를 가져가는 것 또한 필요합니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;최신 정보로 이력서를 관리하라!&lt;/strong&gt; &lt;small&gt;(경력에 대한 지속적인 배포는 필수다.)&lt;/small&gt; 대부분 회사 이직 시 이력서를 갱신하는 분들이 많습니다. 저는 예전에 팀원들에게 매년 연말에 이력서를 업데이트 시키고, 이를 제3자 관점에서 확인해 주기도 했는데 항상 이직을 준비하는 마음으로 지속적인 경력 업데이트가 필요합니다. 지금은 아예 &lt;a href="http://linkedin.com/in/channy"&gt;LinkedIn&lt;/a&gt;을 통해 아예 이력서를 공개를 해 두는 경우가 많을 정도입니다. 구글에 없으면, 존재하지 않는다는 말처럼 이제 링크드인에 없으면 없는 경력이 된 세상입니다. 여러분의 이력서를 늘 지속적으로 배포(Continous Delievery)하셔야 합니다. 또한, 이력을 관리하는 방법도 어떤 것이 어떤 일에 연결되는지 맥락(Context)를 가지고 관리할 필요가 있습니다. &lt;a href="http://channy.creation.net/blog/889"&gt;나만의 경력 지도 만드는 법&lt;/a&gt;을 참고해 보세요.
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Work-Work 균형이 중요하다!&lt;/strong&gt; &lt;small&gt;(업무 경력에도 본업과 부업이 필요하다)&lt;/small&gt; 많은 사람들이 일-삶의 균형이 중요하다고 말합니다. (&lt;a href="http://channy.creation.net/blog/929"&gt;나의 가족과 저녁이 있는 삶&lt;/a&gt; 참고) 하지만, 그에 못지 않게 업무 경력에도 균형과 취미가 필요합니다. 아무리 재미 있던 일도 본업이 되는 순간, 지루해지기 마련입니다. 따라서, 업무 경력을 쌓고 돈을 버는 &amp;#8216;본업&amp;#8217;을 통해 깊이 있는 역량을 쌓는 동시에 재미를 얻기 위한 &amp;#8216;부업&amp;#8217;도 해야 합니다. 이로 인해 새로운 본업을 찾을 수도 있거든요. 저는 커뮤니티 활동, 책쓰기, 블로그, 오픈 소스 활동 등을 통해 새로운 분야에 계속 도전해 볼 수 있었습니다.
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;커뮤니티에서 일생의 인맥을 만들어라!&lt;/strong&gt; &lt;small&gt;(동기가 있는 사람이 모여있다.)&lt;/small&gt; 개발자 커뮤니티는 매우 중요합니다. 다양한 자기 의지와 활동 동기가 있는 사람을 만날 수 있기 때문이죠. 저는 학생 때 부터 &lt;a href="http://channy.creation.net/blog/969"&gt;WWW-KR&lt;/a&gt;이라는 커뮤니티 활동을 했는데, 여기서 인생에서 가장 열정적이던 시기에 일생의 멘토와 친구들을 만났습니다. 또한, &lt;a href="http://channy.creation.net/blog/887"&gt;Mozilla 커뮤니티&lt;/a&gt;를 통해 글로벌의 지인을 만들고, 오픈 소스 모범 사례를 배울 수 있었습니다. &lt;a href="http://webstandards.or.kr"&gt;웹표준 프로젝트&lt;/a&gt;에서 웹 기술에 대한 같은 뜻을 가진 사람을 만나 즐거운 일들을 함께했습니다. 그리고, &lt;a href="http://www.awskr.org/"&gt;AWS사용자 모임&lt;/a&gt;에서 새로 개발자를 돕는 새로운 일을 찾을 수 있었던 동기를 만들어 주었습니다. 다만, 지속적이고 장기적인 커뮤니티 활동은 필수입니다. 개발자 커뮤니티는 생물(生物)과 같습니다. 세심히 키우고 소통해야 합니다.
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;해외 콘퍼런스에 주기적으로 참석한다!&lt;/strong&gt; &lt;small&gt;(직접 눈으로 현장에 가봐야 한다.)&lt;/small&gt; 제 일생의 큰 전환점 몇 개를 뽑으라고 한다면, 주저하지 않고 석사 1년차 &lt;a href="http://channy.creation.net/tour/usa-1997"&gt;미국 학회 참석&lt;/a&gt;을 들 수 있습니다. 직접 눈으로 세상이 어떻게 돌아가는지 볼 수 있었던 계기라고 할까요?  2005년 &lt;a href="http://channy.creation.net/blog/180"&gt;Web2.0컨퍼런스&lt;/a&gt;, &lt;a href="http://channy.creation.net/blog/336"&gt;Ebay 개발자 컨퍼런스&lt;/a&gt;를 다녀와서 &lt;a href="http://dna.daum.net"&gt;Daum 개발자 네트워크&lt;/a&gt;(Daum DNA)를 시작할 수 있는 동기를 얻었습니다. (참고. &lt;a href="http://channy.creation.net/blog/359"&gt;개방형 플랫폼 과연 성공할 수 있을까? – Daum DNA의 도전&lt;/a&gt;) 해외 콘퍼런스는 단순히 강연이 아니라 기술이 변화하는 곳으로 모이는 전 세계 사람들과의 네트워킹을 통해 배우는 현장입니다. 지속적으로 글로벌 동향에 관심을 통해 해외 지인 네트워크도 얻을 수 있죠. 제 &lt;a href="http://channy.creation.net/tour"&gt;해외 여행기&lt;/a&gt;에는 이러한 여정이 담겨있습니다.
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;정보 공유는 항상 옳다!&lt;/strong&gt; &lt;small&gt;(쓰고, 찍고, 정리하여 배워서 남주자.)&lt;/small&gt; 공유라는 게 거창한 게 아닙니다. 그저 일주일에 두 서너개씩 블로그나 소셜 미디어로 나누는 것에 불과합니다. 그런게 모여서 &lt;a href="http://channy.creation.net/blog/1121"&gt;블로그 글 3천개&lt;/a&gt;가 되고, &lt;a href="http://channy.creation.net/lecture"&gt;강연&lt;/a&gt;이 되고, &lt;a href="http://channy.creation.net/about/interview"&gt;인터뷰&lt;/a&gt;가 됩니다. 특히, 공유를 사랑하고 즐기는 사람들과 &lt;a href="http://webappscon.con/2007"&gt;웹앱스콘&lt;/a&gt;, &lt;a href="http://webappscon.con/fwf"&gt;미래웹포럼&lt;/a&gt;, &lt;a href="http://barcamp.org/BarCampSeoul"&gt;바캠프&lt;/a&gt;등을 만들면서 더 큰 인적 네트워크를 얻을 수 있었습니다.
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;올라가는 것만이 능사가 아니다!&lt;/strong&gt; &lt;small&gt;(빨리 올라가면 빨리 내려온다.)&lt;/small&gt; 일정 정도 경력이 올라가면 많은 개발자들이 회사에서 개발이냐? 매니저냐?라는 선택의 기로에 서게 됩니다. 대개 자기 팀을 만들고 조직을 만들어 올라가는 유혹이 많기 때문에 유능한 개발자가 팀 매니저가 되는 일이 종종 있습니다. (그래서 실패하는 경우도 많죠.) 한국에서 CTO가 되면 멋지고 폼나는 일을 할 것 같지만… 직접 스타트업 CTO를 해보고, 많은 CTO와 함께 일해본 결과, 회사에서 가장 더럽고 골치 아프고 뒤치닥꺼리 일만 올라옵니다. 해고, 특허 소송, 장애 해결 등 오히려 자질구레한 일이 더 많습니다. 따라서, 최대한 개발자로의 경력을 이어가는 것이 중요합니다. &lt;a href="https://www.quora.com/Is-software-development-really-a-dead-end-job-after-35-40"&gt;개발자는 40살이 넘으면 경력이 끝나는 건가요?&lt;/a&gt;라는 Quora 질문에 &lt;i&gt;&amp;#8220;구글 시니어 엔지니어인 Rob Pike와 Ken Thompson은 60이 넘은 나이에도 Go 언어를 만들었습니다&amp;#8230; 취업 시장에서는 우리가 가진 기술이 아니라 &amp;#8216;시장이 요구하는 기술&amp;#8217;을 중요시 합니다.  여러분이 시장이 요구하는 기술을 가진 이상, 언제나 일자리를 찾을 수 있습니다.&amp;#8221;&lt;/i&gt;라는 답변은 인상적입니다. 물론 한국에서는 &lt;a href="https://medium.com/engineering-leadership/defining-roles-cto-and-or-vp-engineering-f1c7563643a3"&gt;CTO와 VP of Engineering&lt;/a&gt;을 혼동하고 있는 경우가 많고, 아키텍트와 피플 매니저의 역할과 경력 관리를 정확하게 분리해야 합니다.
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;나이가 들어도 핸즈온이 필수다!&lt;/strong&gt; &lt;small&gt;(손발이 없다고 불평하는 사람이 되지 말자.)&lt;/small&gt; 마지막으로 제가 AWS로 이직할 때, 네이버 송창현 CTO님이 해 주신 이야기입니다. &amp;#8220;나이가 들고 경력이 쌓였는데도 손발이 없다고, 의사 결정 안 해 준다고, 예산 없다고 일하기 힘들다는 사람들이 있다. 자기가 직접 손으로 혼자 다할 줄 알고, 해결할 수 있어야 일을 제대로 하는 사람이 아닐까?&amp;#8221; 그렇습니다. 옆에서 손발이 되는 사람이 없으면 작은 일도 못하는 시니어가 되는 게 아니라, 스스로 직접 손으로 할 수 있는(Hands-on) 사람이 되어야 합니다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="http://channy.creation.net/wp/data/channy/2016/channy-story-2-developer-career-path.png" alt="" title="" /&gt;&lt;/p&gt;
&lt;p&gt;개발자 경력 관리는 정답은 없습니다. 저의 조언도 다 맞지는 않을 거에요. 다만, 나중에 여러분의 경험을 나눠 주시면 누구 한 명에게는 도움이 될 수 있답니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=BNiKSefbj6Y:P0DilD0uL6A:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 26 Dec 2016 22:30:18 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1130#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>[ZDNet 칼럼] AWS re:Invent – 클라우드, 현실 세계에 스며들다</title>
	<link>http://blog.creation.net/605</link>
	<description>&lt;p&gt;지난 주 미국 라스베이거스에는 클라우드 컴퓨팅 마니아 3만 2천 여명이 한 자리에 모였다. &lt;a href="http://reinvent.awsevents.com/"&gt;AWS 리인벤트(re:Invent) 2016&lt;/a&gt;에 참가하기 위해 전 세계에서 모여든 AWS 고객, 개발자 및 파트너 업체 종사자들이다. 클라우드 생태계에 정점에 있는 이들이 모인 이유는 업계의 리더 위치에 있는 AWS의 미래 전략을 듣기 위해서였다. 행사를 주최한 AWS는 re:Invent를 교육 및 배움의 장소로 자리매김하고자 했고, 다양한 고객의 사례를 서로 나누는 장을 마련했다.&lt;/p&gt;
&lt;p&gt;최근 몇 년간 클라우드 컴퓨팅은 어떻게 진화해 왔을까? 넷플릭스의 아키텍처였다가 최근에 AWS 클라우드 전략 담당 부사장으로 영입된 아드리안 코크로프트는 그의 블로그에서 다음과 같이 썼다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“2014년에 엔터프라이즈 기업들이 AWS에 테스트 및 신규 애플리케이션을 개발하기 위한 방편으로 AWS를 사용하기 시작하여, 2015년에는 대량 마이그레이션을 하거나 전체 데이터센터를 퍼블릭 클라우드로 대체하는 패턴을 보았습니다. 올해 들어 이 변화는 미디어 산업, 판매 산업에서 빠르게 도입을 하는가 하면, 은행, 보험 등 퍼블릭 클라우드를 사용하는데 규제가 강한 금융 산업도 여기에 동참하였습니다. 다음은 아마 에너지, 교통, 정부, 제조 및 헬스케어 분야의 얼리 어댑터들이 클라우드 시장을 이끄는 주자가 될 것입니다.” (&lt;a href="https://medium.com/@adrianco/cloud-trends-where-have-we-come-from-and-where-are-we-headed-3d7e5e756d16"&gt;Cloud Trends ? Where have we come from and where are we headed&lt;/a&gt; 중)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;앤디 제시 AWS 최고경영자(CEO)는 2014년에는 클라우드는 새로운 표준(Normal)으로서 엔터프라이즈 기업들이 활용을 시작했음을 알렸고, 2015년 기조 연설에서는 &lt;a href="https://aws.amazon.com/ko/blogs/korea/reinvent-2015-day2/"&gt;클라우드가 가져온 7가지 자유&lt;/a&gt;라는 주제로 클라우드가 스스로 원하는 길을 개척할 수 있는 자유를 준다는 점을 설명하였다. 그렇다면 올해의 화두는 무엇일까? 그는 이제 사용자들이 클라우드를 통해 그 동안 해내지 못했던 것에 대해 무엇이든 해 낼 수 있다는 느낌을 준다는 ‘슈퍼 파워(Super Power)’ 도구로서 자리매김 했다는 점을 기조 연설을 통해 강조했다.&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■클라우드가 없는 곳에도 클라우드를 넣다&lt;/strong&gt;&lt;br /&gt;
아드리안의 언급대로, 앞으로 클라우드 활용이 익숙해진 IT 서비스 분야에서 제조, 유통, 판매 분야 등으로 확대될 전망이다. 이러한 실제 세계는 사실상 네트워크 연결이 제한적이거나 아예 존재하지 않는 극단적인 상황도 있다. 예를 들어, 옥수수 농장, 항공기 제조 공장, 병원등 산업 현장은 우리 생각과 많이 다르다.&lt;/p&gt;
&lt;p&gt;일단 현장에서 생산되는 데이터들은 바로 클라우드로 옮기기도 애매하고, 일시적으로 현장에서 IT 환경을 꾸려야 한다. 이를 위해서는 로컬 환경에 클라우드와 궁합이 맞는 컴퓨팅 및 스토리지 서비스가 필요하다. &lt;a href="https://aws.amazon.com/ko/blogs/korea/aws-greengrass-ubiquitous-real-world-computing/"&gt;Amazon Greengrass&lt;/a&gt;는 AWS Lambda와 AWS IoT를 결합한 로컬 디바이스로 주변 환경의 데이터 수집이나 프로그램 처리 등을 손쉽게 해준다. AWS Lambda는 서버리스 컴퓨팅 환경을 연 클라우드 함수 서비스로 임베디드 형식으로 로컬 컴퓨팅에 이식하게 됨에 따라 그 사용 범위가 훨씬 넓어질 전망이다.&lt;/p&gt;
&lt;p&gt;Greengrass에서 사용된 임베디드 하이브리드 컴퓨팅 방식은 대량 로컬 데이터를 손쉽게 클라우드로 이전할 수 있는 데이터 운송 장치인 AWS Snowball에도 적용됐다. 새로 나온 &lt;a href="https://aws.amazon.com/ko/blogs/korea/aws-snowball-edge-more-storage-local-endpoints-lambda-functions/"&gt;AWS Snowball Edge&lt;/a&gt;는 용량을 100TB로 늘렸을 뿐만 아니라, 현장에서 사용하는 다양한 네트워크 및 데이터 어댑터를 지원한다. 특히, Greengrass에서 적용했던 것과 같이 AWS Lambda 함수를 기기 내부에 탑재하여, 손쉽게 Amazon Simple Storage Service(S3) 스토리지 버킷과 작업이 가능하다.&lt;/p&gt;
&lt;p&gt;이번 기조 연설에서 무엇 보다 깜짝 발표는 기존의 소형 Snowball 기기뿐만 아니라, 엑사바이트(Exabite) 급 데이터를 대량으로 한번에 옮길 수 있는 &lt;a href="https://aws.amazon.com/ko/blogs/korea/aws-snowmobile-move-exabytes-of-data-to-the-cloud-in-weeks/"&gt;AWS Snowmobile&lt;/a&gt;이라는 데이터 선적용 콘테이너 트럭을 선보였던 순간이었다. 인터넷으로 옮기려면 수백 년이 걸릴 데이터를 한꺼번에 몇 주안에 클라우드로 옮길 수 있다는 건, 이제 현실의 데이터 제약을 완전히 극복했다는 것을 입증했다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2016/12/09/yong2_OgXmq6wArSASNU.jpg" alt="AWS Snowmobile 데이터 이동 콘테이너 트럭(출처: @AWSCloud)" width="550px" height="328px" /&gt;&lt;/div&gt;
&lt;div class="img"&gt;AWS Snowmobile 데이터 이동 콘테이너 트럭(출처: @AWSCloud)&lt;/div&gt;
&lt;p&gt;​&lt;br /&gt;
고객 발표사로서 이탈리아 기반 글로벌 전력회사인 Enel 이 올라온 것도 우연은 아니다. Enel은 국내에서는 잘 알려져 있지 않지만, 100 년 이상의 역사를 가지고 있으며, 6천100 만 고객 190만km의 배급망을 가진 전력 업계의 리더이며, Fortune 지의 ‘세상을 바꿀 기업’ 50 개사 중 5 위에 선정된 기업이다. 스마트 전력 이용률에서는 25%, 전 세계 발전량 3위, 신 재생 에너지, 송배전 망 거리 판매액 시가 총액도 1위를 차지했다.&lt;/p&gt;
&lt;p&gt;그러나 리먼 브라더스 사태 이후 전력 수요와 GDP 및 석유 소비량과 결합이 없어지면서, 석유 가격 감소 및 청정 에너지에 대한 투자 증대 등 전력 산업도 변화하고 있다.&lt;/p&gt;
&lt;p&gt;연사로 나선 인프라 담당 임원인 파비오 베로네세는 “이러한 전력 산업 변화 과정에서 Enel 역시 클라우드 우선 전략으로 변화를 꾀하게 되었으며, 2016 년 6 월까지 10,000개의 가상 서버, 30,000 CPU와 6PB 스토리지 마이그레이션을 완료하고 AWS를 활용하여 서버 프로비저닝을 3~4 주 걸리던 것을 이틀 내로 처리할 수 있었다”라고 전했다.&lt;/p&gt;
&lt;p&gt;이를 통해 컴퓨팅 자원의 소비는 21% 감소, 스토리지는 60%까지 절감하였다. Enel의 다음 목표는 사내 IT 업무 100%를 클라우드로 전환하고, 오프라인 스마트 전력망 운영 체계에 있어클라우드 기반 IoT 활용을 목표로 하고 있다고 언급했다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■클라우드 기반 인공 지능으로 현실 세계와 접하다 &lt;/strong&gt;&lt;br /&gt;
이번 기조 연설의 메인 테마 중 하나는 단연 인공 지능(AI)이다. 이미 AWS는 머신 러닝 서비스를 1년 전에 공개한 바 있고, 최근에 GPU 기반 &lt;a href="https://aws.amazon.com/ko/blogs/korea/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/"&gt;P2 인스턴스와 딥러닝 전용 AMI&lt;/a&gt;를 내 놓았고, 최근에는 &lt;a href="https://aws.amazon.com/ko/blogs/korea/aws-deep-learning-FRAMEwork-mxnet/"&gt;MXNet에 대한 지원을 천명&lt;/a&gt; 하는 등 클라우드 기반 인공 지능 서비스 개발 플랫폼으로서 강력한 의지를 보였다.&lt;/p&gt;
&lt;p&gt;이번 키노트에서는 여기에 머물지 않고 인공 지능 분야 킬러 애플리케이션이라고 할 수 있는 서비스 몇 가지를 선보였다. 우선 아마존이 가지고 있는 가장 강력한 음성 인식 스마트 홈 기기인 Amazon Echo를 지원하는 알렉사 서비스를 제공하는 자연어 처리 기법 및 인공 지능 기술을 이용하여 모바일 앱 및 웹 애플리케이션에서 사용가능한 채팅봇을손쉽게 만들고 관리할 수 있는 &lt;a href="https://aws.amazon.com/ko/blogs/korea/amazon-lex-build-conversational-voice-text-interfaces/"&gt;Amazon Lex&lt;/a&gt;이다. Alexa에서 앞뒤 A를 뺀 Lex는 채팅봇을 통해 정보를 제공하고, AWS Lambda 함수를 사용하여 엔터프라이즈급 응용 프로그램 및 데이터를 연결하는 챗봇용 비즈니스 로직을 구현할 수 있다. 장기적으로, 로봇, 드론 및 장난감에 대한 제어 메커니즘을 제공 할 수도 있다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2016/12/09/yong2_DF586aCeXzwn1P.jpg" alt="Amazon Rekognition 서비스가 제공하는 이미지 서비스(출처: AWS홈페이지)" width="550px" height="491px" /&gt; Amazon Rekognition 서비스가 제공하는 이미지 서비스(출처: AWS홈페이지)&lt;/div&gt;
&lt;p&gt;딥러닝 기반 이미지 인식 서비스인 &lt;a href="https://aws.amazon.com/ko/blogs/korea/amazon-rekognition-image-detection-and-recognition-powered-by-deep-learning/"&gt;Amazon Rekognition&lt;/a&gt;은 그 동안 대형 IT 기업의 전유물로 여겨졌던 딥러닝 기반의 이미지 검색 서비스를 개발자들이 손쉽게 쓸 수 있도록 진입 장벽을 낮췄다. 이제 개발자들은 자신의 이미지를 업로드만 함으로써 얼굴 인식, 감정 추출, 장소 및 장면 정보 등의 메타 정보 추출이 가능해졌다. AWS가 가지고 있는 완전 관리형 서비스와 합리적인 가격 조건을 가진 것도 장점이다. 프리티어로 월 5,000개의 이미지에 대해서는 무료로 분석을 제공한다. 사실 아마존은 이미 프라임 회원을 위한 무제한 이미지 서비스에 딥러닝 기반의 이미지 인식 기능을 통한 사진 분류 및 태깅 서비스를 제공하고 있기도 하다.&lt;/p&gt;
&lt;p&gt;마지막으로 선보인 서비스는 흔히 TTS(Text-to-Speech)라고 불리는 음성합성 엔진을 클라우드 서비스로 내놓은 것이다. &lt;a href="https://aws.amazon.com/ko/blogs/korea/polly-text-to-speech-in-47-voices-and-24-languages/"&gt;Amazon Polly&lt;/a&gt;는 24개 언어를 지원하며, 어른, 아이, 남자, 여자 등 서로 다른 47개 음성 종류를 제공한다. SSML이라는 음성 표현 마크업 언어를 통해, 서로 다른 2개국 언어를 섞는 다던가 감정 및강약 표현등을 합성해 낼 수 있다. 사용 비용도 저렴해, 허클베리핀의 모험 영문본을 음성 합성 MP3로 변환하는데 1회 2.41 달러에 불과하다.&lt;/p&gt;
&lt;p&gt;이번에 공개한 아마존 AI 서비스의 특징은 테스트 수준의 API에 머무는 것이 아니라 현실 개발자들이 실제 음성 혹은 이미지 기반 애플리케이션 개발 시, 바로 가져다 쓸 수 있을 만큼 높은 수준의 기술을 클라우드가 가진 장점을 이용하여 단순화했다는 데 있다. 특히, 아마존은 구매 배송 예측, 로봇을 이용한 물류 체계 등이 이미 다양한 머신 러닝 기술을 축적하고 있는데, 이러한 내부 기술을 서비스로 공개하는 패턴이 계속 늘어날 전망이다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■Amazon Go, 인공 지능 결합된 계산대 없는 오프라인 상점&lt;/strong&gt;&lt;br /&gt;
며칠 전 나온 &lt;a href="http://www.amazon.com/go"&gt;Amazon Go&lt;/a&gt;는 클라우드와 인공 지능 그리고 오프라인 매장이 결합된 가장 이상적인 형태의 소비자 경험을 제공해 준다. Amazon Go에서 쇼핑하는 고객 경험을 보여주는 동영상에서는 물건을 사려면 계산대에 줄을 서고 결제를 해야 한다는 통념도 완전히 뒤집었다. 매장에 들어서면서 아마존고 앱을 터치하면 자동으로 소비자를 인식한다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2016/12/09/yong2_KgKaX27UZnt5Sk.jpg" alt="Amazon Go 동영상(출처: Amazon.com 홈페이지)" width="550px" height="241px" /&gt; Amazon Go 동영상(출처: Amazon.com 홈페이지)&lt;/div&gt;
&lt;p&gt;이제 소비자는 원하는 제품을 쇼핑백에 담으면 된다. 매장에 내장된 컴퓨터 시각 센서와 생체인식 센서, 딥러닝 기술 등 AI 기술을 이용해 전자태그(RFID) 같은 센서 없이 정확히 소비자의 쇼핑 목록을 알아낸다. 아마존은 공식 블로그에 &amp;#8220;4년 전부터 &amp;#8216;줄을 서지 않고 계산대도 없이 쇼핑하는 방법이 없을까&amp;#8217;를 연구하기 시작했다&amp;#8221;며 &amp;#8220;컴퓨터 시각 센서와 AI 기술의 발전이 우리의 꿈을 실현할 수 있게 했다&amp;#8221;고 밝혔다.&lt;/p&gt;
&lt;p&gt;이제는 우리가 예측하지 못했던 현장에까지 클라우드의 전선은 계속 확대되고 있다. 그 동안, 기업이 제공하던 구름 뒷편에 애플리케이션이나 IT 업무를 위한 인프라나 솔루션 플랫폼을 넘어, 일반인들도 현실에서 클라우드 컴퓨팅을 만나는 날이 멀지 않았다.&lt;/p&gt;
&lt;p&gt;원문보기:&lt;br /&gt;
&lt;a href="http://m.zdnet.co.kr/column_view.asp?artice_id=20161209121829"&gt; http://m.zdnet.co.kr/column_view.asp?artice_id=20161209121829 &lt;/a&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=ZwcXaxm_on4:Z1DRCP76ALY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Sat, 24 Dec 2016 09:27:31 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>美 대선을 통해 본 현대 민주주의 착시 효과</title>
	<link>http://channy.creation.net/blog/1126</link>
	<description>&lt;p&gt;대이변이었습니다. 여론 조사, 소셜 미디어 심지어 전 세계 모두의 예상을 뒤엎고 도널드 트럼프가 미국 대선에서 승리했습니다. 오로지 당선을 예측한 건 16년전 TV 시리즈인 &lt;a href="https://www.youtube.com/watch?v=il9UZwRYV_c"&gt;‘심슨 가족’&lt;/a&gt; 뿐이었다는 우스개가 있을 정도죠.&lt;/p&gt;
&lt;p&gt;현대 민주주의를 제대로 꽃피운 미국에서 추잡한 과거와 막말 그리고 황당한 분리주의 정책들을 공약으로 내세우고도 트럼프가 당선된 이유가 궁금합니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;착시 1. 소셜 미디어 및 정보 격차&lt;/strong&gt;&lt;br /&gt;
여론 조사나 소셜 미디어를 보면 마치 힐러리 클린턴이 승리할 것으로 보였지만 결과는 달랐습니다. 대의 민주주의와 다수결의 원칙은 공감대 형성이라는 필연적인 절차를 거칠 경우, 유리하게 작동하지만 어느 일방의 목소리가 커지면 반대로 작용하기도 합니다.&lt;/p&gt;
&lt;p&gt;일견 인터넷과 소셜 미디어의 확산이 마치 민주주의와 소통의 창을 넓혀주는 것처럼 보이지만, 온라인과 현실 여론의 괴리가 심화될 수도록 전혀 다른 결과를 보인다는 점은 최근 우리 나라에서도 많이 보이는 현상이었습니다. (10년전 부터 우리 나라는 온라인 소통의 실험장이었죠.)&lt;/p&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2016/161109-twitter-election-feature.jpg" alt="" width="1320" height="816" /&gt;&lt;/p&gt;
&lt;p&gt;소셜 미디어를 사용해 보면, 자신의 의견과 다른 사람은 필터링을 하는 습성이 높고 추천 콘텐츠 자체도 자신의 성향과 유사해져서 사회적 아젠다의 경우, 일방의 의견에 갇히게 되는 폐해가 있습니다. 특히, 온라인에서는 도시에 사는 지식 수준이 높고 진보적인 성향인 사람들이 많아서 때문에 이로 인한 착시 효과가 증대되어 온 것이 사실입니다.&lt;/p&gt;
&lt;p&gt;이러한 정보의 격차로 인해 일방의 목소리가 커질 수록 다른 한편의 의견이 숨어 버리는 것이 현대 민주주의의 약점을 간과하면 안될 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;참고: &lt;a href="http://nypost.com/2016/11/09/this-election-reminds-us-that-social-media-is-not-reflective-of-real-life/"&gt;This election reminds us that social media is not reflective of real life&lt;/a&gt; – New York Post 및 &lt;a href="http://www.bloomberg.com/news/videos/2016-11-10/how-social-media-impacted-the-u-s-election"&gt;How Social Media Impacted the U.S. Election&lt;/a&gt; – Bloomberg&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;착시 2. 민주적 이기주의, 바뀌지 않은 보수&lt;/strong&gt;&lt;br /&gt;
많은 사람들이 주로 시골의 백인 결집 현상이 이번 이변을 낳은 것이라고 합니다만 사실 지난 3번의 미국 대선을 보면 꼭 그렇지만도 않습니다. 아래 도표를 보면, 공화당 투표층의 숫자는 크게 변화가 없습니다. 즉, 보수층의 고정 지지 현상은 예전이나 지금도 변화가 없습니다.&lt;/p&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2016/161109-us-election-results.jpg" alt="" width="1024" height="560" /&gt;&lt;/p&gt;
&lt;p&gt;단지 과거 오바마에게 갔던 투표량이 힐러리에게 없었다는 점이 가장 큰 요인입니다. 미국 대선의 특이한 캐스팅 보트 주(States)인 오하이오, 플로리다, 펜실베이니아 등 중도층에서 민주당 투표가 저조했다는 점 그리고 젊은이, 흑인, 히스패닉 등 과거 투표장으로 갔던 사람이 이번 대선에서는 절대적으로 줄었다는 점이 오히려 크게 좌우했습니다.&lt;/p&gt;
&lt;p&gt;“보수는 부패로 망하고, 진보는 분열로 망한다.”는 격언은 이번에도 유효했습니다. 따라서, 늘 그렇듯이 선거에 이길려면 중도층에 투표장으로 나가야 할 이유를 주는 것이 제일 중요한 승리 요인입니다. 그런 점에서 버니 샌더스가 민주당 후보였다면 어땠을까 하는 생각이 듭니다.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;참고: &lt;a href="http://www.pewresearch.org/fact-tank/2016/11/09/behind-trumps-victory-divisions-by-race-gender-education/"&gt;FactTank: Behind Trump’s victory: Divisions by race, gender, education&lt;/a&gt; – PewResearch Center&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;착시 3. 선거와 민주주의의 본성&lt;/strong&gt;&lt;br /&gt;
많은 분들이 이번 선거를 보고 미국 민주주의에 대한 회의와 실망을 많이 표출하셨습니다. 민주적 절차에 의해 투표로 선출되었다고 해서 항상 그게 옳지는 않습니다. 민주주의의 본성은 바로 “다름에 대한 인정” 그리고 &lt;a href="https://www.facebook.com/channyblog/posts/1273305409400925"&gt;오바마 대통령이 밝혔듯&lt;/a&gt;이 “선거와 민주주의의 본성”을 생각해야 할 때입니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;오늘 많은 사람이 기뻐했고, 덜 기쁜 사람도 많았습니다. 하지만, 그게 선거의 본성입니다. 논쟁적이고 시끄러우면서 항상 고무적인 결과가 나오지 못할 때도 있습니다… 미국이 걸어온 길도 직선은 아니었습니다. 우리는 지그재그로 걷고 어떤 사람은 전진한다고 생각하고 어떤 사람은 후퇴한다고 느낍니다. 그게 민주주의의 본성입니다… 선거에 처음 참여한 사람들은 실망하고 부정적으로 변하지 마세요. 당신이 변화를 이룰 수 있다고 생각해야 합니다. 클린턴 장관이 오늘 아침 이야기했듯이 옳음을 향해 싸워가는 과정은 가치 있습니다… (오늘 우리에게 필요한 것은) 통합, 포용, 우리의 제도와 삶의 방식에 대한 존중, 법에 의한 통치, 그리고 서로에 대한 존경심입니다.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2016/161109-obama-trump.jpg" alt="" width="672" height="378" /&gt;&lt;/p&gt;
&lt;p&gt;양극단에 있다고 평가받는 &lt;a href="http://www.sanders.senate.gov/newsroom/press-releases/sanders-statement-on-trump"&gt;버니 샌더스&lt;/a&gt; 역시 “정책에 따른 협력과 반대”를 확실히 하였습니다.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;트럼프가 일하는 가족의 삶을 향상 시키는 정책을 취한다면, 나와 진보 진영은 그와 함께 일할 준비가 되어 있습니다… 만약 그가 인종과 성별, 이민자를 차별하고 반 환경친화적 정책을 쓴다면 필사적으로 반대 의견을 낼 것입니다.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;이번 대선을 통해 저를 포함해서 한국에 계신 분들도 배운 점이 많습니다. 최근 최순실 게이트로 인해 우리 나라도 앞으로 나가가기 위한 진통의 과정에 있는데, 미래에 더 나은 민주 사회를 만들기 위해 무엇이 중요한지 어떻게 해야 할지 한번 더 고민하는 계기가 되었습니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=VKzOyXsbtHA:qKIldnseTx4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 10 Nov 2016 23:16:32 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1126#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>급하게 먹은 밥은 늘 체하는 법</title>
	<link>http://channy.creation.net/blog/1124</link>
	<description>&lt;p&gt;대학원 석사 1년차였던 1996년 11월 1일 첫 직장에 입사했습니다. 그러니까 딱 만으로 20년입니다. 스타트업으로 시작해서 포털 사이트 그리고 글로벌 기업으로 회사를 옮기면서 다양한 일을 해 봤고, 오픈 소스 및 개발자 커뮤니티 운영 및 대 정부 기관과 협력도 하고, 정부 연구 개발 사업도 많이 진행해 봤습니다. &lt;/p&gt;
&lt;p&gt;이러한 경험 중 한 가지 크게 배운 것은 바로 무엇이든 급하게 하면 항상 탈이 난다는 것입니다. 의도가 순수해도 과정을 무시하는 사람들과 일하면 결과적으로 좋은 결과를 얻지 못했습니다.&lt;/p&gt;
&lt;p&gt;최근 최순실 게이트로 인해 나라 꼴이 말이 아닌 상태로 가고 있는데, 이로 인해 미래 기술을 위한 IT업계의 노력과 스타트업 육성 부분에도 유탄을 맞기 시작했습니다. 아래는 최근에 나온 뉴스 기사들입니다.&lt;/p&gt;
&lt;p&gt;■ &lt;a href="http://media.daum.net/politics/others/newsview?newsid=20161028183359030"&gt;예산 7000억 넘는 문화창조융합벨트..실적은 &amp;#8216;쥐꼬리&amp;#8217;&lt;/a&gt;&lt;br /&gt;
■ &lt;a href="http://media.daum.net/digital/mobile/newsview?newsid=20161031174004662"&gt;은행권청년창업재단, &amp;#8216;금융판 미르&amp;#8217; 의혹에 &amp;#8220;전·현직 대통령과 연결고리 없어&amp;#8221;&lt;/a&gt;&lt;br /&gt;
■ &lt;a href="http://media.daum.net/politics/others/newsview?newsid=20161101202911900"&gt;&amp;#8216;창조경제&amp;#8217;로도 확산되는 최순실 게이트..미래부 &amp;#8216;타격&amp;#8217; 받나?&lt;/a&gt;&lt;br /&gt;
■ &lt;a href="http://media.daum.net/digital/others/newsview?newsid=20161101080509988"&gt;미래부, 750억 국가R&amp;amp;D과제 &amp;#8216;AIRI 몰아주기&amp;#8217; 물거품&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;이 중에는 정상적인 사업인데 의혹 수준에 머무는 것도 있고, 진행 과정 중에 문제가 있어 보이는 것들도 있습니다. 그러다 보니, 선의를 가지고 참여해서 정직하고 양심적으로 운영하던 IT업계 분들이 본의 아니게 심적 고난을 받기도 하고, 옆에 있는 분들이 덩달아 마음 고생을 하고 있습니다.&lt;/p&gt;
&lt;p&gt;특히, 국내 스타트업계의 산실로 알려진 디캠프와 미래 기술을 위해 만들어진 인공 지능 연구소(AIRI)에 방송이나 뉴스 기사의 무분별한 의혹 제기는 저도 충격적이었습니다. 전국에 산재한 창조경제혁신센터에서 일하시는 IT업계 분들도 꽤 있기 때문에 향후 여파가 적지 않을 것 같습니다. 이들 기관에 둥지를 틀고 있는 많은 신생 스타트업들에게도 영향이 적을 수 없습니다.&lt;/p&gt;
&lt;p&gt;&lt;img title="" src="http://channy.creation.net/wp/data/channy/2016/choi-sunsil-it-gate.png" alt="" width="1040" height="623" /&gt;&lt;/p&gt;
&lt;p&gt;돌이켜 보면, 이러한 일들이 벌어지게 된 근본 원인을 한번 살펴볼 필요가 있을 것 같습니다. &lt;strong&gt;공통점은 모두 정부가 주도해서 민간 대기업에게 각출하게 해서 만든 재단 혹은 단체로 인해 벌어진 일이라는 것입니다.&lt;/strong&gt; &lt;em&gt;(청와대와 문화체육관광부가 주도해서 전경련을 통해 대기업으로 부터 각출해 만든 미르재단과 K스포츠재단의 방식과 크게 다르지 않습니다.)&lt;/em&gt; 의도가 순수했다 하더라도, 그 과정이 좋지 않다면 결과는 불을 보듯 뻔할 것입니다.&lt;/p&gt;
&lt;p&gt;사실 정권 초기에 창조경제혁신센터가 만들어질 당시에도 IT업계 계신 대다수 분들이 기업들이 어쩔 수 없이 참여하는 전시 행정 사업이 될 것이라 전망했습니다. 청와대와 정부가 나서면, 어느 기업들도 거절할 수 없는 게 현실이니까요.&lt;/p&gt;
&lt;p&gt;정치 이슈에 대한 글은 극히 아껴왔지만, 지난 대선 전 &lt;a href="http://blog.creation.net/536"&gt;대통령 후보께, IT실험실에서 글로벌로&lt;/a&gt;이란 글에서 IT 분야의 규제 개혁과 자율적이고 공정한 경쟁 환경 육성에 그렇게 바랬습니다. 그러나, 기업을 줄세우고 정부가 주도하겠다는 과거 회귀식 정책 발상이 만든 참사는 피하기 어려울 것 같습니다.&lt;/p&gt;
&lt;p&gt;이제 4년이 지나고, 또 다시 새로운 결정을 해야 하는 대선이 다가오고 있는데 급하게 먹고 체하는 이런 과정이 되풀이되면 안되겠습니다. 저를 포함해 IT업계에 있는 분들도 올바른 과정으로 이루어지는 미래 지향적인 IT 정책인지 선별해서 스스로 검증해서 참여해야 합니다.&lt;/p&gt;
&lt;p&gt;이러한 문제를 빠르게 청산하고 다시는 이런 일이 되풀이 되지 않았으면 좋겠습니다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=m9QdQZKP3AQ:tZEV_rCPHno:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 01 Nov 2016 22:00:07 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1124#comments</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>[ZDNET 칼럼]  클라우드 서버의 혁신, 은밀하게 그리고 위대하게</title>
	<link>http://blog.creation.net/604</link>
	<description>&lt;p&gt;한때 조립 PC가 유행인 적이 있었다. 내 맘대로 원하는 사양의 CPU와 메모리 그리고 그래픽 카드를 조합하여 원하는 PC를 만드는 것이다. 하지만, 정보기술(IT) 서비스를 위한 서버 역시 물리적으로 조립은 가능하지만, 부품이나 안정성을 위해 벤더들이 제공하는 제품을 써야 하는 경우가 많았다. 벤더 중심으로 돌아가던 서버 시장은 클라우드 시대로 접어들면서 완전히 패러다임이 바뀌고 있다.&lt;/p&gt;
&lt;p&gt;클라우드를 사용하는 고객의 서비스 계획과 인프라를 확장하는 요구 사항이 서비스 업체에 지속해서 공유되고 있고, 이러한 점이 서비스에 적극적으로 반영되고 있기 때문이다. 부품과 생산 설비를 중심으로 주기적으로 서버 제품을 생산한다면 불가능했던 일이다.&lt;/p&gt;
&lt;p&gt;클라우드 시장을 선도하고 있는 AWS의 경우, &lt;a href="https://aws.amazon.com/ko/blogs/aws/ec2-instance-history/" target="new"&gt;EC2 인스턴스의 역사&lt;/a&gt;를 보면 IT 변화에 민감하게 가상 서버 사양을 계속 추가해 왔음을 알 수 있다. CPU를 많이 쓰는 프로그램, 메모리를 많이 쓰는 프로그램 그리고 디스크 입출력(I/O)을 많이 쓰는 경우 등등 다양한 애플리케이션에 맞게 입맛에 맞는 인스턴스 타입을 선택할 수 있다.&lt;/p&gt;
&lt;p&gt;따라서, 최근에 추가되는 Amazon EC2의 인스턴스 타입을 보면 IT 시장의 변화를 예측할 수 있는 실마리를 제공한다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 512MB 메모리 최소형 T2.Nano 인스턴스&lt;/strong&gt;&lt;br /&gt;
T2 인스턴스는 AWS에서 가장 낮은 사양의 인스턴스 타입으로 별도 프로세서 사용량 기준을 가지고, (예를 들어, CPU 20%만 사용 가능) 이를 넘어가는 경우 기존에 모아둔 CPU 크레딧을 사용하는 방식으로 효율적으로 자원을 활용할 수 있다. 가격도 저렴할 뿐만 아니라 필요할 때도 컴퓨팅 용량을 크레딧으로 사용할 수 있기 때문에 인기가 높다.&lt;/p&gt;
&lt;p&gt;최근 서비스 아키텍처가 마이크로서비스로 변화하면서 예전에 한 덩어리로 매우 큰 애플리케이션을 잘게 서비스로 쪼개는 경향이 높아졌다. 그러다 보니 개별 서비스의 사이즈는 작고, 컴퓨팅 용량을 많이 사용하지 않는 경우가 많다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-full wp-image-2071" src="http://blog.creation.net/wp-content/uploads/2016/11/aws-t2-instance.jpg" alt="aws-t2-instance" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;re:Invent 2015에서 T2.nano를 발표하는 버너 보겔스 아마존 CTO(출처: AWS)&lt;/small&gt;&lt;/center&gt;소형 웹 사이트를 운영하거나, 테스팅 및 모니터링이나 마이크로서비스용 서비스를 위한 최소형 인스턴스 타입을 원하게 되었는데 이를 위해 만든것이 &lt;a href="https://aws.amazon.com/ko/blogs/korea/ec2-update-t2-nano-instances-now-available/" target="new"&gt;t2.nano 인스턴스&lt;/a&gt;다. 항시 CPU 사용량이 5% 미만인 일반적인 웹 서비스에 딱 맞고, 512MB 메모리의 초소형 서버라고 할 수 있다. (물론 트래픽이 넘치면, 모아둔 크레딧으로 72분가량 100% CPU를 사용 가능하며, 필요 시 오토스케일링으로 서버를 확장할 수 있다.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 2TB급 메모리를 장착한 X1 인스턴스&lt;/strong&gt;&lt;br /&gt;
클라우드 컴퓨팅의 주요 고객은 스타트업과 중소 규모 기업이라고 생각하기 쉽지만, 최근 몇 년 사이 기업 IT의 새로운 표준으로 자리 잡으면서 엔터프라이즈 기업의 수요가 많이 늘어났다. 많은 엔터프라이즈 고객은 많은 메모리 용량을 가진 고성능의 인스턴스를 요구하기도 한다. 예를 들어, 인메모리 기반의 애플리케이션 즉, SAP HANA나 Microsoft SQL Server, Apache Spark 및 Presto 같은 대용량 데이터를 다루는 비지니스 분석 프로그램이다.&lt;/p&gt;
&lt;p&gt;이를 위해 만든 것이 바로 &lt;a href="https://aws.amazon.com/ko/blogs/korea/x1-instances-for-ec2-ready-for-your-memory-intensive-workloads/"&gt;X1 인스턴스 타입&lt;/a&gt;으로 네 개의 Intel® Xeon® E7 프로세서를 기반으로 하고 각 프로세서는 L3 캐시의 메모리 대역폭을 가지며 고성능 대용량인 2TB를 지원하고, 100개가 넘는 vCPU를 통해 이들 인스턴스가 동시 작업을 수행할 수 있다. 네트워크나 디스크 대역폭도 10Gpbs를 지원하고, X1 인스턴스를 클러스터로 묶으면 메모리 용량을 16TB까지 확장할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-2072" src="http://blog.creation.net/wp-content/uploads/2016/11/aws-x1-instance.jpg" alt="aws-x1-instance" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;X1인스턴스를 위한 인텔 Xeon E7을 소개하는 다이앤 브라이언트 인텔 부사장(출처:AWS)&lt;/small&gt;&lt;/center&gt;엔터프라이즈 기업이 원하는 고성능 컴퓨팅을 위해 굳이 비싼 장비를 갖춰 놓지 않아도, 테스트 업무, 주기적인 분석 및 고가용성 구성 등을 위해 클라우드를 활용할 수 있게 되었다. 이는 클라우드를 기업의 핵심 업무에도 사용하기 시작하고 있다는 청신호이다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;■ 16개 GPU를 장착한 P2 인스턴스&lt;/strong&gt;&lt;br /&gt;
최근 IT 업계에 화두는 단연 인공 지능(AI)이다. 특히, 딥러닝에 대한 고성능 컴퓨팅 인프라가 요구됨에 따라 지난 9월 30일 최대 16개의 GPU를 통해 CPU 연산을 병렬 처리할 수 있는 &lt;a href="https://aws.amazon.com/ko/blogs/korea/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" target="new"&gt;P2 인스턴스&lt;/a&gt;가 출시 되었다.&lt;/p&gt;
&lt;p&gt;P2 인스턴스는 192GB의 GPU 메모리, 4만 개의 병렬 처리 코어, 70테라플롭스의 단정밀도 부동 소수점 성능 및 23테라플롭스의 배정밀도 부동 소수점 성능을 갖춘 그야말로 공룡급 인스턴스 타입이다. 16개의 NVIDIA K80 GPU, 64개의 vCPU 및 732GiB의 호스트 메모리를 가진다. P2 인스턴스는 최대 16개의 GPU까지 GPUDirect&lt;img src="https://s.w.org/images/core/emoji/11.2.0/72x72/2122.png" alt="™" class="wp-smiley" /&gt;(피어 투 피어 GPU 통신) 기능을 제공하므로 여러 개의 GPU가 단일 호스트 내에서 함께 작동할 수 있다.&lt;/p&gt;
&lt;p&gt;CUDA 및 OpenCL을 사용하는 범용 GPU 컴퓨팅 애플리케이션을 위해 설계된 P2 인스턴스는 기계 학습, 고성능 데이터베이스, 전산 유체 역학, 컴퓨팅 금융, 내진 해석, 분자 모델링, 유전체학, 렌더링, 그리고 대량의 병렬 부동 소수점 처리 능력이 필요한 서버 측 워크로드에 매우 적합하다.&lt;/p&gt;
&lt;p&gt;또한, 이러한 가상 서버들이 클라우드 상에 보이지 않게 존재하고 있다고 생각하는 것이 일반적이다. 하지만 AWS의 전용 호스팅(Dedicated Hosts)를 이용하면, 우리 회사 가상 서버들만 지정된 AWS 데이터 센터 내부의 단일 물리 서버에 올릴 수 있다. 어떤 소프트웨어의 경우, CPU 코어당 특정 물리 서버에만 운용할 수 있도록 판매 라이선스를 제공하는 경우가 있다. 기업 데이터를 고유한 물리 서버에만 저장하도록 하는 법적 규정이나 규제 준수 사항이 있을 때도 있다. 전용 호스팅 서버는 이러한 기업의 요구 사항에도 충족할 수 있게 만들어졌다. 즉, 클라우드가 물리적인 서버도 빌려주는 시대이다.&lt;/p&gt;
&lt;p&gt;정보 기술의 변화와 컴퓨팅 활용에 대한 고객의 요구 사항이 계속 변화하고 있다. 그러한 변화의 조짐을 알고 싶다면 &amp;#8216;은밀하고도 위대하게&amp;#8217; 바뀌고 있는 클라우드 서비스 기술으 트렌드를 눈 여겨 볼 필요가 있다. 오는 11월 말이면 AWS가 주최하는 &lt;a href="http://bit.ly/awskr-reinvent2016-guide" target="new"&gt;re:Invent 행사&lt;/a&gt;가 라스베가스에서 열린다. 매년 20여가지 신규 서비스와 신 기능을 출시해왔기 때문에 많은 이목이 집중되고 있는 클라우드 업계의 대표 행사이다. 내년 클라우드 기술에 어떤 변화가 오는 서비스가 출시 될지 기대해 봐도 좋을 듯 하다.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=cLbGGTl4tKE:yOOJvGCmr00:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Tue, 01 Nov 2016 16:59:13 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>블로그 13년,  3000개의 여정</title>
	<link>http://channy.creation.net/blog/1121</link>
	<description>&lt;p&gt;가끔 아는 분들을 만나면, &amp;#8220;요즘 블로그가 뜸하시네요.&amp;#8221;라는 말을 듣게 됩니다. 과거 정보 채널이 블로그로 한정되어 있던 2000년대 중반 보다는 덜하지만 그래도 글쓰기는 계속하고 있습니다. 가끔 이긴 하지만, 본 개인 블로그와 Mozilla 커뮤니티 그리고 최근에는 회사에서 업무용으로 하는 AWS 한국 블로그까지 다양합니다.&lt;/p&gt;
&lt;p&gt;2년 전 새로운 회사로 옮기면서 시작한 &lt;a href="https://aws.amazon.com/ko/blogs/korea"&gt;회사 공식 블로그&lt;/a&gt;는 1년 반 만에 300개의 글을 쓰고 있으니, 최근에 제가 가장 많이 쓰고 있는 블로그입니다.&lt;/p&gt;
&lt;p&gt;특히, 2003년 10월 그러니까 지금부터 13년전 시작한 &lt;a href="http://www.mozilla.or.kr/community/blog/1"&gt;Mozilla 한국 커뮤니티 블로그&lt;/a&gt;의 경우, 현재까지 천 여개 글이 쓰여졌습니다. 그 중 일부는 자원 봉사자 몇 분이 도와 주셨지만, 제가 쓴 글도 (대부분 번역글) 700여개가 넘더군요.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter" title="" src="http://channy.creation.net/wp/data/channy/2016/channy-blog-activities-2003-2016.png" alt="" width="1522" height="634" /&gt;&lt;/p&gt;
&lt;p&gt;위의 표를 보시듯 13년 동안 다양한 블로그에 제가 썼던 글은 3,000개 정도입니다. 1년에 평균 200개 이상은 어디엔가 쓰고 있다는 이야기입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;개인: &lt;a href="http://channy.creation.net/archive"&gt;Channy&amp;#8217;s Blog&lt;/a&gt;&lt;small&gt;(계속)&lt;/small&gt;&lt;/li&gt;
&lt;li&gt;커뮤니티: &lt;a href="http://www.mozilla.or.kr/community/blog/"&gt;Mozilla 한국 커뮤니티&lt;/a&gt;&lt;small&gt;(계속)&lt;/small&gt;, &lt;a href="http://hacks.mozilla.or.kr/author/channy-yun/"&gt;Mozilla Hacks&lt;/a&gt;&lt;small&gt;(계속)&lt;/small&gt;&lt;/li&gt;
&lt;li&gt;업무: &lt;a href="http://aws.amazon.com/ko/blogs/korea/index"&gt;AWS 한국 블로그&lt;/a&gt;&lt;small&gt;(계속)&lt;/small&gt;, &lt;a href="http://daumdna.tistory.com"&gt;Daum DNA&lt;/a&gt;, &lt;a href="http://devondaum.tistory.com"&gt;Daum DevOn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;외부 활동: &lt;a href="http://futureweb.tistory.com/"&gt;미래웹포럼&lt;/a&gt;, &lt;a href="http://webappscon.tistory.com/"&gt;웹앱스콘&lt;/a&gt;, &lt;a href="http://liftasia.tistory.com"&gt;LiftAsia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;그래서 저를 만나시면, &amp;#8220;요즘 블로그가 뜸하시네요.&amp;#8221;라는 말 대신&lt;strong&gt; &amp;#8220;요즘은 어디에 글을 쓰시나요?&amp;#8221;&lt;/strong&gt;라고 물어 주시길 바랄께요~&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=gvWO53mYijs:RFgzn6p8tGQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 20 Oct 2016 22:27:44 +0000</pubDate>
	<comments>http://channy.creation.net/blog/1121#respond</comments>
	<author>Channy Yun</author>
</item>
<item>
	<title>[ZDNET 칼럼] 대학생, 무료 클라우드 서비스로 날개를 달아라!</title>
	<link>http://blog.creation.net/606</link>
	<description>&lt;p&gt;2학기 개강이 다가오고 있다. 에반젤리스트로서 해커톤이나 코딩 교육 행사에 가 보면, 꿈 많은 대학생들을 만나게 된다. 자신의 창업 아이템을 생각하고 있는 친구부터 학교 수업에서 하는 프로젝트를 하는 전공 학생들까지, 클라우드 컴퓨팅을 통해 뭔가 시도해 보려는 열정가들이 매우 많다.&lt;/p&gt;
&lt;p&gt;이들 학생들이 의례히 물어보는 것이 있다.바로 “무료로 클라우드 서비스를 이용할 수 없나?” 라는 질문이다. 부족한 것이 많은 대학 시절이기에 뭔가 만들어 보기 위해 사비를 들여 서버, 스토리지, 데이터베이스 등의 인프라를 제대로 갖추기는 어려울 것이다. 이러한 질문을 갖고 있는 수 많은 대학생을 위한 몇 가지 작은 팁을 공유하고자 한다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2016/08/30/delight_3NjTuKpyH34p.jpg" alt="(상) 멋쟁이 사자처럼 3기 (하) 유니톤 대학생 해커톤 현장 (출처: LikeLion.net 및 Unithon 홈페이지)" width="550px" height="356px" /&gt;&lt;br /&gt;
(상) 멋쟁이 사자처럼 3기 (하) 유니톤 대학생 해커톤 현장 (출처: LikeLion.net 및 Unithon 홈페이지)&lt;/div&gt;
&lt;p&gt;&lt;b&gt;■ 일년간 공짜로 제공하는 프리티어&lt;/b&gt;&lt;br /&gt;
클라우드 컴퓨팅 리더로 잘 알려진 아마존 웹 서비스에서는 &lt;a href="https://aws.amazon.com/ko/free"&gt;AWS 프리티어&lt;/a&gt;를 제공한다. AWS 계정을 만들고 나면, 가입 후 1년간 다양한 서비스에 대해 정해진 한도 내에서 무료로 사용할 수 있는 서비스다.&lt;/p&gt;
&lt;p&gt;예를 들어, 가상 서버의 경우 t2.micro(1vCPU 및 1GM 랩)라는 인스턴스 타입에 한해 리눅스 하나와 윈도 서버 각각 하나에 대해서 무료로 이용할 수 있다. 이를 작은 웹 서버로 이용하거나, 자신의 프로젝트 서버로 이용할 수 있다. 재미 있는 점은 클라우드 서비스의 특유의 종량제 덕분에 월간 750시간이 무상이라는 점이다. 즉, 가상 서버를 항시 켜 두게 되면 한달 내내 750시간 미만이므로 하나의 인스턴스에 대해 무상으로 이용 가능하다.&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2016/08/30/delight_B2gw6gnYWHNA.jpg" alt="AWS 프리티어를 통한 무료 웹 사이트 운영 비용 절감 방법- 출처: AWS 간단 계산기" width="550px" height="308px" /&gt; AWS 프리티어를 통한 무료 웹 사이트 운영 비용 절감 방법- 출처: AWS 간단 계산기&lt;/div&gt;
&lt;p&gt;​&lt;br /&gt;
이는 가상 서버 10대를 75시간만 이용해도 무상으로 가능하다는 뜻이다. AWS API를 이용한 간단한 프로그래밍만으로 10대를띄웠다가 중지를 반복할 수 있다. 필요하다면 100대를 7시간 이용해도 무료이다. 다만, 이 한도를 넘어가면, 과금이 될 수 있으니 주의하자. 계정 설정에서 과금 알림을 하거나, 무료 사용량이 얼마나 남았는지 확인할 수 있으니 주의해서 보면 된다.&lt;/p&gt;
&lt;p&gt;가상 서버 말고도 다양한 프리티어 서비스가 있다. 데이터베이스 서버 역시 db.t2.micro에 대해 20GB 스토리지와 천만 I/O에 대해 무료다. 하드디스크 격인 아마존 EBS(Amazon Elastic Block Store) 스토리지는 30GB, 대용량 스토리지인 아마존 S3 (Amazon Simple Storage Service)서비스도 20GB, NoSQL 서비스인 아마존 다이나모 DB(Amazon Dynamo DB)도 25GB를 프리티어로 제공한다. 1년이 지나도 종료되지 않는 무료 서비스도 있다. 예를 들어, 이메일 전송 서비스인 아마존 SES(Amazon Simple Email Service)는 월 6만건, 모바일 푸시 노티를 보내는 아마존 SNS(Amazon Simple Notification Service)는 월 백만건, 모바일 분석 서비스는 월 1억건 등 주요 모바일 서비스 등은 한도 내에서 무상 이용이 가능하다.&lt;/p&gt;
&lt;p&gt;특히, 최근 클라우드 기술 분야의 혁신 중 하나로 손꼽히는 AWS Lambda 서비스 역시, 월 백만건 320만초내에서 무료이다. 이 서비스는 서버없이(Serverless) 프로그램 코드만 업로드해서 실행 시간에만 과금하기 때문에 작은 프로젝트를 서버 관리 부담없이 만들기에 적합하다. AWS IoT 및 Amazon API Gateway를 이용하면, 작은 IoT 기반 API 플랫폼을 손쉽게 만들 수 있다.&lt;/p&gt;
&lt;p&gt;AWS 클라우드에 대해 처음 접하는 학생이라면 &lt;a href="https://aws.amazon.com/ko/blogs/korea/how-to-learn-aws-cloud-books/"&gt;AWS 클라우드를배우는방법시리즈&lt;/a&gt;를 참고하면 좋다. 다른 클라우드 서비스에도 유사한 프로그램을 운영하고 있기도 하다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■ 대학생에게만 주는 무료 크레딧 활용 방법&lt;/b&gt;&lt;br /&gt;
AWS에서는 클라우드 컴퓨팅 분야에 취업을 하기 위해 공부하려는 학생들과 가르치는 교수님을 위해 &lt;a href="https://aws.amazon.com/ko/education/awseducate/"&gt;AWS Educate&lt;/a&gt;라는 프로그램을 작년 5월부터 시작했다. 대학생이라면 학교 메일로 금방 가입할 수 있으며, 가입 후 매년 40달러 상당의 크레딧을 받게 된다. 만약, 여러분의 학교가 기관으로 직접 가입을 했다면 100달러를 졸업시 까지 매년 받을 수 있다. (현재, 전 세계 900여개 학교가 가입 되어 있으며, 국내에서도 서울대, 연세대, 서강대, 한양대 등 12개 학교가 가입되어 있다. 가입 조건은 없기 때문에 학과 교수님 혹은 학교 전산소 담당자에게 요청하면 된다.)&lt;/p&gt;
&lt;div class="img"&gt;&lt;img src="http://image.zdnet.co.kr/2016/08/30/delight_NG8PdL3YJJNn.jpg" alt="클라우드 교육 프로그램을 운영하는 AWS Educate 홈페이지- 출처: AWS" width="550px" height="390px" /&gt; 클라우드 교육 프로그램을 운영하는 AWS Educate 홈페이지- 출처: AWS&lt;/div&gt;
&lt;p&gt;100달러가 적으면 적고 많으면 많은 양일 수 있을 것이다.하지만 프리티어로 제공하는 t2.micro 인스턴스를 유료로 한달 사용할 때 비용이 대략 9.5달러(버지니아 리전 기준)가 되기 때문에, 졸업 시까지 테스트 서버로 운영할 만큼은 되기 때문에 유용하다. 혹시 신용 카드가 없거나, 과금이 많이 나오는 것이 우려된다면 가입 시 AWS Educate Starter Account로 선택 가입할 수 있다. 크레딧은 조금 적게 부여되지만, 부여된 크레딧 이상 청구되지 않는 장점이 있다.&lt;/p&gt;
&lt;p&gt;AWS Educate에 가입한 학생들에게는 600달러 상당의 AWS 기술 에센셜 온라인 교육 과정 및 실습 환경에 무료 접근이 가능하다. 조금만 공부하면 공인 AWS Solution Architect 자격증을 취득할 수 있는데, 취업에도 유리하다. 미국의&lt;a href="https://www.globalknowledge.com/us-en/content/articles/top-paying-certifications/"&gt;15 Top-Paying Certifications for 2016&lt;/a&gt;보고서에 따르면, AWS 공인 자격증이 미국 상위 연봉 순위 1위를 차지하기도 하였다.&lt;a href="https://www.youtube.com/watch?v=BVWZY43tkCg"&gt;AWS Educate 가입방법 (동영상)&lt;/a&gt;그리고&lt;a href="http://awsblogskr.s3-ap-northeast-2.amazonaws.com/aws-educate-for-student.pdf"&gt;Educate 학생용가이드(PDF)&lt;/a&gt;를참고하기 바란다.&lt;/p&gt;
&lt;p&gt;AWS뿐만 아니라 GitHub에서도 &lt;a href="https://education.github.com/pack"&gt;Student Developer Pack&lt;/a&gt;이라는 프로그램을 운영하고 있는데, 다양한 개발 도구 및 교육 사이트를 무상으로 지원하고, AWS 크레딧도 추가로 15달러를 얻을 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;■ 대학생만을 위한 해커톤과 코딩 교육 동아리&lt;/b&gt;&lt;br /&gt;
소프트웨어 코딩 교육이 미국 뿐만 아니라 전 세계 주요 관심사가 되면서, 국내에서도 전공과 관계 없이 전체 학생들에게 코딩 교육을 의무적으로 가르치는 대학교가 늘고 있다. 사실 졸업을 해서 취업을 해보면, 거의 모든 회사들의 업무가 IT 환경을 기반하고 있다. 많은 경우 소프트웨어 개발자와 공동 업무를 진행하는 경우가 많기 때문에 굳이 SW 개발을 하지 않더라도 코딩 교육이 주는 효과는 적지 않을 것이다.&lt;/p&gt;
&lt;p&gt;뿐만 아니라, 동료 학생들과 어울러서 소프트웨어를 활용하여 다양한 아이디어를 구현하는 기회를 갖는 것도 좋은 경험이 될 것이다. 잘 알려진 바와 같이오늘날 가장 인기 있는 소셜 네트워크 서비스이자, 10억명이 넘는 사용자를 자랑하는 페이스북은 하버드대 학생들 사이에서 만든 작은 서비스에서 시작했다.&lt;/p&gt;
&lt;p&gt;국내에서 클라우드 서비스를 활용하는 대학생 교육 및 해커톤 프로그램이 크게 두 가지가 있다. 하나는 매년 컴퓨터 비전공 대학생 중 천 명을 선발해 몇 개월 간 코딩 교육을 해주는 ‘&lt;a href="http://likelion.net/home/index"&gt;멋쟁이사자처럼&lt;/a&gt;’이라는 비영리 교육 동아리이다. 유명 해커인 이두희씨가 설립하여, 올해로 4기를 배출하였다. 4년동안 98개 학교의 1900여명이 참여하여 200여개가 넘는 아이디어를 구현하는 결과를 내놓았다. 본 활동에도 작년 부터 AWS가 교육 지원을 하고 있다. 멋사의 교육 과정의 일부는 &lt;a href="http://codelion.net/"&gt;코드라이언&lt;/a&gt;을 통해서도 맛볼 수 있다.&lt;/p&gt;
&lt;p&gt;대학생 개발 동아리인 &lt;a href="http://startreal.org/"&gt;REAL&lt;/a&gt;, &lt;a href="http://www.yapp.co.kr/"&gt;YAPP&lt;/a&gt;, &lt;a href="http://www.teamnexters.com/"&gt;NEXTERS&lt;/a&gt; 3개 IT 연합동아리가 공동 주관하는 ‘&lt;a href="https://www.facebook.com/unithonWithU/"&gt;유니톤&lt;/a&gt;’이라는 해커톤 역시 매년 2차례씩 개최하고 있다. &lt;span class="textexposedshow"&gt;같은관심사를가진또래들과즐겁고편안하게코딩을 경험하고, 디자인 등 다른 분야 친구들과 협업을 해 보는 경험을 하는 시간으로 2박 3일간 진행한다. 이들 역시 경쟁이 아닌 각 팀의 결과물을 직접 볼 수 있는 시연 부스를 마련해 궁금증을 해결하고 서로의 결과물에 대한 피드백과 격려를 해 주는 행사를 만들고 있다는 점에도 독특하다. 대학생 연합 동아리이고 학기 초인 만큼 신입 회원을 모집 중이니, 꼭 가입 해 보길 권한다.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;구하라 주실 것이요, 찾으라 찾을 것이요, 문을 두드리라 그리하면 열릴 것이라’ 그리고 뜻이 있는 곳에 길이 있다. 도전은 여러분의 몫이다.&lt;/p&gt;
&lt;p&gt;원문보기: &lt;a href="http://blog.creation.net/"&gt; http://m.zdnet.co.kr/column_view.asp?artice_id=20160830104938&lt;/a&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=dnX_ZhsOOcs:CvsA77SxjPw:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Thu, 01 Sep 2016 09:55:07 +0000</pubDate>
	<comments />
	<author>Channy Yun</author>
</item>
<item>
	<title>하이브리드 웹 앱스토어 플랫폼의 종말</title>
	<link>http://blog.creation.net/603</link>
	<description>&lt;p&gt;구글이 며칠 전 윈도우/맥/리눅스용 크롬 브라우저에서 &lt;a href="http://blog.chromium.org/2016/08/from-chrome-apps-to-web.html"&gt;크롬앱 지원을 단계적으로 폐지&lt;/a&gt;하는 계획을 발표했습니다. 크롬 브라우저에 사용하는 앱은 웹 기술을 기반으로 제작된 것으로, 2010년 구글 I/O에서 처음 소개된 후, &lt;a href="http://blog.creation.net/477"&gt;크롬 웹스토어 개념으로 확장&lt;/a&gt;되었습니다. &lt;/p&gt;
&lt;p&gt;&lt;img src="http://blog.creation.net/wp-content/uploads/2016/08/chrome-webstore-firefox-marketplace-1024x391.png" alt="" width="1024" height="391" class="aligncenter size-large wp-image-2068" /&gt;&lt;br /&gt;&lt;center&gt;&lt;small&gt;(좌) 크롬 웹 스토어, (우) 파이어폭스 마켓플레이스&lt;/small&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;물론 크롬앱 지원을 중단해도 현재 크롬에서 유용하게 사용되고 있는 확장 프로그램과 테마는 영향을 받지 않습니다. 향후 2016년 말 윈도우/맥/리눅스 사용자들이 크롬 웹스토어를 이용시 크롬앱이 지원 대상에서 제외되며 크롬 OS에서만 지원되고, 2017년 후반에는 크롬 웹 스토어에 노출되지 않고, 2018년 초에는 이미 설치한 크롬앱도 윈도우/맥/리눅스에서 실행되지 않는다고 합니다. &lt;small&gt;(2011년 발표된 크롬 OS는 웹 브라우저 기반 데스크톱 OS로서, 지금도 이를 탑재한 크롬북은 교육 시장에서는 꽤 인기가 높습니다. 하지만, 이 계획 역시 잠정적인 것으로 보입니다.)&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;구글 블로그에 따르면, 실제 크롬앱 사용자가 크롬 유저 중 패키지 앱 사용자는 1%도 되지 않으며, 호스팅 앱 사용자도 네이티브 앱 수준을 대체할만한 새로운 종류의 웹 표준 API이 생겨나 &lt;a href="http://mozilla.github.io/progressive-apps-hq/"&gt;&amp;#8216;프로그레시브 웹 앱'(Progressive Web Apps)&lt;/a&gt; 개발이 가능해 진 것을 이유로 들었습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;데스크톱 기반 웹앱의 몰락&lt;/strong&gt;&lt;br /&gt;
2010년도 초 당시 크롬 웹스토어와 크롬앱은 차세대 앱 플랫폼이 될 거라는 기대를 많이 받았습니다. HTML5를 기반으로 웹 애플리케이션 및 웹 OS에 대한 관심이 한참 높을 때였고, 구글 내에서도 안드로이드와 크롬 OS가 &lt;a href="https://techcrunch.com/2011/05/17/google-chrome-android/"&gt;플랫폼 경쟁을 벌일 때&lt;/a&gt;였기 때문입니다. &lt;/p&gt;
&lt;p&gt;구글은 크롬 브라우저를 기반으로 데스크톱에서 웹 기반 플랫폼을 모바일에서는 안드로이드를 미는 형국이었습니다. 그런데, 반전은 구글 내부의 크롬을 담당하던 선다 파차이가 안드로이드를 맡은 후 일어났습니다. 데스크톱 보다는 모바일에 힘이 실리면서, 안드로이드를 밀면서 어느 정도 예견되었고, 그 이후로 CEO가 되면서 완전히 정리되는 양상입니다.&lt;/p&gt;
&lt;p&gt;크롬앱은 네이티브 앱과 웹앱의 간극을 좁힐 수 있는 다양한 API를 제공했고, PC에 설치해서 사용할 수 있는 기능을 제공했지만 하이브리드형 앱의 한계를 극복하지 못했습니다. 전형적인 데스크톱 웹 OS인 크롬 OS도 안드로이드 앱을 설치할 수 있는 방향으로 바뀌면서, 장기적으로는 안드로이드 기반으로 확장될 가능성이 높습니다. 구글이라도 역시 웹 플랫폼과 앱 플랫폼의 차이를 좁히기는 어려웠습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;모바일 기반 웹앱의 몰락&lt;/strong&gt;&lt;br /&gt;
구글 크롬 웹 스토어와 함께 주목을 끈 또 하나의 웹 앱스토어는 &lt;a href="http://blog.creation.net/518"&gt;Mozilla의 Firefox Marketplace&lt;/a&gt;입니다. 2011년 오픈 소스로 만든 모바일 웹 OS인 Firefox OS를 지원하기 위해 만든 웹 앱스토어로서, 다양한 Firefox OS 기기 뿐만 아니라 안드로이드와 PC 테스크톱에 앱 설치 기능을 제공하기도 했습니다.&lt;/p&gt;
&lt;p&gt;지난 2월 Mozilla는 &lt;a href="https://discourse.mozilla-community.org/t/firefox-os-connected-devices-announcement/6864"&gt;안드로이드 및 데스크톱 앱 설치 기능을 중단&lt;/a&gt;하기로 하였습니다. 기존 파이어폭스 OS 사용자를 위한 패키지 앱 지원은 2018년 1월까지 지원될 예정이긴 하지만, 모바일 단말기용 웹 OS 개발이 현재 중단된 상태이기 때문에 기존 사용자를 위한 웹 앱 지원 역시 잠정적으로 중단될 수 있습니다.&lt;/p&gt;
&lt;p&gt;Mozilla의 웹앱 생태계에 대한 변화도 구글과 크게 다르지 않습니다. 외견 상으로는 웹 기술 양상이 네이티브 브라우저 기능 개선 위주로 다시 짜여지고 있지만, 기존 PC 혹은 모바일 운영 체제 플랫폼과 경쟁은 쉽지 않다는 점은 다시 한번 입증된 것이라 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;팜이 만들고 HP가 인수했던 webOS (지금은 LG전자가 소유 중), 삼성전자의 타이젠 OS 등이 아직은 웹 OS로서 명백을 이어가고 있습니다만 여전히 성공 여부가 불투명합니다. 아직 TV 등 전자 기기, 웨어러블, 사물 인터넷 등의 영역에서 아직 가능성이 남아 있기는 합니다만&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;웹은 웹으로&amp;#8230; 앱은 앱으로&amp;#8230;&lt;/strong&gt;&lt;br /&gt;
웹 기술이 발전하면 앱 생태계를 완전히 장악할 거라는 장밋빛 희망이 있을 때가 있었습니다. 제 스스로도 Mozilla와 웹 표준 커뮤니티의 일원으로서 그런 전망을 많이 내 놓기도 하였습니다. 하지만, 소프트웨어 플랫폼이라는 것이 그리 쉽게 하나로 합쳐지기엔 너무나 많은 변수가 있습니다. 아마 크롬 웹 스토어, 파이어폭스 마켓 플레이스 등은 이러한 과도기 기술의 실패 사례가 될 것입니다.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://appindex.com/wp-content/uploads/2015/12/progressive-web-apps-konstantinfo.jpg" alt="" class="aligncenter size-large wp-image-2068" /&gt;&lt;/p&gt;
&lt;p&gt;그렇다고, 웹 개발자들이 이러한 미션을 포기한 것은 아닙니다. 2015년 6월 Alex Russel에 의해 기존 플랫폼을 활용한 하이브리드 방식을 벗어나 완전히 네이티브 환경의 앱과 같은 웹 브라우저 기반 앱으로 구현해 보자는 &lt;a href="http://han41858.tistory.com/13"&gt;&amp;#8216;프로그레시브 웹 앱&amp;#8217;&lt;/a&gt;에 대한 개념이 시작되었기 때문이죠. 현재 구글을 주축으로 &lt;a href="https://blogs.windows.com/msedgedev/2016/07/08/the-progress-of-web-apps/"&gt;마이크로소프트&lt;/a&gt;, &lt;a href="http://mozilla.github.io/progressive-apps-hq/"&gt;Mozilla&lt;/a&gt;, &lt;a href="https://pwa.rocks/"&gt;Opera&lt;/a&gt; 등이 동참을 하고 있습니다.&lt;/p&gt;
&lt;p&gt;역사는 돌고 됩니다. 그리고, 경험을 기반으로 다시 새로운 혁신이 일어납니다. 과도기적인 웹앱스토어는 몰락했지만, 앞으로 웹 기술이 어떤 플랫폼 변화를 보여줄 지 기대됩니다. 웹 만큼 빠르고 혁신적으로 움직이기 유연한 플랫폼이 없으니까요. &lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/channy?a=I5dxOs-QbMU:aeLEt4uAV64:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/channy?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;</description>
	<pubDate>Mon, 22 Aug 2016 22:30:45 +0000</pubDate>
	<comments>http://blog.creation.net/603#respond</comments>
	<author>Channy Yun</author>
</item>

</channel>
</rss>
