<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Backup of Brain</title>
	<atom:link href="https://printf.kr/feed" rel="self" type="application/rss+xml" />
	<link>https://printf.kr</link>
	<description>Note, Think, Save</description>
	<lastBuildDate>
	Sun, 24 Feb 2019 09:16:01 +0000	</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.1.1</generator>
	<item>
		<title>[2017년 회고] 주니어 시스템 엔지니어로써의 1년</title>
		<link>https://printf.kr/archives/394</link>
				<comments>https://printf.kr/archives/394#comments</comments>
				<pubDate>Sun, 31 Dec 2017 11:23:41 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[미분류]]></category>

		<guid isPermaLink="false">https://printf.kr/?p=394</guid>
				<description><![CDATA[나에게도 생소했던 시스템 엔지니어로써의 1년을 정리한다 2017년 1월 2일 나는 IT인프라 직군으로 입사를 하였고, 현재는 시스템 엔지니어라는 역할을 가지고 일을 하고 있다. 지난 1년동안 나에게도 생소했던 시스템 엔지니어로써의 1년을 정리해보려고 한다. 입사하면서.. 그동안 학교에서 배운 것 그리고 활동한 것은 개발에 대한 것이었다. 개발자들과 생활하였고 개발에 대한 문화를 배우고 경험해왔다. 하지만 내가 앞으로 지낼 곳은 인프라를&#160;...]]></description>
								<content:encoded><![CDATA[<blockquote><p>
  나에게도 생소했던 시스템 엔지니어로써의 1년을 정리한다
</p></blockquote>
<p>2017년 1월 2일 나는 IT인프라 직군으로 입사를 하였고, 현재는 시스템 엔지니어라는 역할을 가지고 일을 하고 있다.<br />
지난 1년동안 나에게도 생소했던 시스템 엔지니어로써의 1년을 정리해보려고 한다.</p>
<h2>입사하면서..</h2>
<p>그동안 학교에서 배운 것 그리고 활동한 것은 개발에 대한 것이었다. 개발자들과 생활하였고 개발에 대한 문화를 배우고 경험해왔다.<br />
하지만 내가 앞으로 지낼 곳은 인프라를 다루는 곳이다. 평소에 IT인프라에 대해 많은 호기심이 있었다. 쉽게 접할 수 없는 분야이기도 하고 시설과 장비 또한 쉽게 접할 수 없어서 나에겐 우주와 같이 선망의 대상이었다.</p>
<p>인프라에 대해 잘 몰랐던 나는 입사하면서 시스템 아키텍처를 설계하고(예를 들면 MQ는 무엇을 쓰고 어떻게 구성할 것이며.. 디비들은 어떻게 구성할 것인가 등..) 테스트하고 서비스의 인프라를 구축/운영하는 업무를 하게 되지 않을까? 라는 생각을 가졌다.</p>
<p>하지만, 실무에서의 인프라는 내가 처음에 생각했던 것과는 약간 다른 세상이었고, 더 큰 매력을 가지고 있었다.</p>
<h2>신입 사원 교육</h2>
<p>회사에서 자체적으로 진행하는 신입 사원 교육은 건너뛰고, 팀에 배정받은 다음에 진행한 교육이다. 처음 신입 사원 교육에 진행할 때 이용한 도서는 &#8220;<a href="http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=42105813">인프라 엔지니어의 교과서</a>&#8221; 이다.<br />
<img class="border-image" src="https://image.aladin.co.kr/product/4210/58/cover/8966187609_1.jpg" alt="인프라 엔지니어의 교과서" /></p>
<p>책은 매우 얇았지만 IT인프라에 대한 모든 내용의 엑기스를 정말 잘 담아냈다. 이 책을 읽고 구두 시험을 보고, 책에서 설명하는 내용이 실제 회사 인프라에 어떻게 적용되어 있는지를 배웠다. IT인프라에 대해 맛을 보고 싶은 분에게 추천한다.</p>
<p>이 책을 시작으로 &#8220;<a href="http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=3641299">서버/인프라를 지탱하는 기술</a>&#8221; 과 &#8220;<a href="http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=25047966">서버 부하분산 입문</a>&#8221; 을 읽고 세미나를 하면서 시스템 엔지니어로서 기본 역량을 다져나갔다.</p>
<p>동시에 회사 인프라에 대한 교육도 진행되었다. 대충 아래와 같은 내용으로 교육을 받았다.</p>
<ul>
<li>IDC 에 대한 이해
<ul>
<li>공조, 전력, 화재예방, 입/출고 처리, 라벨링, 하드웨어 파트 관리</li>
<li>케이블링, IDC 모니터링 등..</li>
</ul>
</li>
<li>IDC 네트워크 장비에 대한 이해</li>
<li>네트워크 ACL 및 보안 정책</li>
<li>서비스 별 IP 대역 및 IP 할당/관리 정책</li>
<li>다양한 인프라 운영도구
<ul>
<li>CMDB 포함</li>
</ul>
</li>
<li>표준 서버 스펙 및 OS 설정</li>
<li>장애 대응 방법 </li>
<li>등&#8230;</li>
</ul>
<p>적어놓으니 엄청 많다. 저것들이 시스템 엔지니어로써 업무를 하기 위한 기본 지식이고 서비스별로 특화된 지식들도 더 필요하게 된다.</p>
<ul>
<li>DNS</li>
<li>CDN</li>
<li>GSLB</li>
<li>Storage</li>
<li>Backup</li>
<li>VMWare ESXi</li>
<li>Windows Hyper-V</li>
<li>등등&#8230;</li>
</ul>
<p>배워야할 것들이 매우 많기에 지금도 나는 업무를 받으면서 계속해서 공부하고 배워나가고 있다.</p>
<h2>IDC에 대한 이야기</h2>
<p>나에게 IDC는 환상의 나라 에버랜드와 같은 곳이었다. 고등학생 때 친구들과 서버를 구매하여 가지고 놀았기에 몇 번 IDC에 가본적은 있지만, 갈 때 마다 IDC는 신비로운 공간이었고 그 내부를 더 알고 싶었다. 심심할 때면 Youtube 에서 데이터센터, IDC로 검색해서 나오는 데이터센터 소개 영상을 보면서 꿈을 꾸곤 했었다.</p>
<p>이제 나에게 IDC는 더 이상 환상의 나라 에버랜드와 같은 곳이 아니다. IDC에서 직접 일 하는 것은 아니지만 IDC에 있는 서버들과 인프라 요소 하나하나가 나의 삶의 터전이 되었다. 하지만 난 아직도 IDC에 가면 눈이 휘둥그래져서 구경하곤한다.</p>
<p>내가 다니는 회사는 국내에 몇 안되는 자사가 직접 설립한 IDC를 보유한 회사이다. 이것은 주니어 시스템/인프라 엔지니에게 엄청난 복지(?)이다. IDC이용자가 아닌 제공자이기에 IDC에 대한 정말 모든 것들을 생생하게 경험하고 보고 들을 수 있기 때문이다. 상세한 네트워크 구조와 그에 쓰이는 장비들..처음에 IDC에 갔을 때, 통신사와 연결되어있는 코어 라우터를 정말 한동안 바라보곤 했었다..  IDC의 다양한 시설(공조, 전력 등)을 관리하는 방법, 모니터링 대시보드.. 네트워크 현황 등..정말 신기한 것들로 가득차있는 곳이다.</p>
<p>요즘은 클라우드를 많이 쓰기에 많은 주니어 개발자들이 IDC를 경험하진 못할 것 같다. 클라우드라고 부르는 것도 결국 물리적인 장비들로 이루어져있고 IDC가 없으면 안된다는 것을 알고나면 IDC가 궁금해질 것이다. 살면서 꼭! 한번쯤은 IDC에 가보는 것을 추천한다.</p>
<h2>무엇을 공부하였나?</h2>
<p>지금까지 내가 경험한(직접/간접) 시스템 엔지니어의 주요 업무는 아래와 같다.</p>
<ul>
<li>인프라 용량 산정을 통해 선정된 스펙에 맞는 서버 공급
<ul>
<li>디스크, CPU, 메모리 등등.. 상황에 맞게 재조립해서 쓴다</li>
<li>그리고 운영체제를 설치한다.</li>
</ul>
</li>
<li>서비스에 맞는 시스템 설계 및 구축
<ul>
<li>네트워크에 대한 내용은 네트워크팀에서 해준다.</li>
<li>서비스에 맞는 시스템 설정 및 관리</li>
</ul>
</li>
<li>DNS/CDN 설정 및 모니터링</li>
<li>장애 시 1차적 대응 및 원인 분석</li>
</ul>
<p>이 외에도 시스템 엔지니어의 업무영역은 매우 넓다. (아직 1년차라서 모르는게 많을 뿐이다)</p>
<p>1년동안 지내면서 내가 느낀 시스템 엔지니어에게 가장 필요한 스킬은 &#8216;<strong>리눅스</strong>&#8216; 이다. 회사의 대부분 시스템은 리눅스로 구축이 되어있다. 리눅스에 대한 깊은 이해가 없이는 업무를 절대로 진행할 수 없다. 사소한 설정 하나하나가 어떤 것을 의미하는지, 여러 시스템 로그들은 어떻게 분석하는지, 시스템 설정은 어떻게 해야하는지(예: bonding), 파티션 잡는 방법 등등 리눅스라는 운영체제에 대해 아주 깊은 지식이 필요로 한다. 이 지식을 기반으로 더 고급진(Docker라던지 Openstack 이라던지, HAProxy 라던지..) 기술들을 익히고 연마해 나가는 것이다.</p>
<p>그리고! 또 제일 중요한 것이 하나 있었다. HP, DELL, IBM 등 다양한 벤더에서 나오는 제품에 대한 이해. 어차피 엔지니어들은 물리적인 장비를 직접적으로 다루고 그에 대한 이해가 반드시 필요하다는 것을 알게 되었다. 모델별로 어떤 특징을 가지고 있는지, 설정은 어떻게 바뀌는지.. 하나하나가 업무를 하는데 정말 필요한 지식들이었다.</p>
<p>앞으로도 나는 공부할게 너무 많아서 하하 행복하다&#8230;..</p>
<h2>앞으로 나는 어떻게 할 것인가?</h2>
<p>아직 못 배운 기술들이 너무 많다.</p>
<p>우선은 장비에 대한 이해를 먼저 하고 싶다. 선배 엔지니어들을 보면 다양한 벤더사들의 모델별로 어떤 특징이 있고 무슨 문제점이 있는지 정말  콕 찌르면 술술술술 나온다. 물론 경험적인 면이 많이 차지하겠지만 회사에 예비로 되어있는 장비들을 가지고 메뉴얼과 함께 공부를 해야할 듯 하다.</p>
<p>리눅스&#8230; 리눅스를 쓴지는 어느덧 8년이 지나가고 있지만, 시스템 엔지니어로써 일하면서 정말 나는 아무것도 모르고 있었구나를 다시 한번 느끼고 있다. 서점에 가면 볼 수 있는 리눅스 관리하기 같은 책들을 사서 달달 외워야겠다. 업무하는데 리눅스를 몰라서 해맸던 적이 한둘이 아니다&#8230;</p>
<p>최신 기술. 요즘 Ansible이니 DevOps 니 여러 최신 기술들이 나오고 적용하는 사례가 나오고 있다. 지난 1년을 돌이켜보면 난 기존 인프라를 익히는데 모든 리소스를 사용하였고 최신 기술은 쳐다볼 엄두도 나지 않았다. 2018년에는 최신 기술이라 불리는 것들을 조금 들여다볼 수 있지 않을까?</p>
<h2>마무리하며</h2>
<p>간단하게 만 정리한다는 것이 엄청 많이 써버렸다. 두서 없이 써서 빠뜨린 내용도 있겠지만 지금의 나의 생각을 정리하기엔 충분했다. (만족?)<br />
대다수의 또래 전공자들이 선택하지 않은 IT인프라, 시스템 엔지니어로서의 길을 선택했다.<br />
아직은 모르겠다. 1년쯤 더 해보면 이 분야가 어떤지 조금은 알 수 있지 않을까? 아직은 모든게 새롭고 신기하고 흥미진진하다.</p>
<p>많은 사람들이 개발에 관심을 가지고 있다. 인프라에 관심을 가지고 있는 사람도 많아졌으면 좋겠다. 혼자서 놀기엔 심심하다.<br />
2018년에도 열심히 일하는 주니어 일개미가 되어야겠다.<br />
끝!</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/394/feed</wfw:commentRss>
		<slash:comments>2</slash:comments>
							</item>
		<item>
		<title>[ maven ] 운영환경 별, 설정 파일 분리하기</title>
		<link>https://printf.kr/archives/373</link>
				<comments>https://printf.kr/archives/373#comments</comments>
				<pubDate>Mon, 20 Feb 2017 07:27:26 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[JAVA]]></category>

		<guid isPermaLink="false">https://printf.kr/?p=373</guid>
				<description><![CDATA[maven profile 을 이용한 설정 파일 어플리케이션을 개발하다보면 다양한 설정파일들을 관리하게 된다. 데이터베이스 접근 정보, 레디스 접근 정보 등.. 이런 설정들은 개발, 테스트, 운영 단계별 서로 다른 설정을 가질 수 밖에 없다. 만약 이런 내용들이 코드에 정적으로 들어가 있다면 단계별로 코드의 내용을 수정해서 다시 컴파일을 해야하는 매우 번거로운 상황이 일어날 것이다. 만약 별 다른 기능을&#160;...]]></description>
								<content:encoded><![CDATA[<h1>maven profile 을 이용한 설정 파일</h1>
<p>어플리케이션을 개발하다보면 다양한 설정파일들을 관리하게 된다. 데이터베이스 접근 정보, 레디스 접근 정보 등..</p>
<p>이런 설정들은 개발, 테스트, 운영 단계별 서로 다른 설정을 가질 수 밖에 없다. 만약 이런 내용들이 코드에 정적으로 들어가 있다면 단계별로 코드의 내용을 수정해서 다시 컴파일을 해야하는 매우 번거로운 상황이 일어날 것이다.</p>
<p>만약 별 다른 기능을 사용하지 않는다면 아마 코드에는 다음과 같은 내용이 들어갈 것이다.</p>
<p></p><pre class="crayon-plain-tag">if(env.equals("local")) {
    ......
}
else if (env.equals("dev")) {
    .....
}</pre><p></p>
<p>maven 을 이용해서 개발하는 java 프로젝트일 경우, maven profile 을 이용하여 빌드 시, 외부에서 넣어주는 파라미터를 기준으로 서로 다른 설정파일을 참조하게 하여 환경 별로 다른 설정파일을 선택하도록 해줄 수 있다.</p>
<h2>설정 파일</h2>
<p>설정파일은 다양한 포멧을 가질 수 있다. 가장 많이 쓰이는 것이 yaml 와 properties 이다. maven 에서는 properties 파일을 이용해서 설정 파일을 관리하고 있다.</p>
<p>properties 설정 파일의 형태는 다음과 같이 <code>key = value</code> 형태를 가진다.</p>
<p></p><pre class="crayon-plain-tag">url = http://nhnent.com</pre><p></p>
<p>그리고 설정 파일은 대부분 <strong>src/main/resources</strong> 폴더 아래에 위치한다.</p>
<h3>환경 별 리소스 폴더 구분</h3>
<p>위에서 설정 파일은 대부분은 src/main/resources 에 저장된다고 하였다. 그럼 환경별로 리소스 폴더를 다르게 두고 그것을 classpath 에 넣으면 환경 별로 설정파일을 다르게 가져갈 수 있다.</p>
<ul>
<li>개발자 환경
<ul>
<li>src/main/resources-local</li>
</ul>
</li>
<li>개발 서버 환경
<ul>
<li>src/main/resources-dev</li>
</ul>
</li>
<li>운영 서버 환경
<ul>
<li>src/main/resources-real</li>
</ul>
</li>
</ul>
<p>와 같이 리소스 폴더를 구분해 줄 수 있다.</p>
<h2>maven profile 설정</h2>
<p>이제 maven profile 설정을 통해, 빌드 단계에서 원하는 리소스 폴더를 classpath 에 추가하는 작업을 진행할 것이다.</p>
<p>우선, pom.xml 에 아래와 같이 profile 을 추가한다.</p>
<p></p><pre class="crayon-plain-tag">&lt;profiles&gt;
    &lt;profile&gt;
      &lt;id&gt;dev&lt;/id&gt;
      &lt;properties&gt;
        &lt;env&gt;dev&lt;/env&gt;
      &lt;/properties&gt;
    &lt;/profile&gt;
    &lt;profile&gt;
      &lt;id&gt;local&lt;/id&gt;
      &lt;properties&gt;
        &lt;env&gt;local&lt;/env&gt;
      &lt;/properties&gt;
    &lt;/profile&gt;
    &lt;profile&gt;
      &lt;id&gt;real&lt;/id&gt;
      &lt;properties&gt;
        &lt;env&gt;real&lt;/env&gt;
      &lt;/properties&gt;
    &lt;/profile&gt;
&lt;/profiles&gt;</pre><p></p>
<p>이렇게 설정된 profile 을 maven 명령어에서 <strong>P</strong>옵션으로 같이 선택할 수 있다.</p>
<p></p><pre class="crayon-plain-tag">maven clean package -P dev</pre><p></p>
<p>다음은 선택된 profile 에 맞는 리소스 폴더를 classpath 에 넣는 작업이다. 기본즉으로 <strong>src/main/resources</strong> 는 들어가고 부가적으로 환경 별 리소스 폴더를 넣으면 된다. 아래 내용을 pom.xml 의 <strong>build</strong> 부분에 추가한다.</p>
<p></p><pre class="crayon-plain-tag">&lt;resources&gt;
  &lt;resource&gt;
    &lt;directory&gt;src/main/resources&lt;/directory&gt;
  &lt;/resource&gt;
  &lt;resource&gt;
    &lt;directory&gt;src/main/resources-${env}&lt;/directory&gt;
  &lt;/resource&gt;
&lt;/resources&gt;</pre><p></p>
<p>자 여기까지 하면 빌드 시, 환경에 맞는 설정파일이 classpath 로 들어가게 된다.</p>
<h2>코드에서 properties 값 가져오기</h2>
<p>이제 코드에서 properties 에 설정된 값을 가져와야 한다.</p>
<p></p><pre class="crayon-plain-tag">Properties prop = new Properties();
prop.load(OAuthPayco.class.getClassLoader().getResourceAsStream("payco.properties"));
clientId = prop.getProperty("clientId");
clientSecret = prop.getProperty("clientSecret");</pre><p></p>
<p>위 코드와 같이 아주 간단하게 코드에서 properties 에 있는 값을 가져올 수 있다.</p>
<h1>참고 자료</h1>
<ul>
<li>https://www.lesstif.com/pages/viewpage.action?pageId=14090588</li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/373/feed</wfw:commentRss>
		<slash:comments>2</slash:comments>
							</item>
		<item>
		<title>Openstack Network 구축 과정 이해 &#8211; LinuxBridge편</title>
		<link>https://printf.kr/archives/307</link>
				<comments>https://printf.kr/archives/307#comments</comments>
				<pubDate>Sun, 25 Sep 2016 18:32:43 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[Openstack]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=307</guid>
				<description><![CDATA[Test Excerpt]]></description>
								<content:encoded><![CDATA[<p><a href="https://printf.kr/wp-content/uploads/2016/09/networklayout-e1474789168814.png"><img src="https://printf.kr/wp-content/uploads/2016/09/networklayout-e1474789168814.png" alt="networklayout" width="500" height="409" class="alignnone size-full wp-image-308" /></a><br />
그림1. Openstack Mitaka 설치가이드의 네트워크 구성도</p>
<p><a href="http://docs.openstack.org/mitaka/install-guide-ubuntu/environment-networking.html">Ubuntu 14.04 Openstack Mitaka Install Guide</a> 에서는 위의 네트워크 구성도를 기반으로 설정을 진행합니다.. 이 그림만 봐서는 Provider Network 의 역할이 무엇인지, 그리고 대부분의 오픈스택의 네트워크 구성도와는 다른 모습이여서 이해하는데 많은 어려움이 있었습니다.</p>
<p>이 포스트에서는 Install Guide 에서 제시하는 네트워크 구성도에 대한 이해와, 그것이 내부적으로 어떻게 구성되는지 명령어단위로 하나하나 분석하면서 알아보도록 하겠습니다.</p>
<h1>Openstack Network 종류</h1>
<p>많이 공개된 오픈스택 네트워크 구성도를 보면 크게 아래와 같이 3가지 네트워크로 구성됩니다.</p>
<ul>
<li>Management Network : 관리용 네트워크. 각 컴포넌트(Nova, Neutron 등)이 서로 API 를 호출하는데 사용됩니다.</li>
<li>Tunnel Network : vm instance 간 네트워크를 구축하는데 사용되는 네트워크입니다. GRE, VXLAN 등의 기술이 사용됩니다. Overlay Network 라고도 부릅니다.</li>
<li>External Network : vm instance 가 인터넷과 통신하기 위한 네트워크.</li>
</ul>
<p>Host 머신에 오픈스택을 설치하기 위한 인터넷 연결으로는 별도의 네트워크를 연결하거나, Management Network 를 인터넷에 연결되도록 구성합니다.</p>
<p>그림1을 보면 오픈스택 설치 가이드에서는 Management Network 와 Provider Network 이렇게 두 가지의 네트워크로만 구성하고 있습니다. Provider Network 는 무엇이고 Tunnel / External Network 는 어디로 간 것일까요? 이것은 설치 가이드에 나오는 Provider / Self-Service Network 에 대해 먼저 이해하고, 가이드에서는 각각의 네트워크를 어떤 용도로 사용하는지 파악하면 알 수 있습니다.</p>
<h1>Provider / Self-Service Network 란?</h1>
<p><a href="http://docs.openstack.org/mitaka/install-guide-ubuntu/neutron-controller-install.html">설치 가이드 문서</a>의 Neutron 설정부분에서 네트워킹 서비스를 설정하기 위해서 2가지 옵션 중 한가지를 선택해야합니다.</p>
<ul>
<li>Provider Network : 오픈스택을 서비스하는 사람이 구축한 네트워크가 vm instance 에 할당되는 네트워크입니다. <strong>서비스하는 사람이 &#8216;제공&#8217; 하는 네트워크라는 의미에서 Provider Network 라고 합니다.</strong> 이 네트워크는 인터넷에 연결이 되어있는 네트워크입니다. (그렇다고 public ip 를 반드시 가진다는 것은 아닙니다.)</p>
</li>
<li>
<p>Self-Service Network : 오픈스택을 사용하는 사용자(Tenant)가 직접 자신만의 vm instance 를 위한 네트워크를 구축할 수 있는 네트워크입니다. <strong>이 네트워크는 provider network 를 기반으로 GRE, VXLAN 등의 터널링을 통해 구축됩니다.</strong></p>
</li>
</ul>
<p>이 두 가지 옵션과 위에서 설명한 3가지 네트워크 종류와의 연관성을 정리하면 다음과 같습니다.</p>
<ul>
<li>Provider Network = External Network</li>
<li>Self-Service Network = Tunnel Network + External Network</li>
</ul>
<h1>Virtualbox 로 네트워크 구성하기</h1>
<p>이제 오픈스택을 설치하기 위해, virtualbox 로 네트워크를 구성해보도록 하겠습니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/Untitled-Diagram.png"><img src="https://printf.kr/wp-content/uploads/2016/09/Untitled-Diagram.png" alt="untitled-diagram" width="694" height="369" class="alignnone size-full wp-image-320" srcset="https://printf.kr/wp-content/uploads/2016/09/Untitled-Diagram.png 694w, https://printf.kr/wp-content/uploads/2016/09/Untitled-Diagram-300x160.png 300w" sizes="(max-width: 694px) 100vw, 694px" /></a></p>
<ul>
<li>Internet
<ul>
<li>virtualbox의 NAT 인터페이스 </li>
<li>오픈스택 설치용 네트워크</li>
<li>IP : Controller / Compute  가 동일하게 10.0.2.15</li>
</ul>
</li>
<li>Provider Network
<ul>
<li>virtualbox 의 bridge 인터페이스</li>
<li>공유기와 연결된 인터넷을 사용할 수 있는 네트워크</li>
<li>Controller IP : 할당하지 않음</li>
<li>Compute IP : 할당하지 않음</li>
<li>IP 대역 : 172.32.0.0/24</li>
</ul>
</li>
<li>Management Network
<ul>
<li>virtualbox 의 host-only network</li>
<li>host에서 virtualbox 의 vm 에 접속할 수 있는 네트워크</li>
<li>Controller IP : 192.168.56.101/24</li>
<li>Compute IP : 192.168.56.102/24</li>
</ul>
</li>
</ul>
<h2>Neutron 주요 설정</h2>
<p>Neutron 설정은 <a href="http://docs.openstack.org/mitaka/install-guide-ubuntu/neutron.html">Mitaka 설치 가이드</a>의 Self-Service Network 부분대로 진행하였지만, 기록을 위해 주요 설정 부분을 보여드립니다.</p>
<h3>Controller</h3>
<p><strong>/etc/neutron/plugins/ml2/linuxbridge_agent.ini</strong></p>
<p></p><pre class="crayon-plain-tag">[linux_bridge]
physical_interface_mappings = provider:eth1

[vxlan]
enable_vxlan = True
local_ip = 192.168.56.101
l2_population = True</pre><p></p>
<h3>Compute</h3>
<p><strong>/etc/neutron/plugins/ml2/linuxbridge_agent.ini</strong></p>
<p></p><pre class="crayon-plain-tag">[linux_bridge]
physical_interface_mappings = provider:eth1

[vxlan]
enable_vxlan = True
local_ip = 192.168.56.102
l2_population = True</pre><p></p>
<hr />
<h2>Self-Service 네트워크가 구축되는 과정</h2>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/network2-overview-e1474819548741.png"><img src="https://printf.kr/wp-content/uploads/2016/09/network2-overview-e1474819548741.png" alt="network2-overview" width="500" height="469" class="alignnone size-full wp-image-324" /></a><br />
그림 2. Self-Service networks 개요도</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/network2-connectivity-e1474819572641.png"><img src="https://printf.kr/wp-content/uploads/2016/09/network2-connectivity-e1474819572641.png" alt="network2-connectivity" width="500" height="596" class="alignnone size-full wp-image-325" /></a><br />
그림 3. Self-Service networks 연결도</p>
<p>설치 가이드 문서의 Launch Instance 의 인스턴스를 위한 네트워크 구축 &#8211; <a href="http://docs.openstack.org/mitaka/install-guide-ubuntu/launch-instance-networks-selfservice.html">Self-Service 파트</a> 에서 위와 같은 구성도를 보여주고 있습니다.</p>
<p>이제 neutron 에서 네트워크, 서브넷, 라우터를 생성하면서 그림 3의 연결도가 어떤 명령어로 어떻게 생성되는지 하나하나 살펴보도록 하겠습니다.</p>
<p>Self-Service Network 는 provider network 를 통해 인터넷과 통신하기에, neutron에 provider network를 사용할 수 있도록 설정해주어야 합니다.</p>
<h3>Create the provider network</h3>
<p></p><pre class="crayon-plain-tag"># 모든 오픈스택 사용자가 사용할 수 있도록 shared 네트워크를 생성.
neutron net-create --shared --provider:physical_network provider \
  --provider:network_type flat provider

# 위에서 생성한 네트워크에 서브넷을 생성.
neutron subnet-create --name provider --allocation-pool start=172.32.0.100,end=172.32.0.200 --dns-nameserver 8.8.8.8 --gateway 172.32.0.1 provider 172.32.0.0/24</pre><p></p>
<p>위 작업을 하고 나면 Controller 노드에 아래와 같이 구성됩니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/1.png"><img src="https://printf.kr/wp-content/uploads/2016/09/1.png" alt="1" width="668" height="443" class="alignnone size-full wp-image-330" srcset="https://printf.kr/wp-content/uploads/2016/09/1.png 668w, https://printf.kr/wp-content/uploads/2016/09/1-300x199.png 300w" sizes="(max-width: 668px) 100vw, 668px" /></a></p>
<p>Controller 노드에서 bridge 와 네임스페이스는 아래와 같이 구성되어있다.</p>
<p></p><pre class="crayon-plain-tag"># 리눅스 bridge 확인
$ brctl show 
bridge name bridge id       STP enabled interfaces
brqc1997735-77      8000.08002772b679   no      eth1
                            tape9fb1eac-9b

# 리눅스 네트워크 네임스페이스 확인
$ ip netns
qdhcp-c1997735-771e-463c-b64f-7724800bbb1a

$ sudo ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever

2: ns-e9fb1eac-9b@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:07:a8:82 brd ff:ff:ff:ff:ff:ff
    inet 172.32.0.100/24 brd 172.32.0.255 scope global ns-e9fb1eac-9b
       valid_lft forever preferred_lft forever
    inet 169.254.169.254/16 brd 169.254.255.255 scope global ns-e9fb1eac-9b
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe07:a882/64 scope link
       valid_lft forever preferred_lft forever</pre><p></p>
<p>어떤 명령어들이 저런 구성을 완성했는지, neutron 서비스들의 디버그 메세지를 통해 확인해보겠습니다.</p>
<p><strong>아래 내용은 구성을 위한 필수 명령어들만 정리한 내용입니다.</strong></p>
<h4>[DHCP Agent] DHCP Namespace 생성</h4>
<p></p><pre class="crayon-plain-tag"># 먼저 어디에선가 tape9fb1eac-9b 인터페이스가 만들어집니다 (어디서 만들어지는지 찾지 못했습니다. 명령어도..)

# DHCP Agent 에서 DHCP Namespace 생성하는 작업을 진행합니다.
ip netns add qdhcp-c1997735-771e-463c-b64f-7724800bbb1a
ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a ip link set lo up

# 이때 ns-e9 인터페이스가 생성되고, tape9fb 랑 연결됩니다.
ip link add tape9fb1eac-9b type veth peer name ns-e9fb1eac-9b netns qdhcp-c1997735-771e-463c-b64f-7724800bbb1a

ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a ip link set ns-e9fb1eac-9b address fa:16:3e:07:a8:82
ip link set tape9fb1eac-9b up
ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a ip link set ns-e9fb1eac-9b up

# dhcp 서버에 172.32.0.100 을 할당합니다.
ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a ip -4 addr add 172.32.0.100/24 scope global dev ns-e9fb1eac-9b brd 172.32.0.255

ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a ip -4 route replace default via 172.32.0.1 dev ns-e9fb1eac-9b

# dhcp 를 구성합니다.
ip netns exec qdhcp-c1997735-771e-463c-b64f-7724800bbb1a dnsmasq --no-hosts --no-resolv --strict-order --except-interface=lo --pid-file=/var/lib/neutron/dhcp/c1997735-771e-463c-b64f-7724800bbb1a/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/c1997735-771e-463c-b64f-7724800bbb1a/host --addn-hosts=/var/lib/neutron/dhcp/c1997735-771e-463c-b64f-7724800bbb1a/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/c1997735-771e-463c-b64f-7724800bbb1a/opts --dhcp-leasefile=/var/lib/neutron/dhcp/c1997735-771e-463c-b64f-7724800bbb1a/leases --dhcp-match=set:ipxe175 --bind-interfaces --interface=ns-e9fb1eac-9b --dhcp-range=set:tag0172.32.0.0static86400s --dhcp-option-force=option:mtu1500 --dhcp-lease-max=256 --conf-file= --domain=openstacklocal</pre><p></p>
<h4>[Linux-Bridge-Agent] Bridge 생성</h4>
<p></p><pre class="crayon-plain-tag"># 새로운 bridge 생성
brctl addbr brqc1997735-77
ip link set brqc1997735-77 up

# 인터페이스 추가
brctl addif brqc1997735-77 eth1
brctl addif brqc1997735-77 tape9fb1eac-9b</pre><p></p>
<p>여기까지가 provider 네트워크를 생성했을 때 일어나는 일입니다. 여기서 compute 노드에 vm instance 를 생성하면, compute 쪽에서 provider bridge 가 생성되고 instance 와 연결되는 작업이 이루어집니다</p>
<p>이 부분은 self-service 네트워크를 생성 후, vm instance 를 만드는 것과 비슷한 작업이기에 아래에서 알아보도록 하겠습니다.</p>
<hr />
<h3>Create the Self Service Network</h3>
<p></p><pre class="crayon-plain-tag">$ neutron net-create selfservice

$ neutron subnet-create --name selfservice --dns-nameserver 8.8.4.4 --gateway 172.16.1.1 selfservice 172.16.1.0/24</pre><p></p>
<p>위와 같이 self-service network 와 서브넷을 생성하면 아래와 같이 구성됩니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/2.png"><img src="https://printf.kr/wp-content/uploads/2016/09/2.png" alt="2" width="668" height="443" class="alignnone size-full wp-image-357" srcset="https://printf.kr/wp-content/uploads/2016/09/2.png 668w, https://printf.kr/wp-content/uploads/2016/09/2-300x199.png 300w" sizes="(max-width: 668px) 100vw, 668px" /></a></p>
<p>그림을 보면 vxlan-70 이 eth0에 연결되어있는 것을 볼 수 있습니다. eth0 은 Management Network 로 할당된 인터페이스입니다.</p>
<p><strong>이걸 통해서 Openstack Mitaka 설치 가이드에서는 Management Network 를 Tunnel Network와 같이 사용한다는 것을 알 수 있습니다.</strong></p>
<p>사실 이것을 설명드리고 싶어서 이 문서를 작성한 것이 가장 큽니다. Management Network 를 통해서 Tunnel Network 를 사용하고 있었습니다.</p>
<h4>[DHCP Agent] self-service dhcp namespace 생성</h4>
<p></p><pre class="crayon-plain-tag"># 마찬가지로 tapb5ab7d31-e1 가 생성됩니다. 어디에선가..

ip netns add qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899
ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 ip link set lo up

# ns-b5ab7d31-e1를 생성하고 tabb5ab7d31-e1 와 연결합니다.
ip link add tapb5ab7d31-e1 type veth peer name ns-b5ab7d31-e1 netns qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899

ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 ip link set ns-b5ab7d31-e1 address fa:16:3e:76:6d:74
ip link set tapb5ab7d31-e1 up
ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 ip link set ns-b5ab7d31-e1 up

# dhcp 서버 ip를 172.16.1.2 로 설정합니다.
ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 ip -4 addr add 172.16.1.2/24 scope global dev ns-b5ab7d31-e1 brd 172.16.1.255

ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 ip -4 addr add 169.254.169.254/16 scope global dev ns-b5ab7d31-e1 brd 169.254.255.255
ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 ip -4 route replace default via 172.16.1.1 dev ns-b5ab7d31-e1

# dhcp 기능을 활성화합니다.
ip netns exec qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 dnsmasq --no-hosts --no-resolv --strict-order --except-interface=lo --pid-file=/var/lib/neutron/dhcp/34078f0c-3e23-46d5-a03c-4c9af2778899/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/34078f0c-3e23-46d5-a03c-4c9af2778899/host --addn-hosts=/var/lib/neutron/dhcp/34078f0c-3e23-46d5-a03c-4c9af2778899/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/34078f0c-3e23-46d5-a03c-4c9af2778899/opts --dhcp-leasefile=/var/lib/neutron/dhcp/34078f0c-3e23-46d5-a03c-4c9af2778899/leases --dhcp-match=set:ipxe175 --bind-interfaces --interface=ns-b5ab7d31-e1 --dhcp-range=set:tag0172.16.1.0static86400s --dhcp-option-force=option:mtu1450 --dhcp-lease-max=256 --conf-file= --domain=openstacklocal</pre><p></p>
<h4>[neutron-linuxbridge-agent] bridge 생성</h4>
<p></p><pre class="crayon-plain-tag"># vxlan-70 인터페이스를 생성하고, vxlan id 는 70로 설정하고 eth0 과 연결되도록 합니다.
ip link add vxlan-70 type vxlan id 70 dev eth0

ip link set vxlan-70 up

# 새로운 bridge 를 생성합니다.
brctl addbr brq34078f0c-3e
brctl stp brq34078f0c-3e off
ip link set brq34078f0c-3e up

# 인터페이스들을 추가합니다.
brctl addif brq34078f0c-3e vxlan-70
brctl addif brq34078f0c-3e tapb5ab7d31-e1
ip link set tapb5ab7d31-e1 up</pre><p></p>
<h3>Create a router</h3>
<p>self-service network 와 provider network 를 연결해주는 라우터를 생성합니다.</p>
<p></p><pre class="crayon-plain-tag">$ neutron net-update provider --router:external

$ neutron router-create router

$ neutron router-interface-add router selfservice</pre><p></p>
<p>위 작업을 하고 나면 아래와 같은 구성이 만들어집니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/3.png"><img src="https://printf.kr/wp-content/uploads/2016/09/3.png" alt="3" width="668" height="443" class="alignnone size-full wp-image-358" srcset="https://printf.kr/wp-content/uploads/2016/09/3.png 668w, https://printf.kr/wp-content/uploads/2016/09/3-300x199.png 300w" sizes="(max-width: 668px) 100vw, 668px" /></a></p>
<h4>[l3-agent] router namespace 생성</h4>
<p></p><pre class="crayon-plain-tag"># tap19d1547d-a0 가 어디에선가 생성됩니다.

# 새로운 namespace 를 만듭니다.
ip netns add qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip link set lo up
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 sysctl -w net.ipv4.ip_forward=1

# qr-19d1547d-a0 인터페이스를 만들고 tap19d1547d-a0 와 연결합니다
ip link add tap19d1547d-a0  type veth peer name qr-19d1547d-a0 netns qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip link set qr-19d1547d-a0 address fa:16:3e:02:b0:b9
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip link set qr-19d1547d-a0 up

# qr-19d1547d-a0 인터페이스에 self-service 네트워크의 게이트웨이로 설정하기 위해 172.16.1.1 를 할당합니다.
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip -4 addr add 172.16.1.1/24 scope global dev qr-19d1547d-a0 brd 172.16.1.255</pre><p></p>
<h4>[neutron-linuxbridge-agent] brq34078f0c-3e bridge 에 새로운 인터페이스 추가</h4>
<p></p><pre class="crayon-plain-tag"># 이전에 만들어져있는 bridge에 router 와 연결되는 인터페이스를 추가합니다.
brctl addif brq34078f0c-3e tap19d1547d-a0

ip link set tap19d1547d-a0  up</pre><p></p>
<h3>&#8216;router&#8217; 이름의 라우터를 provide 네트워크의 gateway 로 설정</h3>
<p></p><pre class="crayon-plain-tag">neutron router-gateway-set router provider</pre><p></p>
<p>이 작업을 하고 나면 본격적으로 self-service 네트워크와 provider 네트워크가 연결되어, self-service 네트워크에서 인터넷으로 트래픽이 나갈 수 있게 됩니다.</p>
<p>구성은 아래와 같이 됩니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/4.png"><img src="https://printf.kr/wp-content/uploads/2016/09/4.png" alt="4" width="668" height="443" class="alignnone size-full wp-image-359" srcset="https://printf.kr/wp-content/uploads/2016/09/4.png 668w, https://printf.kr/wp-content/uploads/2016/09/4-300x199.png 300w" sizes="(max-width: 668px) 100vw, 668px" /></a></p>
<h4>네트워크 인터페이스 구성</h4>
<p>여기까지 왔을 때, controller 노드의 네트워크 인터페이스 구성은 아래와 같이 됩니다.</p>
<p></p><pre class="crayon-plain-tag">$ brctl show 

bridge name bridge id       STP enabled interfaces
brq34078f0c-3e      8000.7aa146e736c5   no      tap19d1547d-a0
                            tapb5ab7d31-e1
                            vxlan-70
brqc1997735-77      8000.08002772b679   no      eth1
                            tape01914d2-a0
                            tape9fb1eac-9b


$ sudo ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip a

1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: qr-19d1547d-a0@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:02:b0:b9 brd ff:ff:ff:ff:ff:ff
    inet 172.16.1.1/24 brd 172.16.1.255 scope global qr-19d1547d-a0
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe02:b0b9/64 scope link
       valid_lft forever preferred_lft forever
// 새로 생성
3: qg-e01914d2-a0@if14: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:04:32:27 brd ff:ff:ff:ff:ff:ff
    inet 172.32.0.101/24 brd 172.32.0.255 scope global qg-e01914d2-a0
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe04:3227/64 scope link
       valid_lft forever preferred_lft forever</pre><p></p>
<h4>[l3-agent] router namespace 에 인터페이스 추가</h4>
<p></p><pre class="crayon-plain-tag"># tape01914d2-a0 가 어디에선가 생성

# qg-e01914d2-a0 를 생성하고, tape01914d2-a 와 연결합니다.
ip link add tape01914d2-a0 type veth peer name qg-e01914d2-a0 netns qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8

ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip link set qg-e01914d2-a0 address fa:16:3e:04:32:27
ip link set tape01914d2-a0 up
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip link set qg-e01914d2-a0 up

# qg-e01914d2-a0에 provide 라우터 인터페이스 ip로 172.32.0.101 을 할당합니다.
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip -4 addr add 172.32.0.101/24 scope global dev qg-e01914d2-a0 brd 172.32.0.255
ip netns exec qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 ip -4 route replace default via 172.32.0.1 dev qg-e01914d2-a0</pre><p></p>
<h4>[neutron-linuxbridge-agent] brqc1997735-77에 인터페이스 추가</h4>
<p></p><pre class="crayon-plain-tag">ip link set brqc1997735-77 up
brctl addif brqc1997735-77 tape01914d2-a0
ip link set tape01914d2-a0 up</pre><p></p>
<hr />
<p>여기까지 neutron 에서 provider / self-service 네트워크를 생성 및 설정할 경우 controller 노드에서 물리적(?)으로 어떤 작업을 통해 네트워크들이 구성되는지 확인해보았습니다.</p>
<p>이제 self-service 네트워크에 vm instance 를 생성해보고, 어떤 일이 일어나는지 확인해보겠습니다.</p>
<h3>Self-service 네트워크에 vm instance 생성</h3>
<p>vm instance 를 생성하면, controller 와 compute 에 작은 변화가 생깁니다. 우선 vm instance 를 생성하면 어떻게 구성되는지 아래 그림으로 확인해보겠습니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/5.png"><img src="https://printf.kr/wp-content/uploads/2016/09/5.png" alt="5" width="671" height="743" class="alignnone size-full wp-image-360" srcset="https://printf.kr/wp-content/uploads/2016/09/5.png 671w, https://printf.kr/wp-content/uploads/2016/09/5-271x300.png 271w" sizes="(max-width: 671px) 100vw, 671px" /></a></p>
<p>여기서 중요한 것은, compute 노드에서 vxlan70 인터페이스가 생성되고 그것을 통해 controller 노드와 연결이 되고 있는 것입니다.</p>
<p>이제부터 bridge 구성을 포함해서 어떻게 리눅스에서 vxlan 설정이 들어가는지 알아보도록 하겠습니다.</p>
<h4>[compute node 의 neutron-linuxbridge-agent] etables 설정 / 새로운 bridge 생성 / vxlan 설정</h4>
<p></p><pre class="crayon-plain-tag"># tap66aa1ee8-9b 이 생성된다.

# etables 설정 (이더넷 프레임에 대한 테이블)
ebtables -N neutronMAC-tap66aa1ee8-9b -P DROP
ebtables -A FORWARD -i tap66aa1ee8-9b -j neutronMAC-tap66aa1ee8-9b
ebtables -A neutronMAC-tap66aa1ee8-9b -i tap66aa1ee8-9b --among-src fa:16:3e:3f:4b:28 -j RETURN
ebtables -N neutronARP-tap66aa1ee8-9b -P DROP
ebtables -F neutronARP-tap66aa1ee8-9b
ebtables -A neutronARP-tap66aa1ee8-9b -p ARP --arp-ip-src 172.16.1.3 -j ACCEPT
ebtables -A FORWARD -i tap66aa1ee8-9b -j neutronARP-tap66aa1ee8-9b -p ARP

# vxlan-70 인터페이스를 생성하고 vxlan id 를 70으로 할당 후 eth0 에 연결되도록 설정
ip link add vxlan-70 type vxlan id 70 dev eth0
ip link set vxlan-70 up

# bridge 생성 및 인터페이스 추가
brctl addbr brq34078f0c-3e
brctl addif brq34078f0c-3e vxlan-70
brctl addif brq34078f0c-3e tap66aa1ee8-9b

# 여기에서 vxlan 터널링을 설정합니다.
# vxlan-70 의 모든 l2 트래픽의 목적지를 192.168.56.101 으로 설정
bridge fdb add 00:00:00:00:00:00 dev vxlan-70 dst 192.168.56.101

# 172.16.1.1 에 대한 목적지 ip 는 192.168.56.101, mac address 는  fa:16:3e:02:b0:b9 로 설정
# fa:16:3e:02:b0:b9 는 qrouter-594a2a7a-c79b-42f2-9011-8373f3186cb8 의 qr-19d1547d-a0@if13 인터페이스 mac address
ip -4 neigh replace 172.16.1.1 lladdr fa:16:3e:02:b0:b9 nud permanent dev vxlan-70
bridge fdb replace fa:16:3e:02:b0:b9 dev vxlan-70 dst 192.168.56.101

# 172.16.1.2 에 대한 목적지 ip 는 192.168.56.101, mac address 는  fa:16:3e:76:6d:74 로 설정
# fa:16:3e:76:6d:74 는 qdhcp-34078f0c-3e23-46d5-a03c-4c9af2778899 의 ns-b5ab7d31-e1@if10
# 172.16.1.2 는 self-service dhcp 서버 주소
ip -4 neigh replace 172.16.1.2 lladdr fa:16:3e:76:6d:74 nud permanent dev vxlan-70
bridge fdb replace fa:16:3e:76:6d:74 dev vxlan-70 dst 192.168.56.101</pre><p></p>
<h4>[controller node 의 neutron-linuxbridge-agent]</h4>
<p></p><pre class="crayon-plain-tag"># vxlan-70 의 모든 l2 트래픽의 목적지를 192.168.56.102로 설정
bridge fdb add 00:00:00:00:00:00 dev vxlan-70 dst 192.168.56.102

# 172.16.1.3 은 instance ip
# 172.16.1.3 으로 가는 트래픽을 fa:16:3e:3f:4b:28 로 가도록 설정
# fa:16:3e:3f:4b:28 는 compute 노드의 tap66aa1ee8-9b 인터페이스 mac address
ip -4 neigh replace 172.16.1.3 lladdr fa:16:3e:3f:4b:28 nud permanent dev vxlan-70

// fa:16:3e:3f:4b:28 로 가는 트래픽의 목적지 ip 를 192.168.56.102 로 설정
bridge fdb replace fa:16:3e:3f:4b:28 dev vxlan-70 dst 192.168.56.102</pre><p></p>
<hr />
<p>여기까지 아주 긴 과정을 거쳐 provider network / self-service network 를 생성 및 설정이 이루어지는 방법을 알아보았습니다.</p>
<p>내용은 많지만 중요한 것만 정리하면 다음과 같습니다.</p>
<ul>
<li>linuxbridge 를 이용하여 네트워크를 구축할 경우, linux bridge, namespace 를 이용해서 가상 네트워크를 구축한다.</li>
<li>linuxbridge 는 vxlan 만 지원하는데 이는, bridge 라는 명령어를 통해 vxlan 을 설정한다.</li>
<li>self-service 네트워크는 provider 네트워크를 통해 외부와 통신한다.</li>
</ul>
<p>두서없이 정리하느냐 틀린 내용이나 누락된 내용이 있을 수 있습니다. 혹 발견하시면 댓글로 남겨주시면 감사하겠습니다.</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/307/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
							</item>
		<item>
		<title>Virtualbox 네트워크 종류와 설정 방법</title>
		<link>https://printf.kr/archives/285</link>
				<comments>https://printf.kr/archives/285#comments</comments>
				<pubDate>Wed, 14 Sep 2016 15:36:43 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[Openstack]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=285</guid>
				<description><![CDATA[Virtualbox 에서는 가상머신(이하 VM)에 할당할 수 있는 네트워크 종류를 총 6가지를 지원하고 있습니다. 각 네트워크 종류가 어떤 것을 의미하는지 살펴보고, 어떻게 설정을 할 수 있는지 알아보도록하겠습니다. 이 문서에서의 Virtualbox 실행 환경은 OS X 이며, 윈도우와 리눅스/OSX는 기본 설정에서 약간의 차이가 있습니다. 다음은 Virtualbox 에서 지원하는 네트워크 종류입니다. NAT NAT network Bridge Adapter Internal Network Host-only&#160;...]]></description>
								<content:encoded><![CDATA[<p>Virtualbox 에서는 가상머신(이하 VM)에 할당할 수 있는 네트워크 종류를 총 6가지를 지원하고 있습니다. 각 네트워크 종류가 어떤 것을 의미하는지 살펴보고, 어떻게 설정을 할 수 있는지 알아보도록하겠습니다.</p>
<blockquote><p>
  이 문서에서의 Virtualbox 실행 환경은 OS X 이며, 윈도우와 리눅스/OSX는 기본 설정에서 약간의 차이가 있습니다.
</p></blockquote>
<p>다음은 Virtualbox 에서 지원하는 네트워크 종류입니다.</p>
<ul>
<li>NAT</li>
<li>NAT network</li>
<li>Bridge Adapter</li>
<li>Internal Network</li>
<li>Host-only Network</li>
<li>Generic Driver</li>
</ul>
<p>여기에서는 맨 마지막 Generic Driver 를 제외한 나머지 5개에 대해서 설명하도록 하겠습니다.</p>
<p>보다 자세한 내용은 virtualbox 홈페이지의 문서를 참고해주시기 바랍니다.<br />
( <a href="https://www.virtualbox.org/manual/ch06.html">https://www.virtualbox.org/manual/ch06.html</a> )</p>
<h3>NAT</h3>
<p>NAT(Network Address Translation) 는 VM이 외부 네트워크(인터넷)에 접근할 수 있는 가장 간단한 방법입니다.  Host / Guest 에 어떠한 설정을 할 필요가 없기 때문에, Virtualbox 에서 VM을 생성하면 기본적으로 attach 되는 네트워크이기도 합니다.</p>
<p>VM에 NAT 인터페이스를 처음으로 붙이게 되면 10.0.2.0/24 대역이, 그 다음에는 10.0.3.0/24 대역이 할당되게 됩니다.</p>
<p>10.0.2.0/24 대역으로 할당되면, VM에는 10.0.2.15 가 부여되고, 기본 게이트웨이는 10.0.2.2 가 됩니다.</p>
<p>여러개의 VM을 만들고, NAT 를 붙이더라도 모두 동일하게 10.0.2.15 를 가지게 됩니다.</p>
<h3>NAT Network</h3>
<p>NAT Network는 공유기와 같은 환경을 만드는 것으로 이해하면 매우 쉽습니다.</p>
<p>여러 VM을 같은 네트워크에 속하게 하고 싶은데, 모두 인터넷이 가능하게 하고 싶을 때 사용하면 매우 유용합니다.</p>
<p><strong>(보통 VM을 같은 네트워크로 묶고, 인터넷이 가능하게 하려면 VM 에 Host-only, NAT 두 개의 네트워크 인터페이스를 붙입니다)</strong></p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/VirtualBox_-_Network_and_Oracle_VM_VirtualBox_Manager-e1473867146410.png"><img src="https://printf.kr/wp-content/uploads/2016/09/VirtualBox_-_Network_and_Oracle_VM_VirtualBox_Manager-1024x689.png" alt="virtualbox_-_network_and_oracle_vm_virtualbox_manager" width="600" height="400" class="alignnone size-large wp-image-304" /></a></p>
<p>위와 같이 Virtualbox 의 환경설정에서 GUI로 간단하게 NAT Network 를 구성할 수 있지만, 명령어를 통해서도 NAT Network 를 구성할 수 있습니다.</p>
<p></p><pre class="crayon-plain-tag"># 현재 생성되어있는 NAT Network 목록 조회
$ VBoxManage list natnetworks

# 192.168.0.0/24 대역의 DHCP 기능이 있는 NAT Network 생성
$ VBoxManage natnetwork add --netname natnet1 --network "192.168.0.0/24" --enable --dhcp on

# 위에서 생성한 'natnet1' NAT Network 시작하기
$ VBoxManage natnetwork start --netname natnet1

# 'natnet1' NAT Network 삭제하기
$ VBoxManage natnetwork remove --netname natnet1</pre><p></p>
<p>natnetwork 관련 명령어에 대한 자세한 내용은 <a href="https://www.virtualbox.org/manual/ch06.html#network_nat_service">여기</a>를 참고해주시기 바랍니다.</p>
<h3>Bridge Network</h3>
<p>Bridge Network 는 Virtualbox 가 호스트에 있는 네트워크 디바이스를 직접적으로 사용하는 네트워크입니다. 호스트가 사용하는 네트워크 환경을 그대로 이용할 수 있으며, VM이 호스트와 같은 물리적인 네트워크에 존재하게 할 수 있습니다.</p>
<p>사용하는 방법은 VM 설정의 네트워크탭에서 네트워크 어뎁터를 Bridge Network 로 선택하고, 연결할 호스트 네트워크 디바이스를 선택하면 됩니다.</p>
<p>단, OS X와 Linux에서는 무선 네트워크에 대해서는 Bridge Network 사용이 제한됩니다.</p>
<h3>Internal Networking</h3>
<p>Internal Networking 은 NAT Networking 과 같이 Virtualbox 내에서 완벽한 사설망을 구축할 수 있다는 점에서 동일하지만, 인터넷 연결이 되지 않는 점에서 다르다.</p>
<p>또한, Internal Networking 으로 할당된 IP로는 호스트에서 접근이 불가능하다. 오직 같은 Internal Networking 으로 연결된 VM들간에만 통신이 가능하다.</p>
<p>Internal Networking 을 생성하는 방법은 터미널에서만 가능합니다. 아래 명령어를 통해 생성 및 삭제를 할 수 있습니다.</p>
<p></p><pre class="crayon-plain-tag"># testinet 이라는 이름의 Internal Network 생성
vboxmanage dhcpserver add --netname testinet --ip 10.10.10.1 --netmask 255.255.255.0 --lowerip 10.10.10.2 --upperip 10.10.10.240 --enable

# testinet 이름의 Internal Network 제거
vboxmanage dhcpserver remove --netname testinet</pre><p></p>
<p>생성 후, VM 네트워크 설정 부분에서 Attach to 를 &#8216;Internal Network&#8217;로 선택하고, Name 에 위에서 설정한 이름인 testinet을 입력하면 됩니다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/Screen-Shot-2016-07-28-at-1_23_37-PM-e1471975265739.png"><img src="https://printf.kr/wp-content/uploads/2016/09/Screen-Shot-2016-07-28-at-1_23_37-PM-e1471975265739.png" alt="screen-shot-2016-07-28-at-1_23_37-pm-e1471975265739" width="500" height="352" class="alignnone size-full wp-image-302" srcset="https://printf.kr/wp-content/uploads/2016/09/Screen-Shot-2016-07-28-at-1_23_37-PM-e1471975265739.png 500w, https://printf.kr/wp-content/uploads/2016/09/Screen-Shot-2016-07-28-at-1_23_37-PM-e1471975265739-300x211.png 300w" sizes="(max-width: 500px) 100vw, 500px" /></a></p>
<h3>Host-only Networking</h3>
<p>Host-only Networking 은 Internal Networking 과 기능면에서는 동일하다. 하지만 다른점은 Host-only Networking 은 호스트와도 통신이 가능하다는 것이다.</p>
<p>그렇게 때문에 Virtualbox 를 사용하는 사용자들이 호스트에서 VM에 접속하고 싶을 경우, (예를 들면 VM에 웹 서버를 실행하고 호스트에서 접속) Host-only Networking 을 VM에 추가하여 사용한다. (이 경우 VM이 인터넷도 사용해야할 경우 NAT 까지 붙이기도 한다.)</p>
<p>Host-only Networking 은 OS X 의 경우 기본적으로 6개가 생성되어 있다. 이를 이용할 수 도 있고, NAT Networking 에서 확인한 환경설정 화면에서 Host-only Networks 로 들어가서 추가로 생성할 수도 있다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/VirtualBox_-_Network-e1473867396497.png"><img src="https://printf.kr/wp-content/uploads/2016/09/VirtualBox_-_Network-1024x671.png" alt="virtualbox_-_network" width="600" height="400" class="alignnone size-large wp-image-305" /></a></p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/285/feed</wfw:commentRss>
		<slash:comments>3</slash:comments>
							</item>
		<item>
		<title>MQTT 노트 &#8211; QoS, Connection, Security</title>
		<link>https://printf.kr/archives/272</link>
				<comments>https://printf.kr/archives/272#comments</comments>
				<pubDate>Mon, 05 Sep 2016 09:44:53 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[미분류]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=272</guid>
				<description><![CDATA[원문 : https://zoetrope.io/tech-blog/brief-practical-introduction-mqtt-protocol-and-its-application-iot QoS MQTT 는 불안정한 네트워크에서도 동작이 가능하도록 설계되어있고, 그것을 가능하게 하기위해 3가지 타입의 QoS를 지원한다. QoS level 0 가장 간단한 QoS. 클라이언트에게 어떠한 ACK 도 요구하지 않음. 안정성은 TCP/IP 에서 보장 QoS level 1 메세지가 적어도 1번 전송되는 것을 보장한다. 단, 한번 이상 전송이 될 수도 있으니 이 부분은 개발자가 잘 처리해야한다.&#160;...]]></description>
								<content:encoded><![CDATA[<p>원문 : <a href="https://zoetrope.io/tech-blog/brief-practical-introduction-mqtt-protocol-and-its-application-iot">https://zoetrope.io/tech-blog/brief-practical-introduction-mqtt-protocol-and-its-application-iot</a></p>
<h1>QoS</h1>
<p>MQTT 는 불안정한 네트워크에서도 동작이 가능하도록 설계되어있고, 그것을 가능하게 하기위해 3가지 타입의 QoS를 지원한다.</p>
<h2>QoS level 0</h2>
<p>가장 간단한 QoS. 클라이언트에게 어떠한 ACK 도 요구하지 않음. 안정성은 TCP/IP 에서 보장</p>
<h2>QoS level 1</h2>
<p>메세지가 적어도 1번 전송되는 것을 보장한다. 단, 한번 이상 전송이 될 수도 있으니 이 부분은 개발자가 잘 처리해야한다.</p>
<p>클라이언트는 메세지를 받으면 반드시 ACK 를 보내야한다. 이것으로 메세지가 보내졌다는 것을 보장할 수 있다.</p>
<h2>QoS level 2</h2>
<p>가장 흔한 종류의 QoS 이다. 메세지가 반드시 한번만 전송되었다는 것을 보장하는 QoS이다. 4단계를 통해 메세지 전송 과정을 검증한다.</p>
<p>아래 그림은 각 QoS 별로 처리 과정을 그림으로 나타낸 것이다.</p>
<p><a href="https://printf.kr/wp-content/uploads/2016/09/7sC1vzn-e1473067409313.png"><img src="https://printf.kr/wp-content/uploads/2016/09/7sC1vzn-1024x321.png" alt="https://zoetrope.io/tech-blog/brief-practical-introduction-mqtt-protocol-and-its-application-iot" width="1024" height="321" class="alignnone size-large wp-image-277" /></a></p>
<hr />
<h1>LWT(Last Will and Testament)</h1>
<p>MQTT 프로토콜은 keep-alive 를 이용하여, 클라이언트의 비정상적인 연결종료를 감지하는 기능을 제공한다. 클라이언트는 특정 시점에 LWT 패킷을 브로커에게 보낸다. 클라이언트의 비정상 종료를 브로커가 감지하면, 저장되어있던 LWT 메세지를 특정 토픽에 저장하여, 다른 클라이언트가 이 클라이언트의 오프라인 상태를 알도록 한다.</p>
<p><a href="http://www.hivemq.com/blog/mqtt-essentials-part-9-last-will-and-testament">더 자세한 정보</a></p>
<hr />
<h1>Security</h1>
<h2>Username and Password</h2>
<p>MQTT Connect 시, username 과 password 를 전달하여 인증하는 방법이다.</p>
<p>보안에는 취약하지만, TLS 연결로 보완할 수 있다.</p>
<h2>TLS Connection</h2>
<p>MQTT 자체는 TCP 를 기반으로 돌아가기 때문에, 모든 메세지를 TLS 연결에 싣어서 암호화된 채로 전송할 수 있다.</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/272/feed</wfw:commentRss>
		<slash:comments>1</slash:comments>
							</item>
		<item>
		<title>PyCon APAC 2016 &#8211; 자원봉사자에서 준비위원회까지</title>
		<link>https://printf.kr/archives/224</link>
				<comments>https://printf.kr/archives/224#respond</comments>
				<pubDate>Wed, 17 Aug 2016 02:51:38 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[활동]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=224</guid>
				<description><![CDATA[8월 13 &#8211; 15일, 3일간 진행된 PyCon APAC 2016 이 끝난지도 벌써 2일이 지나가고 있다. 원래 장문의 후기를 쓰려고 했는데, 지금 이 순간 느끼는 기분을 지금 당장 적어놓고 싶어서 두서 없는 짧은 후기를 쓰기로 했다. 자원봉사자에서 준비위원회로 PyCon 2014, 2015 에는 자원봉사자로 행사에 참가했었다. 처음 자원봉사자로 참가하게 된 이유는 2014년 군대에서 전역한 이후 다른 사람들에&#160;...]]></description>
								<content:encoded><![CDATA[<p><span id="more-224"></span><br />
8월 13 &#8211; 15일, 3일간 진행된 PyCon APAC 2016 이 끝난지도 벌써 2일이 지나가고 있다.<br />
원래 장문의 후기를 쓰려고 했는데, 지금 이 순간 느끼는 기분을 지금 당장 적어놓고 싶어서 두서 없는 짧은 후기를 쓰기로 했다.</p>
<p><img src="https://printf.kr/wp-content/uploads/2016/08/13680291_1763785157217029_5805990304313049973_o-e1471975364873.jpg" alt="" /></p>
<h2>자원봉사자에서 준비위원회로</h2>
<p>PyCon 2014, 2015 에는 자원봉사자로 행사에 참가했었다. 처음 자원봉사자로 참가하게 된 이유는 2014년 군대에서 전역한 이후 다른 사람들에 비해 뒤쳐져있다는 느낌이 들어서 커뮤니티를 통해 빠르게 간격을 좁히고 싶은 마음과 새로운 사람들을 만나면서 다양한 경험을 해보고 싶었었다.</p>
<p>그러다가 올해 PyCon에서 준비위원회로 활동하게 된 결정적인 이유는 바로 <strong>다른 컨퍼런스에선 찾을 수 없는 무언가</strong>가 있었기 때문이다. 이번 행사를 하기 전까지 그 무언가가 무엇인지 정의할 수 없었다. 아직까지도 그 무언가를 한 단어로 정의할 순 없지만, 아마도 <strong>커뮤니티</strong> 이것인것 같다. 정보를 얻으러/전달하러 오는 행사가 아니라, 새로운 혹은 그동안 못본 사람들을 만나고 이야기를 나누고, 비슷한 관심사를 가지는 사람들끼리 별도로 모여서 이야기꽃을 피우는 그런 것이 바로 PyCon 에만 존재하고 있었다.</p>
<p>암튼, 자원봉사자로 있을 때 보다 준비위원회로 활동한 것이 정말 최고의 경험이고 순간이였다. 나는 프로그램팀에서 활동을 했고 시간표를 테트리스하고 발표자료를 수집하고 공지 이메일을 보내고 행사 당일에는 자원봉사자분들을 케어하는 일 등을 하였다. 행사를 준비하는 것이 힘들지 않았냐고 물어보시는 분들이 많다. 아니요 전혀요. 오히려 엄청 재미있었습니다. 그 이야기는 아래에서 풀어나가도록 해볼게요.</p>
<h2>행사 준비</h2>
<p>프로그램팀은 행사가 시작되기 1~2달전까지는 ~~솔직히 할일이 없었다~~. 그래도 뭔가 해야지 라는 생각으로 페이스북 페이지로 오는 메세지에 열심히 답변을 해주고 있었다. 그 외에는 슬랙에서 오고가는 메세지들을 감상하면서 우와.. 다들 열심히 하신다&#8230; 라고 바라만 보고 있었다.</p>
<p>그리고, 본격적으로 프로그램팀이 움직이기 시작한 것은 call for proposals 가 마감되고 발표자 선정을 위해 제안서 리뷰를 시작한 시점이였다. 이때부터 서서히 가동을 시작하고 최종 선정된 발표자분들에게 축하 및 감사의 이메일을 한글/영문으로, 선정되시지 못한 분들에게도 한글/영문의 이메일을 보내는 것으로 나의 일이 본격적으로 시작되었다. (이때부터인가.. 이메일 발송 담당이 나로 된 것이 .. )</p>
<p>그리고 행사 시간표를 만들기 위해 25/40분 세션, 쉬는 시간, 영어 발표 등등을 고려하여 수작업으로 테트리스를 진행했다. 아. .이게 제일 힘들었다. 계속 바뀐다. 시간 계산도 다시 해보고 빠트린 세션은 없는지 쉬는 시간은 적절한지 등등 계속 체크해가면서 시간표를 만들었다. 중간중간 발표자 사정으로 발표가 취소되면 다시 테트리스를 진행하고.. 그래도 최종 시간표가 나왔을땐 정말 기뻤다.</p>
<p>그 외에도 mailchimp 라는 서비스를 이용해서 참가자 전원(참가자, 스피커, 자봉, 스폰서)에게 안내 메일을 열심히 보내기도 했다. (이상하게 써놓고 보니 한 일이 없네..;) 그 프로모션 탭으로 보내버린 사람이 바로 저에요 !!!! ( 어쩔 수 없었어요 ㅠㅠ 프로모션 탭은..ㅠㅠ 구글 ㅠㅠ)</p>
<p>행사 준비를 하면서 제일 많이 수고를 해주신 분들이 바로 스폰서팀이 아닐까 한다. 정말 준비기간 내내 본업이 있으신데도 불구하고 스폰서를 모집하기 위해 열심히 발품팔아 돌아다니시는 모습이 랜선을 건너와서 느껴질 정도였다. 그리고 정말 이쁜 디자인과 굿즈를 만들어주신 미디어팀 분들까지 최고의 플레이어가 아닐까 생각한다.</p>
<h2>행사 당일</h2>
<p>행사 당일에는 자원봉사자 케어와 세션을 진행하고 관리하는 세션체어/러너 분들의 교육과 세션 준비 및 진행에 대한 전반적인 부분을 담당하였다. 몇몇 세션은 매끄럽게 시작되지 못한 (대부분이 디스플레이 문제) 사고가 있었지만, 정말 열심히 신근우님과 박민우님이 뛰어다녀주셔서 문제를 해결하고 세션을 진행할 수 있었다. 정말 감사하다. 그리고 듣고 싶은 세션이 있으실텐데도 책임감을 가지고 세션 체너/러너로 활동해주신 자원봉사자분들에게도 감사의 말을 전하고 싶다.</p>
<p>사실 행사 당일에 무슨 일을 했는지 잘 기억이 나지 않는다. 너무 정신이 없었다. 세션 체어/러너 교대 시간에는 다시 교육을 진행하고 모든 발표장 점검을 해야하고 계속해서 들어오는 무전에 응답을 해야했다. 그리고 세션이 시작되면, 세션 별 끝나는 시간을 파악하고 빠트린 것이 없는지 다시 점검하는 등 정신없이 보냈었다. (멍 때린 시간이 좀 있었지만..)</p>
<p>세션을 좀 듣고 싶었지만, 이상하게 왜인지 몰라도 뭔가 계속 돌아다녀야해서 세션을 듣지는 못했다. 그리고 좀 쉬고 싶었기에 스태프룸에서 멍.. 때리고 있었다  . 뭐 동영상이 올라오니까 !!</p>
<p>매 행사날 마지막에 진행되는 LT에서는 장비 엔지니어가 되어서 LT 슬라이드를 손으로 넘겨드리고, 발표자가 바뀔 때 마다 화면 전환을 하고 다음 자료를 준비하는 등 정신없이 LT를 진행하였다.</p>
<h2>후기</h2>
<p>막상 쓰고 나니 내가 한 일은 별로 없던 것 같다. 글을 잘 못쓰다보니 내 마음속에 있는 것을 잘 표현하지 못하겠다. 어쨌든 금토일월 까지 3박4일동안 꿈의 나라로 축제를 다녀온 기분이다. 아직까지도 행사의 흥분이 사라지지 않는다. 이번에는 외국인 참가자분들이랑도 많은 이야기를 나누어보았고 더 많은 사람들을 만났던 것 같다. 매번 느끼지만 파이콘은 컨퍼런스가 아니라 개발자 축제이다. 행사를 진행하면서 혹은 끝나고 난 다음 가장 많이 들은 이야기가 준비하는데 정말 많이 힘들었을 것 같다고, 정말 수고 많았다는 이야기다. 하지만 나의 대답은 <strong>아니요!!</strong> 이다. 힘들기는 커녕 더 하고 싶어서 안달날정도 였다. 준비하는 과정, 진행하는 과정, 정리하는 과정 <strong>모두모두 엄청나게 매우매우 진짜로</strong> 재미있었다. 마음같아선 모든 분에게 준비위원회에 참가시켜서 이 재미를 느끼게 해드리고 싶을 정도이다.</p>
<p>2014년엔 막연함, 2015년엔 팬심, 2016년엔 애정과 중독, 과연 2017년에는 어떤 마음으로 파이콘에 참석하게 될지 궁금하다.</p>
<p><strong>20대 나의 인생은 커뮤니티라는 물을 먹고 자라고 있다 !!</strong></p>
<h2>못다한 이야기</h2>
<ul>
<li>유교전 vs 파이콘</li>
<li>외국인과의 만남</li>
<li>LT를 하지 못한 아쉬움</li>
<li>파이콘의 꽃 스프린트</li>
<li>airbnb 에서의 이야기</li>
<li>자봉분들을 많이 못챙겨드린 이야기</li>
<li>준비위원회의 입장에서 느낀 파이콘</li>
<li>등등등..</li>
</ul>
<p>못다한 이야기는 시리즈로 만들어서 올릴게요 !</p>
<p>마지막으로 준비위원회, 스폰서, 스피커, 자원봉사자 그리고 행사에 참석해주신 모든 분들께 정말 감사드립니다 !!!</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/224/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>[virtualbox] dhcp 가 되는 internal network 구축하기</title>
		<link>https://printf.kr/archives/209</link>
				<comments>https://printf.kr/archives/209#comments</comments>
				<pubDate>Thu, 28 Jul 2016 04:41:12 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[TIP]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=209</guid>
				<description><![CDATA[Virtualbox 에서 VM 간의 내부망 구성을 위해 internal network 를 설정하는 방법입니다. ❗ vm 의 network를 internal network 로만 구성할 경우, vm 의 인터넷이 안됩니다. STEP 1. virtualbox 에 dhcp 서버 만들기 먼저, Virtualbox 의 &#8216;intnet&#8217; 이라는 internal network 에 dhcp server 를 만들어보겠습니다. 터미널 혹은 cmd 에서 아래 명령어를 입력해주세요. [crayon-5cd9b8711e22c446440115/] 그럼, intnet 이라는&#160;...]]></description>
								<content:encoded><![CDATA[<p><span id="more-209"></span><br />
Virtualbox 에서 VM 간의 내부망 구성을 위해 internal network 를 설정하는 방법입니다.</p>
<p><img src="https://s.w.org/images/core/emoji/11.2.0/72x72/2757.png" alt="❗" class="wp-smiley" style="height: 1em; max-height: 1em;" /> vm 의 network를 internal network 로만 구성할 경우, vm 의 인터넷이 안됩니다.</p>
<h3>STEP 1. virtualbox 에 dhcp 서버 만들기</h3>
<p>먼저, Virtualbox 의 &#8216;intnet&#8217; 이라는 internal network 에 dhcp server 를 만들어보겠습니다. 터미널 혹은 cmd 에서 아래 명령어를 입력해주세요.</p>
<p></p><pre class="crayon-plain-tag">vboxmanage dhcpserver add --netname intnet --ip 10.10.10.1 --netmask 255.255.255.0 --lowerip 10.10.10.2 --upperip 10.10.10.240 --enable</pre><p></p>
<p>그럼, intnet 이라는 네트워크에 1010.10.0/24 으로 IP 를 할당해주는 DHCP 가 생성됩니다.</p>
<h3>STEP 2. VM 의 네트워크 설정하기</h3>
<p><a href="https://printf.kr/wp-content/uploads/2016/07/Screen-Shot-2016-07-28-at-1.23.37-PM-e1471975265739.png"><img src="https://printf.kr/wp-content/uploads/2016/07/Screen-Shot-2016-07-28-at-1.23.37-PM-e1471975265739.png" alt="Screen Shot 2016-07-28 at 1.23.37 PM" class="alignnone size-large wp-image-216" /></a></p>
<p>위 사진과 같이 VM 의 네트워크의 어뎁터에서 <strong>Internal Network</strong> 를 선택하고, 이름에 <strong>intnet</strong> 을 입력해줍니다.</p>
<p>VM 하나를 더 만들어서 동일하게 설정해주면, Internal Network 가 잘 동작하는지 확인해볼 수 있습니다.</p>
<h3>SETP 3. VM 에서 IP 할당되는지 확인하기</h3>
<p>이제 VM 을 실행시키고, IP 가 제대로 할당되는지 보겠습니다.<br />
<img class="border-image" src="https://printf.kr/wp-content/uploads/2016/07/Screen-Shot-2016-07-28-at-1.31.17-PM-e1471975306381.png" alt="" /></p>
<p>10.10.10.2 로 제대로 할당되는 것을 확인할 수 있습니다.</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/209/feed</wfw:commentRss>
		<slash:comments>1</slash:comments>
							</item>
		<item>
		<title>Virtual switching technologies and Linux bridge</title>
		<link>https://printf.kr/archives/202</link>
				<comments>https://printf.kr/archives/202#comments</comments>
				<pubDate>Sun, 05 Jun 2016 09:56:04 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[자료실]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=202</guid>
				<description><![CDATA[OVS, NFS 등의 기술에 대해 잘 정리한 문서 https://printf.kr/wp-content/uploads/2016/06/LinuxConJapan2014_makita_0.pdf]]></description>
								<content:encoded><![CDATA[<p><span id="more-202"></span><br />
OVS, NFS 등의 기술에 대해 잘 정리한 문서</p>
<p>https://printf.kr/wp-content/uploads/2016/06/LinuxConJapan2014_makita_0.pdf</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/202/feed</wfw:commentRss>
		<slash:comments>2</slash:comments>
							</item>
		<item>
		<title>Ubuntu에서 iscsi target/initiator 구성하기</title>
		<link>https://printf.kr/archives/189</link>
				<comments>https://printf.kr/archives/189#respond</comments>
				<pubDate>Fri, 29 Apr 2016 09:55:12 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[Linux]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=189</guid>
				<description><![CDATA[1. iscsi 구성 요소 Target : iscsi disk 가 mount 되어있고, iscsi를 서비스하는 서버 Initiator : target iscsi disk 를 사용하는 client 2. Target 설정 [crayon-5cd9b8711ef8c017874108/] iscsi 설정에서 target 을 enable 로 설정해줘야한다. [crayon-5cd9b8711ef96776031457/] [crayon-5cd9b8711ef9c217307016/]  스토리지 생성 target 이 사용할 스토리지는 물리 device, 논리 device, raid device, file 등이 될 수 있다. 이 예제에서는 20GB&#160;...]]></description>
								<content:encoded><![CDATA[<h1>1. iscsi 구성 요소</h1>
<ul>
<li>Target : iscsi disk 가 mount 되어있고, iscsi를 서비스하는 서버</li>
<li>Initiator : target iscsi disk 를 사용하는 client</li>
</ul>
<h1>2. Target 설정</h1>
<p></p><pre class="crayon-plain-tag">sudo apt-get install iscsitarget iscsitarget-dkms</pre><p>iscsi 설정에서 target 을 enable 로 설정해줘야한다.</p><pre class="crayon-plain-tag">sudo vim /etc/default/iscsitarget</pre><p></p><pre class="crayon-plain-tag">ISCSITARGET_ENABLE=true</pre><p></p>
<h5><strong> 스토리지 생성</strong></h5>
<p>target 이 사용할 스토리지는 물리 device, 논리 device, raid device, file 등이 될 수 있다. 이 예제에서는 20GB 파일을 생성하여 disk 처럼 사용하도록 한다.</p><pre class="crayon-plain-tag">sudo mkdir /storage
sudo dd if=/dev/zero of=/storage/lun1.img bs=1024k count=20000</pre><p></p>
<h5><strong>ISCSI Target 설정</strong></h5>
<p>스토리지를 생성한 후, ietd.conf 파일에 device 를 아래내용을 추가해준다.</p><pre class="crayon-plain-tag">sudo vim /etc/iet/ietd.conf</pre><p></p><pre class="crayon-plain-tag">[...]
Target iqn.2016.04.kr.printf.sys0
    Lun 0 Path=/storage/lun1.img
    Alias LUN1</pre><p>설정이 완료되면, iscisitarget daemon 을 재시작해준다.</p><pre class="crayon-plain-tag">sudo /etc/init.d/iscsitarget start</pre><p></p>
<h1>3. Initiator 설정</h1>
<p></p><pre class="crayon-plain-tag">sudo apt-get install open-iscsi</pre><p>아래 명령어로 target 을 찾을 수 있다.</p><pre class="crayon-plain-tag">sudo iscsiadm -m discovery -t st -p [ip]</pre><p>그럼 아래와 같이 아까 추가한 정보들이, device 에 설정된 ethernet interface 만큼 나오게 된다.</p><pre class="crayon-plain-tag">127.0.0.1:3260,1 iqn.2016.04.kr.printf.sys0
192.168.11.51:3260,1 iqn.2016.04.kr.printf.sys0</pre><p>iscsi target 에 로그인을 한다.</p><pre class="crayon-plain-tag">iscsiadm -m node --targetname "iqn.2016.04.kr.printf.sys0" --portal "127.0.0.1:3260" --login</pre><p>로그아웃은 아래 명령어.</p><pre class="crayon-plain-tag">iscsiadm -m node --targetname "iqn.2016.04.kr.printf.sys0" --portal "127.0.0.1:3260" --logout</pre><p>로그인이 성공하면 아래와 같은 메세지가 뜬다.</p><pre class="crayon-plain-tag">Logging in to [iface: default, target: iqn.2016.04.kr.printf.sys0, portal: 127.0.0.1,3260] (multiple)
Login to [iface: default, target: iqn.2016.04.kr.printf.sys0, portal: 127.0.0.1,3260] successful.</pre><p>iscsi target 이 제대로 연결되었는지 확인하려면 fdisk 를 통해 확인한다.</p><pre class="crayon-plain-tag">root@nxz-vm:/# fdisk -l
Disk /dev/sda: 931.5 GiB, 1000204886016 bytes, 1953525168 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disklabel type: dos
Disk identifier: 0x9a504ea0

Device     Boot Start        End    Sectors   Size Id Type
/dev/sda1        2048 1953523711 1953521664 931.5G 83 Linux


Disk /dev/sdb: 111.8 GiB, 120034123776 bytes, 234441648 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x52cbbf55

Device     Boot Start       End   Sectors   Size Id Type
/dev/sdb1  *     2048 234440703 234438656 111.8G 83 Linux


Disk /dev/sdc: 19.5 GiB, 20971520000 bytes, 40960000 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</pre><p>마지막 /dev/sdc 가 iscsi target device 이고, 제대로 연결 된 것을 볼 수 있다.</p>
<p>device 를 사용하려면 fdisk 를 이용하여 입맛에 맞게 포맷하여 사용한다.</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/189/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
		<item>
		<title>[Python] 오늘부터 몇일 뒤 날짜 구하기</title>
		<link>https://printf.kr/archives/170</link>
				<comments>https://printf.kr/archives/170#respond</comments>
				<pubDate>Wed, 25 Nov 2015 16:54:26 +0000</pubDate>
		<dc:creator><![CDATA[조성수]]></dc:creator>
				<category><![CDATA[TIP]]></category>

		<guid isPermaLink="false">http://printf.kr/?p=170</guid>
				<description><![CDATA[[crayon-5cd9b8711fa07037213377/] &#160;]]></description>
								<content:encoded><![CDATA[<p></p><pre class="crayon-plain-tag">import datetime

now = datetime.date.today()

tomorrow = now + datetime.timedelta(days=1)

# convert datetime.date to datetime.datetime

td = datetime.datetime(tomorrow.year, tomorrow.month, tomorrow.day)</pre><p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://printf.kr/archives/170/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
							</item>
	</channel>
</rss>
