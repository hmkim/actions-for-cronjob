<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://aa9390.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://aa9390.github.io/" rel="alternate" type="text/html" /><updated>2018-07-12T13:12:51+00:00</updated><id>https://aa9390.github.io/</id><title type="html">SoyeonKim</title><subtitle>공부중!</subtitle><entry><title type="html">인공지능 및 기계학습 개론 Ⅰ - CHAPTER 4 완강</title><link href="https://aa9390.github.io/machine_learning_ch4_f/" rel="alternate" type="text/html" title="인공지능 및 기계학습 개론 Ⅰ - CHAPTER 4 완강" /><published>2018-07-12T00:00:00+00:00</published><updated>2018-07-12T00:00:00+00:00</updated><id>https://aa9390.github.io/machine_learning_ch4_f</id><content type="html" xml:base="https://aa9390.github.io/machine_learning_ch4_f/">&lt;p&gt;&lt;img src=&quot;/images/KakaoTalk_20180712_214632321.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_214633175.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_214633989.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_214634900.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_214635884.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;&lt;/p&gt;

&lt;p&gt;수식 전개가 너무 많아서 중간에 그만 듣고 싶었지만 그래도 ch4를 완강했다.&lt;/p&gt;

&lt;p&gt;완벽하게 이해하려기보다 가볍게 훑어가는 느낌으로 들었는데, 그럭저럭 정리는 된 것 같다.&lt;/p&gt;

&lt;p&gt;지난 시간에 theta에 대한 식이 깔끔하게 떨어지지 않았기 때문에&lt;/p&gt;

&lt;p&gt;이번 시간에 gradient method를 사용하여 최적화하는 과정을 배웠다.&lt;/p&gt;

&lt;p&gt;모든 수식이 이렇게 다 잘 연결되는구나….신기….&lt;/p&gt;

&lt;p&gt;다음 chapter 5에서는 SVM을 배울 예정!&lt;/p&gt;

&lt;p&gt;빨리 다 듣고 파이썬을 이용한 데이터분석 들어야하는데ㅠㅠㅠㅠㅠㅠㅠ너무 할게 많다&lt;/p&gt;

&lt;p&gt;나 자신 괴롭히기…..또륵&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">인공지능 및 기계학습 개론 Ⅰ - CHAPTER 4.3 까지! (Logistic Regression Parameter Approximation 1)</title><link href="https://aa9390.github.io/machine_learning_ch4/" rel="alternate" type="text/html" title="인공지능 및 기계학습 개론 Ⅰ - CHAPTER 4.3 까지! (Logistic Regression Parameter Approximation 1)" /><published>2018-07-12T00:00:00+00:00</published><updated>2018-07-12T00:00:00+00:00</updated><id>https://aa9390.github.io/machine_learning_ch4</id><content type="html" xml:base="https://aa9390.github.io/machine_learning_ch4/">&lt;p&gt;&lt;img src=&quot;/images/KakaoTalk_20180712_022857969.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_022856811.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_022855768.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;
&lt;img src=&quot;/images/KakaoTalk_20180712_022854645.jpg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Decision Boundary, Logistic Function과 역함수인 Logit Function,
Logit Function을 사용하여 Logistic Resgression을 구하고 
머신러닝의 목표를 만족시키기 위해 theta에 대한 식을 최적화하는 과정을 학습했다.
결론적으로 theta를 잘 만져야한다는 것!&lt;/p&gt;

&lt;p&gt;4.5까지는 듣고 싶었는데ㅠㅠ너무 늦어서 내일 마저 해야겠다&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">인공지능 및 기계학습 개론 Ⅰ - CHAPTER 3 까지 완료</title><link href="https://aa9390.github.io/machine_learning_ch3/" rel="alternate" type="text/html" title="인공지능 및 기계학습 개론 Ⅰ - CHAPTER 3 까지 완료" /><published>2018-07-11T00:00:00+00:00</published><updated>2018-07-11T00:00:00+00:00</updated><id>https://aa9390.github.io/machine_learning_ch3</id><content type="html" xml:base="https://aa9390.github.io/machine_learning_ch3/">&lt;p&gt;&lt;img src=&quot;/images/machine_learning.jpeg&quot; alt=&quot;study&quot; title=&quot;study&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NAVER에서 주최하는 Data Science Competition 2018에 지원하여 머신러닝 강의를 듣고 있다. 
고등학생때 이과였지만, 미분 적분이 낯설게 느껴지는건 왜일까ㅠㅠ&lt;/p&gt;

&lt;p&gt;MLE, MAP, decision tree, information gain에 대한 내용을 거쳐 
어제는 Naive Bayes Classifier에 대해 학습했다. 
‘순진한’이라는 뜻을 가진 naive는 각 사건? 데이터를 (실제로는 독립적이지 않지만) ‘순진하게’ 독립적이라고 보고 Classifier하는 것이다. 
어렵지만 너무 재밌는걸….! 오늘 목표는 CHAPTER 4의 반을 완강하는 것으로!&lt;/p&gt;</content><author><name></name></author><summary type="html">NAVER에서 주최하는 Data Science Competition 2018에 지원하여 머신러닝 강의를 듣고 있다. 고등학생때 이과였지만, 미분 적분이 낯설게 느껴지는건 왜일까ㅠㅠ</summary></entry><entry><title type="html">흠..</title><link href="https://aa9390.github.io/first/" rel="alternate" type="text/html" title="흠.." /><published>2018-07-11T00:00:00+00:00</published><updated>2018-07-11T00:00:00+00:00</updated><id>https://aa9390.github.io/first</id><content type="html" xml:base="https://aa9390.github.io/first/">&lt;p&gt;지금은 구루 수업중! 
날씨가 너무 덥고 습하다ㅠㅠ&lt;/p&gt;

&lt;p&gt;테마를 바꾸고 싶은뎅…
hexo로 갈아탈까?&lt;/p&gt;</content><author><name></name></author><summary type="html">지금은 구루 수업중! 날씨가 너무 덥고 습하다ㅠㅠ</summary></entry></feed>