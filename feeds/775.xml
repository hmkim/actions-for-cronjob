<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by 이홍규 on Medium]]></title>
        <description><![CDATA[Stories by 이홍규 on Medium]]></description>
        <link>https://medium.com/@mldevhong?source=rss-b3dc980dcf05------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*O4jK-FAoxyeusNF7rv-Q1g.jpeg</url>
            <title>Stories by 이홍규 on Medium</title>
            <link>https://medium.com/@mldevhong?source=rss-b3dc980dcf05------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 14 May 2019 11:58:30 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/@mldevhong" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[정확도라는 것이 오차의 정도를 알아보는 것이기에]]></title>
            <link>https://medium.com/@mldevhong/%EC%A0%95%ED%99%95%EB%8F%84%EB%9D%BC%EB%8A%94-%EA%B2%83%EC%9D%B4-%EC%98%A4%EC%B0%A8%EC%9D%98-%EC%A0%95%EB%8F%84%EB%A5%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EB%8A%94-%EA%B2%83%EC%9D%B4%EA%B8%B0%EC%97%90-87ddffc9f314?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/87ddffc9f314</guid>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Mon, 09 Jan 2017 13:16:12 GMT</pubDate>
            <atom:updated>2017-01-09T13:16:12.769Z</atom:updated>
            <content:encoded><![CDATA[<p>정확도라는 것이 오차의 정도를 알아보는 것이기에</p><p>데이터가 어떤 형태로 입력되는지는 알 수 없지만 결국 (정답과 결과)의 차이의 정도를 확인하면 원하는 것을 얻을 수 있을 거에요!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=87ddffc9f314" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[오탈자 제보 감사드립니다!!]]></title>
            <link>https://medium.com/@mldevhong/%EC%98%A4%ED%83%88%EC%9E%90-%EC%A0%9C%EB%B3%B4-%EA%B0%90%EC%82%AC%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-7c23fd755779?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/7c23fd755779</guid>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Mon, 09 Jan 2017 13:12:10 GMT</pubDate>
            <atom:updated>2017-01-09T13:12:10.835Z</atom:updated>
            <content:encoded><![CDATA[<p>오탈자 제보 감사드립니다!!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7c23fd755779" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[Tech Blog]Blog 제작 완료]]></title>
            <link>https://medium.com/@mldevhong/tech-blog-blog-%EC%A0%9C%EC%9E%91-%EC%99%84%EB%A3%8C-824f740d2bc1?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/824f740d2bc1</guid>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Mon, 09 Jan 2017 13:08:38 GMT</pubDate>
            <atom:updated>2018-05-02T03:46:23.031Z</atom:updated>
            <content:encoded><![CDATA[<p>.</p><p>Naver에서도 해보고 Medium에서도 Tech Blog를 해보았는데</p><p>.</p><p>둘의 장점 만을 합치고 싶어 결국 직접 만들었습니다.</p><p>.</p><p><a href="http://teamaq.kr">AQ Tech Blog</a></p><p>.</p><p>아직 꾸미진 않았지만 컨텐츠는 꾸준히 채워볼까 합니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=824f740d2bc1" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[Tech Blog] 직접 블로그 제작 시작]]></title>
            <link>https://medium.com/@mldevhong/tech-blog-%EC%A7%81%EC%A0%91-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EC%A0%9C%EC%9E%91-%EC%8B%9C%EC%9E%91-3bddc482d921?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/3bddc482d921</guid>
            <category><![CDATA[wordpress]]></category>
            <category><![CDATA[apache]]></category>
            <category><![CDATA[blog]]></category>
            <category><![CDATA[mac]]></category>
            <category><![CDATA[tech]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Tue, 03 Jan 2017 15:41:19 GMT</pubDate>
            <atom:updated>2017-01-03T15:41:19.940Z</atom:updated>
            <content:encoded><![CDATA[<p>.</p><p>Tech Blog를 직접 만들기 위한 작업에 착수 했습니다.</p><p>.</p><p>이미 하던 블로그 및 홈페이지 들도 모두 통합하고 싶었기에</p><p>(blog.naver.com/hg1286)</p><p>(teamaq.kr)</p><p>.</p><p>설정한 기준은</p><p>.</p><ol><li><strong>신속</strong></li><li><strong>아름다움</strong></li><li><strong>새로운 기능 및 분석</strong></li></ol><p>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*cAJLohgTCnW2CkoauVjT0g.jpeg" /><figcaption>So easy…</figcaption></figure><p>.</p><p>이렇게 정하였고 신속을 최우선으로 생각하여 MAMP로 서버 세팅을 진행하고 워드프레스로 작업을 시작합니다.</p><p>.</p><p>오늘은 세팅 완료 및 워드프레스 만져보기 정도를 진행하였습니다.</p><p>.</p><p>참고 사이트</p><p><a href="https://ko.wordpress.org/2014/01/01/mamp%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%B4%EC%84%9C-%EB%82%B4-%EC%BB%B4%ED%93%A8%ED%84%B0%EC%97%90-%EC%9B%8C%EB%93%9C%ED%94%84%EB%A0%88%EC%8A%A4-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0/">MAMP를 사용해서 애플 컴퓨터에 워드프레스 설치하기</a></p><p>.</p><p>세팅할 때 이미 80 포트가 점유되어 있어 삽질을 좀 하긴 했지만</p><p>.</p><p><a href="http://susemi99.kr/643">[mac] 80 포트를 사용 중인 프로세스 보기 - 쎄미</a></p><p>.</p><p>포트 사용 프로세스 확인 및 맥의 자체 apache 서버 문제를 해결했습니다.</p><p>.</p><p>빠르게 만들고 간단한 강의도 진행하겠습니다.</p><p>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3bddc482d921" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2017 년 Tech Blog 계획]]></title>
            <link>https://medium.com/@mldevhong/2017-%EB%85%84-tech-blog-%EA%B3%84%ED%9A%8D-761e9a11d202?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/761e9a11d202</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[blogging]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Sun, 01 Jan 2017 10:35:41 GMT</pubDate>
            <atom:updated>2017-01-01T10:35:41.944Z</atom:updated>
            <content:encoded><![CDATA[<p>.</p><p>Tech Blog를 제대로 운영해보려 합니다.</p><p>.</p><p>1일 1포스팅은 무리이겠지만 2일 1 포스팅은 최대한 지켜보려 하고</p><p>.</p><p>계획하는 주제는</p><p>.</p><ol><li>머신 러닝 — 실제 프로젝트 진행 예정</li><li>Tech Blog 직접 제작 — python django &amp; angular 2(?)</li><li>간단한 Toy project 진행 — 앱, 웹, 게임, 드론 등등</li><li>과거 네이버 블로그 포스팅들 좀 더 다듬어서 medium으로 이동</li></ol><p>(blog.naver.com/hg1286)</p><p>.</p><p>적다보니 하고 싶은 것들이 정말 많은데 새해 첫날이니 의지를 다져보겠습니다.</p><p>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=761e9a11d202" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[About Designer — Mina Seo]]></title>
            <link>https://medium.com/@mldevhong/about-designer-mina-seo-9ceb22248e6c?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/9ceb22248e6c</guid>
            <category><![CDATA[ui]]></category>
            <category><![CDATA[designer]]></category>
            <category><![CDATA[ui-design]]></category>
            <category><![CDATA[ux]]></category>
            <category><![CDATA[ux-design]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Wed, 21 Dec 2016 14:42:40 GMT</pubDate>
            <atom:updated>2016-12-21T14:42:40.445Z</atom:updated>
            <content:encoded><![CDATA[<p>.</p><p><strong>Latest</strong></p><p>.</p><p>Mathpresso Main Designer (2016.7 ~)</p><p>.</p><p>Nexters 7th Member (2015.8~ )</p><p>.</p><p>Team AQ — Project Manager &amp; Designer (2014.12~)</p><p>.</p><p>LUIX LG U+ UX Supporters (2014.2~2014.11)</p><p>.</p><p>Ewha Womans University — Interactive media</p><p>(2012.3~2017.2(prospective graduate))</p><p>.</p><p>Ewha Womans University — Sculpture</p><p>(2012.3~2017.2(prospective graduate))</p><p>.</p><p>Kyewon Highschool of Art (2009.3~2012.2)</p><p>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9ceb22248e6c" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[About Developer — Hongkyu Lee]]></title>
            <link>https://medium.com/@mldevhong/about-developer-hongkyu-lee-e2350302b64a?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/e2350302b64a</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[automotive]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[developer]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Wed, 21 Dec 2016 14:37:26 GMT</pubDate>
            <atom:updated>2016-12-31T13:02:44.312Z</atom:updated>
            <content:encoded><![CDATA[<p>.</p><p><strong>Latest</strong></p><p>.</p><p>Mathpresso Developer/ ios &amp; machine learning(2016.7.10~2016.12.31)</p><p>.</p><p>Nexters 8th Member (2015.12 ~)</p><p>.</p><p>Creativestudio AQ — Project Manager &amp; Developer (2014.12~)</p><p>.</p><p>Hyundai Motor Research Scholarship Student (2015.09~2016.12)</p><p>.</p><p>Hyundai Motor Research Intern(2015.07.06~2015.07.31)</p><p>.</p><p>Donyang Piston Research Intern (2015.01.02~2015.02.28)</p><p>.</p><p>Military Police — Air Force 10th Fighter Wing</p><p>(2012.1.16~2014.1.15)</p><p>.</p><p>Hanyang University/ major : Automotive Engineering</p><p>(2011.3~2017.2(prospective graduate))</p><p>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2350302b64a" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Welcome to Team AQ!]]></title>
            <link>https://medium.com/@mldevhong/welcome-to-team-aq-616f59743787?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/616f59743787</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[portfolio]]></category>
            <category><![CDATA[ui]]></category>
            <category><![CDATA[welcome]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Wed, 21 Dec 2016 14:28:16 GMT</pubDate>
            <atom:updated>2016-12-21T14:29:00.358Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/966/1*rhQlqZinPpLZfRbTJlWH-w.png" /></figure><p>This is our first start in medium!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=616f59743787" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[논문 번역] RCNN (RNN + CNN) 번역 및 의역(An End-to-End Trainable Neural Network for Image-based Sequence…]]></title>
            <link>https://medium.com/@mldevhong/%EB%85%BC%EB%AC%B8-%EB%B2%88%EC%97%AD-rcnn-an-end-to-end-trainable-neural-network-for-image-based-sequence-recognition-and-its-f6456886d6f8?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/f6456886d6f8</guid>
            <category><![CDATA[image-processing]]></category>
            <category><![CDATA[thesis]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[mlstudy]]></category>
            <category><![CDATA[edutech]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Mon, 12 Dec 2016 09:32:08 GMT</pubDate>
            <atom:updated>2016-12-12T09:33:05.390Z</atom:updated>
            <content:encoded><![CDATA[<h3>[논문 번역] RCNN (RNN + CNN) 번역 및 의역(An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition)</h3><p>.</p><p>Sequence Image Processing을 위한 CNN과 RNN의 결합 RCNN의 제안 논문입니다.</p><p>짧은 영어라 오역이 있을 수 있으니 발견시 피드백 부탁드립니다.</p><p>.</p><h3>초록(Abstract)</h3><p>.</p><p>이미지 기반 시계열 인식은 컴퓨터 비전에서 오랫 동안 연구되던 주제입니다.</p><p>이 분야에서 가장 중요하고 어려운 주제 중 하나인 이미지 글자 인식에 대한 문제를 논문에서 다룰 것입니다.</p><p>feature 추출과 시계열 모델을 통합시키고 하나의 통일된 framework로 합치는 참신한 Neural Network 구조를 소개합니다.</p><p>이전 이미지 글자 인식 시스템과 비교하면 제시하는 구조는 4가지 정도 다른 특징을 가집니다.</p><p>.</p><p>1. 기존의 것들이 부분적으로 학습하고 튜닝하는데 비해서 end-to-end 학습(전체를 모델로 한번에 학습)이 가능합니다.</p><p>2.임의의 길이의 시계열 데이터를 다룰 수 있습니다.(no character segmentation 또는 horizontal scale normalization을 포함)</p><p>3.어떤 미리 정해진 어휘에 제한되지 않습니다. 그리고 사전 free, based 두 분야에서 모두 놀랄만한 성능을 발휘합니다.</p><p>4.현실에서보다 더 타당한 매우 작은 모델을 효율적으로 만들어냅니다.</p><p>다른 벤치 마킹 알고리즘 보다 훨씬 선행 기술임을 증명합니다.</p><p>또한 이미지 기반 음악 악보 인식에서도 좋은 성능을 보여 일반성을 두드러지게 증명합니다.</p><p>.</p><h3>도입(Introduction)</h3><p>.</p><p>최근에 DNN(deep neural network)에 의해 촉진되어 neural network의 큰 부흥이 일고있습니다.</p><p>특히 DCNN(Deep Convolutional Neural Network)는 다양한 컴퓨터 비전 문제에서 대단합니다.</p><p>하지만 DNN과 관련된 최근 작업들의 대부분은 탐지 사물의 카테고리 분류 혹은 탐지에 헌신해 왔습니다.</p><p>이 논문에서 우리는 컴퓨터 비전의 기존 문제에 관심이 있습니다 — 이미지 기반 시계열 인식</p><p>실제 세상에서 글자, 손글씨, 악보 인식 등과 같은 시각에 대한 안정성은 독립적인 것이 아니라 시간에 따라 발생하는 경향이 있다.</p><p>일반적인 데이터 인식과 다르게 시계열 데이터 인식은 한개의 label이 아니라 연속적인 label 예측을 필요로한다.</p><p>그러므로 이런 데이터 인식은 시계열 데이터 인식 문제로 자연스럽게 바뀐다.</p><p>또다른 시계열 데이터의 독특한 특징은 그들의 길이가 매우 매우 다르다는 것이다.</p><p>예를 들어 영어 단어는 OK 처럼 2개의 글자로 이루어질 수도, congratulations 처럼 15개로 이루어 질수도 있다.</p><p>결론적으로 가장 유명한 DCNN 같은 모델은 바로 시계열 예측에 적용할 수 없다.</p><p>DCNN은 보통 고정된 입출력 차원으로 작동하고 다양한 길이의 label sequence를 만들 수 없다.</p><p>.</p><p>이렇게 특수한 시계열 데이터 문제를 해결하기 위해 몇가지 시도들이 있었습니다.</p><p>.</p><p>예를 들어</p><p>- 먼저 글자 하나하나를 발견하고 이 발견된 글자들을 DCNN 모델로 인식하는 것(label된 글자 이미지로 학습)</p><p>이 방법은 종종 정확한 발견 또는 원래 글자 이미지에서 각각을 cropping하기 위한 강력한 글자 detector의 학습을 필요로 합니다.</p><p>- 글자 인식을 이미지 분류 문제로 보고 각각의 영어 단어를 label 합니다(총 90K의 단어)</p><p>이것은 매우 큰 숫자의 class를 큰 학습 모델로 학습해야 합니다.</p><p>또한 기본 조합이 100만이 넘어가는 중국어나 악보와 같은 것에서 일반화 되기 어려운 문제입니다.</p><p>요약하자면 DCNN에 기초한 현재 시스템들은 이미지 기반 시계열 인식에 바로 적용할 수 없습니다.</p><p>.</p><p>DNN에서 또 중요한 분야인 RNN(Recurrent Neural Network) 모델들은 시계열 데이터를 주로 다루도록 설계합니다.</p><p>RNN의 한가지 장점은 학습과 테스트에서 시계열 이미지에서 각각의 요소의 position이 필요 없다는 것입니다.</p><p>하지만 입력 이미지를 이미지 feature의 sequence로 변환하는 전처리 단계가 필수적입니다.</p><p>예를 들어 Graves(논문 저자 이름)은 손글씨에서 기하학적 구조 또는 이미지 특징을 추출했습니다.</p><p>Su와 Lu는 단어 이미지를 시계열 HOG(Histogram of Oriented Gradients) feature로 변환했습니다.</p><p>전처리 단계는 파이프 라인의 후속 구성 요소와 독립적이므로 RNN 기반의 기존 시스템을 end-to-end 방식으로 학습 및 최적화 할 수 없습니다.</p><p>.</p><p>신경망 구조를 기반으로 하지 않는 몇 가지 기존의 글자 인식 방법도 창의적인 아이디어와 새로운 방식을 이 분야에 도입했습니다.</p><p>예를 들어, Almaza`n과 Rodriguez-Serrano는 단어 이미지와 텍스트 문자열을 일반적인 벡터 공간의 부분 공간에 포함시키고 단어 인식을 검색 문제로 변환하는 방법을 제안했습니다.</p><p>Yao와 Gordo는 글자 인식을 위해 중간 단계의 특징을 사용했습니다.</p><p>표준 벤치 마크에서 유망한 성능을 달성했지만, 일반적으로 신경 네트워크 기반의 이전 알고리즘과 이 논문에서 제안된 접근 방식이 더욱 성능이 우수합니다.</p><p>.</p><p>이 논문의 주된 목표는 네트워크 구조가 이미지에서 시계열 데이터를 인식하도록 특별히 설계된 새로운 신경망 모델입니다.</p><p>제안하는 신경망 모델은 DCNN과 RNN의 조합이므로 <strong>Convolutional Recurrent Neural Network (CRNN)</strong>로 명명합니다.</p><p>시계열 데이터의 경우 CRNN은 기존 신경망 모델에 비해 몇 가지 특유의 장점을 가지고 있습니다.</p><p>1) 상세한 주석 (예 : 글자)이 필요없는 시계열 label (예 : 단어)을 직접 학습 할 수 있습니다.</p><p>2) DCNN은 이미지 데이터에서 직접 정보를 얻는 representation을 배우는 데 동일한 속성을 지닌다. (사람이 만든 특징이나 전처리 단계(이진화 / 세분화, 구성 요소 지역화)가 필요하지 않습니다.)</p><p>3) RNN과 동일한 속성을 가지며 시계열 label을 생성 할 수 있습니다.</p><p>4) 학습과 테스트 단계 모두에서 height normalization(이미지 크기 정규화) 만 필요로 하는 시계열 데이터의 길이에 제약을 받지 않습니다.</p><p>5) 이전 기술보다 단어 인식에서 더 우수하거나 매우 좋은 성능을 보여준다.</p><p>6) 표준 DCNN 모델보다 훨씬 적은 매개 변수를 포함하여 저장 공간을 덜 차지합니다.</p><p>.</p><h3>CRNN</h3><p>.</p><p>Convolution Network의 최상부에서 Convolution layer에 의해 출력 된 feature sequence의 각 프레임에 대한 예측을 하기 위해 Recurrent Network가 구축됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/754/1*AcITzfE1-EPNF3G8w5yipw.png" /><figcaption>그림 1</figcaption></figure><p>CRNN의 네트워크 구조는 그림 1에서 Convolution layer, Recurrent layer 및 하단에서 상단으로의 Transcription layer를 비롯한 세 가지 구성 요소로 이루어져 있습니다.</p><p>CRNN의 하단에서, Convolution layer는 각 입력 이미지에서 feature sequence를 자동으로 추출합니다.</p><p>CRNN 상단의 Transcription layer는 Recurrent layer에 의한 프레임 별 예측을 label sequence로 변환하는 데 사용됩니다.</p><p>CRNN은 여러 종류의 Network 구조 (예 : CNN 및 RNN)로 구성되지만 하나의 loss function으로 공동으로 교육 할 수 있습니다.</p><p>.</p><h4>2.1 Feature Sequence Extraction</h4><p>.</p><p>CRNN 모델에서 Convolution layer 구성 요소는 표준 CNN 모델 (Fully-Connected Layer가 제거됨)에서 Convolution 및 max-pooling layer를 가져와 구성됩니다.</p><p>이러한 요소들은 입력 이미지에서 sequential feature를 추출하는 데 사용됩니다.</p><p>네트워크에 입력되기 전에 모든 이미지를 동일한 높이로 조정해야합니다.</p><p>그 다음, feature vector의 sequence는 Convolution layer 구성 요소에 의해 생성 된 feature map에서 추출되며, 이는 다시 Recurrent layer에 대한 입력입니다.</p><p>구체적으로, feature sequence의 각 feature vector는 feature map상의 좌측에서 우측으로 생성된다.</p><p>이것은 i 번째 feature vector가 모든 map의 i 번째 열의 집합임을 의미한다. 우리의 setting에서 각 열의 너비는 단일 픽셀로 고정됩니다.</p><p>Convolution, max-pooling 및 요소 별 활성화 함수의 계층이 local 영역에서 작동하기 때문에 변환 불변입니다.</p><p>따라서, feature map의 각 열은 원본 이미지 (수신 필드라고 함)의 사각형 영역에 해당하며 이러한 사각형 영역은 왼쪽에서 오른쪽으로 feature map의 해당 열과 동일한 순서로 있습니다.</p><p>아래 그림과 같이, 특징 sequence 내의 각 vector는 수용 필드와 관련되며, 그 영역에 대한 이미지 설명하는 것으로 생각할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/372/1*vhx9GhFMtdBUBI3KWGIz0Q.png" /><figcaption>그림 2</figcaption></figure><p>견고하고, 풍부하고, 학습 가능하고, 깊은 Convolutional feature는 다양한 종류의 시각적 인식 작업에 많이 채택되었습니다.</p><p>이전의 접근법 중 일부는 CNN을 사용하여 글자 인식과 같은 시계열 데이터에 대한 강력한 representation을 학습했습니다.</p><p>그러나 이러한 접근법은 일반적으로 CNN에 의해 전체 이미지의 전체 representation을 추출한 다음 시계열 데이터의 각 구성 요소를 인식하기 위해 local의 심층적인 feature를 수집합니다.</p><p>CNN은 고정된 입력 차원을 만족시키기 위해 고정된 크기로 입력 이미지를 스케일링 해야하기 때문에, 길이 변화가 커서 시계열 데이터에는 적합하지 않습니다.</p><p>CRNN에서 시계열 데이터의 길이 변화에 영향을 받지 않기 위해 sequential representation에 깊은 feature을 전달합니다.</p><p>.</p><h4>2.2 Sequence Labeling</h4><p>.</p><p>깊은 양방향 RNN은 Recurrent layer로서 Convolution layer의 상단에 구축됩니다.</p><p>Recurrent layer는 feature sequence x = x1, …, xT에서 각 프레임 xt에 대한 label 레이블 분포 yt를 예측합니다.</p><p>Recurrent layer의 장점은 세 가지입니다.</p><p>첫째, RNN은 sequence 안에서 문맥 정보를 캡처하는 강력한 기능을 갖추고 있습니다.</p><p>이미지 기반 sequence 인식을 위해 상황 별 신호를 사용하는 것은 각 symbol을 독립적으로 처리하는 것보다 더 안정적이며 도움이됩니다.</p><p>글자 인식을 예로 들자면, 넓은 글자는 완전히 기술하기 위해 여러 개의 연속된 프레임을 요구할 수도 있습니다.(그림 2)</p><p>게다가 모호한 문자는 문맥을 관찰 할 때 구별하기가 쉽습니다.</p><p>예를 들어 문자 높이를 대조하여 “il”을 인식하는 것이 각 문자를 개별적으로 인식하는 것보다 쉽습니다.</p><p>둘째, RNN은 error differential을 입력, 즉 Convolution layer에 back-propagate 할 수 있으므로 통합 네트워크에서 Recurrent layer와 Convolution layer을 함께 학습할 수 있습니다.</p><p>셋째, RNN은 시작부터 끝까지 임의의 길이의 sequence에서 작동할 수 있습니다.</p><p>기존의 RNN unit은 입력 layer와 출력 layer 사이에 self-connected hidden layer가 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*6qFGPGqzF5CZf0Yflf7guQ.png" /><figcaption>기존 rnn</figcaption></figure><p>sequence에서 프레임 xt를 수신 할 때마다 현재 상태 xt와 과거 상태 ht-1을 입력으로 갖는 비선형 함수 ht = g (xt, ht-1)로 내부 상태 ht를 갱신한다.</p><p>그런 다음 예측 yt는 ht를 기반으로합니다.</p><p>이런 식으로 과거 컨텍스트 {xt ‘} t’&lt;t가 포착되어 예측에 사용됩니다.</p><p>그러나 기존의 RNN unit은 Vanishing Gradient 문제로 인해 저장할 수있는 컨텍스트의 범위를 제한하고 학습에 부담을 줍니다.</p><p>Long-Short Term Memory (LSTM)는 이 문제를 해결하기 위한 RNN unit의 하나입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/404/1*iu_4JJCt-gFks-P_Zgvbew.png" /><figcaption>그림 3</figcaption></figure><p>LSTM 은 Memory Cell 및 3 개의 곱셈 gate, 즉 input, output 및 forget gate로 구성된다.</p><p>개념적으로, Memory Cell은 과거의 컨텍스트를 저장하고, input 및 output 게이트는 Cell이 오랜 시간 동안 컨텍스트를 저장할 수 있게합니다.</p><p>그 사이에, Cell에 있는 기억은 forget gate에 의해 삭제 될 수 있습니다.</p><p>LSTM은 이미지 기반 sequence에서 자주 발생하는 장기간의 종속을 파악할 수 있게 합니다.</p><p>LSTM은 방향성이 있으며 과거 컨텍스트만 사용합니다.</p><p>그러나 이미지 기반 sequence에서는 양방향의 컨텍스트가 유용하고 서로 보완적입니다.</p><p>그러므로, 우리는 앞, 뒤로 각각 하나씩 두 개의 LSTM을 양방향 LSTM으로 결합합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/363/1*SRZUOdh91iMQY78tOKcT5A.png" /><figcaption>그림 4</figcaption></figure><p>더욱이, 다수의 양방향 LSTM이 적층 될 수 있어서 그림 4와 같이 깊은 양방향 LSTM을 만들수 있습니다.</p><p>심층 구조는 얕은 구조보다 높은 추상화 수준을 허용하며 음성 인식 작업에서 중요한 성능 향상을 달성했습니다.</p><p>Recurrent layer에서, error differential은 위 그림에 나타난 화살표의 반대 방향으로 전파된다. — BPTT (Back-Propagation Through Time)</p><p>Recurrent Layer의 맨 아래에서, 전파된 미분값의 sequence는 map으로 연결되고, feature map을 feature sequence로 변환하는 동작을 갱신시키고, Convolution layer로 feedback된다.</p><p>실제로 우리는 Convolution layer와 Recurrent layer 사이의 다리 역할을하는 “Map-to-Sequence”라는 사용자 지정 네트워크 레이어를 만듭니다.</p><p>.</p><h4>2.3 Transcription</h4><p>.</p><p>Transcription은 RNN에 의해 만들어진 프레임 별 예측을 label sequence로 변환하는 과정입니다.</p><p>수학적으로, Transcription는 프레임 당 예상에 따라 가장 높은 확률로 label sequence를 찾는 것입니다.</p><p>실제로, 번역의 두 가지 모드, 즉 사전 free, based 두가지의 변환이 존재합니다.</p><p>사전은 예측이 제약을 받는 label sequence의 집합입니다. — 맞춤법 검사 사전</p><p>사전 free에서는 사전없이 예측이 수행됩니다.</p><p>사전 based에서는 가장 높은 확률을 갖는 label sequence를 선택하여 예측을 수행합니다.</p><p>.</p><h4>2.3.1 label sequence의 확률</h4><p>.</p><p>우리는 Graves 등이 제안한 CTC (Connectionist Temporal Classification) layer에서 정의 된 조건부 확률을 채택합니다.</p><p>프레임 별 예측 y = y1, …, yT이 1이 될 확률에 대해 정의하며 1이 되는 각 레이블의 위치는 무시합니다.</p><p>결과적으로 네트워크를 훈련시키는 목적으로 이 확률의 negative log-likelihood를 사용하면, 우리는 오직 이미지와 해당 label sequence만 필요하기 때문에 개별 문자의 위치를 labeling하지 않아도 됩니다.</p><p>조건부 확률의 공식은 다음과 같이 간략하게 설명합니다.</p><p>입력은 sequence y = y1,. . . , yT ( 여기서 T는 sequence 길이입니다.)</p><p>여기서 yt ∈ R | L ‘| 는 집합 L ‘= L ∪에 대한 확률 분포이며, 여기서 L은 작업의 모든 label(예 : 모든 영문 문자) 뿐만 아니라 공백 label을 포함합니다.</p><p>sequence-to-sequence mapping function B는 sequence π ∈ L’ T로 정의되며, 여기서 T는 길이입니다.</p><p>B는 먼저 겹쳐진 label을 제거한 다음 π을 l에 매핑 한 다음 공백을 제거합니다.</p><p>예를 들어 B는 “ — hh-e-l-ll-oo — “( “-”는 “공백”을 나타냄)을 “hello”에 매핑합니다.</p><p>그런 다음 조건 확률은 B에 의해 1에 매핑 된 모든 π의 확률의 합으로 정의됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/355/1*ueB4rcXxcxuMDTNVZEWGTA.png" /><figcaption>수식 1</figcaption></figure><p>-</p><p>여기서 π의 확률은 (p (π | y) =</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/182/1*si3gtdJW7ThPU5KV_Tioog.png" /></figure><p>) t 시간에 레이블 πt를 가질 확률입니다.</p><p>수식 1은 기하 급수적으로 많은 수의 합계 항목으로 인해 계산이 불가능합니다.</p><p>그러나 수식 1은 forward — backward 알고리즘을 사용하여 효율적으로 계산할 수 있습니다.</p><p>2.3.2 사전 free transcription</p><p>이 방법에서 sequence 1* 는 위 수식에서 가장 높은 확률을 갖는 것이 예측값이 됩니다.</p><p>솔루션을 정확하게 찾을 수 있는 다루기 쉬운 알고리즘이 없으므로 forward — backward 알고리즘을 사용합니다.</p><p>시퀀스 1 *은 대략적으로</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/339/1*1CQ_grXQQ0okTORHnYr82w.png" /></figure><p>위와 같습니다.</p><p>즉, 각 타임 스탬프 t에서 가장 가능성이 큰 레이블을 취하고 결과 시퀀스를 1 *에 매핑합니다.</p><p>.</p><h4>2.3.3 사전 기반 transcription</h4><p>.</p><p>사전 기반 방식에서 각 테스트 샘플은 사전 D와 연관되어 있습니다.</p><p>기본적으로 label sequence는 수식 1에 정의 된 가장 높은 조건부 확률을 갖는 사전에서 sequence를 선택함으로써 인식합니다.</p><p>즉 1 * = argmax1∈D p (1 | y)이다.</p><p>그러나 큰 사전의 경우, 50k 단어가 있는 Hunspell 맞춤법 검사 사전을 사용한다면 사전에 대한 철저한 검색을 수행하는 데 시간이 많이 걸릴 것입니다. 즉, 사전의 모든 sequence에 대해 수식 1을 계산하고 그 중에서 가장 확률이 높은 것을 선택하는 것</p><p>이 문제를 해결하기 위해 우리는 사전이 없는 transcription를 통해 예측된 label sequence가 편집 거리(edit distance) 지표에서 종종 실제 기준에 가깝다는 것을 관찰한다.</p><p>이것은 가장 가까운 이웃 후보 Nδ (1 ‘)로 검색을 제한 할 수 있음을 나타냅니다.</p><p>여기서 δ는 최대 편집 거리이고 l’은 사전 free 방식에서 y에서 transcription 된 sequence입니다</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/320/1*aLGaHz8TeoqLsFdW2yarFQ.png" /><figcaption>수식 2</figcaption></figure><p>후보 Nδ (l)은 discrete metric space에 특별히 적합한 metric tree 인 BK-tree 데이터 구조 를 사용하여 효율적으로 찾을 수 있습니다.</p><p>BK-tree의 검색 시간 복잡도는 O (log | D |)이며, 여기서 | D |는 사전의 크기입니다.</p><p>따라서 이 체계는 매우 큰 사전으로 쉽게 확장됩니다.</p><p>우리의 접근법에서, BK-tree 데이터 구조는 사전을 위해 오프라인으로 구축됩니다.</p><p>그런 다음 우리는 query sequence에 대한 δ (편집 거리)보다 작거나 같은 sequence를 찾아 트리를 사용하여 빠른 온라인 검색을 수행합니다.</p><p>.</p><h4><strong>2.4 Network Training</strong></h4><p>.</p><p>트레이닝 데이터 세트를 X = {Ii, li} i로 표시하고, 여기서 Ii는 트레이닝 이미지이고 li는 실제 기준의 label sequence입니다.</p><p>목표는 실제 기준의 조건부 확률에 대한 negative log-likelihood을 최소화하는 것이다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/381/1*-LJhUphFhbAbPI3K8Fvq-w.png" /></figure><p>여기서 yi는 Ii의 Recurrent layer와 Convolution layer에 의해 생성 된 sequence입니다.</p><p>이 목적 함수는 이미지와 실제 기준 label sequence로 부터 직접 비용 값을 계산합니다.</p><p>따라서 네트워크는 이미지와 sequence 쌍에 대해 end-to-end 학습을 수행할 수 있으므로 학습 이미지의 모든 개별 구성 요소에 수동으로 label을 지정하는 절차가 필요하지 않습니다.</p><p>네트워크는 Stochastic gradient descent(SGD)로 학습합니다.</p><p>gradient는 back-propagation 알고리즘에 의해 계산됩니다.</p><p>특히, transcription layer에서 error differential은 forward-backward 알고리즘으로 역전파 됩니다.</p><p>Recurrent layer에서 BPTT (Back-Propagation Through Time)를 적용하여 error differential을 계산합니다.</p><p>최적화를 위해 ADADELTA를 사용하여 차원 별 learning rate을 자동으로 계산합니다.</p><p>ADADELTA는 기존의 방식과 비교하여 learning rate을 수동으로 설정할 필요가 없습니다.</p><p>더 중요한 것은 ADADELTA를 사용한 최적화가 기존의 방법(momentum method)보다 빠르게 수렴한다는 사실입니다.</p><p>.</p><h3>Experiments</h3><p>.</p><p>논문에서 제안하는 CRNN 모델의 효과를 평가하기 위해 우리는 비전 작업에 도전하는 글자 인식 및 악보 인식을 위한 표준 벤치 마크에 대해 실험을 수행했습니다.</p><p>훈련과 시험을 위한 데이터 세트와 설정은 섹션 3.1에서 글자 이미지에 대한 CRNN의 세부 설정은 섹션 3.2, 포괄적인 비교 결과는 섹션 3.3. CRNN의 일반성을 더 입증하기 위한 자료는 섹션 3.4.에 서술하였습니다.</p><p>.</p><h4><strong>3.1 Datasets</strong></h4><p>.</p><p>이미지 글자 인식을 위한 모든 실험에서 Jaderberg 등이 발표한 합성 데이터 세트 (Synth)를 사용합니다.</p><p>이 데이터 세트에는 8 백만 개의 학습 이미지와 그에 해당하는 실제 단어 데이터가 포함되어 있습니다.</p><p>이러한 이미지는 합성 텍스트 엔진에 의해 생성되며 매우 현실적입니다.</p><p>우리의 네트워크는 합성 데이터에 대해 한 번 학습을 했으며, 다른 모든 실제 테스트 데이터 세트에서 학습 데이터를 미세 조정하지 않고 테스트했습니다.</p><p>CRNN 모델은 인조 텍스트 데이터로 학습 했지만 표준 텍스트 인식 벤치 마크의 실제 이미지에서 잘 작동합니다.</p><p>ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k 단어 (IIIT5k) 및 Street View Text(SVT)와 같이 이미지 글자 인식을 위한 4 가지 벤치마킹이 성능 평가에 사용됩니다.</p><p>IC03 테스트 데이터 세트는 box 쳐진 label이 있는 251 개의 이미지입니다.</p><p>Wang을 따라서 우리는 알파벳이 아닌 문자를 포함하거나 3 자 미만의 이미지를 무시하고 표 1을 사용하여 테스트 세트를 얻습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*HjG-XpnvJcUsJs19_cSP3g.png" /><figcaption>표 1</figcaption></figure><p>&lt;네트워크 구성 요약. 첫 번째 행이 최상위 계층입니다.&gt;</p><p>&lt;’k’, ‘s’및 ‘p’는 각각 860개의 crop된 글자 이미지의 kernel size, stride 및 padding size로 나타냅니다.&gt;</p><p>각 테스트 이미지는 Wang 등이 정의한 50 단어 사전과 연관되어 있습니다.</p><p>전체 사전은 모든 이미지 별 사전을 결합하여 만듭니다.</p><p>또한, 우리는 Hunspell 맞춤법 검사 사전에 있는 단어로 구성된 50k 사전을 사용합니다.</p><p>IC13 테스트 데이터 세트는 대부분의 데이터를 IC03에서 상속받습니다.</p><p>여기에는 1,015 개의 실제 기준인 crop된 단어 이미지들이 있습니다.</p><p>IIIT5k 에는 인터넷에서 수집한 3,000 자의 crop된 단어 테스트 이미지가 있습니다.</p><p>각 이미지는 50 단어 사전과 1k 단어 사전에 연관되어 있습니다.</p><p>SVT 테스트 데이터 세트는 Google 스트리트 뷰에서 수집 한 249 개의 스트리트 뷰 이미지로 구성됩니다.</p><p>이것에는 crop된 647 단어의 이미지가 있습니다.</p><p>각 단어 이미지는 Wang 등이 정의한 50 단어 어휘집을 가집니다.</p><p>.</p><h4><strong>3.2 Implementation Details</strong></h4><p>.</p><p>실험에 사용 된 네트워크 구성은 표 1에 요약되어 있습니다.</p><p>Convolution layer의 구조는 VGG-VeryDeep 구조를 기반으로 합니다.</p><p>약간의 생각 비틀기로 영어 텍스트를 인식하는데 적합하도록 조정할 수 있습니다.</p><p>제 3 및 제 4 max-pooling layer에서, 기존의 정사각형의 pooling window 대신에 1 × 2 크기의 사각형 pooling window를 채택하는 것입니다.</p><p>이 생각 비틀기는 더 큰 너비의 feature map을 생성하므로 더 긴 feature sequence를 생성합니다.</p><p>예를 들어, 10 글자를 포함하는 이미지는 전형적으로 크기 100 × 32이고, 이로부터 25 개의 프레임이 생성 될 수 있습니다.</p><p>이 길이는 대부분의 영어 단어의 길이를 초과합니다.</p><p>이것을 기반으로 직사각형 pooling window은 직사각형의 수용 필드 (그림 2)를 나타내며 ‘i’와 ‘l’과 같은 좁은 모양의 문자를 인식하는 데 유용합니다.</p><p>네트워크는 깊은 Convolution layer를 가질뿐만 아니라 Recurrent layer도 가지고 있습니다.</p><p>둘 다 학습이 어려운 것으로 잘 알려져있습니다.</p><p>배치 정규화(batch normalization)는 이러한 깊이의 네트워크를 학습시키는데 매우 유용합니다.</p><p>두 개의 batch normalization layer가 각각 5 번째 및 6 번째 Convolution layer 뒤에 삽입됩니다.</p><p>batch normalization layer를 사용하면 학습 시간이 크게 단축됩니다.</p><p>Torch7 / CUDA의 LSTM 유닛, C ++의 transcription layer 및 C ++의 BK-tree 데이터 구조에 대한 사용자 정의 구현을 통해 Torch7 프레임 워크 내에 네트워크를 구현합니다.</p><p>2.50GHz 인텔 제온 E5- 2609 CPU, 64GB RAM 및 Tesla (TM) K40 GPU가 장착 된 워크 스테이션에서 실험을 수행합니다.</p><p>ADADELTA를 사용하여 네트워크 매개 변수 ρ를 0.9로 설정합니다.</p><p>학습 과정에서 모든 이미지는 100x32로 조정되어 학습 과정을 가속화합니다.</p><p>학습 과정은 수렴하는데 약 50 시간이 걸립니다.</p><p>테스트 이미지는 높이가 32로 조정됩니다.</p><p>너비는 높이에 비례하여 조정되지만 최소 100 픽셀입니다.</p><p>사전 없이 IC03에서 측정 한 평균 테스트 시간은 0.16 초 / 샘플입니다.</p><p>근사 사전 검색은 IC03의 50k 어휘에 적용되며 매개 변수 δ는 3으로 설정됩니다.</p><p>각 샘플을 테스트하는 데 평균 0.53 초가 걸립니다.</p><p>.</p><h4><strong>3.3 Compartive Evaluation</strong></h4><p>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-4qMnkZhSf-8KsO2H63IQQ.png" /><figcaption>표 2</figcaption></figure><p>논문에서 제안한 CRNN 모델과 심층 모델에 기반한 접근법을 포함한 최첨단 기술에 의해 얻어진 위의 네 가지 공개 데이터 세트에 대한 모든 정확도가 표 2에 나와있다.</p><p>제한된 어휘집의 경우, 우리의 방법은 대부분의 최첨단 접근법보다 지속적으로 우위에 있으며, 평균적으로 이전 최고의 text reader를 능가한다.</p><p>특히, 우리는 이전 최고와 비교하여 IIIT5k, SVT에서 우수한 성능을 얻었으며 “전체”어휘가 있는 IC03에서만 낮은 성능을 보였습니다.</p><p>이전 최고의 모델은 특정 사전, 즉 각 단어가 class label과 연관되어 있음을 알 수 있습니다.</p><p>그것과 달리, CRNN은 알려진 사전의 단어를 인식하는 것에 한계를 두지 않고 임의의 문자열 (예 : 전화 번호), 문장 또는 중국어 단어와 같은 기타 스크립트를 처리 할 수 있습니다.</p><p>따라서 CRNN의 결과는 모든 테스트 데이터 세트에서 경쟁력이 있습니다.</p><p>제한되지 않은 사전의 경우, 우리의 방법은 SVT에서 최상의 성능을 달성하지만 아직 IC03과 IC13에 대한 일부 접근법에서는 약간 성능이 부족하다.</p><p>표 2의 “없음”열에있는 공란은 그러한 접근법이 사전 없이는 인식에 적용될 수 없거나 제한되지 않은 경우에 인식 정확도를 보고하지 않았음을 나타냅니다.</p><p>우리의 방법은 학습 데이터로 단어 label을 가진 합성 텍스트만을 사용하기에 학습을 위해 문자 label이 있는 790 만 개의 실제 단어 이미지를 사용한 PhotoOCR과 매우 다릅니다.</p><p>가장 좋은 성과는 제약없는 어휘집의 경우에 이전 최고에 의해 보고 되었는데, 이것은 큰 사전을 기반으로 만들었고 앞서 언급 한 바와 같이 사전에 엄격하게 제약되지 않은 모델은 아닙니다.</p><p>이러한 의미에서 제약없는 사전에서 우리의 결과는 여전히 유망합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/483/1*qeuAiOcK7uW3G8x8VxOsLA.png" /><figcaption>표 3</figcaption></figure><p>여기서 제안한 알고리즘의 장점을 다른 텍스트 인식 방법보다 더 잘 이해하기 위해 표 3에 요약 된대로 E2E Train, Conv Ftrs, CharGT-Free, Unconstrained 및 Model Size라는 여러 속성에 대한 포괄적인 비교를 제공합니다.</p><p><strong>E2E Train</strong> : 이 것은 특정 텍스트 읽기 모델이 전처리없이 또는 몇 가지 분리 된 단계를 거쳐 철저히 훈련 가능한지 여부를 보여줍니다. 이는 교육을 위해 우아하고 깨끗한 접근법을 나타냅니다. 표 3에서 볼 수 있듯이, CRNN뿐만 아니라 이전의 방식을 포함한 DNN을 기반으로 한 모델만이 이러한 특성을 갖습니다.</p><p><strong>Conv Ftrs</strong> : 이 것은 접근 방식이 이미지를 직접 학습하거나 basic representation으로 handcraft feature에서 배운 convolutional 특징을 사용하는지 여부를 나타냅니다.</p><p><strong>CharGT-Free</strong> :이 것은 문자 수준의 annotation이 모델 학습에 필수적인지 여부를 나타냅니다. CRNN의 입력 및 출력 label은 sequence 일 수 있기 때문에 문자 수준의 annotation은 필요하지 않습니다.</p><p><strong>Unconstrained</strong> :이 것은 학습된 모델이 특정 사전에 제약되어 있는지, 사전이 아닌 단어 나 임의의 sequence를 처리 할 수 없는지 여부를 나타냅니다. label embedding 및 incremental learning으로 학습 한 최신 모델은 매우 높은 성능을 달성했지만 특정 사전에 제약을 받습니다.</p><p><strong>Model Size</strong> : 이 것은 학습 된 모델의 저장 공간을 보여줍니다. CRNN에서 모든 계층은 가중치 공유 연결을 가지며 fully-connected layer는 필요하지 않습니다. 결과적으로, CRNN의 매개 변수의 수는 CNN의 변형에 대해 학습된 모델보다 훨씬 적으므로 이전 최고에 비해 훨씬 작은 모델이됩니다. 우리 모델에는 830 만 개의 매개 변수가 있으며 33MB RAM (각 매개 변수에 4 바이트)을 사용하므로 모바일 장치로 쉽게 이식 할 수 있습니다.</p><p>표 3은 각기 다른 접근법의 차이점을 세부적으로 보여 주며 CRNN의 장점을 다른 경쟁 방식에 비해 충분히 보여줍니다.</p><p>또한, 파라미터 δ의 영향을 테스트하기 위해 수식 2에서 δ의 다른 값을 실험합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/516/1*xYAOjK_phvUNN_1gZ4ep2g.png" /><figcaption>그림 4</figcaption></figure><p>그림 4에서 우리는 δ의 함수로 인식 정확도를 그려냅니다. δ가 클수록 후보가 더 많아지기 때문에 사전 기반의 정확한 표기가 됩니다. 반면에, 더 긴 BK-tree 검색 시간과 더 많은 수의 후보 sequence를 테스트하기 때문에 δ가 증가할수록 계산 비용도 증가합니다. 실제로, 정확도와 속도 사이의 절충안으로 δ = 3을 선택합니다.</p><p>.</p><h4><strong>3.4 Musical Score Recognition</strong></h4><p>.</p><p>악보는 일반적으로 오선지에 배열된 일련의 음표로 구성됩니다. 이미지에서 악보를 인식하는 것을 OMR (Optical Music Recognition) 문제라고합니다. 이전의 방법들은 종종 이미지 전처리 (대부분 이진화), 오선 탐지 및 개별 음표 인식이 있습니다. 우리는 OMR을 sequence 인식 문제로 생각하고 CRNN을 사용하여 이미지에서 직접 음표의 sequence를 예측합니다. 간단히 하기 위해 모든 음높이를 인식하고 모든 화음를 무시하며 모든 악보에 대해 동일한 장음계 (C 메이저)를 가정합니다.</p><p>우리가 아는 한, 음높이 인식에 대한 알고리즘을 평가하기 위한 공개 데이터 세트는 존재하지 않습니다. CRNN에 필요한 교육 자료를 준비하기 위해 악보 사이트에서 2650 개의 이미지를 수집합니다. 각 이미지는 3 ~ 20 개의 음을 포함하는 악보 단편을 포함합니다. 우리는 모든 이미지에 대해 실제 값 label sequence (반전이 아닌 시퀀스)를 수동으로 labeling합니다. 수집된 이미지는 회전, 크기 조정 및 노이즈 제거, 자연스러운 이미지로 배경이 대체되면서 265k의 학습 샘플로 보강됩니다. 테스트를 위해 세 가지 데이터 세트를 만듭니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/503/1*ajFcnUCj9bma8lOWoBEupQ.png" /><figcaption>그림 5</figcaption></figure><p>1) “Clean”, 악보 사이트에서 수집 된 260 개의 이미지가 포함되어 있습니다. 그림 5 (a)</p><p>2) “Synthesized”, 위에서 언급한 argumentation 전략을 사용하여 “Clean”에서 생성됩니다. 그림 5 (b)</p><p>3) “Real-World”는 휴대 전화 카메라로 음악 서적에서 찍은 스코어 조각 200 이미지를 포함합니다. 그림 5 ©</p><p>.</p><p>학습 데이터가 제한적이므로 모델 용량을 줄이기 위해 단순한 CRNN 구성을 사용합니다. 표 1에 지정된 구성과 다른데 4 번째 및 6 번째 Convolution layer는 제거되고 2 계층 양방향 LSTM은 2 계층 단일 방향 LSTM으로 대체됩니다. 네트워크는 이미지 쌍과 해당 label sequence에 대해 학습 합니다. 인식 성능을 평가하기 위해 다음과 같은 두 가지 방법이 사용됩니다.</p><p>.</p><p>1) 조각 정확도, 즉 올바르게 인식 된 악보 조각의 백분율;</p><p>2) 평균 편집 거리(average edit distance), 즉 예측 음높이 sequence와 실제 값 사이의 평균 편집 거리. 비교를 위해, 우리는 2 개의 상용 OMR 엔진 인 Capella Scan과 PhotoScore를 평가합니다.</p><p>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/494/1*XTLrl-mZL77-WHaxsjvkjA.png" /><figcaption>표 4</figcaption></figure><p>표 4는 결과를 요약합니다.</p><p>CRNN은 두 가지 상용 시스템보다 좋은 성능을 보입니다.</p><p>Capella Scan 및 PhotoScore 시스템은 Clean 데이터 세트에서 상당히 잘되지만 성능은 합성 및 실제 데이터에서 크게 떨어집니다.</p><p>주된 이유는 오선과 음표를 탐지하기 위해 강력한 이진화에 의존하지만 조명 상태가 좋지 않고 노이즈에 의해 손상되고 배경이 어지러운 합성 및 실제 데이터에서는 실패하는 경우가 종종 있습니다.</p><p>한편, CRNN은 잡음 및 왜곡에 매우 강인한 Convolution feature를 사용합니다.</p><p>게다가 CRNN의 Recurrent layer는 악보에서 문맥 정보를 활용할 수 있습니다.</p><p>각 음은 자체 음표뿐만 아니라 근처 음표에서도 인식됩니다. 결과적으로, 일부 음표는 인근 음표와 비교하여 인식 할 수 있습니다. 예시 — 수직 위치 대조</p><p>그 결과는 최소한의 domain knowledge를 필요로 하는 다른 이미지 기반 sequence 인식 문제에 쉽게 적용될 수 있다는 점에서 CRNN의 일반성을 보여주었습니다.</p><p>Capella Scan 및 PhotoScore와 비교해 볼 때, CRNN 기반 시스템은 아직 초기 단계이며 많은 기능이 빠져 있습니다.</p><p>그러나 이것은 OMR을 위한 새로운 시각을 제공하고 음높이 인식에서 유망한 기능을 보여주었습니다.</p><p>.</p><h3>Conclusion</h3><p>.</p><p>본 논문에서는 Convolutional Neural Network (CNN)와 Recurrent Neural Network (RNN)의 장점을 통합한 CRNN (Convolutional Recurrent Neural Network)이라는 새로운 신경망 구조를 제안했습니다.</p><p>CRNN은 다양한 크기의 입력 이미지를 취할 수 있으며 길이가 다른 예측을 생성합니다.</p><p>학습 단계에서 각 개별 요소 (예 : 문자)에 대한 상세한 주석이 필요없는 대용량 level label(예 : 단어)에서 직접 실행됩니다.</p><p>더욱이 CRNN은 기존의 신경 회로망에 사용된 fully-connected layer를 버리기 때문에 훨씬 작고 효율적인 모델이 됩니다.</p><p>이러한 모든 특성 때문에 CRNN은 이미지 기반 sequence 인식을 위한 탁월한 접근 방식입니다.</p><p>이미지 글자 인식 벤치 마크에 대한 실험은 CRNN이 다른 CNN 및 RNN 기반 알고리즘 뿐만 아니라 기존 방법과 비교하여 우월하거나 매우 경쟁력있는 성능을 달성한다는 것을 보여줍니다.</p><p>또한 CRNN은 CRNN의 일반성을 검증하는 OMR (Optical Music Recognition) 벤치 마크에서 다른 경쟁 업체보다 월등히 뛰어납니다.</p><p>실제로 CRNN은 일반적인 framework이므로 이미지의 sequence 예측과 관련된 다른 문제 (예 : 중국어 문자 인식)에 적용 할 수 있습니다.</p><p>CRNN의 속도를 높이고 실제 응용 프로그램에서보다 실용적으로 만드는 것은 향후 탐험의 가치가 있는 방향입니다.</p><p>논문 출처 : <a href="https://arxiv.org/pdf/1507.05717v1.pdf">https://arxiv.org/pdf/1507.05717v1.pdf</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f6456886d6f8" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mathpresso 머신 러닝 스터디 — 16. TensorFlow를 이용한 머신러닝]]></title>
            <link>https://medium.com/qandastudy/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-16-tensorflow%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-99d8e5643a1?source=rss-b3dc980dcf05------2</link>
            <guid isPermaLink="false">https://medium.com/p/99d8e5643a1</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[education]]></category>
            <category><![CDATA[tensorflow]]></category>
            <dc:creator><![CDATA[이홍규]]></dc:creator>
            <pubDate>Sun, 27 Nov 2016 14:17:43 GMT</pubDate>
            <atom:updated>2018-05-02T11:14:29.496Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pidw2QNb9nG5BrcNzbY4NA.jpeg" /></figure><p>오늘은</p><blockquote>TensorFlow</blockquote><p>에 대한 이야기를 해볼까 합니다.</p><p>.</p><p><strong>TensorFlow는 구글에서 연구와 제품 개발을 목적으로 만든 머신러닝을 위한 오픈 소스 소프트웨어 라이브러리 입니다.</strong></p><p>.</p><p>머신러닝을 직접 구현하려면 수학적인 부분이 많이 필요한 것들을</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/959/1*2wK6Sg1ILPYxCxTdqnVYXw.jpeg" /><figcaption>책만 쌓여가고…</figcaption></figure><p>.</p><p>한 줄로 많은 수고를 덜어주는 감사한 친구입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/166/1*iStbGyhJMEiMF20aigQECg.png" /><figcaption>감사한 친구</figcaption></figure><p>.</p><h4>심지어 <strong>무료</strong>이지요!</h4><p>.</p><p>머신러닝은 알고리즘을 알아도 데이터가 필요하고 또 분석하는 시각이 필요하기에 무료 공개를 선택한 것으로 보입니다.</p><p>생태계 구축이 더 중요하다는 것이지요.</p><p>.</p><p>TensorFlow 는 C++ 로 작성되고 Python, C++ API를 제공하기에 접근성 또한 뛰어납니다.</p><p>(API 문서가 아주 잘 나와있어요!)</p><p>.</p><p>또한 다양한 머신러닝 함수들을 제공하고 <strong>GUI를 이용한 직관적인 관찰</strong>이 가능하며 <strong>실제 서비스</strong>까지 이어지도록 기능을 제공합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/746/1*GESu4w2gkDr4traEzVH1Yg.png" /><figcaption>TensorBoard — GUI Interface 제공</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/881/1*e7IzIpf90Tnl8cDnsoXLLQ.png" /><figcaption>TensorFlow Serving — 실제 제품 서비스를 위한 기능</figcaption></figure><p>.</p><p>안타깝게도 Linux, Mac OS X 에서는 지원하고 윈도우에선 지원을 하지 않아 Docker를 써서 사용합니다. (GPU를 못쓰지요 ㅜ)</p><p>.</p><p>윈도우 10에서 <strong>Bash shell</strong>을 지원하면서 시도해보신 분들이 있는 것 같은데 아직은 안정화 되지 않은 것 같습니다.</p><p><a href="http://bryan7.tistory.com/807">Windows 10 Ubuntu Bash Shell 에서 TensorFlow 설치하기</a></p><p>.</p><p>간략한 TensorFlow 소개를 마치고 공부할 때 도움이 되는 것들을 정리하겠습니다.</p><p>(계속 추가될 예정입니다.)</p><p>.</p><p><strong>텐서플로우 공식 홈페이지</strong></p><p><a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></p><p>.</p><p><strong>텐서플로우 홈페이지 번역</strong></p><p><a href="https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/">https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/</a></p><p>.</p><p><strong>텐서플로우 코리아</strong></p><p><a href="https://tensorflow.blog/">https://tensorflow.blog/</a></p><p>.</p><p><strong>텐서(Tensor) 익숙하게 다루기</strong></p><p><a href="http://pythonkim.tistory.com/62">http://pythonkim.tistory.com/62</a></p><p>.</p><p><strong>머신러닝 기법 구현 예제</strong></p><p><a href="https://github.com/nlintz/TensorFlow-Tutorials">https://github.com/nlintz/TensorFlow-Tutorials</a></p><p>.</p><p><strong>TensorFlow를 사용한 머신러닝 강의</strong></p><p><a href="http://hunkim.github.io/ml/">http://hunkim.github.io/ml/</a></p><p>.</p><p><strong>좋은 구현 모음집 — awesome-tensorFlow(현재 진행중인 프로젝트)</strong></p><p><a href="https://github.com/TensorFlowKR/awesome_tensorflow_implementations">https://github.com/TensorFlowKR/awesome_tensorflow_implementations</a></p><p>.</p><p>Mathpresso 학습자료 — 한글 주석 듬뿍</p><p><a href="https://github.com/proauto/ML_Practice">https://github.com/proauto/ML_Practice</a></p><p>.</p><p>cf) 오탈자 혹은 잘못된 개념에 대한 피드백은 항상 환영합니다.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=99d8e5643a1" width="1" height="1"><hr><p><a href="https://medium.com/qandastudy/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-16-tensorflow%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-99d8e5643a1">Mathpresso 머신 러닝 스터디 — 16. TensorFlow를 이용한 머신러닝</a> was originally published in <a href="https://medium.com/qandastudy">MATHPRESSO</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>