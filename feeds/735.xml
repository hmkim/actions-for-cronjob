<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by 이정운 (Jungwoon Lee) on Medium]]></title>
        <description><![CDATA[Stories by 이정운 (Jungwoon Lee) on Medium]]></description>
        <link>https://medium.com/@jwlee98?source=rss-47ecf5e5c7f1------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*W5pMJxahQmN3zdBP-jAOlA@2x.jpeg</url>
            <title>Stories by 이정운 (Jungwoon Lee) on Medium</title>
            <link>https://medium.com/@jwlee98?source=rss-47ecf5e5c7f1------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 13 May 2019 22:08:25 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/@jwlee98" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[좀 더 저렴하게 사용하는 Google Cloud Platform(GCP)]]></title>
            <link>https://medium.com/@jwlee98/%EC%A2%80-%EB%8D%94-%EC%A0%80%EB%A0%B4%ED%95%98%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-google-cloud-platform-gcp-456cd71379f8?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/456cd71379f8</guid>
            <category><![CDATA[gce]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[google]]></category>
            <category><![CDATA[pricing]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sun, 12 May 2019 05:09:18 GMT</pubDate>
            <atom:updated>2019-05-12T05:16:19.965Z</atom:updated>
            <content:encoded><![CDATA[<p>안녕하세요 이정운 입니다.</p><p>Google cloud 를 쓰면서 활용도가 가장 높고 많이 사용되는 서비스 중의 하나는 일반적인 IaaS 개념으로 OS 가 포함된 VM 만 제공되고 자유롭게 해당 OS 위에서 필요한 소프트웨어와 컴포넌트를 설치해서 활용할 수 있는 Google Compute Engine(GCE) 일듯합니다. 이러한 GCE 는 가장 많이 활용되기 때문에 다양한 가격정책을 가지고 있으며 정책의 특성을 고려한 선택을 잘 하게된다면 예상하신 것보다 조금 더 비용을 줄일 수 있습니다. 특히나 GCE 기반의 GKE(Google Kubernetes Engine) 나 Cloud Dataproc(Cloud-native Apache Hadoop &amp; Apache Spark) 도 동일 할인 조건이 적용될 수 있으므로 꼼꼼히 확인해보시기 바라겠습니다.</p><p><a href="https://cloud.google.com/pricing/?hl=ko">https://cloud.google.com/pricing/?hl=ko</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ytSqDILG4_3lQoXo" /></figure><p>상단의 링크에 아주 자세히 각각의 가격정책과 이점이 잘 설명되어 있지만 그냥 읽어만 보면 실제 쓰는 사용자 입장에서는 조금 헷갈리는 부분이 있을수 있어서 이번 기회에 조금 더 자세하게 정리해보려고 합니다.</p><p><strong>#1) 지속 사용 할인(Sustained Use Discounts)</strong></p><p>지속 사용 할인은 말 그대로 별도의 정책을 사용하지 않고도 GCE를 사용하기만 하면 최대 30% 를 할인해주는 Google Cloud 만의 독특하고 간편한 할인 정책입니다. 조금 더 쉽게 말하면, GCE를 생성하고 한 달 간만 사용하면 알아서 자동적으로 30% 를 할인해주는 정책입니다.</p><p><a href="https://cloud.google.com/compute/docs/sustained-use-discounts?hl=ko">https://cloud.google.com/compute/docs/sustained-use-discounts?hl=ko</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/755/0*V2vv9sp4J0wI7jY6" /></figure><p><a href="https://cloud.google.com/pricing/innovation?hl=ko">https://cloud.google.com/pricing/innovation?hl=ko</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/433/0*y3Awv_qRsz_8mEO9" /></figure><p>여기에 덧붙여 Google Cloud 를 사용하다보면 GCE를 하나가 아니라 여러대 사용하게 되고 각 GCE가 저마다 다른 코어를 가지고 있을 수 있으며, 사용 기간도 저마다 달라질 수 있습니다. 이런 환경에서는 한 달 동안 사용되지 않은 GCE가 있거나 중간에 코어를 증가했다면 지속 사용 할인을 받지 못한다고 생각할 수도 있을 텐데 Google 은 친절하게도 이런 상황에서도 지능적으로 알아서 계산하여 할인을 해줍니다.</p><p>예를 들어, 한 달을 4주라고 가정하면 4 Core 짜리 GCE 를 2주 쓰다가 다음 2주에는 8 Core 짜리 GCE 를 사용했다면 이를 Google 이 지능적으로 계산해서 4 Core 를 한달간 쓴 것으로 인정하여 4 Core 에 대해서는 30% 할인을 해주고 나머지 4 Core 에 대해서는 2주간 지속 사용에 따른 할인을 해주는 구조입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Sjq0wOURTNyAIy6p" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/818/0*Qn6-m3X5i_Y0jBd3" /></figure><p>지속 사용 할인의 가장 큰 장점은 어떤한 사전 계약도 제약도 없이 자유롭게 쓰면 된다는 점입니다. 아무 고민 하지 않아도 Google 이 쓴만큼 알아서 할인해주는 나름 똑똑하고 편한(?) 할인 정책입니다.</p><p><strong>#2) 약정 사용 할인(Committed Use Discounts)</strong></p><p>여기서 말하는 약정 사용 할인은 관리 콘솔에서 바로 사용량을 1년 또는 3년으로 약정하여 구매하는 대신에 좀 더 큰 할인을 제공받는 프로그램으로 조건에 따라 최대 57% 할인을 받을 수 있는 정책입니다. (메모리가 최적화된 머신 유형의 할인은 최대 70%입니다.)</p><p>약정 사용 할인은 사전 정의된 머신 유형과 커스텀 머신 유형을 비롯하여 모든 Compute Engine 내 비공유 코어 머신 유형에 적용됩니다.(단, 약정 사용 할인의 대상이 되면 지속 사용 할인을 받지는 못합니다.)</p><p>약정 사용 할인은 다음과 같은 특징이 있습니다.</p><p><a href="https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts?hl=ko">https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts?hl=ko</a></p><ul><li><strong>간단하고 유연함</strong>: 할인이 지역 내 vCPU 또는 메모리의 총 수에 적용되므로(리소스 기반) 인스턴스 머신 유형의 변경에 영향을 받지 않음</li><li><strong>선불 비용 없음</strong>: 약정 사용 할인에는 선불 비용(upfront charge)이 없음. 약정 사용 할인은 매월 청구서에 적용됨</li></ul><p>이를 조금 더 자세히 설명 드리자면, 약정 사용 할인은 하단과 같이 GCP 관리 콘솔의 “Compute Engine &gt; 약정 사용 할인” 메뉴에서 바로 약정을 구매 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aZjXlaDy2UsTgw-_" /></figure><p>여기서 잘 보시면 아실 수 있듯이, Google Cloud 의 약정 할인은 구매할 머신 모델 타입을 선택하는 것이 아니라 CPU 코어와 메모리 수와 같은 자원을 선택한다는 것이며 리전(region) 단위로 구매할 수 있다는 것 입니다. 다시 말해, 특정 모델 타입을 지정하는 것이 아니라 보다 유연하게 내가 선택한 CPU 코어와 메모리같은 리소스 단위를 대상으로 1년 또는 3년 약정 사용 할인 정책을 적용받게 됩니다.</p><p>이해를 돕기 위하여 공식 메뉴얼에 나와있는 하나의 예를 들어보도록 하겠습니다.</p><ul><li>10 custom machine type vCPUs</li><li>30 GB of custom machine type memory</li><li>2 n1-standard-4 predefined machine types</li></ul><p>여기서 약정 사용을 위해 vCPU 코어 15개와 13.5GB 메모리를 구매 했다고 가정하면, 약정 사용 할인은 먼저 custom machine type 에 적용되고, 나머지 할인이 pre-defined machine type 에 적용됩니다.</p><p>이 경우 커스텀 머신 유형의 vCPU 10개 모두 약정 사용 가격으로 청구되며, 커스텀 머신 유형 메모리 13.5GB도 약정 사용 가격으로 청구됩니다. 약정 사용의 나머지 vCPU 5개는 n1-standard-4 머신 유형 2개에서 임의의 vCPU 5개에 적용됩니다. 뿐만 아니라, 약정 사용이 적용되지 않는 리소스는 기 언급한 지속 사용 할인(SUD)을 적용 받게 됩니다.</p><p>결국은 매달 GCE 에 대한 요구 사항이 변경되고 (예 : 영역, 시스템 유형, 운영 체제), 타입 및 위치(동일 region 내 zone)가 변경되더라도, 연간 총 소비량은 변동이 없다면 약정 사용 할인 정책을 적용할 경우 사용 패턴이 다르다 할지라도 모두 동일한 할인을 받을 수 있게 됩니다.</p><p><a href="https://cloud.google.com/blog/products/gcp/committed-use-discounts-for-google-compute-engine-now-generally-available?hl=ko">https://cloud.google.com/blog/products/gcp/committed-use-discounts-for-google-compute-engine-now-generally-available</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/900/0*uhxo1lO6gEeJ3Jdc" /></figure><p><strong>#3) 커스텀 머신 타입(Custom Machine Types)</strong></p><p>커스텀 머신 타입은 클라우드 서비스 제공자가 지정한 표준 VM 타입을 선택하는 것이 아닌, 사용자가 원하는 대로 코어와 메모리 사양을 자유롭게 선택하여 사용할 수 있는 GCE 의 VM 모델입니다.</p><p>이것을 왜 할인 정책 범주에 포함했는지 의문이 있으실 분도 있을텐데, 일반적으로 코어와 메모리 사양이 지정된 표준 VM 타입의 경우 실제로 필요한 사양보다 높은 타입을 사용해야 하는 경우가 대부분입니다. 1 vCore 또는 1 GB 와 같은 사양으로 자유로운 선택이 되지않기 때문에 메모리가 많이 필요한 경우 더 높은 코어를 가지고 있는 VM 타입을 선택해야만해서 필요없는 자원을 낭비하게 되는 상황에 놓여지게 됩니다.</p><p>예를 들어, 최소 CPU 4 코어에 16GB 의 메모리가 필요하다면 n1-standard-4 가 4 코어에 메모리 15GB 를 제공하므로 이 타입으로 요구사항 충족이 안되므로 이보다 큰 n1-standard-8 타입을(8 코어에 메모리 30GB) 고려할 수 밖에 없는데 Google cloud 는 커스텀 타입이 있으므로 낭비없이 VM 에 그냥 CPU 4 코어에 16GB 의 메모리를 선택해서 활용할 수 있습니다.</p><p>커스텀 머신 타입을 통해 요구사항에 따라 맞춤화된 머신 유형을 손쉽게 생성할 수 있으며, 통계적으로 보면 최대 50%의 가격 할인 효과를 볼 수 있습니다 최소 1개 ~ 최대 64개의 범위에서 짝수의 vCPU를 갖춘 머신 유형을 자유롭게 생성이 가능하며, 메모리의 경우 vCPU당 최대 6.5GB의 RAM을 구성할 수 있습니다.</p><p><a href="https://cloud.google.com/custom-machine-types/?hl=ko">https://cloud.google.com/custom-machine-types/?hl=ko</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/356/0*4A-yfqUUjojceCw8" /></figure><p><strong>#4) 선점형 VM 인스턴스(Preemptible VM Instances)</strong></p><p>선점형 VM 인스턴스는 최대 24시간 밖에 유지될 수 없다는 단점이 있지만 일반 인스턴스보다 최대 80% 저렴한 가격으로 만들고 실행할 수 있는 인스턴스 입니다. 아마도 A사의 Spot 인스턴스와 유사한 개념이라고 생각하면 금방 이해하실 듯 합니다. 다만, 타사와 다르게 Google 의 선점형 VM 인스턴스는 경매 방식이 아니라 가격이 정해져 있는 형태로 가격 예측 가능성이 더 높은 형태로 구성되어 있습니다.</p><p><a href="https://cloud.google.com/preemptible-vms/">https://cloud.google.com/preemptible-vms/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iS8vfObwQ0PUKMEa7QdgdA.png" /></figure><p>사용자의 애플리케이션에 내결함성이 적용되어 잠재적인 인스턴스 선점(종료)을 견딜 수 있다면 선점형 VM 인스턴스를 통해 GCE 비용을 크게 절감할 수 있습니다. 예를 들어 일괄 처리 작업을 선점형 인스턴스에서 실행할 수 있는데 처리 과정에서 일부 선점형 인스턴스가 종료되어도 작업이 느려지긴 하겠지만 완전히 중단되지는 않습니다.</p><p>또는, GKE 의 cluster 구성에서 GCE 일반 인스턴스로 구성된 node pool 과 선점형 VM 인스턴스로 구성된 node pool 을 적절히 분배해서 사용하면 훨씬 저렴한 예산으로 더 높은 컴퓨팅 파워를 활용할 수 있습니다.</p><p><a href="https://medium.com/pixboost/save-cash-by-running-kubernetes-services-on-preemptible-vms-in-google-cloud-cca02809ae09">https://medium.com/pixboost/save-cash-by-running-kubernetes-services-on-preemptible-vms-in-google-cloud-cca02809ae09</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*Dcni4wcj5mG9t1fsi4HWyg.png" /></figure><p>선점형 VM 인스턴스는 필요한 경우 선점형 GPU 나 선점형 TPU 를 추가할 수도 있습니다.</p><ul><li>GCE 는 언제나 선점형 인스턴스를 24시간 동안 실행한 후에 종료합니다. (특정 작업은 이 24시간 카운터를 재설정)</li><li>GCE 는 시스템 이벤트가 발생하면 언제든 선점형 인스턴스를 종료할 수 있습니다. (종료전에 알림을 받을 수 있음)</li><li>종료 스크립트를 사용해 알림을 처리하고 인스턴스가 중지되기 전에 정리 작업을 완료할 수 있습니다.</li><li>인스턴스가 30초 후에 중지되지 않으면 GCE 는 시그널을 운영체제에 전송하여 TERMINATE 합니다.</li></ul><p><strong>#5) 대량 약정 구매</strong></p><p>GCE 를 포함하여 Google Cloud Platform의 사용량이 많을 것으로 예상된다면, Google Cloud 영업팀과 직접 협의하여 기존에 언급된 약정 할인과는 다르게 대량 구매를 위해서 오프라인으로 연간 단위의 약정을 맺을 수 있습니다. 대량 약정 구매의 가장 큰 장점은 Google Cloud 내 모든 상품을 특정 제약 없이 약정된 금액 &amp; 할인율 내에서 자유롭게 사용할 수 있도록 되어있다는 것입니다.</p><p>예를 들어 약정이 되어 있는 금액만 맞추면 되는 조건이며 그안에서 GCE 를 써고 되고 Cloud SQL 이나 ML API 를 써도 되는등 자유도를 가지고 있다는 것입니다. 이러한 유연성을 가지고 있으면서도 정책적으로 약정된 금액에 따라 더 큰 할인을 받을 수 있습니다. 특히, 이것도 역시 선불 계약이 아니라 사용한 만큼 다음달에 청구되는 구조로 되어 있습니다.</p><p>여기까지해서 현재 Google cloud 에서 GCE 사용시 좀 더 저렴하게 사용할 수 있는 다양한 가격정책을 확인해봤으며 각 정책에 대한 예시를 통해서 좀 더 명확하게 이해할 수 있는 시간을 가져봤습니다. 기 언급했지만 GCE 기반의 GKE 나 Cloud Dataproc 도 같은 조건을 적용받을수 있으며 적절히 잘 활용되면 조금더 경제적으로 Google Cloud Platform 을 사용할 수 있지 않을까 합니다. 그럼 도움이 되었기를 바라며 이만 줄이도록 하겠습니다. 휘리릭~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=456cd71379f8" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 8탄 — Istio 더 알아보기1 (Traffic Management)]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-8%ED%83%84-istio-%EB%8D%94-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B01-traffic-management-bc7b3674b7f2?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/bc7b3674b7f2</guid>
            <category><![CDATA[istio]]></category>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[service-mesh]]></category>
            <category><![CDATA[kubernetes]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sat, 20 Apr 2019 04:52:52 GMT</pubDate>
            <atom:updated>2019-04-20T04:52:52.087Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]</strong>GKE 차근 차근 알아보기 8탄 — Istio 더 알아보기1 (Traffic Management)</h3><p>안녕하세요 이정운 입니다.</p><p>지난 시간에는 기본적인 GKE 환경에서 한발 더 나아가 요즘 사람들이 많이 이야기하는 Service Mesh 를 이해하기 위해 <a href="https://link.medium.com/r77rNPsZ0V">Istio 를 간단하게 설명드리고 Google cloud 의 GKE 환경에 설치 및 간단하게 테스트를 수행해보는 시간</a>을 가졌었습니다.</p><p>Service Mesh 라는 개념 자체가 쉬운 개념은 아니기 때문에 간단한 설치 및 테스트만 가지고는 이를 이해하기가 쉽지는 않으셨을 것으로 판단됩니다. 따라서, 좀 더 Istio 를 살펴보면서 Service Mesh 가 과연 어떤 구조를 가지고 있으며 MSA 에 어떤 가치를 줄 수 있는지 직접 확인해 볼 수 있는 시간을 추가로 가져보면 좋지않을까 생각했습니다.</p><p>그렇다고 Istio 를 일일이 하나씩 다 설명 드리기에는 너무 오래걸릴 듯 하여 Istio 공식 페이지에도 핵심 기능이라고 언급되어 있는 Traffic management, Security, Observability 를 한번 살펴보도록 하겠습니다. 해당 부분은 사실 이전 이야기에서 간단히 설명드린 트래픽 관리 및 정책 제어, 서비스 통신 보안, 로깅/모니터링 및 서비스 운영 유지를 의미합니다.</p><p><a href="https://istio.io/docs/concepts/what-is-istio/#core-features">https://istio.io/docs/concepts/what-is-istio/#core-features</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0rRz3qGbWHwH2Lw9" /></figure><p>늘 그렇지만 본 이야기는 하단과 같이 좋은 이야기를 기반으로 참고해서 작성되었습니다.</p><p>What is Istio?<br><a href="https://istio.io/docs/concepts/what-is-istio/#core-features">https://istio.io/docs/concepts/what-is-istio/#core-features</a></p><p>Istio<br><a href="https://cloud.google.com/istio/">https://cloud.google.com/istio/</a></p><p>Traffic Management<br><a href="https://istio.io/docs/concepts/traffic-management/">https://istio.io/docs/concepts/traffic-management/</a></p><p>Fault Injection<br><a href="https://istio.io/docs/tasks/traffic-management/fault-injection/">https://istio.io/docs/tasks/traffic-management/fault-injection/</a></p><p>The service mesh era: Advanced application deployments and traffic management with Istio on GKE<br><a href="https://cloud.google.com/blog/products/networking/advanced-application-deployments-and-traffic-management-with-istio-on-gke">https://cloud.google.com/blog/products/networking/advanced-application-deployments-and-traffic-management-with-istio-on-gke</a></p><p><strong>#1) Istio 의 Traffic Management 살펴보기</strong></p><p>Istio 의 Traffic Management 모델을 사용하면 트래픽 흐름과 인프라 확장을 분리할 수 있으며 Pilot 을 통해 특정 Pod 가 트래픽을 수신하는 규칙을 지정할 수 있습니다. 예를 들어 이를 활용하면 운영환경에서 특정 서비스에 대한 트래픽의 5%를 업그레이드 될 v2 버전으로 보내도록 지정하여 실제 전체 버전 업데이트 이전에 실 환경에서 일부 트래픽에 대한 사전 테스트 및 검증들을 해볼 수 있는거죠.(참고: Canary 배포 방식)</p><p><a href="https://istio.io/docs/concepts/traffic-management/">https://istio.io/docs/concepts/traffic-management/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vYXOPr5-MopR67nD" /></figure><p>Istio 는 이러한 Traffic Management 기능을 위해서 내부적으로 Pilot 과 Envoy 라는 컴포넌트를 활용합니다.</p><p><a href="https://istio.io/docs/concepts/what-is-istio/">https://istio.io/docs/concepts/what-is-istio/</a></p><p><strong>Envoy</strong> 는 어떻게 보면 Istio 의 핵심 기능일 수도 있을텐데 이전 이야기에서도 많이 언급했던 proxy 역할을 수행하며 비지니스 로직이 담겨져 있는 컨테이너와 pod 에 sidecar 패턴 형태로 같이 배포되어 Service Mesh 의 모든 서비스에 대한 인바운드 및 아웃바운드 트래픽을 조정할 수 있는 컴포넌트 입니다. Sidecar 패턴의 장점은 코드를 재구성하거나 다시 작성할 필요없이 기존 배포에 Istio 기능을 추가 할 수 있으며 그렇기 때문에 Java 와 같이 특정 프로그래밍 언어에 종속되지 않는 이점을 가지고 있습니다. 즉, 어떤 프로그래밍 언어라도 상관없이 Istio 를 활용할 수 있다는 의미입니다.</p><p>또한, 이러한 배치적인 특성에 의해서 애플리케이션에 대한 풍부한 시그널을 추출 가능하며 Istio 의 Mixer 로 보낼 수 있습니다. 이렇게 데이터를 수집한 Mixer 는 받은 데이터를 사용하여 정책 결정을 시행하고 이를 모니터링 시스템에 보내 전체 Service Mesh 의 동작에 대한 정보를 제공 할 수 있습니다.(Mixer 는 나중에 좀 더 자세히 설명하도록 하겠습니다) Envoy 는 그외에도 하단과 같은 다양한 기능을 지원 가능합니다.</p><ul><li>Dynamic service discovery</li><li>Load balancing</li><li>TLS termination</li><li>HTTP/2 and gRPC proxies</li><li>Circuit breakers</li><li>Health checks</li><li>Staged rollouts with %-based traffic split</li><li>Fault injection</li><li>Rich metrics</li></ul><p><strong>Pilot</strong> 은 Envoy 라는 sidecar 형태의 proxy 에 대한 서비스 검색, 지능형 라우팅 (예 : A / B 테스트, Canary 롤아웃 등) 및 탄력성(resiliency — 시간 초과, 재시도, circuit breakers 등)을 위한 트래픽 관리 기능을 제공합니다. Pilot 은 트래픽 동작을 Envoy 특정 구성으로 제어하는 높은 수준의 라우팅 규칙을 변환하여 런타임에 sidecar 형태로 위치하고 있는 Envoy 에 전달합니다. 조금더 전문적으로 이야기하자면 Pilot 은 플랫폼 별 service discovery 메커니즘을 추상화하고 이를 Envoy dataplain API 를 준수하는 모든 sidecar 형태의 proxy 가 소비 할 수있는 표준 형식으로 전달 및 통합할 수 있습니다.</p><p><a href="https://istio.io/docs/concepts/traffic-management/">https://istio.io/docs/concepts/traffic-management/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NH2A0xxdQd5DBl3d" /></figure><p>Pilot 은 Istio 에 배포 된 Envoy 인스턴스의 수명주기를 담당하게 되며 service discovery, 로드 밸런싱 조정 풀 및 라우팅 테이블에 대한 동적 업데이트를 가능하게 하므로 각 Envoy 인스턴스는 Pilot 에서 가져온 정보와 로드 밸런싱 조정 풀의 다른 인스턴스에 대한 정기적인 Health check 를 기반으로 로드 밸런싱 조정 정보를 유지 관리합니다. 이를 통해서 결국은 지정된 라우팅 규칙에 따라 대상 인스턴스간에 트래픽을 지능적으로 배포 할 수 있습니다.</p><p>다시 말해, Pilot 의 규칙(Rule) 설정을 통해 트래픽 관리 규칙을 지정할 수 있으며 이러한 규칙은 하위 수준 구성으로 변화되고 Envoy 인스턴스에 배포되어 실제 서비스 실행시점에 트래픽 제어가 이루어집니다.</p><p><strong>#2) Istio 의 Traffic Management 테스트 해보기</strong></p><p>그럼 실제로 Istio 에서 제공 가능한 Traffic Management 기능이 무엇인지 확인해보기 위해서 간단하게 이전 이야기에서 사용한 샘플인 Bookinfo 애플리케이션을 가지고 다시 테스트 해보도록 하겠습니다.</p><p><a href="https://istio.io/docs/examples/bookinfo/">https://istio.io/docs/examples/bookinfo/</a></p><p>Istio를 사용하여 Bookinfo 샘플에 대한 버전 라우팅을 제어하려면 Destination rules 을 정의해야 합니다.</p><p><a href="https://istio.io/docs/concepts/traffic-management/#destination-rules">https://istio.io/docs/concepts/traffic-management/#destination-rules</a></p><p>DestinationRule 은 VirtualService 라우팅이 발생한 후 요청에 적용될 정책 집합을 구성하며 Circuit breakers 나 로드 밸런서 설정, TLS 설정 및 기타 설정을 정의할 수 있습니다. DestinationRule 은 해당 대상 호스트의 주소 지정이 가능한 subsets, 즉 이름이 지정된 버전을 정의할 수 있으며 이러한 subsets 은 특정 버전에 트래픽을 보낼 때 VirtualService 라우트 사양에 사용됩니다.</p><p>그럼 Bookinfo 샘플에서 가지고 있는 DestinationRule 을 하단과 같이 적용해 볼까요</p><pre>kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml -n istio</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*c0fGbTHdIbKVt1xl" /></figure><p>실제로 DestinationRule 관련된 yaml 파일을 열어보면 하단과 같은 구조를 가지고 있습니다. (해당 파일의 일부)</p><pre>------- destination-rule-all.yaml -------<br>apiVersion: networking.istio.io/v1alpha3<br>kind: DestinationRule<br>metadata:<br>  name: reviews<br>spec:<br>  host: reviews<br>  subsets:<br>  - name: v1<br>    labels:<br>      version: v1<br>  - name: v2<br>    labels:<br>      version: v2<br>  - name: v3<br>    labels:<br>      version: v3<br>------- destination-rule-all.yaml -------</pre><p>보시면 그리 어렵지 않게 이해하실 수 있는 구조를 가지고 있는데 spec 의 host 는 실제 요청 대상 호스트를 의미하며 VirtualService 구성에 지정된 하나 이상의 요청 대상 호스트에 해당합니다. subset 은 서비스 엔드포인트의 부분집합을 의미하며 subset 은 A/B 테스트 또는 특정 버전의 서비스로 라우팅과 같은 시나리오에 사용할 수 있습니다.</p><p><a href="https://istio.io/docs/reference/config/networking/v1alpha3/destination-rule/">https://istio.io/docs/reference/config/networking/v1alpha3/destination-rule/</a></p><p>예를 들어 reviews 서비스에서 지정된 DestinationRule 이 subsets 를 가지고 정의되었다면 서비스를 수행하는 VirtualService 구성에 하단과 같이 subsets 별로 가중치(weight) 를 추가로 넣어주거나 조정 가능합니다.</p><pre>------- sample-virtual3.yaml -------<br>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: reviews<br>spec:<br>  hosts:<br>    - reviews<br>  http:<br>  - route:<br>    - destination:<br>        host: reviews<br>        subset: v1<br>      weight: 95<br>    - destination:<br>        host: reviews<br>        subset: v2<br>      weight: 5<br>------- sample-virtual3.yaml -------</pre><p>이렇게 v1 에 95% 의 가중치를 주고 나머지 v2 에 5%의 가중치를 주고 테스트를 위해서 해당 yaml 파일을 적용해 봅니다.</p><pre>kubectl apply -f ./sample-virtual3.yaml -n istio</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vg0zYOtBEhOnaKW1" /></figure><p>그리고 테스트를 해보면 거의 대부분 reviews 애플리케이션은 v1만 나오는 것을 확인 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uOUi5fQVQ7wn16sY" /></figure><p>더 쉽게 구별하기 위해서 v1 과 v2 의 가중치를 변경해서 배포하고 테스트 해보면 그 결과를 좀 더 빠르고 명확히 확인하실 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9e-JZ8VbXr-fDP0u" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9ocqUzcPrRbtCf3r" /></figure><p>Istio 의 Traffic Management 는 이렇게 규칙에 따라 트래픽을 관리할 수 있으며 이러한 규칙은 가중치 말고도 다양한 형태가 있습니다. 예를 들어 Istio 에서 수행되는 HTTP 요청은 15초를 기본 Timeout 으로 가지고 있지만 이를 지정해서 미리 타임아웃이 발생되게 하거나 재시도 횟수까지 설정할 수 있습니다.</p><p><a href="https://istio.io/docs/concepts/traffic-management/">https://istio.io/docs/concepts/traffic-management/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/668/0*clB8ZfQ6RLajdZP8" /></figure><p>그럼 이중에 timeout 관련해서 간단하게 테스트를 해볼까요. 우선 하단과 같이 ratings 라는 서비스에 대한 VirtualService 를 다시 설정합니다.</p><p><a href="https://istio.io/docs/tasks/traffic-management/request-timeouts/">https://istio.io/docs/tasks/traffic-management/request-timeouts/</a></p><pre>------- ra-virtual.yaml -------<br>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: ratings<br>spec:<br>  hosts:<br>  - ratings<br>  http:<br>  - fault:<br>      delay:<br>        percent: 100<br>        fixedDelay: 2s<br>    route:<br>    - destination:<br>        host: ratings<br>        subset: v1<br>------- ra-virtual.yaml -------</pre><pre>kubectl apply -f ./ra-virtual.yaml -n istio</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_kP8u9ehbNM8gK5W" /></figure><p>이때, 이전 배포와는 다르게 Fault Injection 을 사용하였는데 fault 를 100% 로 주입하여 2초간 delay 가 발생되도록 설정 하였습니다. 이렇게 하면 해당 결과가 나오는데 2초가 소요됩니다. (참고로 Istio 는 애플리케이션의 탄력성을 테스트하기 위해서 아주 쉽게 다양한 Fault 를 생성할 수 있는 방안을 fault injection 을 통해 제공하고 있습니다.)</p><p><a href="https://istio.io/docs/tasks/traffic-management/fault-injection/">https://istio.io/docs/tasks/traffic-management/fault-injection/</a></p><p>2초 지연관련된 테스트가 문제없이 된다면, VirtualService 에 하단과 같이 timeout 을 추가해서 적용해보도록 하겠습니다.</p><pre>------- sample-virtual-time.yaml -------<br>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: reviews<br>spec:<br>  hosts:<br>  - reviews<br>  http:<br>  - route:<br>    - destination:<br>        host: reviews<br>        subset: v2<br>    timeout: 0.5s<br>------- sample-virtual-time.yaml-------</pre><pre>kubectl apply -f ./sample-virtual-time.yaml -n istio</pre><p>내용을 보면 바로 이해할 수 있으시겠지만 2초의 타임아웃을 준 상태에서 reviews v2 애플리케이션에 대해서만 타임아웃을 0.5초로 제약을 건 것입니다. 그러면 timeout 이 reviews 서비스에 적용되어 하단과 같이 reviews 부분은 에러가 나고 나머지 부분은 정상적으로 나오는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*mAYKfyH-kApzptoY" /></figure><p>여기까지 잘 따라오셨다면 GKE 환경위의 Istio 를 가지고 Traffic Management 기능을 확인해보기 위한 다양한 테스트를 문제없이 잘 수행하신 것 입니다. Istio 가 제공하는 Traffic Management 기능이 무엇인지 감이 오시나요? 테스트를 해보셨으면 아시겠지만 이름 그대로 Istio 는 인프라의 변경없이도 다양한 트래픽 관리 및 제어 기능을 제공할 수 있다는 것을 아실 수 있을 것입니다.</p><p>지금 제가 소개드린 기능은 개념을 이해하기 위한 일부 기능이고 이 이외에 Istio 는 더 많은 Traffic Management 기능을 제공할 수 있기 때문에 관심있는 분들은 차근 차근 Istio 의 공식 홈페이지를 더 참고하시기를 바라겠습니다. 그럼 이번 이야기는 여기서 마치도록 하며 다음 이야기로 다시 돌아오겠습니다. 그럼 이만…휘리릭~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bc7b3674b7f2" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 7탄 — Istio 로 하는 Service Mesh]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-7%ED%83%84-istio-%EB%A1%9C-%ED%95%98%EB%8A%94-service-mesh-9e9363945cbb?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/9e9363945cbb</guid>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[istio]]></category>
            <category><![CDATA[service-mesh]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sat, 06 Apr 2019 04:00:53 GMT</pubDate>
            <atom:updated>2019-04-18T07:19:33.493Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 7탄 — Istio 로 하는 Service Mesh</strong></h3><p>안녕하세요 이정운 입니다.</p><p><a href="https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-6%ED%83%84-cloud-iam-%EA%B3%BC-kubernetes-rbac-f02b52cf538e">지난 시간에는 GKE 관점에서 Cloud IAM 을 살펴보고 GKE 환경안의 Kubernetes 관점에서 RBAC 도 조금 살펴보는 시간을 가져 봤습니다.</a> 또한, 짧게 Cloud IAM 과 RBAC 의 조합에 대해서도 살펴보는 시간을 가졌으며 이를 통해서 GKE 환경의 권한 관리라던가 접근제어에 대한 방안에 대해서 조금은 알 수 있는 시간이 되지 않았을까 기대해 봅니다.</p><p>지금까지 진행된 6개의 이야기를 통해서 기본적인 Kubernetes 와 GKE 에 대해서는 일차적으로 필요한 부분은 얼추 살펴본듯 하니 이번에는 GKE 환경에서 한발 더 나아가 요즘들어 사람들이 많이 이야기하는 Service Mesh 인 Istio 를 살펴보는 시간을 가져보는 것은 어떨까 합니다. Service Mesh 라는 용어는 요즘 여러 군데에서 사용되는데 여기서 다루고자 하는 것은 애플리케이션 또는 마이크로서비스 네트워크, 그리고 이러한 요소 간의 관계와 상호 작용의 연결을 이루는 네트워크를 의미하며 대규모로 분산되는 Microservices Architecture 에서 어떻게 효과적으로 서비스와 네트워크를 관리할 것인가에 대한 방안입니다.</p><p>그러면 조금 더 자세하게 왜 Servie Mesh 를 갑자기 이야기하며 이러한 Service Mesh 는 무엇에 필요한 것일까요?</p><p>이런 부분의 답변을 가장 잘 표현한 것이 제 생각에는 Redhat 에서 이야기하는 하단의 그림과 설명이 아닐까 하는데요.</p><p><a href="http://www.itworld.co.kr/news/109449">http://www.itworld.co.kr/news/109449</a></p><p>“마이크로서비스의 가장 어려운 부분 : 서비스 호출”은 중요한 점을 지적한다. API를 호출할 때 개발자는 A-B 직접 통합(아래 그림 1)을 다룬다고 생각할 수 있다. 그러나 컴퓨터 네트워크는 직접 통신에 최적화되도록 설계되지 않았으며(그림 2), 특히 클라우드 환경을 고려 또는 사용 중인 경우 통제 범위를 벗어난 많은 실제 및 가상 네트워크 디바이스를 필연적으로 다루게 된다. 예를 들어 이런 디바이스 중 하나의 성능이 저하되는 경우 전체 애플리케이션 응답 시간이 영향을 받는다(그림 3)”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/699/0*3FY7H-XxWxjnZ2D7" /></figure><p>이러한 환경에서 클라우드에서 탄력적인 서비스를 제공하기 위해서는 애플리케이션이 환경의 이상 현상으로부터 스스로를 보호하기 위한 방안이 필요하며 이때 언급되는 것이 Service Mesh 입니다. 그리고 이러한 Service Mesh 를 구축하는 여러가지 방법이 있겠지만 요즘에 가장 유명하고 어느덧 대세로 자리잡고 있는 것이 바로 Istio 이며 그래서 이번 이야기에서는 Istio 를 통해서 Service Mesh 가 무엇인지 확인해보는 시간을 가져보고자 합니다. 또한, Istio 를 GKE 환경에 설치 및 간단하게 테스트를 수행해보면서 말이 아니라 직접 만져볼 수 있는 시간을 가져보도록 하겠습니다.</p><p>늘 그렇지만 본 이야기는 하단과 같이 좋은 이야기를 기반으로 참고해서 작성되었습니다.</p><p>Installing Istio on GKE<br><a href="https://cloud.google.com/istio/docs/istio-on-gke/installing">https://cloud.google.com/istio/docs/istio-on-gke/installing</a></p><p>Install Istio on the Google Kubernetes Engine<br><a href="https://istio.io/docs/setup/kubernetes/install/platform/gke/">https://istio.io/docs/setup/kubernetes/install/platform/gke/</a></p><p>Bookinfo Application<br><a href="https://istio.io/docs/examples/bookinfo/">https://istio.io/docs/examples/bookinfo/</a></p><p>Istio step-by-step Part 03 — Deploying an application with Istio in Kubernetes<br><a href="https://medium.com/devopslinks/istio-step-by-step-part-03-deploying-an-application-with-istio-in-kubernetes-d2b1de64fb6b">https://medium.com/devopslinks/istio-step-by-step-part-03-deploying-an-application-with-istio-in-kubernetes-d2b1de64fb6b</a></p><p>Istio Service Mesh, the Step-by-Step Guide, Part 2: Tutorial<br><a href="https://dzone.com/articles/istio-service-mesh-the-step-by-step-guide-part-2-t">https://dzone.com/articles/istio-service-mesh-the-step-by-step-guide-part-2-t</a></p><p>Introduction to Service Management with Istio Service Mesh (Cloud Next ‘18)<br><a href="https://www.youtube.com/watch?v=wCJrdKdD6UM">https://www.youtube.com/watch?v=wCJrdKdD6UM</a></p><p>Best Practices from Google SRE: How You Can Use Them with GKE + Istio (Cloud Next ‘18)<br><a href="https://www.youtube.com/watch?v=XPtoEjqJexs">https://www.youtube.com/watch?v=XPtoEjqJexs</a></p><p><strong>#1) Istio 란?</strong></p><p>Service Mesh 는 굉장히 큰 개념이기 때문에 이를 설명하기 보다는 서두에 말씀드린 것처럼 이를 제공하는 프레임워크인 Istio 를 살펴보고 하나씩 테스트해보는 것이 좋지 않을까 합니다.</p><p>https://istio</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X4CHBMGmA3wTJK3c" /></figure><p>Istio 에 대해서 좀 더 들어가보면 Istio 는 Google, IBM, Redhat, Lyft, VMware 와 같이 다수의 대형업체들이 참여하여 Service Mesh 의 개념을 쉽게 구현할 수 있는 일종의 프레임워크(?) 형태로 제공하는 오픈소스입니다. 그리고 이를 통해서 대규모 마이크로서비스 환경이라고 하여도 서비스를 연결, 보안, 제어 및 관찰 할 수 있는 방안을 쉽게 제공할 수 있도록 지원하고 있습니다.</p><p>아주 쉽게 요약해서 설명한 내용이라 이해가 잘 안된다면 Google cloud 공식 페이지에 나와있는 내용을 가지고 조금더 보충 설명을 드리도록 하겠습니다.</p><p><a href="https://cloud.google.com/istio/">https://cloud.google.com/istio/</a></p><p>Istio는 마이크로 서비스 간 통신의 인증, 승인, 암호화를 확장 가능한 방식으로 제공 및 관리할 수 있습니다. 특히, Istio 가 기본적으로 기반 보안 통신 채널을 제공하므로 이를통해 개발자는 애플리케이션 수준 보안에 집중할 수 있습니다. 다시말해 별도의 보안에 대한 고민 없이도 Istio 가 안전한 보안 커뮤니케이션을 지원할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bdQz-xNSxrnZ8M5q" /></figure><p>Istio 는 기본적으로 추적, 모니터링, 로깅을 제공 가능하며 이를통해 Service Mesh 배포에 대한 심층적인 정보를 제공하여 서비스의 성능과 다른 프로세스에 미치는 성능의 영향을 확인하고 문제를 빠르고 효과적으로 감지 및 분류할 수 있도록 지원할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qzOidK1dyosUK_M1" /></figure><p>마지막으로 Istio 는 트래픽 관리 기능을 제공하여 이를 통해 서비스 간의 API 호출 및 트래픽 흐름을 제어하고 트래픽에 대한 가시성을 향상시켜 문제가 발생하기 전에 포착할 수 있으며 악조건 속에서도 호출을 보다 안정적으로 하고 네트워크를 강화할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*f1XmMsQgN49-vrob" /></figure><p>이렇듯 Istio 를 활용하게 되면 대단위의 마이크로서비스 환경에서 꼭 필요한 서비스 연결, 보안, 제어 및 관찰 기능을 프레임워크 단에서 제공하므로 개발자는 좀더 비즈니스 로직에 집중할 수 있도록 지원할 수 있습니다.</p><p><strong>#2) Istio 테스트 해보기</strong></p><p>기본적인 개념은 이전 파트에서 간단하게 설명드렸고 하단과 같이 Istio 공식 사이트에서도 아주 정리가 잘되어 있으므로 참고하시기를 추천드립니다.</p><p><a href="https://istio.io/docs/concepts/what-is-istio/">https://istio.io/docs/concepts/what-is-istio/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*A7N5EUH_3Y_8oXVu" /></figure><p>특히, 하단의 아키텍처를 보시면 Istio 의 가장 큰 차별점 or 장점이 드러나는데 관리에 필요한 Control plain 부분을 제외하고 실제 서비스를 보면 Sidecar 패턴 형태로 실제 서비스와 proxy(Envoy) 가 병렬로 공존하도록 되어있어서 실제 서비스 호출에서는 직접 서비스를 호출하는 것이 아니라 proxy 를 통해서 호출하도록 되어 있습니다. 이렇게 proxy 를 거쳐 가므로 개발자가 별도의 작업을 하지 않아도 전체적으로 대규모 시스템을 잘 운영하기 위해서 필요한 로깅, 모니터링 정보 뿐만 아니라 보안, 트래픽 제어와 같은 다양한 이점을 누릴 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*mb8zxtJqtP7MKedD" /></figure><p>Kubernetes 환경에서는 pod 안의 각각의 container 형태로 배치하여 쉽게 sidecar 패턴 구현 및 Istio 를 활용할 수 있습니다. 그럼 이전에 말한 것처럼 설명만 하지않고 간단하게 샘플애플리케이션을 통해서 직접 테스트해보고 이를 살펴보는 시간을 가지도록 하겠습니다.</p><p>먼저 설치부터 해야하는데 GKE 의 가장 큰 특징중의 하나는 Istio 를 설치하기 위해 별도로 작업할 필요없이 하단과 같이 설정중에 Add-ons 로 바로 Istio 설치를 추가할 수 있습니다. 이미 설치된 GKE cluster 라고 해도 edit 를 클릭하여 설정을 변경(enable) 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dRaJru7QPB6GxMdf" /></figure><p>그럼 이미 만들어진 GKE cluster 가 있다고 가정하고 Edit 를 클릭하여 Add-on 에서 Istio 를 enable 하고 맨 하단의 Save 를 클릭합니다. 이때 Istio mTLS 는 각 서비스간의 연결을 무조건 mTLS 를 사용할 것인지 아닌지에 대한 설정이므로 우선은 Permissive 로 설정하시면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/950/0*4VtdlNFJ1YtGUSuI" /></figure><p>이렇게 설정만 변경하면 추가적인 작업 없이도 GKE cluster 에 istio 설치를 완료하실 수 있습니다. 설치가 잘 되었는지 확인하기 위해서 istio-system 이라는 namespace 에 정상적으로 Istio 서비스들이 설치되었는지 확인해 봅니다.</p><pre>kubectl get service -n istio-system</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MaBbLPdTcwzEoC82" /></figure><pre>kubectl get pods -n istio-system -o wide</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TPH1oNpngAjhCYv-" /></figure><p>상단에 보시는 것처럼 Istio 가 정상적으로 설치되었다면 이전에 봤던 아키텍처의 Istio 의 control plain 에 있던 Pilot, Mixer, Galley, Citadel, Gateway등의 컴포넌트가 pod 형태로 설치된 것을 확인할 수 있습니다. Istio 가 정상적으로 추가 되었으면 이제 간단하게 샘플 애플리케이션을 설치 및 구동해보도록 하겠습니다. 샘플은 Istio 공식 샘플인 Bookinfo 를 사용하도록 하겠습니다. Bookinfo 는 기본적으로 MSA 구조를 가지고 있으며 이에대한 아키텍처 구조 설명은 하단의 링크에 자세하게 나와있습니다.</p><p><a href="https://istio.io/docs/examples/bookinfo/">https://istio.io/docs/examples/bookinfo/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/771/0*P-eP40vc0Cfc-uJb" /></figure><p>그리고 Bookinfo 의 실제 소스는 하단의 github 에서 받을 수 있습니다.</p><p><a href="https://github.com/istio/istio/tree/master/samples/bookinfo">https://github.com/istio/istio/tree/master/samples/bookinfo</a></p><p>그럼 먼저 이전에 작업해두었던 것과 이번 테스트 구분을 위해서 새로운 namespace 를 하나 만들고 istio-injection 설정을 추가 합니다. 해당 설정을 enabled 로 하면 별도의 추가 명령 없이도 해당 namespace 로 배포되는 모든 deployments 에는 자동으로 Istio 를 위한 proxy 컨테이너가 추가됩니다.</p><pre>kubectl create namespace istio<br>kubectl label namespace istio istio-injection=enabled</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KqdfQeBkVDTkTlX3" /></figure><p>그러면 github 에서 소스를 다운로드 받은 후에 실제 kubernetes 서비스를 배포합니다. 여기서 서비스를 배포한다는 의미는 deployment 와 service 를 의미합니다.</p><pre>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n istio</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YwWhvt4W0rrgDglN" /></figure><p>해당 bookinfo.yaml 파일을 일일이 설명드리기는 어렵지만 지금까지 여러 이야기에서 다루었던 일반적인 형태의 kubernetes 의 deployment 와 service 를 위한 yaml 파일 입니다. 일부 부분만 발췌해 보면, productpage 라는 deployment 를 생성하고 service 를 연결하는데 해당 서비스는 기본 ClusterIP 타입으로 생성해서 cluster 범위 에서만 접근가능하며 외부에서는 접근 불가하도록 설정되어있습니다.</p><pre>--<br>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: productpage<br>  labels:<br>    app: productpage<br>    service: productpage<br>spec:<br>  ports:<br>  - port: 9080<br>    name: http<br>  selector:<br>    app: productpage<br>---<br>apiVersion: extensions/v1beta1<br>kind: Deployment<br>metadata:<br>  name: productpage-v1<br>  labels:<br>    app: productpage<br>    version: v1<br>spec:<br>  replicas: 1<br>  template:<br>    metadata:<br>      labels:<br>        app: productpage<br>        version: v1<br>    spec:<br>      containers:<br>      - name: productpage<br>        image: istio/examples-bookinfo-productpage-v1:1.10.1<br>        imagePullPolicy: IfNotPresent<br>        ports:<br>        - containerPort: 9080<br></pre><p>이렇게 이전과 동일하게 서비스를 배포했지만 Istio 설정이 추가되었기 때문에 실제 눈에 보이는 것과 다르게 내부 배포 결과는 좀 달라집니다. 이를 좀 더 자세히 말씀드리면 관리콘솔 &gt; GKE &gt; cluster 이름 &gt; Workloads &gt; pod 이름 &gt; 관리되는 pod 이름 메뉴로 들어가 보면 배포된 deployment 모두에는 하단과 같이 pod 안에 isito-proxy 라는 컨테이너가 추가되어있는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EzO84MTMCpBsfcqz" /></figure><p>이 부분이 아까 말씀드린 sidecar 패턴의 proxy 공존이라는 의미이며 istio 의 경우에는 서비스의 호출시 proxy 를 통해서 이루어진다고 했는데 istio-proxy 가 그때 사용되는 proxy 의 실체입니다. (참고로, Istio 는 proxy 로 Envoy 를 사용)</p><p>Kubernetes 를 위한 서비스를 모두 배포했으면 이제 실제 서비스를 수행하기 위해서 Istio 에서 필요한 Gateway 와 VirtualService 를 배포해야 합니다. 이에 대해서 조금 더 보충 설명을 하자면 Istio 는 보안을 위해서 기본적으로 인입 서비스는 Ingress gateway 를 통해서만 들어오는 구조로 구성되어 있다고 보시면 됩니다. 실제 서비스를 수행하기 위해서는 Ingress gateway -&gt; VirtualService 라는 구조를 가지고 있어야 하며 VirtualService 는 실제로 kubernetes 의 service 를 목적지로 지정합니다. 따라서, 좀 더 실제적으로 그림을 그려보면 Ingress gateway -&gt; VirtualService -&gt; k8s Service 가 될 수 있을듯 합니다. 그리고 만약 반대로 kubernetes cluster 외부로 호출이 필요한 경우에는 선택적이긴 하지만 Egress gateway 를 선언하고 이를 통해 외부 서비스를 수행할 수 있습니다. 결국은 보시면 아시겠지만 전체 서비스의 흐름이 proxy 를 거치는 것으로 이해하시면 좋을듯 합니다. 하단의 첨부 그림을 참고하시면 이해에 도움이 될듯 합니다.</p><p><a href="https://blog.aquasec.com/istio-service-mesh-traffic-control">https://blog.aquasec.com/istio-service-mesh-traffic-control</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*g_uRFrbKKCW7psAl" /></figure><p>그럼 다시 테스트로 돌아와서 하단과 같은 명령어를 통해서 만들어진 Istio 설정에 대한 배포를 수행합니다.</p><pre>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml -n istio</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZKKTB5mC0bx2F22j" /></figure><pre>kubectl get gateway -n istio<br>kubectl get virtualservice -n istio</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R2p5Szn6C1krUZtg" /></figure><p>이해에 도움이 되기 위하여 bookinfo-gateway.yaml 를 조금 더 분석해보면 상단에는 Istio 서비스를 위하여 bookinfo-gateway 라는 gateway 를 생성하는데 istio 설치시에 기본적으로 설치되는 default controller 를 사용(istio-ingressgateway) 하여 생성하도록 되어있으며 기본 http 프로토콜로 80 포트를 사용하는 설정입니다. 그리고 하단에는 booinfo 라는 bookinfo-gateway 와 연결되어 있으며 ‘/productpage’ 라는 요청이 들어오면 productpage service 에게 9080 포트로 전달하는 VirtualService 를 생성하는 구문으로 구성되어 있습니다.</p><pre>apiVersion: networking.istio.io/v1alpha3<br>kind: Gateway<br>metadata:<br>  name: bookinfo-gateway<br>spec:<br>  selector:<br>    istio: ingressgateway # use istio default controller<br>  servers:<br>  - port:<br>      number: 80<br>      name: http<br>      protocol: HTTP<br>    hosts:<br>    - &quot;*&quot;<br>---<br>apiVersion: networking.istio.io/v1alpha3<br>kind: VirtualService<br>metadata:<br>  name: bookinfo<br>spec:<br>  hosts:<br>  - &quot;*&quot;<br>  gateways:<br>  - bookinfo-gateway<br>  http:<br>  - match:<br>    - uri:<br>        exact: /productpage<br>    - uri:<br>        exact: /login<br>    - uri:<br>        exact: /logout<br>    - uri:<br>        prefix: /api/v1/products<br>    route:<br>    - destination:<br>        host: productpage<br>        port:<br>          number: 9080<br></pre><p>이제 Kuberenetes 를 위한 서비스인 deployment, service 와 Istio 를 위한 gateway, virtualservice 에 대한 배포가 끝났으니 실제 테스트를 해보도록 하겠습니다. 서비스를 위해 생성된 gateway 가 ingressgateway 를 사용하도록 되어 있으니 하단과 같은 명령어로 이를 확인하여 서비스 가능 IP 를 확인합니다.</p><pre>kubectl get svc istio-ingressgateway -n istio-system</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dgONWlcSkJjaC3p8" /></figure><p>IP 확인이 되었으면 이제 브라우저를 통해서 실제 서비스 호출을 수행하여 정상적으로 bookinfo 페이지가 나오는 것을 확인합니다. (현재는 기본설정으로 여러 버전의 review 를 번갈아가면서 호출하도록 되어있어 refresh 버튼을 클릭하면 해당 부분이 변경 되는 것을 확인 가능합니다.)</p><pre><a href="http://34.85.111.253/productpage">http://34.85.111.253/productpage</a></pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qu1Cz8HxxWzFR26U" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Qjyd4VIh0mDb4_uB" /></figure><p>여기까지 잘 수행하셨다면 아주 기본적으로 Servce Mesh 를 지원하기 위한 Istio 를 GKE 에 설치해서 가장 기본적인 샘플 애플리케이션을 배포 및 수행하신 것 입니다. 비록 간단한 이야기이긴 하지만 이해하는데는 시간이 좀 걸리므로 이번 이야기는 여기서 마무리하고 다음에 좀 더 자세한 이야기를 추가로 다루도록 하겠습니다. 그럼 이만 휘리릭~~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9e9363945cbb" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 6탄 — Cloud IAM 과 Kubernetes RBAC]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-6%ED%83%84-cloud-iam-%EA%B3%BC-kubernetes-rbac-f02b52cf538e?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/f02b52cf538e</guid>
            <category><![CDATA[rbac]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp]]></category>
            <category><![CDATA[gke]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sat, 16 Feb 2019 10:59:51 GMT</pubDate>
            <atom:updated>2019-02-16T10:59:51.896Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 6탄 — Cloud IAM 과 Kubernetes RBAC</strong></h3><p>안녕하세요 이정운 입니다.</p><p><a href="https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-5%ED%83%84-cloud-build-%EB%A5%BC-%ED%86%B5%ED%95%9C-ci-cd-47f1486937e4">지난번에 조금은 길었지만 GKE 환경에서 Cloud Build 를 활용하여 CI/CD 적용 설정 및 테스트까지 해보는 시간을 가졌었습니다.</a> 해당 이야기를 진행하는 중간쯤에 Cloud Build 가 GKE cluster 환경을 제어하기 위하여 하단과 같이 Cloud IAM 을 통해서 특정 service account 에 conatiner.developer 권한을 매핑하는 작업을 진행했었습니다.</p><pre>gcloud projects add-iam-policy-binding ${PROJECT} \<br>--member=serviceAccount:${<a href="mailto:PROJECT_NUMBER}&lt;a href=">@cloudbuild</a>.gserviceaccount.com&quot;&gt;PROJECT_NUMBER}<a href="http://twitter.com/cloudbuild">@cloudbuild</a>.gserviceaccount.com \<br>--role=roles/container.developer</pre><p>지난 이야기에서는 그부분이 메인이 아니어서 설명없이 간단히 넘어갔지만 이번 이야기에서 설명을 드려보면 언급된 Cloud IAM 은 Cloud Identity and Access Management 를 의미하여 Google cloud 에서 권한을 관리하고 접근을 제어할 수 있는 기능을 제공하는 기본 솔루션 입니다. 당연히 GKE 에 대한 접근 제한도 Cloud IAM 을 통해서 제어 가능합니다.</p><p><a href="https://cloud.google.com/iam/">https://cloud.google.com/iam/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4NNyODXzVsq2DQ8B" /></figure><p>Cloud IAM 을 통해서 권한 관리를 할 수 있지만 GKE 는 Managed Kubernetes 이기 때문에 Kubernetes 자체에서 제공하는 RBAC 도 기본 지원 가능하므로 이를 통한 역할별 접근 제어 기능을 제공할 수도 있습니다</p><p>말이 나온 김에 이번 이야기에서는 짧게 GKE 관점에서 Cloud IAM 을 살펴보고 GKE 환경의 Kubernetes 관점의 RBAC 도 조금 살펴보는 시간을 가져 보도록 하겠습니다.</p><p>본 이야기는 하단과 같이 좋은 이야기를 기반으로 참고해서 작성되었습니다.</p><p>액세스 제어 개요<br><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/access-control?hl=ko">https://cloud.google.com/kubernetes-engine/docs/concepts/access-control?hl=ko</a></p><p>Using RBAC Authorization<br><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></p><p>Making Sense of Kubernetes RBAC and IAM Roles on GKE<br><a href="https://medium.com/uptime-99/making-sense-of-kubernetes-rbac-and-iam-roles-on-gke-914131b01922">https://medium.com/uptime-99/making-sense-of-kubernetes-rbac-and-iam-roles-on-gke-914131b01922</a></p><p>Simplifying Granular Access Control on Kubernetes(GKE) Using IAM and RBAC<br><a href="https://medium.com/google-cloud/simplifying-granular-access-control-on-kubernetes-gke-using-iam-and-rbac-19a627e3aa18">https://medium.com/google-cloud/simplifying-granular-access-control-on-kubernetes-gke-using-iam-and-rbac-19a627e3aa18</a></p><p>권한을 찾아서: GitHub Team을 이용하여 Kubernetes 계정 인증하기 (3)<br><a href="https://medium.com/rainist-engineering/k8s-auth-with-github-team-part3-7e976adcf4c6">https://medium.com/rainist-engineering/k8s-auth-with-github-team-part3-7e976adcf4c6</a></p><p>Demystifying RBAC in Kubernetes<br><a href="https://www.cncf.io/blog/2018/08/01/demystifying-rbac-in-kubernetes/">https://www.cncf.io/blog/2018/08/01/demystifying-rbac-in-kubernetes/</a></p><p>Google cloud IAM 계정, 권한 및 조직 관리</p><p>https://www.slideshare.net/mobile/JerryJeong2/google-cloud-iam</p><p><strong>#1) Cloud IAM</strong></p><p>Cloud IAM 은 기 언급했지만 GCP 내에서 프로젝트와 리소스에 대한 접근을 관리하는 역할을 제공합니다. 프로젝트에 사용자를 추가한 후 프로젝트와 클러스터 내에서 작업을 수행할 수 있는 권한을 부여하는 역할을 사용자에게 할당 할 수 있으며 이를 통해서 해당 사용자에 대한 권한 제어가 가능합니다. 좀 더 쉽게 설명하면 ‘<strong>누가</strong>’ ‘<strong>어떤 자원에 대해서</strong>’ ‘<strong>무엇을 할수 있는지</strong>’ 에 대한 설정 및 제어 기능을 제공 할 수 있습니다.</p><p><a href="https://medium.com/@doctusoft/how-to-make-your-google-cloud-platform-project-more-secure-iam-245dcf05b18f">https://medium.com/@doctusoft/how-to-make-your-google-cloud-platform-project-more-secure-iam-245dcf05b18f</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/638/0*4_RHASHnzD6EG3Zo" /></figure><p>여기에 덧붙여, GCP 의 특성 중에 하나인데 사람이 아니라 Service account 를 만들고 해당 Service account 에 권한을 부여할 수 있습니다. Service account 란 사용자를 대신하여 작업을 수행하는, 프로젝트에 연결된 Google 계정이며 이러한 Service account 에는 사용자와 동일한 방식으로 역할과 권한을 할당 할 수 있습니다. 다시 말해, Service account 는 반드시 사람이 아니라 머신이나 VM 에 부여할 수 있으며 이렇게 부여된 Service account 는 사용자와 동일한 방식으로 역할과 권한을 할당 할 수 있습니다.(예를 들어, 특정 사용자가 아니라 VM 자체에 특정 API 를 호출할 수 있는 권한을 부여 가능)</p><pre>gcloud projects add-iam-policy-binding ${PROJECT} \<br>--member=serviceAccount:${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com \<br>--role=roles/container.developer</pre><p>이전 이야기에서 사용된 상단과 같은 Cloud IAM 명령을 한번 살펴보면 roles/container.developer 라는 이름의 역할을 부여하고 있습니다. 이 역할을 실제 살펴보면 Kubernetes Engine Developer 라는 이름을 가지고 하단과 같이 267 개의 권한을 가진 미리 정의된 역할입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LPvkNdAY0M_xk9OS" /></figure><p>역할을 확인해 보시면 아시겠지만 GKE 환경에 대한 접근을 가능하게 허용된 개발자 권한입니다. 이를 다시 해석해 보면 GKE 환경에서 구동중인 kubernetes 리소스에 접근 가능한 개발자 권한을 Service account 인 ${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com 로 부여한다는 의미 입니다. (참고로 ${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com 는 Cloud Build 가 기본으로 사용하는 Service account 입니다.) Service account 입장에서는 이제 GKE 환경에 대한 개발자 권한을 받았으며 그에 맞는 작업을 언제든 할 수 있다는 의미이기도 합니다.</p><p>해당 명령 수행 후에 관리콘솔에서 IAM&amp;Admin &gt; IAM 메뉴를 클릭하면 하단과 같이 ${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com 이 기본 가지고 있는 Cloud Build Service account 에 더해서 Kubernetes Engine Developer 권한이 추가된 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ohxLqA_V7fgkD6LM" /></figure><p>좀 더 들어가보면 roles/container.developer 는 개발자에게 필요한 각각의 권한을 사전 정의해 둔 역할을 의미하며 Cloud IAM 에는 다음과 같이 GKE 를 위한 사전 정의된 역할을 더 가지고 있습니다. 다시 말하면 필요시 Cloud IAM 의 사전 정의해 둔 역할의 할당을 통해서 GKE 에 대한 권한 배분이나 관리가 가능하다는 의미 입니다.</p><p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/iam">https://cloud.google.com/kubernetes-engine/docs/how-to/iam</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uLlM0MuKS5VT-CX6" /></figure><p><strong>#2) Kubernetes RBAC</strong></p><p>Kubernetes 에서 제공하는 RBAC 은 Role Based Access Control 을 의미하며 역할 기반 접근 제어를 의미합니다. Kubernetes 기본 액세스 제어 API를 사용하여 클러스터 또는 네임스페이스 수준에서 Kubernetes 리소스와 작업을 위한 세분화된 권한이 있는 역할(Role)을 만들 수 있는 기능을 제공할 수 있으며 역할을 만든 후 역할을 사용자와 Kubernetes 서비스 계정에 할당하는 역할결합(RoleBinding) 을 만들어서 연결 가능합니다. 특히나, Kubernetes RBAC 은 Kubernetes 내부의 액세스 제어에 이미 익숙하고 클라우드와 상관없는 방식의 액세스 관리를 선호하는 경우에 유용합니다.</p><p>역할 기반 액세스 제어<br><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control">https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control</a></p><p>상단의 링크에 자세한 설명이 있지만 Cloud IAM은 프로젝트별 기준으로 작동하게 되며 사용자가 특정 GCP 프로젝트에 있는 클러스터에 액세스하기 위해서는 프로젝트 수준의 권한이 필요합니다. 그에 비해서 Kubernetes RBAC 권한은 각 클러스터 내의 리소스 액세스에 대해 세부적인 제어 기능을 제공하기 때문에 프로젝트 수준의 권한 없이도 클러스터에 있는 리소스 사용을 제어 가능합니다. 그렇기 때문에 Cloud IAM 과 Kubernetes RBAC 은 함께 조합해서 사용할 수도 있으며 필요시 더 세밀한 제어를 가능하게 만들수 있습니다.</p><p><a href="https://medium.com/uptime-99/making-sense-of-kubernetes-rbac-and-iam-roles-on-gke-914131b01922">https://medium.com/uptime-99/making-sense-of-kubernetes-rbac-and-iam-roles-on-gke-914131b01922</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bR5G7iQb0DAtyB_Q" /></figure><p>Kubernetes RBAC 을 사용하려면 Role 이나 ClusterRole 이라는 객체를 생성한 후 RoleBinding 이나 ClusterRoleBinding 을 사용해서 연관된 역할과 사용자를 연결해주면 됩니다. 여기서 Role 과 ClusterRole 은 이름을 보면 알겠지만 동일하게 역할을 지정하는 기능을 수행하지만 ClusterRole 은 이름 그대로 범위가 더 넓은 클러스터 레벨이나 리소스 단위가 아닌 엔드포인트(예: /healthz) 등에 적용할 수 있는 객체입니다.</p><p>실제 하단과 같이 GKE 환경에서 기본 생성되어 있는 Role 을 확인 가능합니다.</p><pre>kubectl get role — all-namespaces</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-W5zOYwSZscrfiZc" /></figure><p>또한, 하단의 kubectl 명령을 통해서 Role 의 상세 내용도 확인 가능합니다.</p><pre>kubectl get role gce:cloud-provider -n kube-system -o=yaml</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zKxBwwE2SddL9NPM" /></figure><p>당연한 이야기 이겠지만 ClusterRole 도 하단과 같이 확인 가능합니다. 보시면 아시겠지만 더 넓은 클러스터 단위의 역할을 의미하기 때문에 namespace 와 연동되지 않은 admin, edit 등의 역할도 확인할 수 있다는 것을 보실 수 있습니다.</p><pre>kubectl get clusterroles</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Yh6kN9TR58eQYQN0" /></figure><p>예를 들어 현재의 account 를 확인한 후 kubectl 명령을 통해서 해당 사용자에게 cluster-admin 권한을 하단과 같은 방식으로 clusterrolebinding 을 만들어서 제공 가능합니다.</p><pre>gcloud info | grep Account</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Fx1laN2VqyMYDb2i" /></figure><pre>kubectl create clusterrolebinding myname-cluster-admin-binding — clusterrole=cluster-admin — user=jwlee98@gmail.com</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ACozAqy79ZwwUpAS" /></figure><pre>kubectl get clusterrolebinding</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Oqj9I9iDIvhccHiy" /></figure><pre>kubectl get clusterrolebinding myname-cluster-admin-binding -o=yaml</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JCu9-9cgsy2YQHq9" /></figure><p><strong>#3) Cloud IAM &amp; RBAC</strong></p><p>Kubernetes 에서 제공하는 RBAC 을 위해서 User 를 생성하려면 하단의 링크에서 확인되는 것과 같이 credential 을 만들기 위해서 key 생성과 같은 번거로운 작업을 거쳐야 합니다.</p><p>Configure RBAC In Your Kubernetes Cluster<br><a href="https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/">https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/</a></p><p>어떻게 보면 간단할 수도 있겠지만 credential 을 만들고 관리하기 위해서는 key 생성과 같은 많은 부수적인 작업들이 추가되어야 합니다. 이를 GCP 에서 제공되는 Cloud IAM 을 활용하게되면 좀 더 편리하게 조합해서 활용할 수 있습니다. 예를 들어 새로운 개발자가 들어와서 해당 개발자에게 권한을 제공해야 한다고 가정하면 관리콘솔의 Cloud IAM 을 통해서 하단과 같이 프로젝트의 viewer 권한을 제공할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hZENRpvGQ_r4cDw4" /></figure><p>이제 해당 개발자는 GCP 프로젝트에 Viewer 권한을 얻었으므로 해당 개발자로 새로 GCP 관리콘솔에 로그인 하게 되면 프로젝트의 GKE cluster 에 접근 가능합니다. 그러나 Cloud IAM 상의 viewer 권한만 있기 때문에 보는 것은 가능하나 하단과 같이 GKE 의 상태를 변경하는 작업을 수행하게 되면 권한이 없다는 오류를 받게 됩니다.</p><pre>gcloud info | grep Account<br>kubectl scale deployment gceme-frontend-production -n production — replicas=3</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Py7FZ7hVy1SjoL4O" /></figure><p>여기서 개발자에게 GKE cluster 를 변경하는 것도 허용하고자 한다면 다시 임시 개발자 계정이 아니라 원래 계정으로 돌아와서 해당 개발자에게 하단과 같이 RoleBinding 을 만들어서 ‘edit’ 라는 미리 정의된 ClusterRole 을 부여해 주면 됩니다. (당연히 Cloud IAM 을 통하여 추가 권한을 줘도 가능합니다.)</p><pre>-----rolebinding-edit.yaml-----<br>kind: RoleBinding<br>apiVersion: rbac.authorization.k8s.io/v1<br>metadata:<br> name: edit-role<br> namespace: production<br>subjects:<br> - kind: User<br> name: jungwoon@freejava.co.kr<br> apiGroup: rbac.authorization.k8s.io<br>roleRef:<br> kind: ClusterRole<br> name: edit<br> apiGroup: rbac.authorization.k8s.io<br>-----rolebinding-edit.yaml-----</pre><pre>kubectl create -f rolebinding-edit.yaml</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yBS_qVM_IKBxw32n" /></figure><p>RoleBinding 을 통해서 Cloud IAM 으로 추가한 개발자 계정에 ‘edit’ 권한 부여가 완료되었다면 다시 개발자 계정으로 들어가서 권한 문제가 있었던 kubectl scale 명령을 수행하여 pod 의 replica 를 증가시켜 봅니다.</p><pre>kubectl scale deployment gceme-frontend-production -n production — replicas=3</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MTyD7S4icd3Rvn4y" /></figure><p>보시면 아시겠지만 Cloud IAM 으로 생성된 개발자 계정에 Kubernetes RBAC 을 이용하여 ‘edit’ 권한이 부여되어서 문제없이 kubectl scale 명령을 수행할 수 있는 것을 확인 가능합니다.</p><p>여기까지 잘 따라오셨다면 간단하게 GCP 내에서 프로젝트와 리소스에 대한 액세스를 관리하는 역할을 제공하는 Cloud IAM 과 Kubernetes RBAC 을 간단하게 잘 살펴본 것입니다. 간략하게 소개드렸긴 하지만 실제적으로 프로젝트를 수행하거나 직접 권한 관리를 수행하게 되면 많이 어려워지는 부분이오니 한번씩 더 챙겨서 연습해 보시기 바라겠습니다.</p><p>그럼 이번 이야기는 여기서 줄이도록 하겠으며 다음에 다시 돌아오도록 하겠습니다. 휘리릭~~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f02b52cf538e" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 5탄 — Cloud Build 를 통한 CI/CD]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-5%ED%83%84-cloud-build-%EB%A5%BC-%ED%86%B5%ED%95%9C-ci-cd-47f1486937e4?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/47f1486937e4</guid>
            <category><![CDATA[cloud-build]]></category>
            <category><![CDATA[gcp]]></category>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[kubernetes]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Wed, 30 Jan 2019 18:06:19 GMT</pubDate>
            <atom:updated>2019-01-30T18:10:06.795Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 5탄 — Cloud Build 를 통한 CI/CD</strong></h3><p>안녕하세요 이정운 입니다.</p><p><a href="https://link.medium.com/PWzYRBFCST">지난번에</a> Kubernetes 의 configMap 과 Secrets 를 활용하여 GKE cluster 위에서 구동되는 container 화 된 애플리케이션의 환경 정보를 분리하여, 조금 더 유연하고 이식성 있게 유지할 수 있는 구조를 가져보는 방안을 살펴보고 테스트 해봤습니다. 꼭 해당 이야기에서 언급한데로 구조를 가지고 갈 필요는 없겠지만 그래도 기존 방식이 아니라 stateless 라는 kubernetes 의 특성을 잘 이해하고 살릴 수 있는 구조로 가지고 가야지 조금 더 GKE 의 장점을 잘 얻을 수 있지 않을까합니다.</p><p>그건 그렇고, 지난번까지 GKE 에 대한 다양한 이야기를 하나씩 하다보니, 결국 이야기가 진행되면서 애플리케이션을 수정하고 다시 빌드하고 배포하는 작업을 계속 반복했는데, 이런 반복작업은 굉장히 시간 소비가 클 뿐만이 아니라 수작업으로 계속 수행하기 번거로운 작업 입니다. 지금 테스트처럼 작은 단위의 테스트면 몰라도 조금 더 큰 시스템을 일일이 수작업으로 빌드, 배포하는 것은 쉽지 않은 일 입니다. 이미 GKE 환경이 아닌 다른 환경에서도 많이 사용해서 잘 알고 계시겠지만 이러한 반복 작업들을 자동화하기 위해서 CI/CD(Continuous Integration/Continuous Deploy&amp;Deployment) 라는 도구를 활용해서 파이프라인을 만들고 자동화 작업을 수행합니다. 당연히 GKE 도 이러한 CI/CD 도구를 활용해서 파이프라인을 만들거나 자동화 하는 것이 가능하며 이때 사용할 수 있는 Google cloud 에서 제공되는 것이 Cloud Build 라는 솔루션 입니다.</p><p>GCP 의 Cloud Build 를 사용하면 특정 언어에 상관없이 소프트웨어를 신속하게 빌드 및 배포 할 수 있으며 VM, 서버리스, Kubernetes, Firebase 등 다양한 환경에서 커스텀 빌드, 테스트, 배포 워크플로를 정의해서 사용하는 것이 가능합니다. 여기에 당연히 Managed Kubernetes 환경인 GKE 도 포함되며 오늘은 Cloud Build 를 한번 살펴보고 실제로 테스트 하는 시간을 가져보면서 이해할 수 있도록 이야기를 진행해 보겠습니다.</p><p><a href="https://cloud.google.com/cloud-build/?hl=ko">https://cloud.google.com/cloud-build/?hl=ko</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wtQQbOJlg41ZAftv" /></figure><p>본 이야기는 하단과 같이 좋은 이야기를 기반으로 참고해서 작성되었습니다.</p><p>Building, testing, and deploying artifacts<br><a href="https://cloud.google.com/cloud-build/docs/configuring-builds/build-test-deploy-artifacts">https://cloud.google.com/cloud-build/docs/configuring-builds/build-test-deploy-artifacts</a></p><p>Running builds using the GitHub app<br><a href="https://cloud.google.com/cloud-build/docs/run-builds-on-github">https://cloud.google.com/cloud-build/docs/run-builds-on-github</a></p><p>Automating builds using build triggers<br><a href="https://cloud.google.com/cloud-build/docs/running-builds/automate-builds">https://cloud.google.com/cloud-build/docs/running-builds/automate-builds</a></p><p>GKE Deployments with Cloud Builder<br><a href="https://github.com/GoogleCloudPlatform/container-builder-workshop">https://github.com/GoogleCloudPlatform/container-builder-workshop</a></p><p>Continuous Delivery Using Google Kubernetes Engine and Google Cloud Build<br><a href="http://stephenmann.io/post/continuous-delivery-using-google-kubernetes-engine-and-google-cloud-build/">http://stephenmann.io/post/continuous-delivery-using-google-kubernetes-engine-and-google-cloud-build/</a></p><p>Continuous Deployment with Cloud Build<br><a href="https://codelabs.developers.google.com/codelabs/cloud-builder-gke-continuous-deploy/index.html?index=..%2F..cloud#0">https://codelabs.developers.google.com/codelabs/cloud-builder-gke-continuous-deploy/index.html?index=..%2F..cloud#0</a></p><p>GoogleCloudBuild/gcbapp-example<br><a href="https://github.com/GoogleCloudBuild/gcbapp-example">https://github.com/GoogleCloudBuild/gcbapp-example</a></p><p><strong>#1) GKE 샘플 애플리케이션 구성</strong></p><p>먼저 새로운 GKE 샘플 애플리케이션을 이용해서 테스트를 위한 구성을 해보도록 하겠습니다. 이전 이야기에서 사용한 샘플 애플리케이션을 그대로 쓰고 싶으신 분들은 그대로 활용해도 되지만 이것 저것 뒤지다가 Google cloud platform 의 공식 github 에 조금 데모가 편한 샘플 애플리케이션이 있어서 이를 활용하도록 하겠습니다. 또한, 샘플 애플리케이션 내의 deployment 나 service 의 yaml 을 보니 잘 정의되어 있어서 이 부분을 공부하실 때도 도움이 될듯 합니다.</p><p>GKE Deployments with Cloud Builder<br><a href="https://github.com/GoogleCloudPlatform/container-builder-workshop">https://github.com/GoogleCloudPlatform/container-builder-workshop</a></p><p>참고적으로 다음 파트에서 실제 배포 작업을 수행하게되면 소스 변경을 하고 commit 등의 작업을 해야 하므로 Google cloud platform 의 공식 github 에서 샘플소스를 개인 계정의 github 의 저장소로 Fork 하는 작업을 먼저 수행해 둡니다. (해당 github 에서 Fork 버튼을 클릭하면 됩니다- 본 이야기가 github 강의는 아니기때문에 github 설명은 가급적 최소로 할 예정입니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*G_FQo6sesGIVyPX3" /></figure><p>테스트할 github 저장소가 준비되었다면 GKE 샘플 애플리케이션을 실제로 배포하기 위해서 GKE 의 해당 cluster 로 가서 상단 메뉴에 있는 ‘CONNECT’ 를 클릭하여 kubectl 수행을 위한 credential 을 가지고 옵니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*o6opRaH8VGwb98rm" /></figure><p>복사한 명령을 Cloud Shell 에서 수행하면 하단과 같이 GKE cluster 환경에 접속하여 자동으로 필요한 credential 을 가지고 오게 되고 GKE 차원에서 필요한 기본 준비가 된 것입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*P-zR6xE53ibfrBa1" /></figure><p>이제 자신의 github 로 Fork 한 샘플 애플리케이션을 직접 빌드하고 배포하기 위해서 해당 소스를 로컬 환경으로 clone 합니다.</p><pre>git clone <a href="https://github.com/jwlee98/container-builder-workshop.git">https://github.com/jwlee98/container-builder-workshop.git</a><br>cd ./container-builder-workshop</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xwYm8c4JCsbCjrMt" /></figure><p>샘플 애플리케이션 준비가 되었으니 실제 배포를 위해서 kubernetes 환경에서 새로운 namespace 인 production 을 하단과 같이 kubectl 을 통해서 만듭니다.(여기서 namespace 를 별도로 만든것은 같은 GKE cluster 를 사용하면서 환경을 구분하고 관리를 편하게 하기 위한 목적이며 반드시 필요한 것은 아닙니다. 나중에 기회가 되면 설명하겠지만 kubernetes 의 특징중의 하나는 namespace 를 이용해서 물리적인 자원을 논리적으로 구분해서 사용 가능하다는 점 입니다. – <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/</a>)</p><pre>kubectl create ns production</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ughU6SoxsTINJCWV" /></figure><p>Namespace 가 생성되었다면 github 에서 다운 받은 샘플 애플리케이션의 deployments 와 services 를 생성합니다. 제가 이전 이야기에 일부 yaml 을 설명했기 때문에 이번 이야기에서 상세하게 yaml 을 설명하지는 않지만 가급적 kubernetes/deployments/prod 에 있는 두 개의 yaml 파일과 kubernetes/services 에 있는 두 개의 yaml 파일은 직접 확인해보고 그 의미를 이해해보는 시간을 갖는것을 추천드립니다.</p><pre>kubectl apply -f kubernetes/deployments/prod -n production<br>kubectl apply -f kubernetes/services -n production</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*F8TS5wfUIthunxwH" /></figure><p>deployment 와 service 가 정상적으로 배포되었다면 하단과 같이 노출된 service 의 external-IP 를 확인해서 서비스가 정상적으로 나오는지 테스트 해볼 수 있습니다.</p><pre>export FRONTEND_SERVICE_IP=$(kubectl get -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;  --namespace=production services gceme-frontend)</pre><pre>curl <a href="about:blank">http://$FRONTEND_SERVICE_IP/version</a></pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CJl8GI-1Xr_jilFV" /></figure><p>실제 브라우저를 이용해서 접속하면 하단과 같이 두 부분으로 구성된 간단한 웹 페이지가 나오는 결과를 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vNMJrTMZvb9vsdfP" /></figure><p>해당 github 에서 기공유된 아키텍처를 보셨겠지만 저희가 사용한 샘플 애플리케이션은 kubernetes 에서 frontend pod 와 backend pod 라는 2 개의 pod 로 분리된 하단과 같은 구조를 통해서 간단하게 서비스를 보여주는 애플리케이션 입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/737/0*I1dChteZjK5a6_7O" /></figure><p>실제 GKE 환경에서도 미리 지정된 label 을 통해서 하단과 같이 frontend 와 backend 로 명명된 pod 가 구동중인 것을 직접 확인 가능합니다.</p><pre>kubectl get pods -n production -l app=gceme -l role=frontend<br>kubectl get pods -n production -l app=gceme -l role=backend</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lGMHDDOrfH4SPY3j" /></figure><p><strong>#2) Cloud Build 를 통한 CI/CD</strong></p><p>지난 파트까지 간단하게 새로운 샘플 애플리케이션을 GKE 환경으로 다시 배포해보고 정상적으로 서비스가 되는지 테스트를 해봤습니다. 이제는 이번 이야기의 본 게임인 Cloud Build 를 통한 CI/CD 구성을 해보도록 하겠습니다.</p><p>우선 가장 먼저 GKE 작업을 위한 권한에 문제가 없도록 하기 위하여 Cloud IAM 을 통해서 cloudbuild 의 service account 에 conatiner.developer 권한을 매핑하는 작업을 하단과 같이 수행합니다.</p><pre>export PROJECT=$(gcloud info --format=&#39;value(config.project)&#39;)<br>export PROJECT_NUMBER=&quot;$(gcloud projects describe \<br>$(gcloud config get-value core/project -q) --format=&#39;get(projectNumber)&#39;)&quot;</pre><pre>gcloud projects add-iam-policy-binding ${PROJECT} \<br>--member=serviceAccount:${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com \<br>--role=roles/container.developer</pre><p>다음으로 샘플 애플리케이션의 Dockerfile 을 수정하여 golang 을 최신 1.8 버전의 환경을 가져오도록 변경합니다. (테스트 해보니 github 의 소스를 그대로 사용하면 빌드시에 문제가 발생하여 빌드할 golang 버전을 올려주어야 합니다.)</p><pre>FROM golang:1.8-onbuild</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*XI5lIaAM0urITLc8" /></figure><p>이제 본격적으로 하단의 링크를 참고하여 Cloud Build 를 위한 설정 파일인 cloudbuild.yaml 파일을 작성합니다. 가이드를 보시면 아시겠지만 결국 해당 설정 파일에서 빌드/배포를 위한 파이프라인 구성이 가능하다는 것을 확인 가능합니다. (설명과 함께 다양한 예제가 포함되어 있으니 꼭 확인해보시기 바라겠습니다.)</p><p>Build configuration overview<br><a href="https://cloud.google.com/cloud-build/docs/build-config">https://cloud.google.com/cloud-build/docs/build-config</a></p><p>좀 편하게 이미 다운 받아 놓은 샘플 애플리케이션의 builder/cloudbuild-prod.yaml 파일을 그대로 쓰고 싶었는데 약간의 이슈가 있어서 그걸 그대로 복사해다가 CLUSTER, ZONE 등의 변수를 받아오는 부분만 살짝 변경하여 사용하도록 하겠습니다. cloudbuild.yaml 의 구성은 각 단계에서 수행할 entrypoint: ‘bash’ 와 args 로 이루어진 조합이며 보시는 것처럼 bash shell 명령을 사용가능하므로 원하는 다양한 액션을 직접 만들어서 변경할 수 있습니다.</p><pre>-----cloudbuild-prod2.yaml----</pre><pre>steps:</pre><pre>### Build<br>  - id: &#39;build&#39;<br>    name: &#39;gcr.io/cloud-builders/docker&#39;<br>    entrypoint: &#39;bash&#39;<br>    args: <br>      - &#39;-c&#39;<br>      - |<br>          docker build -t gcr.io/$PROJECT_ID/gceme:$TAG_NAME .<br>### Test</pre><pre>### Publish<br>  - id: &#39;publish&#39;<br>    name: &#39;gcr.io/cloud-builders/docker&#39;<br>    entrypoint: &#39;bash&#39;<br>    args: <br>      - &#39;-c&#39;<br>      - |<br>          docker push gcr.io/$PROJECT_ID/gceme:$TAG_NAME</pre><pre>### Deploy<br>  - id: &#39;deploy&#39;<br>    name: &#39;gcr.io/cloud-builders/gcloud&#39;<br>    env:<br>      - &#39;CLOUDSDK_COMPUTE_ZONE=${_CLOUDSDK_COMPUTE_ZONE}&#39;<br>      - &#39;CLOUDSDK_CONTAINER_CLUSTER=${_CLOUDSDK_CONTAINER_CLUSTER}&#39;<br>      - &#39;KUBECONFIG=/kube/config&#39;<br>    entrypoint: &#39;bash&#39;<br>    args:<br>      - &#39;-c&#39;<br>      - |<br>          CLUSTER=${_CLOUDSDK_CONTAINER_CLUSTER}<br>          PROJECT=$$(gcloud config get-value core/project)<br>          ZONE=${_CLOUDSDK_COMPUTE_ZONE}<br>          gcloud container clusters get-credentials &quot;$${CLUSTER}&quot; \<br>            --project &quot;$${PROJECT}&quot; \<br>            --zone &quot;$${ZONE}&quot;  <br>          sed -i &#39;s|gcr.io/cloud-solutions-images/gceme:.*|gcr.io/$PROJECT_ID/gceme:$TAG_NAME|&#39; ./kubernetes/deployments/prod/*.yaml<br>          <br>          kubectl get ns production || kubectl create ns production<br>          kubectl apply --namespace production --recursive -f kubernetes/deployments/prod<br>          kubectl apply --namespace production --recursive -f kubernetes/services</pre><pre>-----cloudbuild-prod2.yaml----</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JF3Jqt63pJAn5jhN" /></figure><p>이렇게 Cloud Build 를 위한 설정 파일에 대한 준비가 되었다면 먼저 정상적으로 잘 수행되는지 파악하기 위해서 로컬 테스트를 수행해 보도록 합니다. 로컬 테스트의 경우에는 gcloud builds 명령을 사용하며 TAG_NAME 이나 cloudbuild.yaml 파일에서 내부적으로 사용된 변수를 직접 넣어주어야 한다는 점을 유념하시기 바라겠습니다.</p><pre>export PROJECT_ID=$(gcloud info --format=&#39;value(config.project)&#39;)<br>export CLUSTER=kub-standard-cluster-01<br>export ZONE=asia-northeast1-c</pre><pre>gcloud builds submit --config builder/cloudbuild-prod2.yaml --substitutions=TAG_NAME=1.0.1,_CLOUDSDK_COMPUTE_ZONE=${ZONE},_CLOUDSDK_CONTAINER_CLUSTER=${CLUSTER} .</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RmksfMdYWYANQX8X" /></figure><p>로컬 테스트 후에 정상적으로 배포가 되었는지 확인합니다. 기본적으로 큰 변화가 없다면 추가 테스트를 하지 않고도 해당 pod 의 AGE 가 변경되었는지 여부를 통해서 반영 여부를 확인할 수도 있습니다.</p><p>kubectl get pods -n production -o wide</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nWhwYR9NIxr91uGj" /></figure><p>로컬 테스트에 큰 이슈가 없다면 git add 및 commit 해서 해당 내용을 내 github 의 저장소에 반영합니다. (향후 Cloud Build 의 trigger 를 사용할때 cloudbuild.yaml 을 활용하기 위한 목적입니다. 따라서, 만드신 cloudbuild.yaml 파일의 경로는 숙지하고 있어야 합니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*G-Lk1dJ2u4zu4eI9" /></figure><p><strong>#3) Github 에서 App 을 통한 Cloud Build 의 trigger 연동</strong></p><p>로컬 테스트를 수행해서 정상적으로 결과가 나오는 것을 확인했으면 이제 본격적으로 좀 더 CI/CD 스럽게 github 연동 작업을 수행하도록 하겠습니다. GCP 의 Cloud Build 는 github 의 App 을 이용해서 저장소 변경시에 자동으로 Cloud Build 의 trigger 를 동작 시킬 수 있습니다. 이를 다시 정리하면 github 의 저장소가 변경되는 이벤트 만으로도 자동으로 Cloud Build 를 수행해서 애플리케이션을 빌드/배포 할 수 있습니다.</p><p>이를 위해서 Google Cloud Build 라는 github application 에 대한 설치가 필요합니다.(설치해보시면 아시겠지만 로컬에 실제 설치되는 것은 아니며 개인 github 계정에 등록되는 것입니다.) 하단의 링크로 접속하여 install 버튼을 클릭합니다.</p><p><a href="https://github.com/apps/google-cloud-build">https://github.com/apps/google-cloud-build</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*mW82Ncwrbeou8wMA" /></figure><p>그러면, 하단과 같이 어떤 저장소를 쓸지를 선택하고 Install 을 클릭하여 설치하면 됩니다. (당연히 상단에서 Fork 한 본인 github 의 저장소를 선택하면 됩니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/824/0*wjfHBueuFWMRsTJe" /></figure><p>github 의 Google Cloud Build App 설치가 마무리 되었다면 이제 Cloud Build 의 Trigger 를 생성해보도록 하겠습니다. 관리 콘솔에서 Cloud Build &gt; Triggers 메뉴에서 new 를 클릭합니다. 처음에 소스 위치가 나오는데 Github 를 선택합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VLaJkcaahL1_mXXa" /></figure><p>그러면 하단과 같이 github 페이지로 redirect 되어서 권한 확인을 위한 github 계정정보를 입력하게 되어 있습니다. 각자 자신이 가지고 있는 github 계정을 넣어주면 됩니다 .</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Fk1ceD_aelGcX-pl" /></figure><p>계정 확인이 잘 되었으면 이제 github 내의 어떤 저장소랑 연결할 건지 지정해주고 continue 를 클릭하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*__oOnz9YFu63lTcM" /></figure><p>마지막으로 trigger 를 위한 세부 설정을 합니다. 예를 들어 Trigger type 을 어떤 것으로 할 것인지(Branch 가 변경될 때 반응 or Tag 가 변경될 때 반응) 나 cloudbuild.yaml 파일의 위치(github 기준으로 폴더 위치를 지정 — builder/cloudbuild-prod2.yaml 이 되어야 합니다.), 맨 앞에 ‘_’ 로 시작하는 변수(cloudbuild.yaml 파일 내에서 사용한 변수)의 값 등을 넣을 수 있습니다. 크게 이슈가 없다면 해당 설정은 하단과 같이 해주고 Create trigger 버튼을 클릭하면 trigger 가 생성됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*3bpwM1-Mfpc9ZaI5MzXykw.png" /></figure><p>해당 작업이 정상적으로 수행되면 하단과 같이 Trigger 가 생성된 것을 확인할 수 있습니다. 특히, 해당 Trigger 는 ‘Run a trigger’ 명령을 클릭하여 바로 테스트 해보실 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_AwQSxdEQNSOhKiy" /></figure><p>언급한데로 ‘Run a trigger’ 명령을 클릭하여 메뉴얼로 테스트를 해보면 하단과 같이 History 에서 자동으로 Build 가 수행된 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TWVs_lLjBd7xRGVe" /></figure><p>해당 빌드를 클릭하면 로그와 같이 좀 더 자세한 내용을 확인해 보실 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JF1ThhM_90_LMA5G" /></figure><p>당연하겠지만 이 작업 중에 kubectl 명령으로 pods 를 확인해보면 하단과 같이 새로운 버전의 배포작업이 일어나서 pods 가 변경되는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*c_T5QcUirVYxNLIq" /></figure><p><strong>#4) Github 에서 소스 변경을 통한 테스트</strong></p><p>자 이제 거의 다 왔습니다. ^^&amp; 모든 구성이 다 되었으므로 마지막으로 github 의 내 저장소에서 직접 소스를 변경하여 Cloud Build 를 거쳐 실제 GKE 환경까지 애플리케이션이 배포되는 작업을 테스트 해보도록 하겠습니다.</p><p>Github 의 샘플 애플리케이션이 있는 내 저장소에 가서 루트에 있는 html.go 파일의 &lt;div class=”card blue”&gt; 구문을 &lt;div class=”card green”&gt; 로 모두 변경 합니다. (github 에서 연필 모양 아이콘을 통해서 바로 수정 가능하며 보시면 직관적으로 아시겠지만 파란색을 녹색으로 변경하는 것 입니다.) 그리고 하단과 같이 해당 변경 사항을 commit 합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K0TCZsWLyJP5NetF" /></figure><p>현재 Cloud Build 의 trigger 변경 기준을 Tag 로 했기 때문에 commit 한다고 해서 변경이 발생되지는 않으며 github 에서 버전이 변경되었다는 의미로 tag 를 업데이트 해줍니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ihybLeHJJ2DIEMJq" /></figure><p>해당 작업을 수행하면 자동으로 github 내에서 Webhook 이 발생되고 App 을 통해 만들어 둔 Cloud Build 의 Trigger 가 작동합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NmbqjleJpOH7_RFW" /></figure><p>미리 지정된 Cloud Build 의 설정 파일인 cloudbuild.yaml 에서 정의한 순서대로 빌드/배포 작업이 진행됩니다. (여기서는 github 내에 배포된 builder/cloudbuild-prod2.yaml)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yBlnjZS1hk09VfCT" /></figure><p>이 작업이 완료되고 테스트를 해보면 github 의 소스 변경 및 commit 과 tag 업데이트 만으로 자동으로 소스 빌드 및 배포작업이 수행되고 GKE 샘플 애플리케이션의 배경 카드가 파란색에서 녹색으로 변경된 것을 하단과 같이 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zg8Ay91ay5ozsPzX" /></figure><p>여기까지 잘 따라오셨다면 조금 길긴 하였지만 GCP 의 CI/CD 도구인 Cloud Build 를 활용하여 GKE 환경에 대한 자동화된 빌드/배포 구성을 수행하였고 실지 테스트까지 문제 없이 수행하신 것 입니다. 특히나, Cloud Build 뿐만 아니라 개발자들이 많이 사용하는 소스 저장소인 github 의 변경에 대한 부분을 시작점으로 Trigger 를 통해서 이벤트 발생을 캐치하는 작업도 같이 잘 수행해보신 것 입니다. 반드시 GKE 환경의 CI/CD 도구를 GCP 의 Cloud Build 를 사용할 필요는 없을 수도 있겠지만 테스트 해보셔서 느낀 것처럼 쉽게 사용할 수 있게 구성되어 있으며 비용도 거의 무료수준에 가까울정도로 저렴해서 큰 부담없이 사용하기에는 간편하고 좋지 않을까 합니다.</p><p>긴 이야기 였지만 여기까지 따라와 주신 점 감사드리며 이번 이야기는 여기서 마무리 하고 다음 이야기로 다시 돌아오도록 하겠습니다. 그럼 이만 휘리릭~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=47f1486937e4" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 4탄 — GKE 설정 외부화 (configMap, Secrets)]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-4%ED%83%84-gke-%EC%84%A4%EC%A0%95-%EC%99%B8%EB%B6%80%ED%99%94-configmap-secrets-354da3a91edf?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/354da3a91edf</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[gcp]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sun, 06 Jan 2019 11:59:27 GMT</pubDate>
            <atom:updated>2019-01-06T11:59:27.329Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 4탄 — GKE 설정 외부화 (configMap, Secrets)</strong></h3><p>안녕하세요 이정운 입니다.</p><p><a href="https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-3%ED%83%84-gcp-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%97%B0%EA%B2%B0-%ED%95%B4%EB%B3%B4%EA%B8%B0-ae608b1b4338">지난번 이야기까지</a> Managed kubernetes 서비스인 Google Kubernetes Engine (GKE) 도 살펴보았고, DB 서비스를 연결하기 위해서 GCP 의 Cloud SQL 도 간단히 연동해서 테스트 해봤습니다. 해당 이야기를 보셨으면 아시겠지만 GKE 를 통해서 아주 쉽게 kubernetes 를 사용할 수 있으며 GKE 에서 VPC native 기능을 지원하고 있기 때문에 필요한 경우에 별도의 고려없이 다양한 GCP 의 서비스도 바로 internal IP로 연동해서 활용할 수 있는 장점을 가지고 있습니다.</p><p>그런데 지난번 테스트한 구조를 다시 한번 잘 살펴보면 아시겠지만 Spring boot 애플리케이션을 container 이미지로 만들때 연결되는 DB의 사용자나, 비밀번호, IP 주소등을 로컬 디텍토리에 있는 application.properties 에서 읽어 오는 형태로 만들어서 독립적이지 않고 유연성이 조금은 떨어지는 구조를 가지고 있습니다. 다시 말하면, 분리된 DB 의 암호를 변경하게 되었을 때 해당 정보가 container 이미지의 로컬 파일에 저장되어 있으므로 변경의 영향을 받게되고, 결국 암호를 변경 및 반영해서 다시 container 이미지를 새롭게 만들고 배포해야 하는 의존성을 가진 구조를 가지게 됩니다.</p><p>그렇다고 이번 이야기에서 거창하게 MSA(MicroServices Architecture) 나 Cloud native 를 논의 하려는 것은 아니지만 이러한 구조라면 stateless 를 지향하는 kubernetes 에서 각 pod 간의 의존성이 생길 수 있으므로 이 구조 보다는 조금은 유연하고 조금 더 kubernetes 의 사상에 맞는 구조를 가질 수 있는 방법을 고민하면서 한 걸음 나아가는 것으로 이번 이야기를 진행해보고자 합니다. 그래서 이번 이야기에서 한번 다루어 보고 테스트 할 것은 환경과 연관된 정보를 container 에 두지 않고 kubernetes 에 저장하고 불러 올 수 있는 configMap 과 secrets 기능을 살펴보려고 합니다.</p><p>configMap 은 container 이미지에서 사용하는 환경변수와 같은 세부 정보를 분리하여 container 화 된 애플리케이션을 이식성 있게 유지할 수 있는 구조를 지원할 수 있는 kubernetes 의 기능이며 Secrets 은 이와 동일하지만 좀 더 보안화된 방법을 제공 가능한 기능입니다. 그냥 이렇게 설명만 듣고서는 이해가 어렵기 때문에 이제 부터는 실제 테스트를 해보면서 직접 확인해보는 시간을 가져보도록 하겠습니다.</p><p>참고적으로 본 이야기는 하단과 같이 좋은 이야기를 기반으로 참고해서 작성되었습니다.</p><p>ConfigMap<br><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/configmap?hl=ko">https://cloud.google.com/kubernetes-engine/docs/concepts/configmap?hl=ko</a></p><p>Configure a Pod to Use a ConfigMap<br><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/</a></p><p>Secret<br><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/secret?hl=ko">https://cloud.google.com/kubernetes-engine/docs/concepts/secret?hl=ko</a></p><p>Secrets<br><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>Configuring Spring Boot on Kubernetes With ConfigMap<br><a href="https://dzone.com/articles/configuring-spring-boot-on-kubernetes-with-configm">https://dzone.com/articles/configuring-spring-boot-on-kubernetes-with-configm</a></p><p>Spring Boot — spring.profiles<br><a href="https://blog.naver.com/handhwang/221375095974">https://blog.naver.com/handhwang/221375095974</a></p><p>Configuring Spring Boot on Kubernetes with ConfigMap<br><a href="https://developers.redhat.com/blog/2017/10/03/configuring-spring-boot-kubernetes-configmap/">https://developers.redhat.com/blog/2017/10/03/configuring-spring-boot-kubernetes-configmap/</a></p><p><strong>#1) Spring boot 의 profiles 를 통한 configMap 사용하기</strong></p><p>몇 번 이야기 했지만 본 이야기는 직접적인 개발 이야기는 아니기때문에 이전 이야기에서 사용한 github 에서 받은 애플리케이션 소스를 그대로 재활용 하도록 하겠습니다. 다만, 추가적으로 설정 정보를 container 안의 파일이 아니라 kubernetes 에서 동적으로 받아오는 작업을 하기 위해서 환경에 따라 설정 파일을 동적으로 변경할 수 있는 Spring boot 의 profiles 라는 기능을 추가해서 테스트 해보도록 하겠습니다.(제가 이리 저리 찾아봤는데 그나마 이 방안이 제일 나은거 같아서 선택해봤습니다. 반드시 이렇게 할 필요는 없을듯 하며 더 나은 방안이 있다면 그 방법을 사용해도 무방합니다.)</p><p>Spring boot — Spring profiles<br><a href="https://blog.naver.com/lsc401/221339383234">https://blog.naver.com/lsc401/221339383234</a></p><p>Spring Boot — spring.profiles<br><a href="https://blog.naver.com/handhwang/221375095974">https://blog.naver.com/handhwang/221375095974</a></p><p>상단의 가이드에 나온 것처럼 Spring boot 의 profiles 기능을 사용할 수 있도록 설정 파일을 하단과 같이 추가로 만들어 둡니다.</p><p>vi src/main/resources/application-k8s.properties</p><pre>## Spring DATASOURCE (DataSourceAutoConfiguration &amp; DataSourceProperties)<br>spring.datasource.url = jdbc:mysql://${MYSQL_HOST}:${MYSQL_PORT}/notes_app?autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;useSSL=false<br>spring.datasource.username = ${MYSQL_USER}<br>spring.datasource.password = ${MYSQL_PASSWORD}</pre><pre>## Hibernate Properties<br># The SQL dialect makes Hibernate generate better SQL for the chosen database<br>spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MySQL5InnoDBDialect</pre><pre># Hibernate ddl auto (create, create-drop, validate, update)<br>spring.jpa.hibernate.ddl-auto = update</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*blRItlG_y47-aBLs" /></figure><p>상단의 설정을 보시면 아시겠지만 이전에 작성한 application.properties 와 동일한데 설정 파일에 직접 IP 나 암호등을 미리 적어놓는 것이 아니라 변수로 지정해서 동적으로 받아오도록 설정 한 부분만 다른 형태를 가지고 있습니다.</p><p>이제 설정을 변경하였으니 해당 Spring boot 애플리케이션을 다시 package 하고 container 이미지로 만듭니다. 이때 Spring boot 의 profiles 기능을 사용할 것이므로 Dockerfile 의 실행부분에 ‘-Dspring.profiles.active=k8s’ 옵션을 추가해야 합니다.</p><pre>FROM openjdk:8-jdk-alpine<br>EXPOSE 8080<br>ADD easy-notes-1.0.0.jar easy-notes-1.0.0.jar<br>ENTRYPOINT [&quot;java&quot;, &quot;-Dspring.profiles.active=k8s&quot;, &quot;-jar&quot;, &quot;easy-notes-1.0.0.jar&quot;]</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9SV0lGAxcUfgRZIE" /></figure><p>Dockerfile 을 만들었다면 이전에도 해봤지만 하단과 같이 Cloud build 를 이용해서 container 이미지를 만들고 GCR 에 저장합니다.</p><pre>gcloud builds submit --tag gcr.io/jwlee-myproject-01/bootsample ./</pre><p>container 이미지가 준비되었으니 다음으로 해당 설정 정보와 연동하기 위한 kubernetes 의 configMap 을 생성해보도록 하겠습니다. configMap 의 자세한 설명은 하단의 링크들에 자세히 나와있지만 가장 중요한 포인트는 pod 와 환경정보를 분리할 수 있어 작업 부하를 이식 가능하게 유지하는 데 도움이 되고, 구성을 쉽게 변경하고 관리할 수 있으며, 구성 데이터를 pod 에 하드코딩하는 것을 방지하도록 도움을 줄 수 있다는 것입니다.</p><p>ConfigMap<br><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/configmap?hl=ko">https://cloud.google.com/kubernetes-engine/docs/concepts/configmap?hl=ko</a></p><p>Configure a Pod to Use a ConfigMap<br><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/</a></p><p>configMap 은 yaml 파일로 만들수 있지만 하단과 같이 kubectl 명령을 통해서도 쉽게 생성 가능합니다.</p><pre>kubectl create configmap configmap-sample --from-literal=db.address=&quot;172.16.3.3&quot; --from-literal=db.port=&quot;3306&quot; --from-literal=db.user=&quot;root” --from-literal=db.password=&quot;q1w2e3r4&quot;</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*frK49yK79rJC6-Ow" /></figure><p>생성된 configMap 은 GCP 의 관리콘솔의 메뉴에서도 바로 확인이 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bOFcMX_8U9fV9Yp4" /></figure><p>이제 configMap 이 적용된 버전을 GKE 환경에 배포하기 위해서 deployment 작성을 위한 yaml 파일을 하단과 같이 작성합니다. 이때 이전과 다른 점은 ‘env’ 라는 설정이 추가되었으며 Spring boot 의 profiles 기능을 통해서 호출되는 application-k8s.properties 에 있는 변수들에 대한 실제 값으로 configMap 정보를 지정하게 됩니다.</p><pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: bootsample<br>  labels:<br>    app: bootsample<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>      app: bootsample<br>  template:<br>    metadata:<br>        labels:<br>          app: bootsample<br>    spec:<br>      containers:<br>      - name: bootsample<br>        image: gcr.io/jwlee-myproject-01/bootsample<br>        env:<br>          - name: MYSQL_HOST<br>            valueFrom:<br>             configMapKeyRef:<br>                name: configmap-sample<br>                key: db.address<br>          - name: MYSQL_PORT<br>            valueFrom:<br>             configMapKeyRef:<br>                name: configmap-sample<br>                key: db.port                <br>          - name: MYSQL_USER<br>            valueFrom:<br>             configMapKeyRef:<br>                name: configmap-sample<br>                key: db.user          <br>          - name: MYSQL_PASSWORD<br>            valueFrom:<br>             configMapKeyRef:<br>                name: configmap-sample<br>                key: db.password                       <br>        ports:<br>        - containerPort: 8080</pre><p>위와 같이 sample.yaml 이라는 deployment 를 작성하고 ‘kubectl apply -f sample.yaml’ 명령을 통해서 배포하면 됩니다.</p><p>이때 kubernetes 의 장점을 하나 보실 수 있는데 하단의 첨부 그림을 보면 바로 이해하실 수 있는 것처럼 ‘kubectl apply’ 명령을 이용해서 deployment 를 업데이트 하게 되면 해당 pod 가 업데이트가 되지 않고 새로운 pod 가 생성된 후에 기존 pod 를 교체하는 형태로 업데이트 된 다는 것을 확인할 수 있습니다. (나름 rolling update 방식이 자동 적용됩니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UF59mfqNanEavlF3" /></figure><p>이제 이전과 동일하게 curl 을 이용해서 서비스를 테스트해보면 하단과 같이 이전과 동일하게 결과가 잘 나오는 것을 확인하실 수 있습니다.</p><pre>curl -v &quot;<a href="http://35.190.236.174:80/api/notes">http://35.221.95.11:80/api/notes</a>&quot;</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*k3DDZsIlZCgRb3Ou" /></figure><p><strong>추신 #1 </strong>: 참고적으로 이러한 방식의 장점중의 하나는 배포중에 이슈가 발생하거나 새롭게 업데이트된 pod 에 이슈가 있다면 하단과 같이 쉽게 rollback 할 수 있다는 것입니다.</p><pre>kubectl rollout undo deployment/bootsample</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wWbChiKMfJf9MHcm" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kBoOUo1xihgO2maK" /></figure><p><strong>#2) Secrets 사용하기</strong></p><p>Secrets 은 configMap 과 동일한 기능을 제공하면서도 좀 더 보안화된 방법을 제공 가능한 기능입니다. 좀 더 보안화된 방법이란 의미는 설정 정보를 암호화하여 데이터의 민감성을 제어하고 승인되지 않은 사용자에게 데이터가 노출될 위험을 줄여준다는 의미입니다.</p><p>Secret<br><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/secret?hl=ko">https://cloud.google.com/kubernetes-engine/docs/concepts/secret?hl=ko</a></p><p>Secrets<br><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>Secrets 생성은 configMap 과 유사하게 kubectl 을 통해서 하단과 같이 생성 가능합니다.</p><pre>kubectl create secret generic securitytoken --from-literal user=root --from-literal password=q1w2e3r4</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*12QxgEg2EZNNIhKl" /></figure><p>당연히 이렇게 생성된 Secrets 도 GCP 의 관리콘솔에서 하단과 같이 확인 가능합니다. 다만, Secrets 의 특성상 관리화면에서 해당 값은 볼 수 없도록 가려져서 표시됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pfg59uT3Bqrki3CW" /></figure><p>실제 사용 방식에서도 Secrets 는 configMap 과 동일 기능을 제공해서 그런지 굉장히 유사합니다. 하단의 deployment 를 참고하시면 아시겠지만 configMap 과 같은 형태로 deployment 의 yaml 파일 내에 설정 가능합니다.</p><pre>          - name: MYSQL_USER<br>            valueFrom:<br>             secretKeyRef:<br>                name: securitytoken<br>                key: user  <br>          - name: MYSQL_PASSWORD<br>            valueFrom:<br>             secretKeyRef:<br>                name: securitytoken<br>                key: password</pre><p>이전과 동일하게 해당 deployment 를 GKE cluster 에 배포하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KnyWP8sX83Nwm9Pd" /></figure><p><strong>추신 #2 </strong>: 참고적으로 kubectl apply 시에 계속 하단과 같은 이슈가 발생해서 깔끔하게 삭제하고 kubectl create 를 통해서 생성하는 형태로 진행했었습니다. 혹시 저와 유사하게 이슈가 있으신 분은 새롭게 생성하는 형태로 해보시기 바라겠습니다.</p><pre>The Deployment &quot;bootsample&quot; is invalid: spec.template.spec.containers[0].env[3].valueFrom: Invalid value: &quot;&quot;: may not have more than one field specified at a time</pre><p>마지막으로 curl 로 테스트해보면 정상적으로 결과가 나오는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BVoyvKCpKVry__f-" /></figure><p>여기까지 잘 따라 오셨다면 GKE 를 이용한 서비스에서 설정 정보를 configMap 이나 Secrets 로 분리해서 동일하게 서비스 하는 것을 테스트 해보신 것 입니다. 테스트 해보시면 아시겠지만 Kubernetes 의 pod 에서 설정 정보를 분리해서 container 화 된 애플리케이션을 이식성 있게 유지할 수 있는 구조를 지원할 수 있는 장점을 얻을 수 있습니다. 상황과 환경에 따라 달라질 수는 있겠지만 좀 더 유연한 아키텍처 구조를 가지고 갈 수 있다는 의미입니다.</p><p>여하튼 이만하면 충분히 살펴본 것 같으니 이번 이야기는 여기서 마무리 하도록 하겠으며 다음에 또 다른 주제로 다시 찾아오겠습니다. 그럼 이만 휘리릭~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=354da3a91edf" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 3탄 — GCP 서비스 연결 해보기]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-3%ED%83%84-gcp-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%97%B0%EA%B2%B0-%ED%95%B4%EB%B3%B4%EA%B8%B0-ae608b1b4338?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/ae608b1b4338</guid>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[gcp]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gke]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sat, 22 Dec 2018 08:35:43 GMT</pubDate>
            <atom:updated>2019-01-02T02:21:33.339Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 3탄 — GCP 서비스 연결 해보기</strong></h3><p>안녕하세요 이정운 입니다.</p><p>지난번 까지 2회에 걸쳐서(“<a href="https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-1%ED%83%84-gke-%EA%B0%9C%EC%9A%94-382dc69b2ec4">GKE 차근 차근 알아보기 1탄 — GKE 개요</a>”, “<a href="https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-2%ED%83%84-gke-%EC%84%9C%EB%B9%84%EC%8A%A4-%EB%B0%8F-%ED%99%95%EC%9E%A5-%ED%95%B4%EB%B3%B4%EA%B8%B0-5c9b137e72c8">GKE 차근 차근 알아보기 2탄 — GKE 서비스 해보기</a>”) GKE 서비스를 살펴보고, 간단한 샘플 서비스를 배포하여 테스트까지 해봤습니다. 이제 이를 더 확장해서 GKE cluster 에 애플리케이션을 올려서 다른 Google cloud 서비스와 연결하는 것을 한번 살펴보려고 합니다.</p><p>잘 아시겠지만 Kubernetes 가 제공하는 기능은 아주뛰어나며 container 환경을 잘 조정하고 관리 할 수 있지만 그렇다고 모든 것을 Kubernetes 로 올려서 사용하는 것은 비효율적인 아키텍처 가 될 수도 있습니다. 예를 들어서 변화되지 않고 고정된 형태가 더 어울리는 DB 같은 경우에는 이를 kubernetes 로 하게 되면 생각보다 고려사항들이 많아지고 훨씬 많은 노력이 들어갈 수 있습니다. 이러한 경우에는 어쩌면 Google cloud 의 Cloud SQL 과 같이 클라우드 업체에서 제공하는 관리형 서비스를 사용하는 것이 더 쉽고 효율적이지 않을까 합니다. (늘 강조드리지만 아키텍처에는 정해진 정답이 없으며 각 환경별로 다 다를 수 있습니다. 요즘에는 전체 환경의 일관성 및 통합에 방점을 두어 Kubernetes 의 StatefulSets 를 활용하여 DB 를 운영하는 시도도 많이 나오고 있습니다.)</p><p>이러한 경우를 가정하여 어떻게 GKE 환경에서 다른 GCP 서비스를 연동하고 사용할 것인지에 대해서 이번 이야기에서 간단하게 살펴보고 실제 테스트 해보도록 하겠습니다. 아마도 이번 이야기를 보시면 바로 아시겠지만 Cloud SQL 이 아니라 다른 GCP 서비스도 쉽게 연동 가능하며 이를 통해서 원하시는 형태대로 유연하게 아키텍처를 구성하는 것이 가능합니다.</p><p>본 이야기는 하단과 같이 좋은 이야기를 기반으로 참고해서 작성되었습니다.</p><p>Configuring Private IP Connectivity<br><a href="https://cloud.google.com/sql/docs/mysql/configure-private-ip">https://cloud.google.com/sql/docs/mysql/configure-private-ip</a></p><p>Introducing VPC-native clusters for Google Kubernetes Engine<br><a href="https://cloud.google.com/blog/products/gcp/introducing-vpc-native-clusters-for-google-kubernetes-engine">https://cloud.google.com/blog/products/gcp/introducing-vpc-native-clusters-for-google-kubernetes-engine</a></p><p>Creating VPC-native clusters using Alias IPs<br><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips">https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips</a></p><p><strong>#1) Cloud SQL 준비하기</strong></p><p>이미 이전 이야기에서 GKE cluster 환경은 생성해서 준비해 두었으니(참고적으로 GKE 의 VPC native 기능을 enable 했다는 전제입니다) 이제 DB 목적으로 사용할 Cloud SQL 을 간단하게 준비해보도록 하겠습니다. Cloud SQL은 클라우드에서 관계형 PostgreSQL 및 MySQL 데이터베이스를 손쉽게 설정, 유지, 관리할 수 있는 완전 관리형 데이터베이스 서비스입니다. 다시 말해, 귀찮은 설치나 설정 없이 손쉽게 사용할 수 있는 클라우드 DB 서비스 입니다. 좀 더 자세한 기능이 궁금하신 분들은 하단의 링크를 참고하시기 바라겠습니다.</p><p><a href="https://cloud.google.com/sql/">https://cloud.google.com/sql/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bz6atapRc77T2MlB" /></figure><p>그럼 이제 실제로 Cloud SQL 을 생성해보도록 하겠습니다. GCP 관리 콘솔에서 Cloud SQL 을 선택하고 생성하면 하단과 같은 생성 마법사가 나옵니다. 처음에는 가장 중요한 포인트인 MySQL 과 PostgreSQL 중에 어느 것을 선택할 지에 대한 질문이 나옵니다. 원하시는 DB 를 골라주면 되며 본 이야기에서는 많이 사용되는 MySQL 을 선택하도록 하겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/784/0*dOB-U2n70lqdu50E" /></figure><p>그렇게 되면 하단과 같이 다시한번 1세대로 할건지, 2세대로 할건지에 대해서 물어보는데 이건 2 세대를 선택합니다. (제 개인적 견해인데 아마도 하위 호환성 때문에 1세대를 계속 유지하는게 아닐까하며 1세대는 이제 거의 사용되지 않습니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/992/0*s1Qx07mvuiZ8feWP" /></figure><p>그 뒤에는 실제적인 상세 설정 메뉴를 확인할 수 있습니다. 원하시는 형태로 Instance ID 나 Root password 를 입력하고 어느 Region, zone 에 위치할 지 등을 선택하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JuL8ckXjLoyrYZadbo5esA.png" /></figure><p>이때 ‘Set Connectivity’ 부분을 열어서 기본 값인 Public IP 를 사용하지 않고 Private IP 를 선택하는 것을 추천드립니다. Private IP 는 같은 region 에서만 사용 가능하다는 제약이 있긴 하지만(그러므로 GKE cluster 를 생성한 같은 region 으로 Cloud SQL 을 생성해야 합니다) 이름 그대로 커뮤니케이션을 private ip 로 하기 때문에 보안에 더 안정하며 GCP 내부 네트워크 라인을 통해서 통신하므로 더 빠른 성능을 제공 가능합니다. 좀 더 자세한 내용에 대해서는 하단의 링크를 참고하시기 바라겠습니다.</p><p>Configuring Private IP Connectivity<br><a href="https://cloud.google.com/sql/docs/mysql/configure-private-ip">https://cloud.google.com/sql/docs/mysql/configure-private-ip</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/650/0*XgrqaDG8XbY33FfA" /></figure><p>그 뒤에 남은 설정들은 보시면 이름을 보고 직관적으로 이해가 가능한 설정들인데 사이즈를 조정하거나, 스토리지 타입 변경(SSD or HDD) , 자동 스토리지 증설 기능에 대한 옵션입니다. 원하시는 요구사항에 맞추어서 설정을 해서 사용하면 되지 않을까 합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*F2gEPwrHvLOI3ury" /></figure><p>추가적으로 잘 안보고 넘어가는 경우가 많은데 Cloud SQL 의 Maintenance schedule 을 설정하는 기능이 있으니 가급적 문제가 없을 새벽시간으로 설정하시기 바라겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/910/0*cpeoXGUvSQUy8s7s" /></figure><p>이렇게 설정을 완료하고 생성을 하게 되면 하단과 같이 Cloud SQL 인스턴스를 확인 가능하며 세부 설정도 관리콘솔에서 바로 확인 가능합니다. 여기서는 중요한 것 중의 하나가 Private IP 주소를 잘 확인해두면 될듯 합니다. (나중에 해당 Private IP 주소로 직접 접근 할 것입니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MFdPjxvhBHzbIfhc" /></figure><p><strong>#2) 샘플 서비스 테스트 및 GKE 에 올리기</strong></p><p>이전 파트에서 Cloud SQL 에 대한 설정을 완료했으니 이제 해당 Cloud SQL 을 사용하는 간단한 샘플을 테스트 해보고 서비스를 GKE 에 올려서 서비스 테스트도 해보도록 하겠습니다.</p><p>본 이야기는 개발 이야기가 아니기 때문에 직접 개발을 하거나 소스 설명은 하지는 않을 예정이며 하단과 같이 github 에 오픈된 소스를 활용해서 테스트 해보는 형식으로 진행하도록 하겠습니다. (즉, 원하신다면 다른 다양한 샘플을 가지고 동일하게 테스트 해보시면 될듯 합니다.)</p><p>기 언급한 것처럼 github 에 오픈된 소스중 간단하게 Rest API 를 제공하는 Spring boot 애플리케이션을 가지고 테스트 해보도록 하겠습니다.</p><p><a href="https://github.com/callicoder/spring-boot-mysql-rest-api-tutorial">https://github.com/callicoder/spring-boot-mysql-rest-api-tutorial</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*18iMpU8UOt32p9SW" /></figure><p>github 의 설명을 참고하여 애플리케이션을 github 에서 복제한 후 application.properties 설정 파일을 지금 생성해둔 Cloud SQL 과 맞추어서 변경 작업을 수행합니다. 이때 GKE 의 VPC native 가 enable 된 상태에서 Cloud SQL 을 private ip 로 작업했기 때문에 그냥 ip 만 변경해주면 되며 특별히 별도의 추가 설정을 안해도 되는 장점을 가지고 있습니다.</p><pre>vi src/main/resources/application.properties</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rwsmuGMtXIl2-DhT" /></figure><p>테스트를 위하여 mvn 을 이용해서 받아온 Spring boot 애플리케이션을 수행합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VuW2bWKHUJNKHDx5" /></figure><p>해당 애플리케이션을 수행후에 하단과 같이 REST API 호출을 수행하여 Cloud SQL 과 정상적으로 연동되어 서비스 결과를 가지고 오는지 확인합니다. 하단과 같이 정상적으로 return 값을 가지고 오면 Cloud SQL 에 문제없이 연동이 완료된 것 입니다.</p><pre>curl -v “<a href="http://localhost:8080/api/notes">http://localhost:8080/api/notes</a>&quot;</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eADwNkyHiLlz6vpc" /></figure><p>Mysql client 를 이용해서 직접 Cloud SQL 에 접속하여 API 호출을 통해서 정상적으로 데이터가 저장되었는지 확인합니다.</p><pre>mysql --host=172.16.3.3 --user=root --password</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*P3kuBIXgjZMZqnaysEhLXg.png" /></figure><p>확인 후 이상이 없다면 정상적으로 애플리케이션과 Cloud SQL 연동 테스트가 완료된 것이므로 GKE 에 배포를 위해서 Spring boot 애플리케이션을 jar 로 패키징 합니다.</p><pre>mvn package</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fgCrVnoJ8fgH8FBc" /></figure><p>성공적으로 패키징이 완료되었다면 ./target 폴더에 easy-notes-1.0.0.jar 파일이 생성된 것을 확인 할 수 있습니다. 해당 폴더에서 GKE 에 배포하기 위한 docker 생성을 위해서 Dockerfile 을 하단과 같이 만듭니다. (Spring boot 의 docker 를 만드는 부분에 대한 좀 더 자세한 내용이 필요하시면 Spring 의 공식 페이지를 참고하시기 바라겠습니다. — <a href="https://spring.io/guides/gs/spring-boot-docker/">https://spring.io/guides/gs/spring-boot-docker/</a> )</p><pre>FROM openjdk:8-jdk-alpine<br>EXPOSE 8080<br>ADD easy-notes-1.0.0.jar easy-notes-1.0.0.jar<br>ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;easy-notes-1.0.0.jar&quot;]</pre><p>Dockerfile 을 만들었으면 이제 하단의 명령을 사용해서 GCP 의 Cloud Build 를 통해서 실제 docker 이미지를 만들고 이를 Google Container Registry(GCR) 에 저장합니다.</p><pre>gcloud builds submit --tag gcr.io/jwlee-myproject-01/bootsample ./</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7OYlWJFpzDgQ_omr" /></figure><p>정상적으로 해당 작업이 완료되었다면 GCR 에서 하단과 같이 방금 생성되어서 저장된 docker 이미지를 확인할 수 있습니다. 해당 repository 를 클릭하면 tag 별로 좀 더 자세한 내용도 확인 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tn13EYM8SNoUT5ba" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gO3V0c6CbgMMtFIr" /></figure><p><strong>추신 #1</strong> : 참고적으로 Container Registry 의 설정에 들어가면 하단과 같이 Vulnerability scanning 기능을 enable 할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RfED1drHrdOuSQS1" /></figure><p>해당 기능을 enable 하게 되면 하단과 같이 Container 이미지가 생성될때 자동으로 GCR 에서 하단과 같이 취약점 점검을 수행해서 좀 더 보안적인 측면에서 점검할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wEnWos-7SfQppAkc" /></figure><p>이제 docker 이미지를 생성하고 GCR 에 저장했다면 마지막 단계로 실제 GKE 환경에 배포를 수행해보도록 합니다. 해당 부분은 지난 이야기에서 테스트 했던 것과 동일하게 kubernetes 의 CLI 도구인 kubectl 을 사용해서 수행하면 됩니다.</p><pre>kubectl run bootsample --image gcr.io/jwlee-myproject-01/bootsample --port 8080</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Spcf3f1fJKfWj7Ek" /></figure><p>다음으로 테스트를 위해서 service 를 LoadBalancer 타입으로 생성해서 외부로 노출하는 작업을 수행합니다.</p><pre>kubectl expose deployment bootsample --type LoadBalancer --port 80  --target-port 8080</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_fNRj6MXEcjnWeW7" /></figure><p>이제 GKE 로 서비스를 배포하고 노출했으니 하단과 같이 curl 이나 웹브라우저로 호출을 해보시면 정상적으로 GKE 환경에서 GCP 의 Cloud SQL 서비스를 호출해서 결과가 잘 나오는 것을 확인할 수 있습니다.</p><pre>curl -v “<a href="http://35.190.236.174:80/api/notes">http://35.190.236.174:80/api/notes</a>&quot;</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wor7c4596nz-aYUH" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*H1qfzS8KqVZIGz8h" /></figure><p>테스트 결과를 보시면 아시겠지만 GKE 에 애플리케이션을 배포하고 실제 서비스는 GCP 의 Cloud SQL 과 같은 외부 서비스를 사용한다고 해도, GKE 에 같은 VPC 를 공유할 수 있기 때문에 NAT 설정 과 같은 추가 작업 없이도 IP 를 통해서 외부 서비스를 바로 호출하고 사용할 수 있다는 것을 잘 확인 하실 수 있으실 것 입니다.</p><p>그럼 이번 이야기는 여기서 마무리 하도록 하겠으며 다음에 다른 주제를 가지고 다시 돌아오겠습니다. 그럼 이만 휘리릭~~~</p><p><strong>추신 #2</strong> : 모든 시스템이 그렇겠지만 GKE 도 하다보면 문제가 있을 수도 있으며 로그 분석이나 이슈 디버깅이 필요할 수도 있습니다. 타사와 다른 GKE 의 가장 강점중의 하나는 모든 로그를 Stackdriver 를 통해서 통합적으로 관리되므로 GUI 환경에서 편하게 로그를 확인할 수 있고 디버깅 가능하다는 점입니다.</p><p>예를 들어 GKE 의 pod 에서 이슈가 있다면 관리콘솔에서 하단과 같이 exit code 와 함께 모니터링이 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wUZFYetM3UYNKycT" /></figure><p>또한, 해당 pod 의 로그를 확인하고 싶다면 ‘Container logs’ 를 클릭하여 Stackdriver logging 화면으로 이동하여 자동으로 설정된 필터에 따른 해당 pod 의 로그를 모니터링 가능하므로 문제분석을 쉽게 시작 할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gGXO_CR6lTg_Tqb-" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fFVoZHK1_53SbeC5" /></figure><p>추가적으로 GKE 환경에 문제 발생 및 해결에 관련된 가이드 링크를 공유드리오니 좀 더 상세한 내용이 궁금하신 분은 하단의 링크를 참고하시기 바라겠습니다.</p><p><strong>Troubleshooting</strong><br><a href="https://cloud.google.com/kubernetes-engine/docs/troubleshooting?hl=en_US&amp;_ga=2.170271648.-1620698413.1544525164#CrashLoopBackOff">https://cloud.google.com/kubernetes-engine/docs/troubleshooting?hl=en_US&amp;_ga=2.170271648.-1620698413.1544525164#CrashLoopBackOff</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ae608b1b4338" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 2탄 — GKE 서비스 및 확장 해보기]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-2%ED%83%84-gke-%EC%84%9C%EB%B9%84%EC%8A%A4-%EB%B0%8F-%ED%99%95%EC%9E%A5-%ED%95%B4%EB%B3%B4%EA%B8%B0-5c9b137e72c8?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/5c9b137e72c8</guid>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[gcp]]></category>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sun, 02 Dec 2018 08:41:38 GMT</pubDate>
            <atom:updated>2018-12-13T02:01:31.897Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 2탄 — GKE 서비스 및 확장 해보기</strong></h3><p>안녕하세요 이정운 입니다.</p><p>지난번에 “<a href="https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-1%ED%83%84-gke-%EA%B0%9C%EC%9A%94-382dc69b2ec4">GKE 차근 차근 알아보기 1탄 — GKE 개요</a>” 를 통해서 간단하게 Google Kubernetes Engine(GKE) cluster 를 생성해보고 이를 통해서 GKE 개요에 대해서 살펴봤었는데 이번엔 조금 더 들어가서 지난번 이야기에서 생성한 GKE cluster 를 가지고 샘플 container 를 배포하고 간단하게 서비스가 동작하는 것을 살펴보도록 하겠습니다. 이를 통해서, GKE 안에서 어떻게 실제 Kubernetes 가 동작하는지를 보고/테스트 해보면서 이야기하는 시간을 가져 보도록 하겠습니다. 간단하고 작은 테스트이긴 하지만 이를 통해서 GKE 가 어떤 일을 할 수 있고 어떤 특성 및 장점을 가지고 있는지 파악이 가능하지 않을까 합니다.</p><p>기 언급 드렸지만 Kubernetes 에 대해서 일일이 설명하지는 않을 예정이며 GKE 입장으로 주로 이야기하도록 하겠습니다. 그러나 결국 GKE 의 동작을 설명 하다보면 아마도 그 얘기가 그 얘기가(?) 되지 않을까 합니다. ^^&amp;</p><p>늘 강조하지만 본 이야기는 하단과 같은 좋은 링크를 참고해서 작성되었고 테스트 되었습니다.</p><p>Kubernetes engine Quickstart<br><a href="https://cloud.google.com/kubernetes-engine/docs/quickstart">https://cloud.google.com/kubernetes-engine/docs/quickstart</a></p><p>Kubernetes 101: Pods, Nodes, Containers, and Clusters<br><a href="https://medium.com/google-cloud/kubernetes-101-pods-nodes-containers-and-clusters-c1509e409e16">https://medium.com/google-cloud/kubernetes-101-pods-nodes-containers-and-clusters-c1509e409e16</a></p><p>Viewing Pods and Nodes<br><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/">https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/</a></p><p><strong>#1) GKE 로 기본 서비스 해보기</strong></p><p>역시나 이번에도 이런 저런 설명을 길게 하는 것보다 직접 한번 해보는 것이 더 나으실거라 GKE 로 기본 서비스를 배포해서 테스트하는 것을 먼저 진행해보도록 하겠습니다. 이를 위해서 GCP 환경이라면 Cloud Shell 로 접속한 후 하단의 kubectl 명령을 통해서 샘플 container 를 GKE cluster 에 배포 합니다.</p><pre>kubectl run hello-server --image gcr.io/google-samples/hello-app:1.0 --port 8080</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UebNepZarvRnItGb" /></figure><p>여기서 기본 Kubernetes 를 해본 분들도 ‘gcr.io/google-samples/hello-app:1.0’ 라는 부분이 생소할텐데 이건 Google 의 비공개 container 저장소인 Google Container Registry(GCR) 의 주소 이며 더 정확히 들어가면 GCR 내의 hello-app 이라는 이미지의 주소입니다.</p><p>GCR 을 쉽게 말씀드리면 Docker 이미지를 관리하고, 취약성 분석을 수행할 수 있으며 세분화된 액세스 제어 기능으로 보다 높은 보안성을 제공할 수 있는 비공개 Docker 저장소 입니다. (말은 거창하지만 Google cloud 에서 쉽게 container 를 저장할 수 있는 비공개 저장소로 보시면 됩니다.)</p><p><a href="https://cloud.google.com/container-registry/">https://cloud.google.com/container-registry/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TTX-X3ONzjkRkWDf" /></figure><p>결국 방금 실행한 명령은 GCP 에 있는 비공개 Docker 저장소인 GCR 의 hello-app 이라는 샘플 Docker 이미지를 가지고와서 GKE cluster 에 배포 및 실행하는 명령어로 보시면 됩니다.</p><p>지금 수행한 kubectl 명령은 단순하게 run 을 수행하지만 내부적으로는 Kubernetes 의 deployment 를 생성하게 됩니다. Kubernetes 의 CLI 도구인 kubectl 을 통해서 명령어로 deployment 상태를 확인할 수도 있으며 하단과 같이 관리콘솔의 Workload 메뉴에서도 지금 배포한 deployment 의 상태를 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NaybBFAJgfV1l-6n" /></figure><p>또한, GUI 형태가 아니라 개발자들에게 친숙한 yaml 형태로도 바로 메뉴에서 확인이 가능합니다. 저도 해보면서 느꼈는데 kubectl 로 간단하게 GKE cluster 에 배포해도 필요한 설정들이 자동으로 기본 적용되어 yaml 은 생각보다 내용이 많았으며, 이를 조금 더 찬찬히 공부하게 되면 Kubernetes 를 더 이해하는데 도움이 되지 않을까 합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fW01dbDZGtVI9LKi" /></figure><p>테스트로 hello-app 이라는 container 를 GKE cluster 환경에 배포했으니 이제 이를 실제적으로 외부에서 서비스하기 위해 노출해주는 작업을 수행하도록 하겠습니다. 이번에도 kubectl 을 이용해서 하단과 같이 수행합니다. (Kubernetes 를 테스트해보시면 아시겠지만 생각보다 많은 작업이 kubectl 이라는 CLI 도구를 통해서 이루어집니다. 해보시면 작업은 이게 제일 편하긴 합니다.) 그리고 나서 ‘kubectl get service’ 명령을 통해서 service 를 확인해보면 지정된 이름으로 service 가 생성되고 LoadBalancer 타입이기 때문에 external-ip 가 매핑된 것을 확인할 수 있습니다.</p><pre>kubectl expose deployment hello-server --type LoadBalancer --port 80 --target-port 8080</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xLhkeAQp5B_H28FI" /></figure><p>그러면 이제 실제 서비스를 확인해보기 위해서 브라우저에서 external-ip 로 호출하면 하단과 같이 샘플 서비스의 서비스 결과가 잘 나오는 것을 확인할 수 있습니다.</p><p><a href="http://35.194.99.138:80/">http://35.194.99.138:80/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/752/0*LTYHuUNVXgU2xkTN" /></figure><p><strong>추신</strong>: Kubernetes 를 공부하다 보면 처음에 제일 헷갈리는 것이 service 타입으로 ClusterIP, NodePort, LoadBalancer, Ingress 등등 각 차이가 잘 이해가 안됩니다. 제가 봤을때 그 부분에 대해서 각각의 특징을 그림을 통해서 제일 잘 정리해둔 이야기가 하단의 글이지 않을까 하며 혹시 자세한 내용이 궁금하신 분들은 링크 참고하시기 바라겠습니다.</p><p>Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what?<br><a href="https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0">https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/913/0*_b_fO5eC5e8u0kiI" /></figure><p>브라우저를 이용해서 테스트 서비스가 잘 되는걸 확인했으면, 급 궁금해지는 부분은 “실제 LoadBalancer 역할은 누가 하고 있을까” 이지 않을까 합니다. 그래서 찾아보니 하단과 같이 GKE cluster 에서 LoadBalancer 객체를 만들때 자동으로 GCP 의 Internal Loadbalancer 를 생성해서 하단과 같이 IP 와 backend 설정까지 알아서 해주고 있더군요. (관리콘솔에서 GKE cluster &gt; Services &gt; Service 명 &gt; Details 에 생성한 Load Balancer 이름을 확인 가능합니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yn1Z7WCQEf2mRu3o" /></figure><p>추가적으로 방금 생성한 service 도 관리콘솔의 Services 메뉴에서 상태를 확인할 수 있으며 yaml 파일로도 확인 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6Wa_8HigyUfPQgXD" /></figure><p><strong>#2) GKE cluster 확장 하기</strong></p><p>이전 파트에서 간단하게 샘플 container 를 이용해서 GKE cluster 로 서비스를 배포하고 브라우저를 이용해서 서비스 테스트를 해봤습니다. 이제는 다음 단계로 해당 서비스를 이리 저리 확장해 보면서 GKE cluster 가 어떻게 동작하는지 확인 해보도록 하겠습니다. (사실 결국은 Kubernetes 가 어떻게 동작하는지 확인이 될듯 합니다.)</p><p>GKE cluster 에서 서비스를 수행하는 pod 의 확장은 굉장히 쉬우며 하단과 같은 kubectl 명령으로 바로 pod 를 확장 할 수 있습니다. (참고로 pod 는 Kuberentes 서비스의 최소 단위이자 집합이며 하나 이상의 Container 로 이루어 집니다. 이 부분이 더 궁금하신 분은 해당 링크를 참고하세요 — <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/">https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/</a> )</p><pre>kubectl scale deployment hello-server --replicas=3</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0GJ49Tnyv13UvrPk" /></figure><p>당연히, kubectl scale 명령만이 아니라 refplica 가 3 으로 설정된 deployment yaml 자체를 새로 만들어서 적용하는 방식을 통해서도 pod 확장을 수행할 수 있습니다. 이렇게 replica 의 개수를 증가하면 Kubernetes 는 내부적으로 ReplicaSet 을 통해 실제 pod 의 개수도 3개로 맞추게 됩니다.(ReplicaSet 은 지정된 수의 pod 복제본이 수행되도록 하는 기능을 가지고 있으며 좀 더 자세한 사항이 궁금한 분들은 이 링크를 참고하세요 – https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)</p><p>그런데 이때, pod 를 확장하면 생각하는 가장 쉬운 오류중의 하나는 pod 를 3개로 확장했을 때, 당연히 node 가 3개이니 고르게 잘 node 당 pod 하나씩 올라가서 구동할 것이라는 착각 입니다. 확인해 보시면 아시겠지만 kubernetes 는 그렇게 쉽게(?) 예전 분산 환경처럼 구동되지 않으며 Stateless 상태인 pod 의 사상에 맞추어서 구동됩니다. Kubernetes 입장에서는 별도로 지정된 정책이 없다면 node 를 보고 자원적으로 가능하다면 pool 에서 꺼내서 그냥 pod 를 위치 시킵니다. Pod 를 3개로 늘려달라는 명령이므로 그저 큰 고민 없이 node pool 안에 pod 를 3개만 만드는 거죠. (사실 scheduler 가 고민을 많이 하긴 하는데 지금은 그런 조건에 닿지 않을 것이기 때문입니다. 이리 저리 숫자를 변경해서 테스트 해보시면 아시겠지만 들쭉 날쭉 합니다.)</p><pre>kubectl get pod -o wide</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TIFdCJaDythfDHgx" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*J4_8XdQYkDXKIspG" /></figure><p>일반적인 분산 운영환경을 경험해보신 분들에게는 이해가 가지 않는 상황이겠지만 Kubernetes 입장에서 보면 현재로서는 문제도 없고 당연할 수도 있습니다. (기존 개념을 깨는게 이해가 더 편할수 있습니다 ^^&amp;)</p><p>만약 좀 더 분산환경에 적합하고 안정적으로 3 개의 pod 가 이쁘게(?) 3개의 Node 로 각각 분산되기를 원한다면 kubernetes 의 다양한 정책을 통해서 그렇게 구성할 수도 있습니다. 이러한 작업을 위해서 Kubernetes 에서는 라벨을 보고 pod 를 지정된 node 로 배포할 수 있는 nodeSelector 를 제공하며, 이에 추가하여 조금 복잡하기는 하지만 사용자가 다양한 조건을 지정해서 조절할 수 있는 affinity 와 anti-affinity 를 node 단위와 pod 단위 정책으로 적용하여 사용할 수도 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*29xh8ctuwpyCuyga" /></figure><p>예를 들어 상단과 같이 podAntiAffinity 를 적절히 만들어서 기존 배포된 환경에 적용하게 되면 하단과 같이 3개의 pod 가 3개의 node 에 분산되어서 서비스 가능하게 만들 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EwILljVLO34nFu3J" /></figure><p>이러한 Advanced scheduling 에 대해서 조금 더 자세한 내용이 궁금하신 분들은 하단의 링크를 참고하시기 바라겠습니다.</p><p>Advanced Scheduling and Pod Affinity and Anti-affinity<br><a href="https://docs.openshift.com/container-platform/3.6/admin_guide/scheduling/pod_affinity.html">https://docs.openshift.com/container-platform/3.6/admin_guide/scheduling/pod_affinity.html</a></p><p>마지막으로 pod 에 대한 확장을 넘어서 Node 에 대한 확장을 원하는 경우가 있을 수 있습니가. 이런 경우라면 GCP 의 관리콘솔에서 GKE cluster 를 선택하고 Edit 를 클릭한 후에, 하단에서 보는 것과 같이 Size 만 변경하시면 Node 에 대한 확장 및 축소를 아주 손쉽게 하실 수 있습니다. (각자 편한 방식을 사용하면 됩니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*o_f_HvNi7LHQNXy_" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JNFRlUcBAbdVVpku" /></figure><p>여기까지 잘 따라오셨다면 “GKE 차근 차근 알아보기 2탄 — GKE 서비스 해보기” 를 무사히 테스트 해보신 것입니다. 이미 이야기를 보셔서 아시겠지만 크게 복잡한 내용을 다루기 보다는 간단한 샘플 container 를 실제적으로 만들어둔 GKE cluster 에 배포하고 서비스를 해보는 형태로 진행 하였습니다. 그리고 이에 덧붙여 Kubernetes 의 장점 중의 하나인 pod 나 node 를 확장하면서 기본적으로 GKE cluster 가 어떻게 동작하는지를 살펴봤습니다.</p><p>아주 쉽고 간단하게 테스트 해봤지만 GKE 를 사용하는 기본적인 방법으로 향후 실제로 GKE 를 사용해서 운영하실 때에도 많이 사용하지 않을까 합니다. 그럼 이번 이야기는 여기서 마무리 하고 다음에 다른 이야기로 다시 돌아오겠습니다. 그럼 이만 휘리릭~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5c9b137e72c8" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[GCP]GKE 차근 차근 알아보기 1탄 — GKE 개요]]></title>
            <link>https://medium.com/@jwlee98/gcp-gke-%EC%B0%A8%EA%B7%BC-%EC%B0%A8%EA%B7%BC-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-1%ED%83%84-gke-%EA%B0%9C%EC%9A%94-382dc69b2ec4?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/382dc69b2ec4</guid>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[google]]></category>
            <category><![CDATA[gke]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Sat, 24 Nov 2018 11:11:12 GMT</pubDate>
            <atom:updated>2018-11-24T11:11:12.389Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>[GCP]GKE 차근 차근 알아보기 1탄 — GKE 개요</strong></h3><p>안녕하세요 이정운 입니다.</p><p>정신 없이 바쁜 시간이 좀 지나가고 근질거리는 몸을 부여잡고, 이번에 공부도 할 겸 정리하고자 하는 부분은 Google cloud 의 관리형 kubernetes 서비스인 Google Kubernetes Engine(GKE) 입니다. Kubernetes 는 이미 잘 알고 계시겠지만 Container 를 위한 오픈소스 Orchestrator 로 이미 많은 분들이 관심을 가지고 있고 사용하고 있는 솔루션입니다. GKE 는 Kubernetes 를 쉽게 사용자가 활용할 수 있도록 관리형으로 제공하는 Google cloud 의 솔루션으로 하단과 같이 다양한 기능을 제공 가능합니다.</p><p><a href="https://cloud.google.com/kubernetes-engine/">https://cloud.google.com/kubernetes-engine/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IadcADXQkOKZ0Wl5" /></figure><p>결국, GKE 는 Container 애플리케이션 배포를 위한 관리형 환경으로 이를 통해서 개발자 생산성, 리소스 효율성, 자동화된 작업, 오픈소스 유연성을 얻을 수 있으며 특히 Google 은 15년 이상 Container 로 운영 환경 작업을 실행해 왔으며 이러한 작업으로부터 얻은 노하우의 정수가 GKE 의 기반을 이루고 있습니다.</p><p>그렇다고 GKE 가 Kubernetes 환경을 모두 Google cloud 에서 관리형으로 제공하는 것은 아니고 운영환경에서 변화의 영역이 적은 Master Node 에 대한 완전 관리 서비스를 제공하며, 실제 서비스를 수행하는 Container 의 집합인 Pod 가 올라가서 운영되는 Worker Node 는 반 관리형(?)으로 설정만 하면 기본 Kubernetes 에 필요한 컴포넌트를 준비해둔 GCE(VM) 를 제공하여 개발자가 직접 Pod 를 올리거나 작업을 수행할 수 있도록 지원합니다.</p><p><a href="https://cloud.google.com/solutions/distributed-load-testing-using-kubernetes">https://cloud.google.com/solutions/distributed-load-testing-using-kubernetes</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kD3YgVaI416z8I2p" /></figure><p>GKE 관련 이야기를 진행하다 보면 결국 Kubernetes 이야기가 많이 필요하겠지만 이미 하단과 같이 훌륭한 분들이 정리를 잘해주셔서 저는 가급적 Kubernetes 의 기본 내용은 제외하고 GKE 관점에서 이야기 진행을 해볼까 합니다. 따라서, Kubernetes 를 시작하는 분들은 하단의 블로그들을 먼저 살펴 보기를 권장드리며 기본적으로 Kubernetes 에 대한 기본 개념은 있는 분들을 대상으로 본 이야기는 진행하도록 하겠습니다.</p><p>Kubernetes #1 — 소개 — 조대협의 블로그<br><a href="http://bcho.tistory.com/1255">http://bcho.tistory.com/1255</a></p><p>Kubernetes Engine from scratch<br><a href="https://medium.com/google-cloud/kubernetes-engine-from-scratch-2c2369c46841">https://medium.com/google-cloud/kubernetes-engine-from-scratch-2c2369c46841</a></p><p>늘 강조드리지만 저는 그렇게 창조적이거나 똑똑하지 않기 때문에 이번 강좌도 하단과 같은 좋은 자료를 기반으로 작성하였습니다.</p><p>Google Kubernetes Engine Documentation<br><a href="https://cloud.google.com/kubernetes-engine/docs/">https://cloud.google.com/kubernetes-engine/docs/</a></p><p>GKE Quickstart<br><a href="https://cloud.google.com/kubernetes-engine/docs/quickstart">https://cloud.google.com/kubernetes-engine/docs/quickstart</a></p><p>Creating VPC-native clusters using Alias IPs<br><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips">https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips</a></p><p>Medium — GKE<br><a href="https://medium.com/google-cloud/tagged/gke">https://medium.com/google-cloud/tagged/gke</a></p><p>Kubernetes in the Google Cloud<br><a href="https://www.qwiklabs.com/quests/29?locale=en">https://www.qwiklabs.com/quests/29?locale=en</a></p><p>그럼 “백문이 불여 일타!” 한번 시작해 보도록 하겠습니다.</p><p><strong>#1) GKE cluster 생성 하기</strong></p><p>가장 먼저 Google cloud 에서 GKE 를 생성하는 부분부터 이야기를 시작하고자 합니다. 설명도 없이 벌써 생성인가 생각하시는 분들도 있으실 수 있지만 GKE 는 굉장히 쉽게 생성할 수 있기 때문에 뒷 이야기를 보면 바로 그 이유를 알 수 있으실 것 입니다. Google cloud 의 관리콘솔에 가서 Kubernetes Engine 을 선택하면 하단과 같은 화면을 확인할 수 있으며 ‘create cluster’ 라는 메뉴를 통해서 GKE cluster 를 생성할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*e8dUnRKr5U385AJ1" /></figure><p>‘create cluster’ 메뉴를 클릭하면 하단과 같이 GKE cluster 생성 마법사가 나옵니다. 가장 먼저 왼쪽의 메뉴를 보면 여러 종류의 클러스터를 큰 카테고리로 선택 가능합니다. 해당 메뉴는 다양한 운영환경을 고려하여 그에 적합한 옵션들이 사전 선택된 템플릿으로 보면 되며 해당 카테고리를 선택하면 자동으로 해당 설정들이 오른쪽 세부 설정에 들어가게 됩니다. 예를 들어 GPU Accelerated Computing 이면 자동으로 노드에 사용되는 GCE(VM)에 GPU 가 자동 설정으로 추가 됩니다. 그렇다고 완벽히 고정된 템플릿은 아니라 가이드 정도이며 세부 설정은 필요한 경우 추가 변경 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*XVl9HqRgvkb8-G3P" /></figure><p>우측의 세부 설정에는 GKE 를 Zonal 형태로 만들지 Regional 형태로 만들지 Location type 에 대한 설정 및 버전, Node pool 등을 지정할 수 있습니다. GKE 는 zone 이라는 Google cloud 의 데이터 센터에서 마스터 노드가 하나 존재하는 Zonal 형태가 있고 GKE 의 마스터 노드에 대한 고가용성을 추가하고자 한다면 Region 안의 여러 zone 별로 마스터를 두는 Regional 형태의 GKE cluster 를 선택할 수 있습니다. (Google cloud 의 Zone, Region 의 개념을 좀 더 자세히 알고 싶으신 분들은 링크를 참고하시기 바라겠습니다. — <a href="https://cloud.google.com/docs/geography-and-regions">https://cloud.google.com/docs/geography-and-regions</a>)</p><p>With Google Kubernetes Engine regional clusters, master nodes are now highly available<br><a href="https://cloud.google.com/blog/products/gcp/with-google-kubernetes-engine-regional-clusters-master-nodes-are-now-highly-available">https://cloud.google.com/blog/products/gcp/with-google-kubernetes-engine-regional-clusters-master-nodes-are-now-highly-available</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*54FrZb-G3TSaWDfa" /></figure><p>다음으로 Node pools 에 대한 설정입니다. Node pools 는 이름 그대로 Node 의 집합이며 Node 는 결국 GCE(VM) 이기 때문에 이미지 타입과 머신 타입등을 설정하게 되어 있습니다. 메뉴를 보시면 아시겠지만 어떤 GCE(CPU, Memory 등) 를 사용할지와 Node 개수만 넣어주면 간단하고 쉽게 설정을 완료할 수 있습니다. 또한, 하나가 아닌 여러 Node pools 를 만들어서 GKE 버전이나 GCE 머신 타입을 다르게 구성하여 다양한 환경을 위한 GCE 그룹을 운영하는 것도 가능합니다.</p><p>Cutting costs with Google Kubernetes Engine: using the cluster autoscaler and Preemptible VMs<br><a href="https://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms">https://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Y4uAXV4Tlosd9Wmd" /></figure><p>그리고 필요하다고 하면 좀 더 유연하게 GKE cluster 의 Node pool 에 대한 Autoscaling 기능을 사용할 수도 있습니다. 즉, 요청에 따라서 Node 자체를 동적으로 늘리거나 줄일 수 있다는 의미입니다. (해당 옵션을 켜보시면 아시겠지만 최소/최대 node 수를 입력하는 화면이 나옵니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/940/0*3Px2JsKOg3d7xgAi" /></figure><p>Node pools 메뉴를 닫고 상세 메뉴에서 고급 옵션을 펼치면 GKE cluster 에 대한 세부 설정이 가능합니다. 여기서는 중요한 포인트 몇 가지만 살펴보도록 하겠습니다. 먼저 GKE 는 관리형 Kubernetes 서비스이기 때문에 귀찮으면서도 중요한 작업들은 자동화된 기능을 제공합니다. 그 중에 대표적인 기능이 Auto Upgrade 기능입니다. Kubernetes 는 많은 관심을 받고 빠르게 성장하고 있습니다. 그래서 업데이트가 중요한데 이를 매번 수동으로 업데이트 해야하는 고민을 없애주는 기능이 Auto Upgrade 기능입니다. 그리고 Maintenance windows 는 Auto upgrade 를 수행할 시간을 지정할 수 있는 기능입니다. (일반적으로 부하가 가장 적은 새벽으로 해두면 되겠죠. 또는 자동이 아닌 설정 변경을 통해서 수동으로 하실 수도 있습니다.)</p><p>다음으로 Network 에 VPC-native 기능에 대한 설정을 확인할 수 있는데 해당 기능은 타사의 관리형 Kubernetes 와 다르게 Google cloud 만이 제공할 수 있는 기능으로 별도의 NAT 없이도 독립적인 Kubernetes 의 네트워크 구조를 사용하는 GKE 를 다른 Google cloud 서비스와 바로 연동할 수 있도록 Google cloud 의 VPC 와 엮는 옵션입니다. 언급드린 것처럼 이를 enable 하면 추가 작업 없이 다양한 Google cloud 서비스와 VPC 를 통한 통신이 가능하므로 바로 internal IP 로 호출할 수 있습니다. (향후 이야기 진행을 위해서는 꼭, enable 하시기 바라겠습니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/942/0*sUOAYmdE8gTjhyEU" /></figure><p>또한, 별도 설치 작업없이 Network policy 설정을 바로 enable 가능하여 GKE cluster 에서 Project Calico 를 통해서 네트워크 정책을 적용하여 보다 유연하고 안전한 설정을 하실 수 있습니다. 그 뒤에 나오는 Network security 항목은 엔터프라이 환경에 적합하도록 보안을 추가해서 네트워크를 설정을 할 수 있는 기능으로 예를 들어 보안화된 엔터프라이즈 환경을 위해서 인터넷에서는 접근이 안되는 private cluster 로 만들거나 특정 IP 만 master node 에 접근하도록 허용하는 기능들에 대한 것으로 좀 더 자세한 내용이 궁금하신 분들은 하단의 블로그를 참고하시기 바라겠습니다.</p><p>Bringing enterprise network security controls to your Kubernetes clusters on GKE<br><a href="https://cloud.google.com/blog/products/networking/bringing-enterprise-network-security-controls-to-your-kubernetes-clusters-on-gke?fbclid=IwAR2dUiOcb-wVDnaiXHfzMmwsIf_u8nGav6lZDf-Uykq-ecVv5DOP_w5UZ9M">https://cloud.google.com/blog/products/networking/bringing-enterprise-network-security-controls-to-your-kubernetes-clusters-on-gke?fbclid=IwAR2dUiOcb-wVDnaiXHfzMmwsIf_u8nGav6lZDf-Uykq-ecVv5DOP_w5UZ9M</a></p><p>언급한 것처럼 다양한 보안에 대한 옵션을 선택할 수 있으며 추가적으로 Stackdriver logging 과 monitoring 을 바로 연동할 수 있는 옵션도 포함되어 있습니다. 저 개인적으로는 GKE 를 테스트해보면서 다른 타사 솔루션 대비 가장 나은점 중의 하나는 Stackdriver 연동이 바로 가능해서 전체 Google cloud 의 다른 솔루션을 로깅/모니터링하는 것과 동일하게 GKE 도 로깅/모니터링 할 수 있으며 통합 대시보드 형태로 모니터링 하거나 관리할 수 있다는 점 입니다. (추가로 Beta 이긴 하지만 새로운 Kubernetes 통합 모니터링 기능도 사용할 수 있습니다.)</p><p>그리고 마지막으로 원하신다면 GKE cluster 의 node 에 머신러닝을 위해서 GPU 가 아니라 TPU 를 사용할 수도 있습니다. (GKE cluster 환경을 이용해서 ML 작업을 하신다면 유용하겠죠.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/978/0*gtGe3_WVzB5hmWG8" /></figure><p>이렇게 설정을 하고 완료하면 하단과 같이 GKE cluster 가 아주 쉽게 5분내로 만들어지는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*sIvZJmSmDfA2_Lat" /></figure><p>어때요 처음 이야기 드린 것처럼 아주 쉽게 시작하실 수 있죠? ^^&amp;</p><p><strong>#2) GKE cluster 살펴보기</strong></p><p>그럼 이제 생성되어진 GKE cluster 를 하나씩 살펴보는 시간을 가지도록 하겠습니다. 생성된 GKE cluster 의 이름을 클릭하면 하단과 같이 상세 정보를 확인할 수 있습니다. Master node 버전이라던가 사이즈등 설정에 넣었던 정보를 여기서 한눈에 확인할 수 있습니다. 당연히 Node pools 에 대한 상세 정보도 하단에서 확인 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VRLLaiNG-hqctCxL" /></figure><p>Storage 탭을 클릭하면 GKE cluster 에 할당된 storage 를 확인할 수 있으며 기본 형태로 생성했다면 standard 라는 이름으로 PD(Persistent Disk) 가 할당되어 있는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fANUDxYXIPwg_mj2" /></figure><p>Nodes 탭을 클릭하면 Node 명과 기본적인 정보들을 확인할 수 있으며 이전에 GKE cluster 를 만들때 기본 사이즈 3으로 설정했기 때문에 3개의 Node 를 확인가능 합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jo50Ups2HFJyD7jB" /></figure><p>각 Node 를 클릭하면 좀 더 상세한 모니터링 정보와 Pods 정보를 하단과 같이 확인 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KVhwMTXuMMbjO-fS" /></figure><p>당연히 여기서 추가로 Pod 이름을 클릭하면 Pod 의 상세 정보 및 모니터링 화면을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YQRFzUIVAbDZtEkf" /></figure><p>이러한 GKE cluster 의 모니터링 정보는 Stackdriver Monitoring 이 훨씬 상세하고 잘 나오며 통합 대시보드로 연동도 가능하기 때문에 해당 도구를 추천 드립니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IGIdflSTgYiVPHx9" /></figure><p>특히 이전에 잠깐 언급드린 새로운 Kubernetes 통합 모니터링 기능 Beta 기능을 enable 했다면 Resources &gt; Kubernetes 를 통해서 하단과 같이 보다 직관적이며 통합적인 GKE 모니터링 기능을 사용할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HQSt4x6boLSMejDm" /></figure><p>다음으로 Workloads 메뉴를 선택하면 이름 그대로 GKE cluster 의 Deployment 나 Job 과 같은 워크로드를 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9EC-dYxoity7IA2S" /></figure><p>그 밑에 있는 Applications 메뉴는 GKE cluster 를 조금 더 쉽게 활용하게 할 수도 있고 엔터프라이즈 환경에 보다 적합하게 보이는데 Marketplace 에서 파트너를 통해서 제공되는 다양한 Kubernetes 애플리케이션을 설치해서 바로 사용할 수 있는 메뉴입니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-br1Bry631vB9KFh" /></figure><p>‘Deploy from Marketplace’ 버튼을 클릭하면 일반적인 3rd party 솔루션을 Google cloud 에 바로 설치할 수 있는 Marketplace 와 동일하지만 Kubernetes 환경인 GKE cluster 에 클릭만으로 설치해서 사용할 수 있는 Kubernetes 애플리케이션들을 확인하실 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nUDo2ZnFqNeqvoaD" /></figure><p>Services 는 메뉴명 그대로 service 와 service type, endpoints 등을 일목요연하게 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LNX1-_egzjQrWJj9" /></figure><p>Configuration 메뉴는 Container 의 환경 설정 정보등이 담긴 config map 과 secret 정보를 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dAeXX3sQw1YCPA8h" /></figure><p>마지막으로 Storage 메뉴는 Kubernetes 의 Persistent volume claims 와 storage classes 를 확인 가능합니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dau6-AqeR2VSVJjq" /></figure><p><strong>#3) GKE cluster 에 CLI 접근해보기</strong></p><p>관리 콘솔을 통해서 GKE cluster 를 생성해보고 다양하게 살펴봤으니 이제는 마무리로 개발자들이 많이 사용하는 방식인 CLI 로 접근해보고 살펴보는 시간을 가져보도록 하겠습니다. 이를 가장 쉽게 하는 방법은 GKE cluster 메뉴에서 Connect 메뉴를 선택한 다음 ‘Run in Cloud Shell’ 을 클릭하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*n_Wfd_jwKF7dv_AE" /></figure><p>그러면 브라우저 화면에서 Google cloud 의 장점인 Cloud shell 이 실행된 후 자동으로 GKE credential 을 받아오는 명령이 수행되며 그 이후부터는 편하게 kubectl 명령을 이용해서 GKE cluster 답게 Kubernetes 의 모든 액션을 수행할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VO0He_4NNYR27P-h" /></figure><p>어때요? 간단하게 GKE cluster 를 잘 살펴보셨나요? 보시면 아시겠지만 GKE 는 관리형 Kubernetes 기능에 충실하게 개발자나 사용자가 쉽게 Kubernetes 를 활용하고 운영할 수 있는 다양한 기능을 제공하고 있습니다. 기 언급한 것처럼 오늘 준비한 이야기는 GKE cluster 를 살펴보는 것 이었으며 이정도면 개략적으로 살펴보는 것은 충분한 것 같아 준비한 이야기는 여기까지로 마무리 하도록 하며 다음에는 조금 더 자세한 이야기로 찾아오도록 하겠습니다. 그럼 이만 휘리릭~~~</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=382dc69b2ec4" width="1" height="1">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[G Suite]G Suite 에 원하는 기능을 추가 할 수 있는 Google App Script(GAS) 살펴보기]]></title>
            <link>https://medium.com/@jwlee98/g-suite-g-suite-%EC%97%90-%EC%9B%90%ED%95%98%EB%8A%94-%EA%B8%B0%EB%8A%A5%EC%9D%84-%EC%B6%94%EA%B0%80-%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-google-app-script-gas-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-de130645de07?source=rss-47ecf5e5c7f1------2</link>
            <guid isPermaLink="false">https://medium.com/p/de130645de07</guid>
            <category><![CDATA[g-suite]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[gas]]></category>
            <category><![CDATA[google]]></category>
            <dc:creator><![CDATA[이정운 (Jungwoon Lee)]]></dc:creator>
            <pubDate>Tue, 20 Nov 2018 14:48:08 GMT</pubDate>
            <atom:updated>2018-11-20T14:48:08.790Z</atom:updated>
            <content:encoded><![CDATA[<p>안녕하세요 이정운 입니다.</p><p>간만에 시간이 좀 생겨서 그간 생각하고 있던 것중에 하나를 꺼내서 간단히 정리해보고자 합니다. 오늘 이야기 해보고 테스트 해볼 주제는 G Suite 에서 원하는 기능을 직접 개발해서 추가 할 수 있는 Google App Script(GAS) 입니다.</p><p>일반적으로 GMail 이나 Google Drive, Docs, Sheets 등의 G Suite 제품은 커스터마이징이 아예 안되는 것으로 알고 있을 수도 있지만 사실 원하신다면 GAS 라는 Javascript 개발을 통해서 필요한 기능을 개발해서 추가 할 수 있습니다. GAS 는 G Suite 와 통합되는 비즈니스 애플리케이션을 빠르고 쉽게 만들 수있는 신속한 애플리케이션 개발 플랫폼으로서 Javascript 로 코드를 작성하면 Gmail, Calendar, Drive 등과 같은 G Suite 애플리케이션을 위한 내장 라이브러리에 액세스 할 수 있으며 필요한 기능을 추가 개발 가능합니다.</p><p>Overview of Google Apps Script<br><a href="https://developers.google.com/apps-script/overview">https://developers.google.com/apps-script/overview</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZFez6oOjewL4fYRX" /></figure><p>Google 에서는 이런 GAS 를 쉽게 개발하고 활용할 수 있도록 G Suite Developer Hub 를 제공하며 여기서 바로 브라우저 코드 편집기를 통해서 작업하고 스크립트는 별도 작업 없이 Google 서버에서 실행할 수 있는 기능을 제공합니다.</p><p>G Suite Developer Hub<br><a href="https://sciprt.google.com">https://sciprt.google.com</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TTOO-km6QefyCEM3" /></figure><p>본 이야기를 진행하면서 개발이라고 해서 어렵게 생각하시는 분들도 있겠지만 Javascript 자체가 조금은 쉽게 사용할 수 있는 개발 언어이며 하단의 이야기를 따라가 보면 아시겠지만 실제로 개발하는 부분은 생각만큼 어렵지 않고 다양한 샘플들이 존재하므로 적절히 활용하면 G Suite 에서 조금은 개인이 원하는, 자동화되는 기능을 추가 할 수 있지 않을까 합니다.</p><p>Google Apps Script Samples<br><a href="https://github.com/gsuitedevs/apps-script-samples">https://github.com/gsuitedevs/apps-script-samples</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KyJmU9-7C4mQv-mD" /></figure><p><strong>#1) Google App Script(GAS) 테스트 해보기</strong></p><p>Google App Script(GAS) 가 실제로 어떤 역할을 할 수 있는지 이해하는 것은 한번 직접 샘플 테스트를 돌려보는게 제일 빠르지 않을까 합니다. 이를 위해서 간단하게 하단의 링크에 있는 내용들을 참고하여 샘플 테스트를 바로 해보도록 하겠습니다.</p><p>Create Time-driven Trigger in Gmail Add-on using Google Apps Script<br><a href="https://goolgedev.blogspot.com/2018/05/create-time-driven-trigger-in-gmail-add.html?m=1">https://goolgedev.blogspot.com/2018/05/create-time-driven-trigger-in-gmail-add.html?m=1</a></p><p>Take Control of Gmail, Only Check it Once Per Hour with Google Apps Script<br><a href="https://www.bettercloud.com/monitor/the-academy/take-control-of-gmail-only-check-it-once-per-hour-with-google-apps-script/">https://www.bettercloud.com/monitor/the-academy/take-control-of-gmail-only-check-it-once-per-hour-with-google-apps-script/</a></p><p>본 이야기에서 진행할 GAS 샘플 테스트는 상단의 링크를 참고하여 Gmail 에서 특정 문구가 들어간 메일을 가지고 간단히 “filter” 라는 label 을 설정하는 필터를 만든 후에 GAS 를 사용하여 해당 메일을 읽어들여 읽음 표시를 추가하고 자동으로 특정 사용자에게 포워딩해보도록 하겠습니다. (필터에서도 메일 자동 포워딩이 되긴 하지만 조금 다르게 해보고 싶기도 하고 하나의 예시로 보면 좋을듯 합니다. 참고로 혹시 Gmail 의 필터 기능에 대해서 잘 모르시는 분들은 이 링크 참고하시기 바라겠습니다. — <a href="http://ohknow.tistory.com/54">http://ohknow.tistory.com/54</a> )</p><p>이를 조금 쉽게 단계 별로 이야기 하면 “메일을 받으면 조건에 따라 필터를 만들어 두어서 해당 메일에 ‘filter’ 라는 라벨 생성 -&gt; GAS 를 사용하여 메일 중에 ‘filter’ 라는 라벨이 있으면 포워딩 하고 라벨삭제 -&gt; 해당 GAS 를 트리거로 등록하여 10분마다 자동 수행” 이런 구조가 될듯 합니다.</p><p>그럼 먼저 Gmail 에서 필터를 만드는 것으로 시작하도록 하겠습니다. 이를 위해 본인 계정의 Gmail 의 설정 메뉴 &gt; 필터 및 제한 조건으로 가서 ‘새 필터 만들기’ 를 클릭하면되며 필터를 적용할 조건을 넣어주면 됩니다. 참고로 모든 받은 메일에 해당 필터를 적용하고자 하는 경우에는 메일 상에는 나오지 않을 단어를 ‘제외할 단어’ 에 추가하는 꼼수(?)를 사용하면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CPxkGr_2JiQeuZSA" /></figure><p>다음으로 ‘계속’ 을 클릭하면 필터에서 적용한 다양한 액션을 선택할 수 있는데 테스트를 위해서 ‘받은편지함 건너뛰기(보관)’ 와 ‘라벨 적용’ 을 설정해주시면 됩니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Z1o8e4Mj3KNKECZm" /></figure><p>그럼 하단과 같은 설정이 완료되고 조건에 맞는 메일을 받으면 자동으로 지정된 ‘filter’ 라는 라벨이 해당 메일에 추가되는 것을 확인할 수 있습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*siG0iZO344_KuIJh" /></figure><p>다음으로 실제 Google App Script(GAS) 를 작성해 보기 위해서 <a href="https://script.google.com">https://script.google.com</a> 으로 접속합니다. 그러면 하단과 같이 G Suite Developer Hub 로 접속이 가능하며 별도의 도구 없이 이 안에서 App Maker 나 GAS 를 작성할 수 있습니다. (App Maker 는 G Suite 에서 템플릿, 드래그 앤 드롭 UI 디자인 및 선언적 데이터 모델링이 제공되어 IT 개발자는 물론 아마추어 코더도 쉽게 팀의 역량을 강화해 주는 앱을 구축할 수 있는 도구로서 G Suite Business 및 Enterprise 에 기본 포함되어 있습니다 . (<a href="https://gsuite.google.com/intl/ko/products/app-maker/">https://gsuite.google.com/intl/ko/products/app-maker/</a>) 나중에 기회가 되면 App Maker 를 소개하도록 하겠으며 여기서는 우선 GAS 를 선택하도록 합니다.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*b42CbUoOer9OiMEn" /></figure><p>그러면 브라우저에서 새로운 창이 열리면서 하단과 같이 브라우저 상에 코드 편집기가 나옵니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/964/0*3rzS6Ktwm55tqhh4" /></figure><p>코드 편집기의 코드 입력창에 하단과 같이 Javascript 로 되어 있는 소스를 넣어 줍니다.</p><p>//함수 이름<br>function myTrigger() {<br>// 로그 출력<br>console.log(“Trigger on work.”);<br>// 메일에서 filter 라는 라벨이 있는 메일을 찾음<br>var label = GmailApp.getUserLabelByName(“filter”);<br>var count = 0;</p><p>if (label) {<br>// filter 라는 라벨이 있는 메일이 하나가 아닐수 있으므로 배열로 가지고 옮<br>var threads = label.getThreads();<br>for (var i = 0; i &lt; threads.length; i++) {<br>++count;<br>var thread = threads[i];</p><p>// 배열에서 실제 개별 메세지를 가지고 옮<br>var message = thread.getMessages()[0];<br> // 메일을 읽음 표시로 변경<br>message.markRead();<br>/메일을 포워딩<br>message.forward(“minsoo@xxxxxxxx.co.kr”);<br>// label 을 삭제<br>thread.removeLabel(label);<br>} }<br>console.log(‘completed myTrigger() : ‘ + count);<br>};</p><p>소스를 작성했으면 저장을 하고(디스크 모양 아이콘) 테스트로 수행을 해봅니다.(삼각형 모양 아이콘) 문제가 있다면 디버깅 해볼 수도 있습니다.(벌레 모양 아이콘) 참고적으로 GAS 작성 후 맨 처음 수행시에는 개인 계정의 메일 박스에 접근하므로 권한 요청 화면이 나오는데 이때는 동의를 눌러주시면 됩니다.</p><p><strong>추신 #1</strong> : GAS 작성 화면에서 보기 &gt; Stackdriver 로깅 메뉴를 클릭하면 Google Cloud Platform(GCP) 의 Stackdriver 로그 기록 화면으로 새창이 열리고 접속 가능합니다. 그리고 그 안에서 GAS 에서 찍은 다양한 로그를 확인 가능합니다. (아마도 GCP 를 하시는 분들은 보자 마자 G Suite 의 GAS 와 GCP 가 어떻게 연결되었는지 이해가 될듯 합니다. 참고로 Stackdriver 로깅에 대해서 좀 더 알고 싶으신 분들은 이 링크를 참고하세요 — <a href="https://medium.com/@jwlee98/gcp-stackdriver-logging-monitoring-%EA%B8%B0%EB%B3%B8-6050acc4695d">https://medium.com/@jwlee98/gcp-stackdriver-logging-monitoring-%EA%B8%B0%EB%B3%B8-6050acc4695d</a> )</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rxL7lviBV4F4853y" /></figure><p><strong>추신 #2</strong> : GAS 메뉴에서 보기 &gt; 메니페스트 파일 표시를 클릭해서 Manifest 파일 정보를 확인하고 필요한 경우에 수정하실 수도 있습니다. 참고로 Manifest 파일은 기본적으로 GAS 수행을 위한 프로젝트 정보를 담고 있는 파일로 이해하시면 됩니다.</p><p>Manifests<br><a href="https://developers.google.com/apps-script/concepts/manifests">https://developers.google.com/apps-script/concepts/manifests</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/938/0*bqyIiZGoDTYZ7svz" /></figure><p>테스트를 해봤을때 수동이긴 하지만 우리가 원하는 형태대로 정상적으로 GAS 가 동작된다면 이제 주기적으로 해당 GAS 를 자동으로 수행하도록 하기 위하여 트리거 아이콘(시계 모양 아이콘)을 클릭하여 트리거를 만드는 메뉴로 이동합니다.</p><p>다음으로 실행할 함수로 방금 작성한 GAS 의 함수명을 넣어주며 ‘시간 기반’ 으로 ‘10분마다’ 동작하는 트리거를 하나 만듭니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R_sKjxs2Q9iQCtT7" /></figure><p>이렇게 해두면 해당 GAS 와 트리거가 알아서 Google 서버에서 동작합니다. 즉, 트리거에 따라 10분마다 만들어둔 GAS 함수를 호출하여 받은 편지함에 ‘filter’ 라는 라벨이 있으면 메일을 자동으로 포워딩하고 해당 라벨을 제거합니다. 여기까지 완료하시면 간단한 GAS 샘플 애플리케이션을 작성 및 돌려본 것이고 이를 통해 GAS 동작구조가 대략적으로 이해가 되지 않았을까 생각합니다.</p><p>어때요 굉장히 쉽죠? 중간에 개발 or 프로그래밍이 들어가므로 조금 어렵게 보시는 분들도 있겠지만 기 언급한 것처럼 GAS 는 Javascript 기반이라 그나마 직관적인 개발이 가능하며 구글링 해보면 많은 예제를 참고하실 수 있으므로 그 예제를 활용해서 직접 다 짜는 것이 아니라 간단한 수정만 해서도 사용할 수 있는 경우가 많이 있습니다. 무엇보다도 G Suite 의 좋은 기능에 더해서 내가 원하는 기능을 간단하게 커스터마이징 하거나 추가할 수 있으므로 프로그래밍을 조금 할줄 아는 분들에게는 더 유용한 방법을 제공할 수 있습니다. 특히나 수작업을 자동화하는데는 최적입니다. ^^&amp;</p><p>그럼 오늘 이야기는 여기까지 마무리 하는 것으로 하고 이번 이야기는 이만 줄이도록 하겠습니다.</p><p><strong>추신 #3</strong> : G Suite Developer Hub 의 장점은 이렇게 쉽게 GAS 를 개발해 볼 수 있을 뿐만 아니라 하단과 같이 통계나 모니터링 기능도 같이 제공하므로 참고하시기 바라겠습니다.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*71RkWYeJ17r1GvW7" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qpcAYG09fSJL_Prp" /></figure><p><strong>추신 #4</strong>: 참고적으로 하단의 공식 사이트를 참고하면 G Suite Developer Hub 에 대한 상세한 설명이 있습니다.</p><p><a href="https://developers.google.com/gsuite/">https://developers.google.com/gsuite/</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gVPrmDl869s4vEcI" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=de130645de07" width="1" height="1">]]></content:encoded>
        </item>
    </channel>
</rss>