<?xml version='1.0' encoding='UTF-8'?><?xml-stylesheet href="http://www.blogger.com/styles/atom.css" type="text/css"?><feed xmlns='http://www.w3.org/2005/Atom' xmlns:openSearch='http://a9.com/-/spec/opensearchrss/1.0/' xmlns:blogger='http://schemas.google.com/blogger/2008' xmlns:georss='http://www.georss.org/georss' xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr='http://purl.org/syndication/thread/1.0'><id>tag:blogger.com,1999:blog-8166686140676460430</id><updated>2019-05-09T18:00:17.205+09:00</updated><category term="Spark"/><category term="TensorFlow"/><category term="Deep Learning"/><category term="Mesos"/><category term="Hadoop"/><category term="SparkML"/><category term="GPU"/><category term="InfluxDB"/><category term="Druid"/><category term="Keras"/><category term="AI Serving"/><category term="Azure Batch AI"/><category term="CNTK"/><category term="ElasticSearch"/><category term="Grafana"/><category term="Horovod"/><category term="MS R"/><category term="Python"/><category term="R"/><category term="Scala"/><category term="SparkR"/><category term="Word2Vec"/><category term="Anaconda"/><category term="AzureML"/><category term="ChatBot"/><category term="Cognitive Services"/><category term="Data Lake"/><category term="Docker"/><category term="Kafka"/><category term="OCR"/><category term="Reinforcement Learning"/><category term="TensorFlowOnSpark"/><title type='text'>HoonDongKim&#39;s BigData &amp; AI</title><subtitle type='html'>Hadoop, Spark, NoSQL, Machine Learning , Deep Learning ....</subtitle><link rel='http://schemas.google.com/g/2005#feed' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/posts/default'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/'/><link rel='hub' href='http://pubsubhubbub.appspot.com/'/><link rel='next' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default?start-index=26&amp;max-results=25'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><generator version='7.00' uri='http://www.blogger.com'>Blogger</generator><openSearch:totalResults>58</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-3648800230124890979</id><published>2019-03-31T23:48:00.001+09:00</published><updated>2019-04-02T02:15:14.722+09:00</updated><title type='text'>XGBoost 와 Deep Learning Regressor 사용한 Regression 문제에서의 성능지표 정의와 앙상블(ensemble) 방법</title><content type='html'>Python 이나 R에서 앙상블(ensemble)하는 방법을 package에 의존하다가, Deep Learning Model 과의 앙상블을 하려고 하면, 의외로 그 방법을 모르고, 질문을 해오시는 분들이 많았다.&lt;br /&gt;&lt;br /&gt;그래서, &lt;b&gt;&lt;u&gt;앙상블 방법을 회귀문제를 예를 들어 설명해 보고자 한다. 또한 회귀문제에 대한 성능지표 재정의 부분도 살펴보도록 하겠다.&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;우선 앙상블은 기법이 많고, 다양한데, 금일 공유하는 방법은 가장 심플하면서도 보편적인 앙상블 방법이라고 할 수 있을 것이다.(Kaggle 등에서도 이 방법이 많이 사용된다.)&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;u&gt;1. 우선 Regression 문제의 성능 측정 지표를 재 정의해 보겠다.&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;회귀(Regression) 문제를 위한 성능측정 지표의 Custom 정의라 할 수 있을 것이다. 성능측정 지표 정의는 실무에서 문제 정의 이후 하게 되는 매우 중요한 단계인데, 즉, 어떻게 성능을 측정하고, 어떻게 지표를 가져갈 것인지에 관한 문제이다.&lt;br /&gt;&lt;br /&gt;보통 모델의 성능을 측정하고자 하는 모델러들은 회귀 문제의 경우 MSE(Mean Squre Error)값을 보는 것이 보통이다.&lt;br /&gt;&lt;br /&gt;하지만, 실무에서는 현업분들과 Communication 하는 경우 MSE 값이 얼마에서 얼마로 개선되었어요 라고, 설명하는 경우, 그 모델의 정확도나 개선 정도를 가늠하기가 매우 힘들어, 원활한 소통이 되지 않는 경우가 많다.&lt;br /&gt;&lt;br /&gt;이 경우 개인적으로 많이 사용하는 방법은 회귀문제를 Accuracy 문제로 재정의 하는 방법이다.&lt;br /&gt;물론, Business 의 특성에 맞추어, 이정도면 맞추었다고 할 수 있다 라는 것을 현업분들의 의견을 구하여, 미리 허용 범위를 정해놓고 시작하는 것도 방법일 것이다.&lt;br /&gt;&lt;br /&gt;배송소요시간 예측 문제를 예를 들어 보겠다. 현업과 이야기해 본 결과 0.5일 이내로 예측을 맞춘 경우, 그것은 맞게 예측한 것으로 본다 라고 이야기를 들었다고 가정해 보자. 예를 들어 배송소요 시간이 3.8일인데, 이를 3.1일 이라고 예측하여 반나절 이상 시간 차가 발생한 경우, 이는 잘못 예측했다 라고 판단 할 수 있을 것이다. 반면, 3.8일을 3.6일 이라고 예측하여, 오차범위가 반나절 이하의 시간차만 발생한 경우, 이는 잘 예측했다 라고 판단 가능하다고 가정해 보자.&lt;br /&gt;&lt;br /&gt;이 경우 회귀문제에 해당하는 배송소요시간 예측 문제를 Classification 문제에서 사용하는 Accuracy 문제로 재 정의 가능하다.&lt;br /&gt;&lt;br /&gt;아래는 이를 위한 코드 이다.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-BMSm04I1Bto/XKJAWXwkw-I/AAAAAAAAJjY/We4xX35vqE8TkTBh-vwk270x7VwehmMcQCLcBGAs/s1600/Screen%2BShot%2B2019-04-02%2Bat%2B1.45.53%2BAM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;770&quot; data-original-width=&quot;1336&quot; height=&quot;368&quot; src=&quot;https://1.bp.blogspot.com/-BMSm04I1Bto/XKJAWXwkw-I/AAAAAAAAJjY/We4xX35vqE8TkTBh-vwk270x7VwehmMcQCLcBGAs/s640/Screen%2BShot%2B2019-04-02%2Bat%2B1.45.53%2BAM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;Boosting 방식으로 Kaggle 스타일 회귀문제에서 끝판왕 중의 하나인 XGBoost 모델의 정확도를 MSE가 아닌 Accuracy 문제로 재정의 한 코드 이다. 위 처럼 처리하는 경우 정확도를 수치로 뽑아 볼 수 있으며, 위 모델은 0.5일 이내로 예측에 성공할 정확도가 84.04% 라 할 수 있다.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;u&gt;2. 다음으로 두가지 알고리즘을 예를 들고 두 알고리즘을 앙상블 해 보겠다.&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;(1) 위에서도 언급했던 XGBoost 의 0.5일 이내 예측 성공율에 대한 정확도 산정 부분이다.&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-BMSm04I1Bto/XKJAWXwkw-I/AAAAAAAAJjY/We4xX35vqE8TkTBh-vwk270x7VwehmMcQCLcBGAs/s1600/Screen%2BShot%2B2019-04-02%2Bat%2B1.45.53%2BAM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;770&quot; data-original-width=&quot;1336&quot; height=&quot;368&quot; src=&quot;https://1.bp.blogspot.com/-BMSm04I1Bto/XKJAWXwkw-I/AAAAAAAAJjY/We4xX35vqE8TkTBh-vwk270x7VwehmMcQCLcBGAs/s640/Screen%2BShot%2B2019-04-02%2Bat%2B1.45.53%2BAM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;br /&gt;앞에서도 살펴본 것처럼 &lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;XGBoost 예측모델이 0.5일 이내의 오차범위에서 예측 성공한 정확도(성공비율)는 84.04% 였다&lt;/u&gt;&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;(2) 다음으로 Deep Learning LSTM Regressor 를 이용한 동일한 Data 에 대한 0.5일 이내 예측 성공률에 대한 정확도 산정 부분이다.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-HN1b0v2zqKE/XKJFaiZBGCI/AAAAAAAAJjw/LeNDQ4RxOM4IExPned4hnrYHfBKQ986DgCLcBGAs/s1600/Screen%2BShot%2B2019-04-02%2Bat%2B2.07.37%2BAM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; height=&quot;198&quot; src=&quot;https://2.bp.blogspot.com/-HN1b0v2zqKE/XKJFaiZBGCI/AAAAAAAAJjw/LeNDQ4RxOM4IExPned4hnrYHfBKQ986DgCLcBGAs/s640/Screen%2BShot%2B2019-04-02%2Bat%2B2.07.37%2BAM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;Deep Learning LSTM Regressor 는 정확도가 86.94% 였다.&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Deep Learning 이 XGBoost 보다도 정확도가 높게 나왔다. 이는 항상 이런 양상을 보이지는 않는다. 데이타의 성격에 따라 둘이 역전되기도 하며, 어느 알고리즘이 항상 이긴다고 장담할 수 없다.&lt;br /&gt;&lt;br /&gt;(3) 마지막으로 위 두 알고리즘을 앙상블 해 보겠다.&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-Rl6mMbMVLfk/XKJGd8JvalI/AAAAAAAAJj4/dpxWKEr1SGUoxF_k8A1-tqPUZ0jFd3B0gCLcBGAs/s1600/Screen%2BShot%2B2019-04-02%2Bat%2B2.12.00%2BAM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;688&quot; data-original-width=&quot;1600&quot; height=&quot;274&quot; src=&quot;https://4.bp.blogspot.com/-Rl6mMbMVLfk/XKJGd8JvalI/AAAAAAAAJj4/dpxWKEr1SGUoxF_k8A1-tqPUZ0jFd3B0gCLcBGAs/s640/Screen%2BShot%2B2019-04-02%2Bat%2B2.12.00%2BAM.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;앙상블을 하자 정확도가 94.11%로 크게 상승하였다.&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;위에서 처럼 단순하게 두 수치를 산술평균 해 주었다. 허무하게 들리겠지만. 이것도 역시 앙상블이다.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;u&gt;보통 모델이 3개 이상일 때는 산술평균보다 기하평균이나 조화평균을 많이 사용한다.&lt;/u&gt;&lt;/b&gt; 그리고 모델이 위에서 처럼 2개 일 때는 산술평균을 사용해도 무방하다. 모델이 3개 이상일 때 기하평균을 사용하는 이유는 너무 예외적인 소수 모델에 의해 값이 너무 틀어지는 것을 막고, 다수의 모델이 선정한 값이 더 평활하게 값이 유지되도록 하기 위함이다. 보통 그런 것이 더 현실세계의 문제에서 잘 먹히는 것이 사실이다.&lt;br /&gt;&lt;br /&gt;물론 문제의 특성 상 Exact Matching Accuracy 보다 F1 Score 등이 더 의미가 있는 경우가 있을 수 있다. 이 경우는 만장일치일때 좀더 맞다고 선정하는 형태로 앙상블 로직을 재정의 할 수도 있다. 즉, 앙상블 방식 또한 문제에 따라 재정의 혹은 실험을 통한 선별이 가능하다가 정답이라 할 수 있다.&lt;br /&gt;&lt;br /&gt;결론 : 앙상블은 보통 모델이 2개일때는 산술평균, 3개 이상일때 기하평균을 많이 쓰는데, 때에 따라 이를 재정의 하기도 하고, 때에 따라서는 조화 평균을 쓰기도, 때로는 중간값을 쓰기도 하고 때예 따라서는 True False 모델로 바꾸어, 만장일치 여부로 판단하기도 한다. 문제에 가장 최적의 앙상블 방법론을 실험 데이타 셑으로 찾아 가는 것이 정답이라 하겠다.&amp;nbsp;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/3648800230124890979/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2019/03/windws10-tensorflow-on-docker.html#comment-form' title='0개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/3648800230124890979'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/3648800230124890979'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2019/03/windws10-tensorflow-on-docker.html' title='XGBoost 와 Deep Learning Regressor 사용한 Regression 문제에서의 성능지표 정의와 앙상블(ensemble) 방법'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-BMSm04I1Bto/XKJAWXwkw-I/AAAAAAAAJjY/We4xX35vqE8TkTBh-vwk270x7VwehmMcQCLcBGAs/s72-c/Screen%2BShot%2B2019-04-02%2Bat%2B1.45.53%2BAM.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-3023910077156839554</id><published>2019-03-31T23:41:00.000+09:00</published><updated>2019-05-09T00:45:55.964+09:00</updated><title type='text'>추천(Recommendation) 시스템 - 알고리즘 Trend 정리</title><content type='html'>&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;추천 알고리즘의 트랜드를 시계열로 정리해 보았습니다.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;여기에서 언급된 년도는 논문 년도라기 보다는 산업계에서 주로 유행했던 시점에 대한 개인적인 추정치 년도 입니다.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;언급하고 있는 알고리즘 또한 학계에서 유명한 알고리즘 보다는 산업계에서 주로 Production 에서 많이 사용되었던 알고리즘을 나열하고 있습니다. 예를들어, Netflix 는 Competition 에서 우승한 Accuracy 가 높은 복잡한 알고리즘이나, 가장 최신의 학계 SOTA 알고리즘을 사용하는 것이 아닌, 개인화와 최신성, 그리고, 100ms 이하의 빠른 서빙이 가능한, 그로부터 Accuracy 보다는 ABTest 에서 더 효율이 좋은, 기민한 알고리즘을 사용하고 있습니다. 그리고 무엇보다, 그날 그날 시시각각 User 의 미묘한 반응의 변화에 Model 이 즉각적으로 개인화 진화 가능하도록 하는 Realtime Lambda 아키텍처 + 원순환 Continuous 진화 Serving 이 가능한 Simple 한 Model 을 이용하여 추천을 하고 있습니다. ( 2018년 Spark + AI Summit at Sanfrancisco 발표 내용 참고 )&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;본 글에서 소개 드리는 추천 알고리즘은 모든 도메인에 사용 가능한 범용적인 추천 알고리즘은 아닐 수도 있습니다. 하지만, 다양한 추천 알고리즘 접근 방식 및 각각의 장단점을 이해하여, 사고의 넓이를 키우자는 입장에서 전반적인 Trend 를 나열해 보았습니다. 혹시 제가 언급하지 못한 중요 Approach 가 있거나, 제가 잘못 이해하고 있었던 부분이 발견된다면, 언제든 컴멘트 부탁드립니다.&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;span data-mce-style=&quot;color: #003366;&quot; style=&quot;color: #003366;&quot;&gt;(1) 10년 ~ 15년 전에는 Apriori 알고리즘. 대표적인 연관 상품 추천 알고리즘.&lt;/span&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 현재는 현업에서 거의 쓰이고 있지 않지만, Confidence , Support , Coverage , Lift 등의 추천 관련 용어를 이해함에 있어, 교과서적인 지식을 주는 알고리즘.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;https://ratsgo.github.io/machine%20learning/2017/04/08/apriori/&quot; href=&quot;https://ratsgo.github.io/machine%20learning/2017/04/08/apriori/&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://ratsgo.github.io/machine%20learning/2017/04/08/apriori/&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(2) 5~10년 전에는 Apriori 다음으로 Collaboration Filtering&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 2010년도 이후, 상당기간 거의 추천의 표준으로 자리 메김.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 손쉬운 구현체 많음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; User Base , Item Base , Contents Base, 혹은 복수개의 결합. etc…&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; T-Store 추천 시스템 관련 이야기가 있는 아래 Blog 추천. 좀 오래된 블로그인데, 그만큼 옛 방식이기 때문 임.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;https://readme.skplanet.com/?p=2509&quot; href=&quot;https://readme.skplanet.com/?p=2509&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://readme.skplanet.com/?p=2509&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 아래는 CF on Spark 에 관한 spotify 의 Approach&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;https://www.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark&quot; href=&quot;https://www.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://www.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; spotify 는 이후 song2vec 등 다양한 시도로 좀더 진화 함. (item2vec 항목에서 재 언급)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(3) 4 ~7년 전에는 FPGroth . Apriori 의 BigData&amp;nbsp; 버전&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Spark ML 라이브러리에 구현되어 있으며, Apriori 와 거의 동일한 Input 과 Output 을 보이지만, 계산 속도 및 BigData Scale 병렬처리에 있어 훨씬 더 효과적인 알고리즘.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 이로부터 Output 이 Apriori 와 거의 동일하지만, 대용량 Data 를 통한 커버리지 개선 및 최신성이 개선 되어 결과론적으로 ABTest Score 를 향상 시키는 것이 가능해짐.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;http://blog.naver.com/PostView.nhn?blogId=sindong14&amp;amp;logNo=220661064114&amp;amp;parentCategoryNo=&amp;amp;categoryNo=48&amp;amp;viewDate=&amp;amp;isShowPopularPosts=true&amp;amp;from=search&quot; href=&quot;http://blog.naver.com/PostView.nhn?blogId=sindong14&amp;amp;logNo=220661064114&amp;amp;parentCategoryNo=&amp;amp;categoryNo=48&amp;amp;viewDate=&amp;amp;isShowPopularPosts=true&amp;amp;from=search&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;http://blog.naver.com/PostView.nhn?blogId=sindong14&amp;amp;logNo=220661064114&amp;amp;parentCategoryNo=&amp;amp;categoryNo=48&amp;amp;viewDate=&amp;amp;isShowPopularPosts=true&amp;amp;from=search&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(3.5) 4 ~ 5년 전에는 Collaboration Filltering + Deep Learning&amp;nbsp; 혹은 유사 Approach&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 모두의 연구소에서 정리한 버전. 크게 성공한 방식은 별로 없음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;https://www.whydsp.org/291&quot; href=&quot;https://www.whydsp.org/291&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://www.whydsp.org/291&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(4) 3~5년 전에는 Matrix Factorization&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; ALS, SGD 가 많이 쓰이며, 모두 Spark ML 에 구현되어 있어, BigData Scale Approach 에서도 많이 사용 되었음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 나온지는 좀더 오래되었지만, Mahout 과 Spark ML 에 구현되면서 BigData 와 결합 가능한 알고리즘으로서 3~5년 전에 더 유행 하였음. (개인적인 경험으로, 정말 Big한 Data 에서는 Spark ML 보다 Mahout Hadoop 버전이 좀더 안정적 이었음.)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 특히 Netflix 추천 Competition 에서 가장 우수한 알고리즘으로 알려지면서 유명세를 탓던 알고리즘이기도 하다. ( ps. 뒤에 Factorization Machine 이 나오는데, MF에 비하여 좀더 Sparse 한 Matrix 에서 유용하다.)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;http://sanghyukchun.github.io/73/&quot; href=&quot;http://sanghyukchun.github.io/73/&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;http://sanghyukchun.github.io/73/&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(5) 2~4년 전에는 Item2Vec + CF&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Doc2Vec , User2Vec , Item2Vec 등을 CF 에 결합하면, Un Seen Feature 에 대하여 좀더 강건해 짐.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Doc2Vec 은 상품상세 Text 나 댓글, Search Engine Tag 등을 이용하여 구성 가능. 이후 Doc =&amp;gt; Item 으로 치환하고, 카테고리, 제목, 이미지Search Simillarity Score 등을 동원하여, Item2Vec 을 만들어 낼 수 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Microsoft 사의 Market 상품 추천에 사용된 바 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [Microsoft 논문]&amp;nbsp;&lt;a data-mce-href=&quot;https://arxiv.org/vc/arxiv/papers/1603/1603.04259v2.pdf&quot; href=&quot;https://arxiv.org/vc/arxiv/papers/1603/1603.04259v2.pdf&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://arxiv.org/vc/arxiv/papers/1603/1603.04259v2.pdf&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; MS의 원 논문에서 파생되어 다양하게 응용되어 사용되고 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [관련 블로그]&amp;nbsp;&lt;a data-mce-href=&quot;https://brunch.co.kr/@goodvc78/16&quot; href=&quot;https://brunch.co.kr/@goodvc78/16&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://brunch.co.kr/@goodvc78/16&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 예,&amp;nbsp;&lt;span style=&quot;font-size: 13.3333px;&quot;&gt;item2vec : 아프리카TV에서 Live 방송을 벡터화.&lt;/span&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; song2vec : Spotify(음원 스트리밍)에서 플레이리스트로 노래를 벡터화. ( &amp;lt;- 개인적으로 드는 생각. 추천되는 목록에서 사용자가 Skip 또는 Listen 행위를 할 때. Vector 공간에서 방향성을 + 혹은 - 함으로써, 더 원하는 취향의 노래를(실시간으로 지금 딱 이순간에 맞게) 찾아 들어가는 취지에서…. 음원 서비스 추천에 매우 적합할 듯한 느낌. Word2Vec 이 그러하듯이 Song2Vec 또한 수백차원의 방향성 벡터 공간일 수 있으므로...)&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;( word2vec 이 그러하듯, 복수의 차원에서 공간적으로 +, - 연산을 통해 수백차원의 공간 내부를 특정하여, 해당 위치 근처의 음원들에 대하여, cosign similarity 순 정렬을 빠르게 해 올 수 있음. )&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;(가수와 장르, 남성, 여성, 혼성, 듀오, 년도 등등이 모두 공간 차원 요소일 수 있음.)&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-JX3YL20jXsk/XKIxR2fWyiI/AAAAAAAAJjE/UuajsWcsS189diw5IWzi-sPtFZOMjzCdQCLcBGAs/s1600/1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https://2.bp.blogspot.com/-JX3YL20jXsk/XKIxR2fWyiI/AAAAAAAAJjE/UuajsWcsS189diw5IWzi-sPtFZOMjzCdQCLcBGAs/s640/1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; meta-pro2vec : Criteo(개인화 광고)에서 상품을 벡터화.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(6) 2~3년 전에는 You-tube Recommendation 스타일 Deep Learning Approach&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 사실 딥러닝 보다도 You-tube 는 머문시간으로 Measure 를 바꾼 것 자체가 보다 큰 성공 요인이었음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 딥러닝으로 추천을 바라보는 Bible 격 논문.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [논문]&amp;nbsp;&lt;a data-mce-href=&quot;https://static.googleusercontent.com/media/research.google.com/ko/pubs/archive/45530.pdf&quot; href=&quot;https://static.googleusercontent.com/media/research.google.com/ko/pubs/archive/45530.pdf&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 시간없으신 분들은 이거라도…&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [논문 요약]&amp;nbsp;&lt;a data-mce-href=&quot;http://keunwoochoi.blogspot.com/2016/09/deep-neural-networks-for-youtube.html&quot; href=&quot;http://keunwoochoi.blogspot.com/2016/09/deep-neural-networks-for-youtube.html&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;http://keunwoochoi.blogspot.com/2016/09/deep-neural-networks-for-youtube.html&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [슬라이드 쉐어]&amp;nbsp;&lt;a data-mce-href=&quot;https://pt.slideshare.net/lekaha/deep-neural-network-for-youtube-recommendations&quot; href=&quot;https://pt.slideshare.net/lekaha/deep-neural-network-for-youtube-recommendations&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://pt.slideshare.net/lekaha/deep-neural-network-for-youtube-recommendations&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 유튜브 알고리즘에 대한 다양한 인문학적 고찰&amp;nbsp;&lt;a data-mce-href=&quot;https://www.bloter.net/archives/301890&quot; href=&quot;https://www.bloter.net/archives/301890&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://www.bloter.net/archives/301890&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(7) 1~3년 전부터 Wide &amp;amp; Deep Model&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 구글 Play스토어의 추천에 사용되면서 높은 효율 개선 이력을 보인 알고리즘으로 알려지면서, 유명세를 탔음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 추천이 아닌 예측 문제에서도 유용함. (마치 Factorization Machine 처럼...)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Cold Start Problem 에 대하여 좀더 능동적으로 대처하고, Memorization (wide)에 의존성을 덜 받기 위한 Generalized 가 가미된 방식(deep).&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 여기서 시사점. 단건 단건 접근하여 Merged 된 Model 을 Wide Model 이라고 할때, 좀더 Generalized 된 Deep Model 은, User 의 Minimal 한 정보만으로도 다양하게 추천이 되는, 그로부터 Sparse 한 부분까지를 유추해 내는 효과를 주지는 않을 런지… ( 이런 효과는 뒤에서 언급되는 Factorization Machine 에서도 언급 됨.)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [논문]&amp;nbsp;&lt;a data-mce-href=&quot;https://arxiv.org/abs/1606.07792&quot; href=&quot;https://arxiv.org/abs/1606.07792&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://arxiv.org/abs/1606.07792&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [조대협님이 정리한 코드가 있는 블로그]&amp;nbsp;&lt;a data-mce-href=&quot;https://bcho.tistory.com/tag/wide%20and%20deep%20model&quot; href=&quot;https://bcho.tistory.com/tag/wide%20and%20deep%20model&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://bcho.tistory.com/tag/wide%20and%20deep%20model&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(8) 1~2년 전부터 개인화 추천이 뜨면서 다시 각광 받는 Factorization Machine. (논문은 사실 좀 오래 되었음.)&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 특히 Big Sparse Matrix 일때 유리. &amp;nbsp;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 특히, (6),(7)의 모델이 Inference 및 Serving 에서 극한의 Engineering 작업을 필요로 하고, BigData Lambda 아키텍처 + Deep Learning Continuous Training 을 통하여 실시간성을 부여하는데 Engineering 복잡도 크게 증가하는 단점을 보여, 단순한 Approach 로 개인화를 구현함에 있어서 상대적으로 Factorization Machine 이&amp;nbsp; 다시금 각광을 받음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 기존 스타일의 추천 Legacy Approach 및 Legacy Search Engine 과 결합하여, 뒷 Layer 로서, 개인화 랭킹을 추가하는 용도로도 많이 쓰임.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 기존 검색엔진의 Output에 Factorization Machine 을 적용하면, 개인화 re-Ranking 을 할 수 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 복수의 모델의 Output 을 Mash-Up 함에 있어, 모델(영역)을 Factorization Machine 과 결합하면, 개인화된 담벼락을 얻을 수 있음. ( amazon 첫페이지나 You-tube 첫페이지 처럼)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 룰엔진의 결과에 적용 시 Rule Engine + 개인화 Re-Ranking 형태로 손쉽게 개인화 추천 적용 가능.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 모델별 개별 접근의 결과가 개인별로 있는 경우, 특정 이력이 전혀없어 Sparse 한 부분을 Factorization Machine 으로 보완하는 경우, 개별 모델로 부터 General 모델을 파생하는데 도움이 될 수 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Factorization Machine 은 다양한 구현체가 있음. 특히, AWS 에는 SageMaker 안에 PaaS 위에서 돌아가는 손쉬운 Library 수준의 Service 구현체가 있어( Code Snippet 이라 SaaS 라고 하기에는 무리가 있음) 빠르게 자사의 데이터로 Training 및 Serving 할 수 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 개인적으로 가지고 있는 Jupyter Notebook 소스코드가 있으므로, 필요하신 분은 요청 주세요.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 아래는 정리가 잘 되어있는 Hands On Lab 예제.&lt;br /&gt;-&amp;gt; [HOL]&amp;nbsp;&lt;a data-mce-href=&quot;https://cloud.hosting.kr/techblog_180709_movie-recommender-with-factorization-machines/&quot; href=&quot;https://cloud.hosting.kr/techblog_180709_movie-recommender-with-factorization-machines/&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://cloud.hosting.kr/techblog_180709_movie-recommender-with-factorization-machines/&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;(8.5) Matrix Factorization 과 Factorization Machine 과의 차이점.&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; MF와 FM의 차이점을 간단하게 써 놓은 글.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;a data-mce-href=&quot;https://stats.stackexchange.com/questions/108901/difference-between-factorization-machines-and-matrix-factorization&quot; href=&quot;https://stats.stackexchange.com/questions/108901/difference-between-factorization-machines-and-matrix-factorization&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://stats.stackexchange.com/questions/108901/difference-between-factorization-machines-and-matrix-factorization&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;논문 참고 :&amp;nbsp;&lt;a data-mce-href=&quot;https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf&quot; href=&quot;https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;Key 내용은 아래와 같다.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;So compared to Matrix Factorization, here are key differences:&lt;/div&gt;&lt;ol style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; margin: 10px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;li&gt;In&amp;nbsp;recommended&amp;nbsp;systems, where Matrix Factorization is generally used, we cannot use side-features. Ex for a movie&amp;nbsp;recommendation&amp;nbsp;system, we cannot use the movie genres, its language etc in Matrix Factorization. The factorization itself has to learn these from the existing interactions. But we can pass this info in Factorization Machines&lt;/li&gt;&lt;/ol&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;MF는 유저-상품 데이터만 가지고, 숨겨진 정보들(side-features, latent feature -&amp;gt; 상품카테고리, 검색어, 이전에 본 상품 등등등) 을 표현(학습) 하는 알고리즘이라,&lt;span style=&quot;font-size: 13.3333px;&quot;&gt;입력 데이터로 유저-상품(클릭 여부) 만 사용 가능. 하지만 FM에서는 이러한 side-feature들을 직접 입력으로 넣어서&amp;nbsp;학습이 가능하다.&lt;/span&gt;&lt;/div&gt;&lt;ol style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; margin: 10px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;li&gt;Factorization Machines can also be used for other prediction tasks such as Regression and Binary Classification. This is usually not the case with Matrix Factorization&lt;/li&gt;&lt;/ol&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;FM은 MF보다 더 일반적이고 확장된 모델이여서, 추천 뿐만 아니라 회귀나 이진분류 와 같은 다양한 ML에서도 사용 가능하다.(MF는 불가능)&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;Just some extension to Dileep&#39;s answer.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;If the only features involved are two categorical variables (e.g. users and items) then FM is equivalent to the matrix factorization model. But FM can be easily applied to more than two and real valued features.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;FM의 가장 큰 장점은 이렇듯 기존 user,item 외에 2개이상의 feature들을 실제로 사용 할 수 있다는 부분.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;(9) 최근. 개인화 추천. ( 2017 re-invent By Amazon )&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Amazon 이 사용하였으나, 크게 알려지진 않은 Deep Learning Base 개인화 추천 알고리즘.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 그 이전에 Amazon 도 Factorization Machine 을 썼다는 이야기가 나옴.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 어떻게 추천이 Deep Learning 으로 진화 되었는지 시계열적인 설명을 하고 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 최종 DSSM 버전은 Google 의 You-tube Recommendation 과도 유사. 다양한 Feature 의 Embedding Vector 를 Concat 하여 Neurual Network 에 넣었다는 것이 핵심. 그래서 사실 You-Tube 의 그것보다 그다지 새롭지는 않음.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-e484LjEIOmY/XKIybzl2RiI/AAAAAAAAJjM/I_snmxJL39sZJjmKxSVlkOXiMGKhSchewCLcBGAs/s1600/2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;913&quot; height=&quot;340&quot; src=&quot;https://2.bp.blogspot.com/-e484LjEIOmY/XKIybzl2RiI/AAAAAAAAJjM/I_snmxJL39sZJjmKxSVlkOXiMGKhSchewCLcBGAs/s640/2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 최종 버전의 소스코드 및 PaaS/SaaS 서비스가 Open 되지는 않았으나, 일부는 github 에 spark + mxnet + gluon 버전의 코드가 부분 공개되어 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [슬라이드쉐어]&amp;nbsp;&lt;a data-mce-href=&quot;https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017&quot; href=&quot;https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;(10) 최근. 개인화 추천. Hierarchical RNN ( 2018 re-invent By Amazon )&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 2018 re-invent 행사에서 Amazon 이 알파버전을 PaaS/SaaS 개인화 추천 서비스로 공개하여, 주목받음.&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [논문]&amp;nbsp;&lt;a data-mce-href=&quot;https://arxiv.org/pdf/1706.04148.pdf&quot; href=&quot;https://arxiv.org/pdf/1706.04148.pdf&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://arxiv.org/pdf/1706.04148.pdf&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 아직 알파버전이라 당장 사용해볼 수는 없으나, github 에 위 논문 저자가 올린 공개 버전 코드가 있음.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [구현체] 저자의 구현체 공개버전 :&amp;nbsp;&lt;a data-mce-href=&quot;https://github.com/mquad/hgru4rec&quot; href=&quot;https://github.com/mquad/hgru4rec&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://github.com/mquad/hgru4rec&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(11) 최근. 개인화 Re-Ranking. (개인화 Reinforcement Learning Re-Ranking By 알리바바)&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 개인화 re-Ranking 에 Reinforcement Learning 이용.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; [논문]&amp;nbsp;&lt;a data-mce-href=&quot;https://arxiv.org/pdf/1803.00710.pdf&quot; href=&quot;https://arxiv.org/pdf/1803.00710.pdf&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://arxiv.org/pdf/1803.00710.pdf&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; 아카데믹하게 알려진 논문은 아니지만, 타오바오와 알리바바의 사례를 담고있는 논문이라, 매우 현실적인 접근이며, 의미있는 새로운 Approach 임.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt; Factorization Machine 과 마찬가지로, 기존 Legacy 추천 시스템 뒤에서, 혹은 복수개의 모델의 개인화 노출 순서를 정하는 부분, 혹은 Search 엔진 뒤에서 개인화를 구현하는데, 적합.&lt;/div&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3 style=&quot;color: #172b4d; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 16px; font-variant-ligatures: normal; line-height: 1.5; margin: 30px 0px 0px; orphans: 2; widows: 2;&quot;&gt;&lt;strong&gt;(12) Deep Learning 기반 최신 추천시스템 동향 관련 Survey 논문&lt;/strong&gt;&lt;/h3&gt;&lt;div style=&quot;color: #555555; font-family: &amp;quot;맑은 고딕&amp;quot;, seoul, arial, helvetica; font-size: 13.3333px; font-variant-ligatures: normal; line-height: 1.3; margin-top: 10px; orphans: 2; overflow-wrap: break-word; widows: 2;&quot;&gt;-&amp;gt;&amp;nbsp;&lt;a data-mce-href=&quot;https://arxiv.org/pdf/1707.07435.pdf&quot; href=&quot;https://arxiv.org/pdf/1707.07435.pdf&quot; style=&quot;color: rgb(59, 115, 175) !important; text-decoration: none;&quot;&gt;https://arxiv.org/pdf/1707.07435.pdf&lt;/a&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/3023910077156839554/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2019/03/recommendation-trend.html#comment-form' title='0개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/3023910077156839554'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/3023910077156839554'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2019/03/recommendation-trend.html' title='추천(Recommendation) 시스템 - 알고리즘 Trend 정리'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-JX3YL20jXsk/XKIxR2fWyiI/AAAAAAAAJjE/UuajsWcsS189diw5IWzi-sPtFZOMjzCdQCLcBGAs/s72-c/1.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-1163724847865021053</id><published>2018-10-10T23:47:00.001+09:00</published><updated>2018-10-11T13:56:29.281+09:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><title type='text'>강화학습(Reinforcement Learning)으로 접근하는 E-commerce Dynamic Pricing 논문리뷰 </title><content type='html'>요즘 취미생활중인 강화학습!&lt;br /&gt;강화학습의 코딩 Deep Dive 를 하기 위한 Play Ground 를 뒤지던 중, 우리 파트, deep learning specialist 한성국파트너가 &lt;b&gt;&lt;u&gt;좋은 논문&lt;/u&gt;&lt;/b&gt;을 소개해 주었다.&lt;br /&gt;&lt;br /&gt;일단, &lt;b&gt;&lt;u&gt;나의 기준으로 좋은 논문&lt;/u&gt;&lt;/b&gt;은&lt;br /&gt;(1) Goal 이 Practical 한지,&lt;br /&gt;(2) 구현 가능할 것 같은지,&lt;br /&gt;(3) Goal 뿐 아니라, 구현 난이도 등에서도 현실성을 갖추고 있는지(너무 두리뭉실하게 표현하고, 많이 숨기고 있으면 탈락이다.)...&lt;br /&gt;&lt;br /&gt;에 가중치를 높게 두고 있음을 알려둔다. (물론 저 기준은 산업계에 몸담고 있는 나의 지극히 개인적인 기준일 뿐이다. 저런 류의 Practical 한 논문 또한 Ideal 한, Research 논문을 바탕으로 그 위에 등장할 수 있으므로, 모든 Research 논문들이 결국은 뿌리이며, 진화의 시작이라 할 수 있다.)&lt;br /&gt;&lt;br /&gt;이 논문은 아직 학술적으로는 검증된 논문이 아니다. ICLR 2019 에 제출되었으나, 아직 Review 중인, 그래서, 저자가 누구인지도 모르는 상태의 논문이기 때문이다.&lt;br /&gt;&lt;br /&gt;하지만, 내용을 읽다 보면, 실무에서 바로 적용하기 위한 연구를 진행하였고, 그 과정에서의 시도와 결과 그리고 인사이트 등을 경험을 통해 공유하고 있음을 알수 있다. 더군다나 Tmall.com (By Taobao) 이라고 하는 걸출한 E-commerce 플랫폼 위에서 꽤 많은 양의 데이타와 기간으로 실험한 결과를 보여주고 있어, 검증 자체에도 강한 신뢰가 가는것이 사실이다.&lt;br /&gt;&lt;br /&gt;이런류의 논문은 학술적인 가치를 논외로 치더라도, 그 실험 방법, 문제의 정의, 그리고, 그것을 적용한 결과 자체가 매우 Practical 하여, 이런류의 문제를 동일하게 Real World 에서 접근 시 매우 큰 참고가 될 수 있어서 좋다.&lt;br /&gt;(아마도 저자는 Taobao 의 직원일 것이다. 꼭 그렇지 않더라도, 해당 플랫폼 위에서의 실험 자체가 Real World 를 많이 반영하고 있어서 맘에 든다.)&lt;br /&gt;&lt;br /&gt;본론으로 들어가서 내가 읽은 부분을 요약해 보겠다.&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;논문 제목&lt;/li&gt;&lt;ol&gt;&lt;li&gt;DYNAMIC PRICING ON E-COMMERCE PLATFORM WITH DEEP REINFORCEMENT LEARNING&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;논문 위치&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HJMRvsAcK7&quot;&gt;https://openreview.net/pdf?id=HJMRvsAcK7&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;서론부에 등장하는 타사 사례&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Uber 사례&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Uber 는 &#39;surge&#39; 라고 하는 dynamic pricing 전략을 구현하고 발표한바 있는데, driving time 을 늘리는데 있어 매우 큰 기여(significant impact)를 줬다고 한다.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;[사족] @김상우 님. 저희 예전에 쏘카에서 Reinforcement Learning 으로 dynamic Pricing 적용 시의 파급효과및 구현 가능성에 대한 대화를 잠시 나눈적 있었죠? 딱 그 모델이네요. 물론 Uber 는 다른 알고리즘을 사용했을지도 모르지만요...&lt;/span&gt;&lt;/li&gt;&lt;li&gt;Chen &amp;amp; Sheldon (2016) 에서 소개되었다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Zara 사례&amp;nbsp;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;systematic 한 dynamic pricing 모델을&amp;nbsp; 구현 적용하였으며, Markdown pricing strategy 에&amp;nbsp; 적용.&lt;/li&gt;&lt;li&gt;Caro &amp;amp; Gallien (2012) 에서 소개.&lt;/li&gt;&lt;li&gt;&lt;i style=&quot;background-color: yellow;&quot;&gt;[사족] Markdown은 좀 옛 방식이긴 하지만, 여전희 Off-line 등에서 Human 에 의해 직관으로 대부분 수행되고 있으며, 국내 E-commerce 에서는 이 조차도 Model 화 되지 않은 곳이 대부분이다.&lt;i style=&quot;background-color: yellow;&quot;&gt;(2018년 현재 기준).&lt;/i&gt;&amp;nbsp;기술이 어렵다기 보다는 실행력이 어려운 분야가 또 이 분야일 것이다. 가격과 관련된 프로젝트는 장고를 뒤따르게 만들기도 하거니와, 책임, 위험부담 등등, 넘어서야 하는 것들이 많은, 기술 이외의 것들이 더 어려운 분야가 또 이 분야이다. Zara 방식 소개 논문이 2012년 인것으로 보아, 벌써 6년 전 이므로, Zara 는 지금 이 분야가 매우 진화 되어 있을 것으로 추측 된다.&lt;/i&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Kroger 사례&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Offline 에서 Dynamic Pricing 을 적용하고 개선하기 위해, Electronic price tag 를 스토어에 적용 개선 중.&lt;/li&gt;&lt;li&gt;Nicas (2015)&lt;/li&gt;&lt;li&gt;&lt;i style=&quot;background-color: yellow;&quot;&gt;[사족] 매우 오래전, 벌써 3년전에 Off에서조차 이런시도를 했으니, 그때부터 Kroger 는 매우 발빠른 유통기업이었던거 같다. 현재(2018년)의 Kroger&amp;nbsp; 는 Google에서 무인자동차를 하던 핵심멤버들이 나와서 창업한 실리콘벨리의 무인자동차 관련 Startup 과 제휴하여, 무인배송자동차를 통한 유통 혁명을 실험하고 있고, 아마존 월마트 못지 않게 Data 를 잘 다루는 선진적인 유통기업의 면모를 다양한 곳에서 보여주고 있다. 아니나 다를까... 2015년 부터 이런 시도를 했었넹....&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Amazon.com 상품수&lt;/li&gt;&lt;ol&gt;&lt;li&gt;2017년 기준 562 million = 5억 6천 2백만 상품이 있다고 함.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Wallmart.com 상품수&lt;/li&gt;&lt;ol&gt;&lt;li&gt;4.2 millon = 420만 상품이 있다고 함.&lt;i&gt; &lt;span style=&quot;background-color: yellow;&quot;&gt;([사족] 뭐 별거 아니네....물론 온라인전용 상품 포함이 아닌 오프라인 상품 수가 저 정도라고 한다면, 말이 다르지만...)&lt;/span&gt;&lt;/i&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Taobao.com 상품수&lt;/li&gt;&lt;ol&gt;&lt;li&gt;2 billion = 20억 상품.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;i&gt;([사족] 중국이 대단한건 알고 있었지만, World Top Level 아마존이 5억 6천인데.... 2017년이라고 하더라도, China Top Level 타오바오가 20억 상품 이라뉘.... 물론 Tmall 은 이미 Global 화가 많이 진행되고는 있지만...)&lt;/i&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Amazon.com 사례&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Automatic Pricing System 을 만들었음.&lt;/li&gt;&lt;li&gt;매 15분 마다 Price를 갱신. (2016년 기준)&lt;/li&gt;&lt;li&gt;Chen et al. (2016)&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;학계의 Approach&amp;nbsp;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;기존연구들&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Statistical Learning.&amp;nbsp;&lt;/li&gt;&lt;li&gt;price optimization.&lt;/li&gt;&lt;li&gt;stochastic model.&lt;/li&gt;&lt;li&gt;Bayesian dynamic pricing policies.&lt;/li&gt;&lt;li&gt;non-parametric approaches.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;위 연구들은 큰 한계가 있었음.&lt;/li&gt;&lt;li&gt;그 한계를 뛰어 넘는 최신 연구.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;MAB(multi armed bandit) 을 이용. Misra et al. (2018)&amp;nbsp;&lt;/li&gt;&lt;li&gt;본 논문은 MAB 를 그래서, base model 비교군으로 삼았음.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;본 논문의 알고리즘 요약&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Dynamic Pricing Problem 을 Deep Reinforcement Learning 으로 접근.&lt;/li&gt;&lt;li&gt;E-Commerce 플랫폼에 적용하여 evaluation. (taobao 의 Tmall.com)&lt;/li&gt;&lt;li&gt;주요접근 방법&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Markov Decision Process 를 이용한 D.P. 의 첫 논문이라고 밝힘.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;[사족] 이 시점 부터 AlphaGo-Lee(이세돌버전)가 강하게 떠오른다. Alphago-Lee 는 Markov Decision Process 를 활용한, Monte Carlo tree search , 그리고 마찬가지로 Deep Reinforcement Learning 이 주요 알고리즘들 중 하나이다.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;discrete and continuous 한 price 액션 스페이스 에서 다양한 보상 functions 을 정의.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;discrete pricing action model&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;deep Q-networks (DQN) 이용.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;continuous pricing action model&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;(DQN) 과 함께 Deep Deterministic Policy Gradient (DDPG) 가 이용.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;historical data 로부터 pre-train 모델을 적용.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;pre-training 의 중요성이 길게 언급되었었음.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;pricing period d 는 하루나 한주가 될 수 있음.&lt;/li&gt;&lt;li&gt;보상(reward) function 은 revenue 혹은 sales. 수익 혹은 매출로 가능하지만...&lt;/li&gt;&lt;ol&gt;&lt;li&gt;그러나 위 두 값은, 요일, 계절, 행사 및 이벤트 여부 등 다양한 외부 변수에 의하여 매우 가변적인 값이다.&lt;/li&gt;&lt;li&gt;그러한 값들에 독립적인 measure 가 필요.&lt;/li&gt;&lt;li&gt;본 논문에서는 이를 revenue conversion rate 로 칭하였고, 다음과 같이 정하였다.&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-4sYTsa3lxC4/W738gYyLnEI/AAAAAAAAH-U/HF7hUTjLkrUgIL06zBWSIJ7vDhKT9PKzgCLcBGAs/s1600/4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;53&quot; data-original-width=&quot;212&quot; src=&quot;https://2.bp.blogspot.com/-4sYTsa3lxC4/W738gYyLnEI/AAAAAAAAH-U/HF7hUTjLkrUgIL06zBWSIJ7vDhKT9PKzgCLcBGAs/s1600/4.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;&amp;nbsp; dividing &lt;b&gt;&lt;span style=&quot;font-size: large;&quot;&gt;revenue&lt;/span&gt;&lt;/b&gt;i by &lt;b&gt;&lt;span style=&quot;font-size: large;&quot;&gt;uv&lt;/span&gt;&lt;/b&gt;i -&amp;gt; 이를 시간에 따른 변화량으로 표현.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;다양한 features&lt;/li&gt;&lt;ol&gt;&lt;li&gt;price features&lt;/li&gt;&lt;li&gt;sales features&lt;/li&gt;&lt;li&gt;customer traffic features&lt;/li&gt;&lt;li&gt;competitiveness features&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;[사족] 경쟁사의 동일 상품의 상태는 중요하고 상관도 높은 feature 일 수 있는데, 유일한 외부 feature 이기도 하다.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;[사족] 그래서, 크롤링이 동원되어야 하고, 크롤링 해서 가져올 내용은 경쟁사 가격, 행사 및 이벤트 여부, 그리고 댓글 달리는 속도(이는 조회 수를 유추 할 수 있어서...)&lt;/span&gt;&lt;/li&gt;&lt;li&gt;저자 역시 이런 언급을 했다.&amp;nbsp;&amp;nbsp;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;The comments and the states of the similar products contribute to competitiveness features.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;실험결과&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Offline 결과&lt;/li&gt;&lt;ol&gt;&lt;li&gt;4만개의 서로 다른 fast moving&amp;nbsp;consumer product 를 사용&lt;/li&gt;&lt;li&gt;60일 판매데이타 사용&lt;/li&gt;&lt;ol&gt;&lt;li&gt;59일을 가지고 pre-training&amp;nbsp;&lt;/li&gt;&lt;li&gt;마지막 1일을 가지고 evaluation&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;240만 튜플이 이용되었다.&lt;/li&gt;&lt;li&gt;offline policy evaluation 에서 MAB 알고리즘 구현체로 LinUCB 를 사용. 이는 Deep Reinforcement Learning 을 앞도했음.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;MAB 는 즉각적인 reward 를 maximize 하는 경향 있음.&lt;/li&gt;&lt;li&gt;DRL 은 long-term reward 를 maximize 하려는 경향 있음.&lt;/li&gt;&lt;li&gt;이는 offline 의 특성에 기인.&lt;/li&gt;&lt;li&gt;하지만, E-commerce 플랫폼에서는 이 optimal policy 는 적합하지 않음.이는 뒤이는 online 결과에서 입증 됨.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;서로다른 accuracy 모델(error rate, rate of right estimation, etc)에서는 DRL 이 더 높은 accuracy 를 보임.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Online 결과&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&amp;nbsp;revenue&amp;nbsp; conversion rate 와&amp;nbsp; profit conversion rate 모두 DQN 이 사람의 manual 판단을 앞도 했음.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-nuaFcjGxk58/W74DnVGx8SI/AAAAAAAAH-k/PptGVFZAllcCea2DwvlxvXlbc_Syt7JSgCLcBGAs/s1600/5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1032&quot; height=&quot;446&quot; src=&quot;https://2.bp.blogspot.com/-nuaFcjGxk58/W74DnVGx8SI/AAAAAAAAH-k/PptGVFZAllcCea2DwvlxvXlbc_Syt7JSgCLcBGAs/s640/5.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;유사 상품 그룹은 유사 결과를 보여준다는 것(a)과 DQN과 LinUCB 를(b) 그리고 다시 DQN과 DDPG 를(c) 비교하였음.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-y5D7GiuZy5A/W74IY8GnfSI/AAAAAAAAH-w/p9IgfG_8idYdjQYZKLioau3P2YvsTdqpgCEwYBhgL/s1600/6.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: left; margin-bottom: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;307&quot; data-original-width=&quot;572&quot; height=&quot;342&quot; src=&quot;https://1.bp.blogspot.com/-y5D7GiuZy5A/W74IY8GnfSI/AAAAAAAAH-w/p9IgfG_8idYdjQYZKLioau3P2YvsTdqpgCEwYBhgL/s640/6.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;결론&lt;/li&gt;&lt;ol&gt;&lt;li&gt;DDPG와 DQN 로 구현한 pricing policies는 기존의 다른 pricing policies 를 앞도하였다.&lt;/li&gt;&lt;li&gt;markdown pricing 에 있어서도 deep reinforcement learning approach 가 기존 manual markdown pricing strategy 를 앞도하였다.&lt;/li&gt;&lt;li&gt;본 연구에서는 각각 개별 상품별로 pre-training 을 수행했다.&lt;/li&gt;&lt;li&gt;그러므로, 잘 팔리지 않는 상품에 대하여는 별도의 처리를 해주어야 하며, 이 부분은 transfer learning 및 meta learning 기법이 이용될 수 있다.&lt;/li&gt;&lt;li&gt;향후 promotion pricing feature 와 membership pricing fetures 를 추가할 예정이다.&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;[사족] 본 논문은 다이나믹 프라이싱을 Markov Decision Process 를 사용하여 가장 최신의 기법인 MAB 기법보다 낳은 결과를 보여주었음을 보여주었다.&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;[사족] 이 논문이 읽는 독자의 시각에서 크게 기여하고 있는 바는 아래 정도일 듯 한다. 내가 느낀 바 이다.&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;Pricing 문제를 RL 로 접근함에 있어, 문제정의 그리고 접근 방법에 충실한 가이드라인을 제공하고 있다.&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;시간단위별 revenue conversion rate 를 가지고 reward function을 만든 부분은 간단하면서도 꽤 reasonable 한 approach 였다.&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;중요 feature 들을 잘 정리해주었고, 유사상품 그룹의 특성까지 실험해 주었다.&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;어느정도의 데이타로 어느정도의 결과에 도달하는지 가이드를 제시해 주었다. 향후 이 수치는 우리가 맞게 접근하고 있는지 가늠하는데 큰 도움이 될 수 있다.&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;접근한 DQN 과 DDPG 의 hyper parameter 도 제시하고 있다. 유레카~~ ^^&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;b&gt;[사족] 내가 이런류의 Practical 한 논문을 좋아하는 이유는 이를 그대로 구현해 보는 것이 최초의 Base line 모델 구축 이후 다음 2번째 혹은 3번째 시도 정도로서 매우 빠르고 유용한 접근 가이드를 제시하기 때문이다.&amp;nbsp;&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/1163724847865021053/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2018/10/reinforcement-learning-e-commerce.html#comment-form' title='1개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/1163724847865021053'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/1163724847865021053'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2018/10/reinforcement-learning-e-commerce.html' title='강화학습(Reinforcement Learning)으로 접근하는 E-commerce Dynamic Pricing 논문리뷰 '/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-4sYTsa3lxC4/W738gYyLnEI/AAAAAAAAH-U/HF7hUTjLkrUgIL06zBWSIJ7vDhKT9PKzgCLcBGAs/s72-c/4.png" height="72" width="72"/><thr:total>1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-4002061305904015269</id><published>2018-08-29T10:25:00.000+09:00</published><updated>2018-08-29T10:56:01.361+09:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI Serving"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><title type='text'>Production Deep Learning 서비스 Pain Point 에 대한 단상 - E-Commerce Product Overview 기능을 통한 분석</title><content type='html'>&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;dngsv-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;dngsv-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span data-offset-key=&quot;dngsv-0-0&quot; style=&quot;font-family: inherit;&quot;&gt;요즘 해외 배송이 빨라지고 저렴해 지면서, 전자 제품 및 노트북 액서서리 등등 은 거의 Amazon 및 Alibaba 를 이용하고 있다. 특히 고가의 IT기기의 경우 국내가격대비 20% 이상 저렴한 것을 감안하면, 가격 Save 도 큰 역할을 한다. 하지만 그 무엇보다, Amazon 및 Alibaba 의 IT 기기 Search와 상품상세 view에 있어서의 site 의 편리함과 Data Driven Overview 기능의 사용성은 그 모든것을 능가하는 쐐기를 박는 요소가 아닐 수 없다.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;4fr2f-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;4fr2f-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;우선 alibaba 는 IT 기기들의 경우 카테고리별로 사용자들이 주로 보는 속성들을 Quick Details 라는 메뉴로 요약해서 보여준다. 사실 이 부분만 보면 거의 98%에 가까운 확인 사항을 모두 확인 할 수 있어서, 상품상세를 모두 봐야 할 필요가 없다. &lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;3acqu-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-yZLa-p3yXbo/W4XirozyweI/AAAAAAAAHnI/a-o_P9EuCu0X-vyaTqycsaLrDAko_nu_wCLcBGAs/s1600/2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1280&quot; height=&quot;492&quot; src=&quot;https://2.bp.blogspot.com/-yZLa-p3yXbo/W4XirozyweI/AAAAAAAAHnI/a-o_P9EuCu0X-vyaTqycsaLrDAko_nu_wCLcBGAs/s640/2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;3acqu-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span data-offset-key=&quot;3acqu-0-0&quot; style=&quot;font-family: inherit;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;3acqu-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;우리나라 e-commerce 의 경우 무겁고 어마어마한 크기의 카탈로그 scan 이미지를 모두 봐야 제품 상세 정보를 알수 있는 경우가 많고, 요약 속성을 제공하는 경우에도, 업자가 자체적으로 제공하여, 상품마다, 노출 위치나 항목, 요약 정보의 제공 패턴이 모두 제각각이다. 불리한 속성은 일부러 언급하지 않는 경우도 있고, 그 경우에는 어마어마한 크기의 카달로그를 죄다 뒤져야 한다.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-JZEFp6w6j_k/W4Xk5rFMqnI/AAAAAAAAHnU/RFhwqDK026EvGpYhYSjU4rx8xC4xLhGuwCLcBGAs/s1600/3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1600&quot; data-original-width=&quot;631&quot; height=&quot;640&quot; src=&quot;https://1.bp.blogspot.com/-JZEFp6w6j_k/W4Xk5rFMqnI/AAAAAAAAHnU/RFhwqDK026EvGpYhYSjU4rx8xC4xLhGuwCLcBGAs/s640/3.png&quot; width=&quot;251&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;3acqu-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;Amazon 은 한단계 더 진화 했다. 최근 보았던 2개를 포함하여 총 3개를 비교하여 한눈에 difference 의 요약 정보를 일목 요연 하게 보여 준다.  &lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;5endc-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-3PDaAC-2Ty0/W4XlFJtSt-I/AAAAAAAAHnY/BXeXfdYrxnYw0Aws0pwzwvrKvuOOk4pIgCLcBGAs/s1600/1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1039&quot; data-original-width=&quot;1535&quot; height=&quot;432&quot; src=&quot;https://2.bp.blogspot.com/-3PDaAC-2Ty0/W4XlFJtSt-I/AAAAAAAAHnY/BXeXfdYrxnYw0Aws0pwzwvrKvuOOk4pIgCLcBGAs/s640/1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;5endc-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span data-offset-key=&quot;5endc-0-0&quot; style=&quot;font-family: inherit;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;6mmr1-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;6mmr1-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;이는 비단, 사용성과 편리함의 문제로 그치지 않는다. 상품 카테고리별로 중요한 속성이 뭔지를 아는것, 그리고 각 상품별로 그 속성의 값이 무엇인지를 아는 것은 e-commerce site 에서 어마어마한 자산일 수 있다.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;b2040-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;b2040-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;일반적인 e-commerce site 의 대부분이 상품별로 제목, 상품상세 text, 카테고리, 브랜드 등만을 관리하는 것과 달리, 위 선진 site 들 처럼 각각의 대표속성, 대표 키워드 등을 관리한다는 것은 Semantic Search , 자연어 기반 챗봇 검색, 속성기반 유사도 추천, 품절 대체 추천, 기타 검색 품질 관리에 있어서도 많은 유리함을 가져다 준다. 다차원의 속성이 있으면, 모든 상품을 N차원 공간에 배치할 수도 있다. 그리고, 그런 유사 상품의 공간 백터 상 관리는, 상품별로 가격, 배송,물류 관리 등 다방명의 Data Driven Model 을 만듦에 있어, 매우 큰 기초 Data 역할을 한다. 약간의 Deep Learning NLP 를 가미하면, 상품명을 몰라도 그 상품을 수식하는 형용사 만으로도 상품을 찾아들어갈 수 있다. &lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;f8hlt-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;f8hlt-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;최근의 e-commerce Text NLP AI use case 관련 논문들은 자연어 문장에서 단순하게 NER 만 뽑아내는 것이 아닌, NER 을 뽑아냄과 동시에 이러한 중요 속성을 함께 뽑아내는 연구가 이루어지고 있음을 보여준다. 문장중에 언급이 없어도, 성별을 찾고, 어른용을 찾는지 아이들용을 찾는지, 선물용인지 등등의 중요속성 정보를 NER 뽑아내듯, 화자 혹은 검색 사용자의 일부 단서나 나열된 keyword sequence 로부터 뽑아준다는 의미이다.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;qel0-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;qel0-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;해당 논문을 몇달 전부터 구현레벨에서 검토하였는데, 역시, 구현보다 어려운것이, 그 데이타의 Training 을 위한 Input Data 가 legacy 에 존재하지 않는 다는 점이다. 요즘 내가 alibaba 나 amazon 의 저런 제품 속성 기반 Generated(or Model Managed) Overview 기능에 관심을 갖는 이유라 하겠다. &lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;4ltlp-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;4ltlp-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span data-offset-key=&quot;35v66-0-0&quot; style=&quot;font-family: inherit;&quot;&gt;그렇다면, 해당 속성 데이타가 없다는 것은 누구의 잘못인가? 의사결정자의 잘못인가? 기획자? 전략부서? 데이타부서? AI부서? 그 누구의 잘못도 아니다. 그걸 100% 수작업으로 만들어 내려는 시도는 10년전에도 많은 기획자들이 고민했던 ROI 답 안나오는(그래서 검색 Tag 정도만 최소로 관리하는) 문제 중 하나이다. 그래서 그런것을 만들어내는 것 또한, 요즘은 &lt;/span&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;Deep Learning Model 로 접근하고 있다. 혹은 Model 이 95% 정도를 자동으로 해주고, 소수의 운영자 및 알바생들이, Accuracy 점수가 낮은 순으로 정렬하여, quick 하게 eye checking 만 하면서, click, drag 가 대부분 작업이도록 하면서  supervised learning  되도록 operation 관리자 페이지를 구성하여 구성하는 사례가 선진사례에서 등장하고 있다. 즉, ROI 안나오던 작업이 AI Assist 로 인하여, ROI 가 나오는 해볼만 한 작업이 된 것이다.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;35v66-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;35v66-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;하루에 수천건의 신규 상품이 들어와도 자동 분류 되고, 자동 tagging 되고, 공간상에 자동으로 꼳혀 들어가야 하고, operator 에 의한 정교화 보정 작업은 후처리 사항이며 모든것을 전수 관리할 필요가 없어졌다. 이를 프로세스화 하고 입력시점에 업체들에게 위임 할 수도 있으나, 업체는 낚시성 Tag 를 다는데 더 최적화 되어 있으며, 이는 Noise 에 해당한다. 감지된 데이타에 대한 보정 작업은, Supervised Learning, 좀더 구체적으로는 online learning 의 재료로 쓰이며, 모델 진화에 기여 한다. 그것보다 더 잘 기획된 AI 서비스는 그 과정에서, 모든 training 진화를 Operator가 죄다 하는 것이 아닌, 사용자의 반응 및 행위가 model 의 진화에 기여 하도록 UI 기획을 하는 것이라 할 수 있다.(요즘 기획자들은 그래서 Deep Learning  개념을 좀 알고 있어야 한다.)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;3vlka-0-0&quot; style=&quot;background-color: white; color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;3vlka-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;&quot; data-block=&quot;true&quot; data-editor=&quot;9sk98&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;color: #1d2129; font-family: Helvetica, Arial, sans-serif; font-size: 14px; white-space: pre-wrap;&quot;&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span data-offset-key=&quot;fphpv-0-0&quot; style=&quot;background-color: yellow; font-family: inherit;&quot;&gt;&lt;b&gt;&lt;u&gt;결론을 내 보겠다.&lt;/u&gt;&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;background-color: white; direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;Production AI 서비스를 하다보면, &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;background-color: white; direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;ol&gt;&lt;li&gt;Main Problem  의 model 을 만드는 일이 사실 제일 쉬운 문제이다. 연구도 많이 되어 있고, 논문 혹은 바로 쓸 수 있는 오픈소스나 github 구현체를 찾아서 활용할 수도 있다. &lt;/li&gt;&lt;li&gt;그보다 어려운 부분이 다수동접처리 및 빠른 서빙, 무중지 배포, Auto Scale Out 등의 문제이다. 여기서부터는 Lab 에서 연구하지 않는 분야이고, 참고 자료가 많지 않기 때문이다. &lt;b&gt;&lt;u&gt;이를 위한 방법론은 여러차례 블로깅 한 적이 있었다.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;그보다 더 어려운 부분은 만드는 AI Production 서비스에 Freshness 와 personalization 을 추가하는 문제이다. Freshness 는 병렬성이 핵심이며, realtime 인입이 필요하여 kafka, spark streamming 등의 도움이 필요할 수 있으며, 람다 아키텍처가 필요하고, 요즘 스타일로는 Serverless Microservice 기술이 필요하고, kubernetess 및 docker 를 끌여들여야 할 수 있다. Personalizaion 으로 가면, 모델의 차원이 커지고, 사람별로 혹은 상품별로 Training 을 하기 위해 BigData Scale 의 Computing 과 분산 Deep Learning 을 위한 BigData 기술 연계가 동원된다. (개발 생산성과 low level tensorflow 직접 분산처리보다 더 빠른 컴퓨팅을 위해서 이기도 하거니와, 수많은 heavy pre-processing 을 위해서이기도 하다.)  &lt;b&gt;&lt;u&gt;이를 위한 방법론은 여러차례 블로깅 한 적이 있었다.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;그보다 더 어려운 문제는 양질의 Data 를 확보하는 문제이다. 결국은 Production 직전에 혹은 직 후에 이 부분에 직면하게 된다. Garbage in Garbage out 이다. 모델에 집중하면 정확도 3% 올리는데 반년이 걸릴 수 있지만, Input Data 에 집중하면 몇달을 투자하여 정확도가 10%가 올라갈 수도 있다. 국내에 AI Production Service 를 잘하는 곳도 여기까지 와서 주저 앉는 경우가 많다. 일반적인 IT 프로젝트 처럼 오픈 이후 바로 결과를 확인 하는 습관 때문이기도 하다. AI 프로젝트는 여기서부터 시작이라고 해도 과언이 아니다. 반응을 보지 않고, 상상으로 개발한 1차 결과물보다, 실 데이타가 3달 이상 쌓인 후 이를 보면서, 2차 3차 고도화를 통해, 상상 Training Data 와 Real Input Data 간의 괴리를 줄이는 작업이 이후에 지속적으로 투자 되어야 하는데, 이를 기달리지 못하고, 초기 결과를 보고 스폰을 받지 못하여 Operation 과 이와 맞물린 관리적 고도화가 되지 못하는 AI 서비스를 주변에서 많이 보았다.   &lt;/li&gt;&lt;li&gt;그보다 더 어려운 문제는 양질의  Data를 수작업을 통해서가 아닌 자동으로 혹은, 자동80% 수동 20% 정도로 확보하게 만드는 일이다. e-commerce 를 예를 들자면, OCR, 댓글 텍스트, 크롤링 등등 다양한 방식으로 이런한 Data 를 Generation 화 할 수 있으며, 중요속성을 뽑아내는 다양한 알고리즘이 존재한다. 하지만, 이 부분은 경험해 보니, 모델이 어려운게 아니라, 이것 자체가 또하나의 프로젝트가 되어버리는 것이 문제였다. A 프로젝트를 잘하기 위해 B 프로젝트를 또 해야 해요! 가 조직에 잘 설득되지 않는 경우가 많았다. 사실 할일도 많고, 인력도 없는데, A 프로젝트도 겨우 인력 짜내서 하는 형국에 B 는 Minor 로 치부되는 것이, 당연한 현실 일 수 있다. &lt;/li&gt;&lt;li&gt;그것보다 더 어려운 문제는 양질의 Data 와 이후 Production 중 보정 후 재 Training 이 사용자들에 의하여 &lt;b style=&quot;font-family: inherit;&quot;&gt;&lt;u&gt;원순환&lt;/u&gt;&lt;/b&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;으로 알아서 진화되도록 UI를 잘 만드는 일이다. 덧붙여 이에 맞는 원순환 맞춤 관리자 페이지도 나와줘야 한다. 덧붙여 모델 자체도 원순환 training 이 가능하도록, 람다 아키텍처 및 batch training,   그리고, 무중지 online training 이 삼박자가 잘 맞게 개발되어야 한다. 아쉽지만, 이는, R&amp;amp;R 지옥을 뚫고 기획을 개발자가 하거나, 기획자가 개발자가 되거나 하는 정도의 Mix 된 융합 Co-Work 이 필요하며, 딱 그 만큼(정말로 개발자가 기획을 하고, 기획자가 개발하는 것 만큼이나)이나 어렵다. &lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;오늘 언급한 Production Deep Learning 서비스의 Pain Point 에 대한 단상은 결국, 기획과 모델링과 개발의 유기적인 결합이 중요한 Key 라는 결론으로 귀결 되었다. &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;background-color: white; direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;경험해 보건데, 현실적으로 이를 위해 제일 쉬웠던 방법은....&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;background-color: white; direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;ol&gt;&lt;li&gt;끊임없이 개발자나 모델러들이, 기획자를, 윗분들을 설득하고 , 또 말하고, 또 설득하고... 그 과정에서 싸우기도 하고, 장문의 편지도 많이 쓰고.. 설레발도 많이 떨고, 페이스북에 기사같은것도 막 퍼 날라주고....TT 지난한 일이지만, 그래야....느린속도로 라도 둘 간에 혹은 셋간에(경우에 따라서 넷 이상이 될 수도 있는게 현실이다.  TT) 바라보는 곳이 같아지고, Over lap 되어 일하는 부분이 늘어나는 것 같다.&lt;/li&gt;&lt;li&gt; 그리고, 그들을 설득하고, 우리들의 가장 큰 약점인 말빨에서 밀리지 않기 위해서는, 우리들도 끊임없이 문과적으로 사고하고, 비지니스도 좀  가중치를 올려주고, 기획적인 사고도 더 연습하고 더 자주해야 한다는 점이다.&lt;/li&gt;&lt;li&gt;기획자에게  Deep Learning 에 대한 꿈과 밝은 미래를 자주 주입해주고, 공짜 컨퍼런스 티켓도 좀 사비로라도 사서 나 시간없어서 사놓고 못간다고 하면서 좀 던져 주고, 기획자에게 패스트캠퍼스나 모두의 연구소 같은것도 좀 추천해주고.... 이것이 은근히 정말 큰 도움이 된다. 기획자가 50%를 이해하면, 회의시간이 3배 이상 줄어 들고, 서로 딴이야기를 하지 않을 확률이 200% 이상 증가하는 듯 하다. 실제 경험담이다.&lt;/li&gt;&lt;li&gt;위 6단계를 warter fall 로 한다는 것은 말도 안되는 일이다. 외주에 맞기고, Production 시점부터 즉, 위 6단계의 4부터 외주업체가 나간체로 내부에서 한다는 것도 말이 안되는 일이다. 물론 협력하면서 프로젝트를 한 경우는 가능할 수 있다. 결국 초반엔 첫 프로젝트는 개고생을 감수해야 할지도 모르겠다. 역시 경험담이다. 개고생한만큼 기술도, 그리고 위를 이끌어 낼려는 간절함의 동인으로 부터 야기된 설레발 능력(1에서 많이 활용되는 능력이다)도 성숙해지는 듯 하다. &lt;/li&gt;&lt;li&gt;성공을 한번 맞보게 하는것이 매우 중요해 보인다. 그런데 매우 초기에 빨리 맛보게 해줘야 한다. 그렇지 못한다면 자멸할 수도 있다. 이후 그것을 발판삼아야 스폰도 받고, 시간도 얻고, 인력도 얻어서, 시간을  Parallel 화 시킬 수 있다.(초기에는 누구도 기달려주는 사람이 없다.) 결국 AI 서비스는 아이에게 말을 가르치듯, 단계를 하나하나 잘 밟고, 시간 투자를 해야 하는데, 그런데 시간을 기달려주지 않는다....그렇다면, 조직에서 스폰서쉽이라도 받아서 Parallel 하게 모델들을 개발 하는게 중요하다. &lt;/li&gt;&lt;/ol&gt;&lt;div&gt;요즘 박항서가 2002년도 히딩크 만큼이나 베트남에서 축구 감독으로 활약을 하고 있다. 우리나라에서 AI 하기는 우리나라 대표팀으로 월드컵 16강 가기 정도로, 생각할게 많은 듯 하다. AI와 축구는 멀티플레이어가 매우 중요한것도 닮아 있다. 그리고, 골을 넣는것도 중요하다. 결국 골을 못넣으면, 과정이 어찌 되었든 비난 받는다. 수많은 시선과 실시간으로 쏟아지는 매우 short-term 의 비판을 뚫고서도, long term 의 전략을 세우지 않으면, 본질적으로 결코 쉬운일이 아니다. 너무 log term 의 전략은 자칫 Short-term 의 비판에 나가 떨어지거나, 포기하거나, 이직하거나, 인사고과  D 맞기 십상이다. 하지만...본질적으로 우리나라는 16강 가기 위해, Long term 의 전략과 본질적인 기본기 확충을 위한 시간 투자와, 시스템의 개편과, 축구 협회의 성찰과 혁신도 필요하다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;대한민국 AI 개발자 화이팅 합시다.!!!!  &lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;_1mf _1mj&quot; data-offset-key=&quot;fphpv-0-0&quot; style=&quot;background-color: white; direction: ltr; font-family: inherit; position: relative;&quot;&gt;&lt;span style=&quot;font-family: inherit;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/4002061305904015269/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2018/08/production-deep-learning-pain-point-e.html#comment-form' title='0개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/4002061305904015269'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/4002061305904015269'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2018/08/production-deep-learning-pain-point-e.html' title='Production Deep Learning 서비스 Pain Point 에 대한 단상 - E-Commerce Product Overview 기능을 통한 분석'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-yZLa-p3yXbo/W4XirozyweI/AAAAAAAAHnI/a-o_P9EuCu0X-vyaTqycsaLrDAko_nu_wCLcBGAs/s72-c/2.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-2900231299385751683</id><published>2018-01-11T21:59:00.001+09:00</published><updated>2018-02-03T17:01:05.271+09:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Azure Batch AI"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="GPU"/><category scheme="http://www.blogger.com/atom/ns#" term="Horovod"/><category scheme="http://www.blogger.com/atom/ns#" term="Keras"/><category scheme="http://www.blogger.com/atom/ns#" term="TensorFlow"/><title type='text'>Deep Learning Multi Host &amp; Multi GPU Architecture #2 - Keras 를 이용한 Scale Up, Horovod 를 이용한 Scale Out 성능 비교</title><content type='html'>Deep Learning Multi Host, Multi GPU 를 사용하고, BigData Scale 데이타를 처리하며, Auto Scale Out 확장 까지 고려한 아키텍처 구성에 대하여 연재 중이다.&lt;br /&gt;&lt;br /&gt;개요는 이곳에서 확인 가능하다. [아키텍처 주안점 및 설계를 위한 고찰 : &lt;a href=&quot;https://hoondongkim.blogspot.kr/2018/01/deep-learning-multi-host-multi-gpu.html&quot;&gt;https://hoondongkim.blogspot.kr/2018/01/deep-learning-multi-host-multi-gpu.html&lt;/a&gt; ]&lt;br /&gt;&lt;br /&gt;오늘은 두번째로,&lt;span style=&quot;background-color: yellow;&quot;&gt; &lt;b&gt;&lt;u&gt;High Level 딥러닝 프레임워크 인 Keras 를 이용한 GPU Scale Up. 그리고 Horovod 를 이용한 Multi Host GPU Scale Out 의 성능에 대한 비교를 해보도록 하겠다.&lt;/u&gt;&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;우선 Tensorflow 나 pyTorch 나 CNTK , Caffe2 등에서 이미 GPU 나 Host 에 대한 확장을 이미 지원하고 있는데, 이러한 실험이 무슨 의미가 있는지 의아할 듯 하여, 그 의미에 대하여 다시 언급해 보도록 하겠다.&lt;br /&gt;&lt;ol&gt;&lt;li&gt;기존 Deep Learning 프레임워크에서의 Low&amp;nbsp; Level 병렬 수행 최적화 코드를 별도로 구현해야 하는 수고를 덜 수 있다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Keras 의 경우 Multi GPU 를 지원하기 시작한지 불과 3달도 체 지나지 않았다. 2017년 10월 경 버전 2.0.9 부터 지원되기 시작했다.&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;하지만, Keras는 단 1줄로 Multi GPU Scale Up 이 된다. (Horovod 와 함께 Keras 를 함께 실험한 이유이기도 하다.)&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Multi Host 는 훨씬 복잡하다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Keras 의 경우 Multi Host 는 아직 지원하지 않는 한계가 있다.&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;Multi GPU Scale Up 은 8 GPU 가 Max 이다. 즉, BigData Scale 확장을 위해서는 Multi Host 도 동원해야 한다.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;Keras등은 자체적으로는 Multi Host 를 지원하지 않는데, Multi Host 까지 최소의 노력으로 사용가능 하도록 구성하기 위해서는 tensorflowOnSpark 나 Horovod, elephas 등을 이용해야 한다.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;개발 생산성 뿐 아니라, 성능 까지 더 좋았으면 한다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Horovod (made by Uber ) 의 official 페이지에는 Horovod 개발 배경을 다음과 같이 설명하고 있다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-1oBFCSCe0RY/WlMs1SHJ0qI/AAAAAAAAGlM/0WzsQubj6D0lB4d6pquA35tXDhJ7RjZrACLcBGAs/s1600/why.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;204&quot; data-original-width=&quot;1019&quot; height=&quot;128&quot; src=&quot;https://1.bp.blogspot.com/-1oBFCSCe0RY/WlMs1SHJ0qI/AAAAAAAAGlM/0WzsQubj6D0lB4d6pquA35tXDhJ7RjZrACLcBGAs/s640/why.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;이런 언급도 있다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-w3M9w-O2QBQ/WleM03Oox5I/AAAAAAAAGn4/uUP7kACiozo3vKFeZ4Rj-4t9O-n1ufroQCLcBGAs/s1600/uber.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;60&quot; data-original-width=&quot;908&quot; height=&quot;42&quot; src=&quot;https://4.bp.blogspot.com/-w3M9w-O2QBQ/WleM03Oox5I/AAAAAAAAGn4/uUP7kACiozo3vKFeZ4Rj-4t9O-n1ufroQCLcBGAs/s640/uber.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;ol&gt;&lt;/ol&gt;&lt;div&gt;이제 위에서 언급된 &lt;b&gt;&lt;u&gt;(1) 병렬 코딩에 있어서의 개발생산성&lt;/u&gt;&lt;/b&gt; , &lt;b&gt;&lt;u&gt;(2) 수행 시간 단축 효과&lt;/u&gt;&lt;/b&gt;가 어느 정도 인지 확인 해 보자. (맛보기 정도의 예제이기는 하지만, 느낌을 공유 하고자 한다.)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;테스트는 아래 환경에서 수행되었다.&lt;br /&gt;&lt;b&gt;1. GPU Scale Up Test&lt;/b&gt;&lt;br /&gt;=&amp;gt; Azure DSVM Image NC-Series .&lt;br /&gt;=&amp;gt; K80 GPU.&lt;br /&gt;=&amp;gt; Tensorflow&amp;nbsp;+ Keras&lt;br /&gt;&lt;b&gt;2. GPU Scale Out Test&lt;/b&gt;&lt;br /&gt;=&amp;gt; Azure Batch AI Cluster with NC-Series vm.&lt;br /&gt;=&amp;gt; K80 GPU.&lt;br /&gt;=&amp;gt; Tensorflow&amp;nbsp;+ Keras&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Keras 를 이용한 Multi GPU Scale Up 코드&lt;/li&gt;&lt;ol&gt;&lt;li&gt;아래 코드 1줄이면 된다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-q0YAwJx2i38/WlR5cIIj5VI/AAAAAAAAGlc/no9pK7LoOIEhNZ_dY-zZzRzC70zwzhDhgCLcBGAs/s1600/multi_gpu.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;139&quot; data-original-width=&quot;877&quot; height=&quot;100&quot; src=&quot;https://4.bp.blogspot.com/-q0YAwJx2i38/WlR5cIIj5VI/AAAAAAAAGlc/no9pK7LoOIEhNZ_dY-zZzRzC70zwzhDhgCLcBGAs/s640/multi_gpu.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;위에서 사용한 메소드를 호출하기 위해 필요한 package import 는 아래처럼 해주면 된다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-8E_X-IDgCUI/WlR59ZNEQWI/AAAAAAAAGlk/kNfB7gdbdScVP3LPjw9IGdHqnGZxN-WEQCLcBGAs/s1600/multi_gpu2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;94&quot; data-original-width=&quot;877&quot; height=&quot;68&quot; src=&quot;https://3.bp.blogspot.com/-8E_X-IDgCUI/WlR59ZNEQWI/AAAAAAAAGlk/kNfB7gdbdScVP3LPjw9IGdHqnGZxN-WEQCLcBGAs/s640/multi_gpu2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;여기서 매우매우 유의해야 할 점이 있다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Keras 는 Multi GPU 사용 시 epoch 가 나눠져서 수행되는 것과 유사하게 동작한다.&lt;/li&gt;&lt;li&gt;&lt;b&gt;&lt;u&gt;즉, Epoch 를 GPU 갯수로 나누는 코드 적용이 필요하다.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;이론적으로는 이 경우 GPU 갯수가 2배가 되면, Training 속도가 2배 빨라져야 할 것이다. 그러나, 실험을 해보면 그정도 까지 개선되지는 않는다. 그것은 GPU 가 증가 시 GPU 간 Data 의 동기화 Copy 등에 부가적인 Overhead 가 소요되기 때문이다.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;Keras 를 이용한 Multi GPU Scale Up 성능 비교&lt;/li&gt;&lt;ol&gt;&lt;li&gt;General 한&amp;nbsp; LSTM 모델이다. 실험을 위해 Training Data Size 는 줄여놓은 모델이다.&lt;/li&gt;&lt;li&gt;GPU 1개 (NC6)&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-vKKMuAqOrXQ/WlR69oQe9pI/AAAAAAAAGlw/zATYWAqHzOcan0AU3ZUfZi_cT111s4-AACLcBGAs/s1600/multi_gpu3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1280&quot; height=&quot;492&quot; src=&quot;https://2.bp.blogspot.com/-vKKMuAqOrXQ/WlR69oQe9pI/AAAAAAAAGlw/zATYWAqHzOcan0AU3ZUfZi_cT111s4-AACLcBGAs/s640/multi_gpu3.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;150초 정도가 소요되었다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;GPU 2개 (NC6)&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-17-LvQYM-ps/WlR8ZkJi4RI/AAAAAAAAGl8/seUpUr_FcMwPdw_IvGoEokJWx6yaMdOPgCLcBGAs/s1600/multi_gpu4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1280&quot; height=&quot;492&quot; src=&quot;https://1.bp.blogspot.com/-17-LvQYM-ps/WlR8ZkJi4RI/AAAAAAAAGl8/seUpUr_FcMwPdw_IvGoEokJWx6yaMdOPgCLcBGAs/s640/multi_gpu4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;127초가 소요되었다.&lt;/li&gt;&lt;li&gt;그리고 약간 성능도 개선되었다.&lt;/li&gt;&lt;li&gt;이 시점의 nvidia GPU 사용량은 다음과 같다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-ljgbyU0dTQ8/WleOlt_QgOI/AAAAAAAAGoA/W3aX_iEoTfQi-E1g-NveU99KjSABv47xwCLcBGAs/s1600/keras_2gpu.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;662&quot; height=&quot;454&quot; src=&quot;https://3.bp.blogspot.com/-ljgbyU0dTQ8/WleOlt_QgOI/AAAAAAAAGoA/W3aX_iEoTfQi-E1g-NveU99KjSABv47xwCLcBGAs/s640/keras_2gpu.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;위 처럼 2번째 GPU 는 보편적으로 Fully 일하지는 못한다. (그러나, 모델 및 알고리즘에 따라 그 양상에는 차이가 있었다.)&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;GPU 4개 (NC12)&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-BnNmRwpKNYg/WlR9QRmigQI/AAAAAAAAGmE/dSlNPmJeLpUZ9jdEz_orf4zIz4As-7dWwCLcBGAs/s1600/multi_gpu6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1280&quot; height=&quot;492&quot; src=&quot;https://3.bp.blogspot.com/-BnNmRwpKNYg/WlR9QRmigQI/AAAAAAAAGmE/dSlNPmJeLpUZ9jdEz_orf4zIz4As-7dWwCLcBGAs/s640/multi_gpu6.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;82초가 소요 되었다.&lt;/li&gt;&lt;li&gt;성능도 좀더 향상 되었다.&lt;/li&gt;&lt;li&gt;GPU 갯수와 성능과의 관계는 뒤에서 다시 언급 토록 하겠다.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;Horovod 를 이용한 Multi GPU Scale Out 코드&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Keras 의 Scale Up 코드 만큼 심플하지는 않지만, Horovod 의 경우도 몇줄 코드만으로 Deep Learning 을 Multi Host 로 Scale Out 가능하다. 중요한 것은 Keras와 달리 Scale Up 도 되고, Scale Out 도 된다는데 있다.&lt;/li&gt;&lt;li&gt;import 패키지&lt;/li&gt;&lt;ol&gt;&lt;li&gt;import horovod.keras as hvd&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;horovod init 코드&lt;/li&gt;&lt;ol&gt;&lt;li&gt;hvd.init()&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Process 당 GPU 하나씩 할당하기 위한 코드&lt;/li&gt;&lt;ol&gt;&lt;li&gt;config = tf.ConfigProto()&lt;/li&gt;&lt;li&gt;config.gpu_options.allow_growth = True&lt;/li&gt;&lt;li&gt;config.gpu_options.visible_device_list = str(hvd.local_rank())&lt;/li&gt;&lt;li&gt;K.set_session(tf.Session(config=config))&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Epoch 분산 용으로 변경&lt;/li&gt;&lt;ol&gt;&lt;li&gt;EPOCH = 200.0&lt;/li&gt;&lt;li&gt;epochs = int(math.ceil(EPOCH / hvd.size()))&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Optimizer 분산용으로 변경&lt;/li&gt;&lt;ol&gt;&lt;li&gt;opt = optimizers.Adadelta(1.0 * hvd.size())&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Callbacks 추가&lt;/li&gt;&lt;ol&gt;&lt;li&gt;hvd.callbacks.BroadcastGlobalVariablesCallback(0)&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;아래는 위 코드 Example 이다.&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-QOrdKbG1N78/WldHj9t0I3I/AAAAAAAAGmY/L67WU8RdeLEQjsvZn780hCt-7ArfBtrZwCLcBGAs/s1600/horovod_code.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;274&quot; data-original-width=&quot;661&quot; height=&quot;264&quot; src=&quot;https://4.bp.blogspot.com/-QOrdKbG1N78/WldHj9t0I3I/AAAAAAAAGmY/L67WU8RdeLEQjsvZn780hCt-7ArfBtrZwCLcBGAs/s640/horovod_code.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;epoch 를 본 예에서는 직접 입력하였다. 이후 Optimizer 와 Callbacks 관련 2줄만 각각 model compile 과 model fit 전에 수행해 주면 된다.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;Horovod 를 이용한 Multi Host Scale Out 성능비교&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Node 1개&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Jupyter 를 통해 Azure Batch AI 클러스터 위로 Training Job 을 구동하였다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-AXADldjqisQ/WldNKXUm7ZI/AAAAAAAAGmo/3qTFGyPv4h8zz2kYPQo3oo9E-sjG_BGNgCLcBGAs/s1600/horovod%25EC%2584%25B1%25EB%258A%25A51.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;972&quot; data-original-width=&quot;1378&quot; height=&quot;450&quot; src=&quot;https://1.bp.blogspot.com/-AXADldjqisQ/WldNKXUm7ZI/AAAAAAAAGmo/3qTFGyPv4h8zz2kYPQo3oo9E-sjG_BGNgCLcBGAs/s640/horovod%25EC%2584%25B1%25EB%258A%25A51.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Node 1개에서 166초가 소요되었다.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Node 2개&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Node 2개 에서 멀티 Host 모드로 수행하자 아래처럼 로그가 2번씩 찍힌다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-2XfpIsCYUmg/WldNxkjCt0I/AAAAAAAAGms/w8Phlc0acbc_0GAxCWxz-DCi1dZIKJyeACLcBGAs/s1600/horovod%25EC%2584%25B1%25EB%258A%25A52-1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1309&quot; height=&quot;362&quot; src=&quot;https://1.bp.blogspot.com/-2XfpIsCYUmg/WldNxkjCt0I/AAAAAAAAGms/w8Phlc0acbc_0GAxCWxz-DCi1dZIKJyeACLcBGAs/s640/horovod%25EC%2584%25B1%25EB%258A%25A52-1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-MMV_2cgDQAk/WldOfCOCjtI/AAAAAAAAGm4/nEYKVuEFJN0J1xH0Yg5deVSfkyTMdI4pACLcBGAs/s1600/horovod%25EC%2584%25B1%25EB%258A%25A52-2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;613&quot; data-original-width=&quot;1312&quot; height=&quot;298&quot; src=&quot;https://3.bp.blogspot.com/-MMV_2cgDQAk/WldOfCOCjtI/AAAAAAAAGm4/nEYKVuEFJN0J1xH0Yg5deVSfkyTMdI4pACLcBGAs/s640/horovod%25EC%2584%25B1%25EB%258A%25A52-2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Horovod Cluster Size 가 2가 되자 initial epoch가 200 이었지만, 각 노드별 병렬 epoch 는 100으로 줄었다.&lt;/li&gt;&lt;li&gt;143초로 수행시간이 단축 되었다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Node 4개&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Horovod 는 계속 NC6 으로도 병렬 확장 테스트가 가능하나, Keras Scale Up 과의 비교를 위해 , 이 시점에 NC12로 바꾸었다. 왜냐하면, Scale Up 의 경우 NC6 은 GPU 2개 까지밖에 지원하지 않아서, GPU 4개를 테스트하는 시점 NC12로 바꾸었었기 때문이다.&amp;nbsp;&lt;/li&gt;&lt;li&gt;동일한 테스트를 위해 이시점은 NC12 이미지 4대에서의 테스트 이다. 각 멀티 노드에서 GPU 는 1개씩만 사용하였다.&lt;/li&gt;&lt;li&gt;즉, 4 Host * 1 GPU = 4GPU 환경이다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-MJftEoWyFnI/WldPbFBa34I/AAAAAAAAGm8/_6tPffjdF3k44FO0mJyGNaB2tLiqZVcHACLcBGAs/s1600/horovod%25EC%2584%25B1%25EB%258A%25A53-1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;654&quot; data-original-width=&quot;1293&quot; height=&quot;322&quot; src=&quot;https://1.bp.blogspot.com/-MJftEoWyFnI/WldPbFBa34I/AAAAAAAAGm8/_6tPffjdF3k44FO0mJyGNaB2tLiqZVcHACLcBGAs/s640/horovod%25EC%2584%25B1%25EB%258A%25A53-1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;예상했던 것처럼 로그가 4번씩 나오고 있다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-tUXW9lyfhhs/WldQIC9dq2I/AAAAAAAAGnM/RIBnGEt5t-ofy_03CSMqhLR5jVOoAfzDQCLcBGAs/s1600/horovod%25EC%2584%25B1%25EB%258A%25A53-2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;581&quot; data-original-width=&quot;1274&quot; height=&quot;290&quot; src=&quot;https://4.bp.blogspot.com/-tUXW9lyfhhs/WldQIC9dq2I/AAAAAAAAGnM/RIBnGEt5t-ofy_03CSMqhLR5jVOoAfzDQCLcBGAs/s640/horovod%25EC%2584%25B1%25EB%258A%25A53-2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;수행속도는 89초로 줄어 들었다.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;div&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;[결론]&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Keras 의 Scale Up과 Horovod 의 Scale Out 모두 Epoch 를 나눠가졌다. 이런 양상은 TensorflowOnSpark 와는 조금 다른 양상이다. TensorflowOnSpark 는 Epoch 를 나눠갖지 않고, Multi Node 의 Multi Job 들이 Mini Batch 를 1/N 로 나눠 갖는다. 각 특성이 갖는 양상에 따른 차이도 그 차이점을 고려해볼만 한 주제인듯 하다.(향후 시간이 허락한다면 이 부분도 파 보도록 하겠다.)&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white;&quot;&gt;우선 오늘 실험해본 내용의 종합 결과는 아래와 같다.&lt;/span&gt;&lt;br /&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-Hu_1L-1ARVU/WldXSyXak5I/AAAAAAAAGnY/-Qufjtsa6086x9Tz3VCH6FDcGE-vt1jkgCLcBGAs/s1600/4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;617&quot; data-original-width=&quot;1040&quot; height=&quot;378&quot; src=&quot;https://3.bp.blogspot.com/-Hu_1L-1ARVU/WldXSyXak5I/AAAAAAAAGnY/-Qufjtsa6086x9Tz3VCH6FDcGE-vt1jkgCLcBGAs/s640/4.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;앞에서도 언급 했던 것처럼 Keras 를 사용하면, Tensorflow Backend 의 GPU Scale Up 은 우선 매우 매우 쉽다. 성능 또한, 기본적인 initial time 을 제외 하고는 어느정도 선형적인 증가를 보여 준다. 좀 더 특이한 양상이 발견되었던 점은, (적은 양의 데이타에서 특히) GPU 갯수가 늘어나자 좀더 성능이 빨리 좋아지는 양상이 발견되었다는 점이다. 이는 앙상블 효과 처럼 보인다. 그리고, 마치 가끔 Mini Batch Size 를 키웠더니, 정확도가 오히려 개선되었을 때와 비슷한 양상이다.(일반적으로 그 반대가 더 많지만...)&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;background-color: white;&quot;&gt;모든 weight 의 실시간 공유가 꼭 좋은것 만은 아니다. 4개의 독립적인 GPU 가 어느정도는 독자적인 local minigma 를 찾고, 각 GPU 가 1번씩 epoch 가 끝났을때, 지연 동기화를 하게 되면, local minigma 에 빠질 확률이 훨씬 줄어들기 때문에, 분산 지연 weight 동기화가 오히려 training 에 긍정의 효과를 준 것으로 보여진다.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Horovod 를 통한 Scale Out 형태의 GPU 확장은 예상 했던 것 처럼, Scale Up 모드보다는 무거운 연산임이 실험을 통해서도 확인 되었다. 노드 갯수가 작을때에는 Single Node 의 멀티 GPU 보다 더 느린 양상을 보였다. 이는 노드갯수가 4개 정도로 늘어나면 큰 차이가 나지 않는다. (&lt;b&gt;&lt;u&gt;이 곳에 언급하진 않았지만, 노드 갯수가 많아질수록 성능은 역전된다&lt;/u&gt;&lt;/b&gt;.) 즉, Horovod 시나리오는 좀더 BigData Scale Deep Learning 에 가깝다.&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Production 시스템에서 Training Data 가 쌓이고, 그 크기가 점점 커지기 시작하면, 점점 training 전에 전처리 단계에서 부터, 1대 머신의 물리 Disk 를 모두 동원해도 감당이 안될 정도로 Data가 커지는 경우가 있다. 이 경우 Scale Out 시나리오에서는 해당 노드에 간단하게 Hadoop 정도를 설치해주면, 데이타는 복수 Node 에 펼쳐져 저장되게 되고, Disk IO 는 분산되어, 훨씬 성능도 좋아진다. ( Tensorflow 의 공식 github 에는 이러한 경우를 위해hadoop 에 저장된 Training Data를 핸들링 할 수 있는 Hadoop Connector 유틸리티가 제공되고 있다. Spark Connector 도 있다. TensorflowOnSpark 는 내부적으로 해당 유틸리티를 활용하고 있다.) Data 의 크기는 크지 않지만, Disk IO 를 20~30배 이상 성능적으로 개선해보고자 할때는 Hadoop 대신에 해당 노드에 Spark 나 Tachyon (이름이 바뀌긴 했으나, original 이름으로 더 알려진) 이나, Apache ignite 를 설치하고 수행할 수도 있다.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;위 실험에서도 알 수 있는것처럼, 데이타의 크기가 1대의 머신에 담을 수 있는 크기이고, GPU는 8개 미만만 사용해도 되는 수준이라고 한다면, Scale Up 이 더 유리하다. ( Deep Learning Infra 설계시 참고하기 바란다. 작은 데이타는 1Node 4GPU 가 1GPU 4Node 보다 성능이 더 좋음을 위해서 보여 준 바 있다.)&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;하지만, Scale Up 은 Nvidia 의 dependency 로 대부분의 Deep Learning 프레임워크들이 8개 까지만 지원하는 경우가 많다. 하지만, Scale Out 은 그런 한계를 극복 할 수 있다.&amp;nbsp;&lt;/span&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Horovod 는 그래서, 1대 Node 에 GPU 를 4개씩 꼳아서 8대를 클러스터로 묶어서 수행하는 경우 Scale Up 과 Scale Out 의 성능을 동시에 활용할 수 있다. 즉, 그 경우 4*8 = 32개 GPU 를 사용할 수 있다. Keras 는 8개가 한계 이지만, Horovod 그리고, Horovod&amp;nbsp;+ Cloud PaaS (본 실험에서는 이 부분을 Azure batch AI 를 이용했다.) 를 사용하는 경우 수백대 이상 까지도 Node 및 GPU를 동원하여, 분산 Training 의 장점을 몇번의 클릭으로 수행 해 볼 수 있다.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Keras 는 1줄 코드 수정으로 Scale Up 이 되었지만,&amp;nbsp; Horovod 는 6~8 줄 정도의 수정이 필요하였다. 하지만, 익숙해지면, 1분안에 적용 가능한 수준이었다. 때문에, &lt;/span&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;개&lt;/u&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;발생산성 향상은 검증되었다고 본다.&lt;/u&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Horovod&amp;nbsp; 의 위키에는 다양한 Horovod 성능 수치 비교자료들이 존재한다. 아래는 그중에 어떤 사용자의 경험을 보여주는 성능비교 수치이다. Scale Out 이 아닌 Scale Up 의 경우에도 8개 GPU 이상일때는 Horovod 방식이 성능이 더 잘 나오는 것을 표현해주고 있다. 내가 했던 실험 데이타 또한 처음에는 Horovod 가 느리다가 Node 4개를 기점으로 역전되는 모양을 보여주었었다.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-nDyMRdnwf-c/WlddafPIuVI/AAAAAAAAGno/hJ6mPPYt79Q75tot6f_gwsvzu_Gl2hW6QCLcBGAs/s1600/5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;104&quot; data-original-width=&quot;792&quot; src=&quot;https://4.bp.blogspot.com/-nDyMRdnwf-c/WlddafPIuVI/AAAAAAAAGno/hJ6mPPYt79Q75tot6f_gwsvzu_Gl2hW6QCLcBGAs/s1600/5.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;Scale Out 부분에 있어서도, 대부분의 벤치마크 자료나, 대다수의 비교 자료에서 Horovod 는 Tensorlfow 에서 자체적으로 제공하는 Parameter Server 를 활용한 방식 보다, 수행 속도 측면에서 성능이 더 잘 나오는 것을 확인 해 볼 수 있다.&lt;/span&gt;&lt;br /&gt;&lt;u&gt;&lt;br /&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;즉, 성능 적인 개선도 검증되었다고 여겨진다.&lt;/span&gt;&lt;/u&gt;&lt;br /&gt;&lt;u&gt;&lt;br /&gt;&lt;b style=&quot;background-color: yellow;&quot;&gt;개발이 훨씬 쉬워지면서 성능이 뒤쳐지지 않는다면... 아키텍처링 입장에서는 마다할 이유가 없다.&lt;/b&gt;&lt;/u&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;background-color: white;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;ol&gt;&lt;ol&gt;&lt;ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/2900231299385751683/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2018/01/deep-learning-multi-host-multi-gpu_11.html#comment-form' title='0개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/2900231299385751683'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/2900231299385751683'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2018/01/deep-learning-multi-host-multi-gpu_11.html' title='Deep Learning Multi Host &amp; Multi GPU Architecture #2 - Keras 를 이용한 Scale Up, Horovod 를 이용한 Scale Out 성능 비교'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-1oBFCSCe0RY/WlMs1SHJ0qI/AAAAAAAAGlM/0WzsQubj6D0lB4d6pquA35tXDhJ7RjZrACLcBGAs/s72-c/why.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-1794233329898911671</id><published>2018-01-03T01:37:00.002+09:00</published><updated>2018-01-08T17:09:21.365+09:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Azure Batch AI"/><category scheme="http://www.blogger.com/atom/ns#" term="CNTK"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="GPU"/><category scheme="http://www.blogger.com/atom/ns#" term="Horovod"/><category scheme="http://www.blogger.com/atom/ns#" term="Keras"/><category scheme="http://www.blogger.com/atom/ns#" term="TensorFlow"/><title type='text'>Deep Learning Multi Host &amp; Multi GPU Architecture #1 - 고찰 및 구성 with tensorflow, cntk, keras, horovod, Azure Batch AI</title><content type='html'>지난번 Deep Learning Serving Layer 아키텍처 수립을 위한 고찰(&lt;a href=&quot;http://hoondongkim.blogspot.kr/2017/12/deep-learning-inference-serving.html&quot;&gt;http://hoondongkim.blogspot.kr/2017/12/deep-learning-inference-serving.html&lt;/a&gt;) 이후 , 이번에는 Deep Learning Training Layer 아키텍처 수립에 대하여도 고찰해 보았다.&lt;br /&gt;&lt;br /&gt;아키텍처 수립에서 고려한 주요 주안점은 다음과 같다.&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Training 은 Fine Tuning 이 이루어지는 주기적인 Batch Training 이 있을 수 있고, 시시각각 혹은 실시간으로 수행후 바로 반영될 필요가 있는 RealTime &amp;amp; Online Training 이 있을 수 있다.&amp;nbsp;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Online Training 은 Fine Tuning 을 하는 시나리오는 아니지만, 기존의 Fine Tuning 된 weight 값을 불러 들여, 빠르게 변경분에 대하여 반영하는 로직이다.&lt;/li&gt;&lt;li&gt;이 경우 부분적으로 해당 Data 로 약간의 overfit 이 발생될 소지는 있다. 하지만, 변경분 Data 는 크기가 매우 작고, 변경 분 Data 의 왜곡을 최소화 하기 위해 일정량의 랜덤 sampling 데이타를 전 class 에 대하여 부분 추출후 섞어 주는 기법을 사용, 왜곡 현상을 줄일 수 있다.&lt;/li&gt;&lt;li&gt;RealTime Training 시나리오를 Production 에 반영하는 경우 가장 중요한 고려 점은&amp;nbsp;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;긴급한 Training 변경 요건을 빠르게 수행하여 운영중 모델에 적용할 수 있는가? 그리고,&lt;/li&gt;&lt;li&gt;다수의 Model 관리 Operator 가 실시간적으로 Training Data 를 손보는 경우 이를 받아들일 만큼 시스템이 유연한가? 일 것이다.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;우리가 구상하는 시스템은 BigData Scale Data 를 포함하고 있다. 1대의 GPU 로는 Disk 에 담기 힘든 규모의 Large Scale Data 로 확장 가능하다.&lt;/li&gt;&lt;li&gt;주기적으로 수행되는 다수의 Deep Learning 모델이 Pipe Line 을 통해 구성될 수 있으며, 일정 시간안에 모두 완료되어야 한다.&lt;/li&gt;&lt;li&gt;일시적으로 Deep Learning 모델 프로젝트가 다수개 동시 수행되는 경우 On-Premise GPU 리소스 로는 부족한 경우도 있다. Production Batch Training 을 위해 구성한 GPU 클러스터 시스템은 내부 모델 개발의 유연한 확장에도 활용가능 했으면 한다.&lt;/li&gt;&lt;li&gt;Multi GPU 그리고 Multi Host 를 지원하는 Deep Learning 모델을 만드는 것은 불가능하지 않지만, Tensorflow 에서는 너무 Low level 접근이 필요하고, Keras 에서는 한계가 크다. GPU Cluster 가 그런 부분을 프레임워크 레벨에서 보완해주는 아키텍처가 필요하다. (예, BigDL , TensorlfowOnSpark , Horovod ,&amp;nbsp;Azure Batch AI ...) 예를 든 솔루션에 대하여 뒷 부분에서 설명토록 하겠다.)&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;위 4번, 5번 때문에, 우리는 경험상 On-premise 보다는 Cloud 에 GPU 리소스가 존재하길 원했다. 프로젝트 시작 초기에는 모델개발에 굉장히 많은 GPU 리소스가 필요하다. 운영중에는 오픈 초기 여러번의 on-line Training&amp;nbsp; 이 운영자에 의해 시시각각 수행될 수 있으며, 매 새벽 배치를 통해 fine tuning 배치가 수행될 필요도 있다. 이 경우 GPU 1대 혹은 1대의 single 머신에서 모델이 Training&amp;nbsp; 되는 경우 최고의 성능에 도달하기 까지 매우 많은 시간이 소요 될 수도 있을 것이다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;이러한 이유로 우리는 아래와 같은 환경을 고려하게 되었다.&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;운영상의 효율성. 안정성. 모델개발 시점의 시간 및 투자비용 Save 를 위해 Cloud GPU 인프라가 효율적이다.&lt;/li&gt;&lt;li&gt;BigData Scale Data 를 통한 Training 및 빠른 Training 퍼포먼스를 위해 GPU Cluster 구성이 필요하다.&lt;/li&gt;&lt;li&gt;GPU 클러스터는 다양한 Deep Learning 프레임워크를 지원해야 한다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;우리가 사용하는 Deep Learning 프레임워크는 다음과 같다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Tensorflow&lt;/li&gt;&lt;li&gt;Keras&lt;/li&gt;&lt;li&gt;CNTK&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;다수의 Operator 가 관리자 페이지에서 online Training 을 동시에 여러 Job 을 수행 시킬때, 혹은 다수의 Deep Learning 모델러가 복수의 Job을 수행할때, 유연하게 자사의 GPU 클러스터가 반응할 수 있는 플랫폼이 필요했다.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;우리는 이를 위해 최종 적으로 다음과 같은 사전 시스템을 구성 및 테스트 해 본 적이 있다.&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Hadoop&amp;nbsp;+ Spark&amp;nbsp;+ BigDL&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Data Pre Processing 에 강점이 크며, 기존 Machine Learning 배치와도 잘 결합 가능하다.&lt;/li&gt;&lt;li&gt;기존의 Spark Job 과 한몸처럼 구성이 가능하다.&lt;/li&gt;&lt;li&gt;BigDL 이 아직은 구현된 모델이 많지 않다는 가장 큰 단점이 존재한다. 속도도 GPU 클러스터 보다는 떨어진다.&amp;nbsp;&lt;/li&gt;&lt;li&gt;내부에 기존 BigData 클러스터가 충분히 많고 Spark 를 중요 Machine Learning 개발 도구로 사용하는 경우 활용할 가치가 있다. (참고로 BigDL 은 Intel 이 주도하는 Open Source 이며, Spark 에 내장 라이브러리 처럼 활용 가능하다.)&lt;/li&gt;&lt;li&gt;이 시스템은 이미 On Premise 에 구축하여 일부 모델의 검증에 활용하고 있다. 하지만 최종 시스템으로 선정하지는 않았다.&lt;/li&gt;&lt;li&gt;이 시스템에 대한 경험은 아래에 블로그 했었다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://hoondongkim.blogspot.kr/2017/08/deep-learning-text-nlp-with-spark.html&quot;&gt;http://hoondongkim.blogspot.kr/2017/08/deep-learning-text-nlp-with-spark.html&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Hadoop&amp;nbsp;+ Spark&amp;nbsp;+ tensorflowOnSpark&lt;/li&gt;&lt;ol&gt;&lt;li&gt;위 장점을 수용하면서 Tensorflow 를 사용할 수 있다.&lt;/li&gt;&lt;li&gt;BigDL 과 달리 CPU 뿐 아니라 GPU 도 사용 가능하다.&lt;/li&gt;&lt;li&gt;Tensorflow 에서 Multi GPU, Multi Host 병렬 확장을 10줄 미만의 간단한 코드 수정으로 수행할 수 있다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;VGG, inception 등 널리 알려진 모델은 이미 multi gpu 및 multi host 지원이 내장 구현되어 있다. 이런 경우는 tensorflow 만 쓰거나, 혹은 high level api 를 쓰더라도 큰 문제가 되지 않는다.&lt;/li&gt;&lt;li&gt;그러나, 대부분의 custom 모델이거나, 특히 RNN, LSTM 의 수많은 공개된 변종 알고리즘들은 병렬성이 구현된 구현체들이 거의 존재하지 않는다.&lt;/li&gt;&lt;li&gt;이를 직접 병렬성 구현해 주는 것은 매우 지난한 작업이다.&lt;/li&gt;&lt;li&gt;즉, tensorflowOnSpark (made By Yahoo) 는 이런 부분을 보완해 주는 Deep Learning 확장 도구이다.&lt;/li&gt;&lt;li&gt;이 시스템은 또한 On Premise 에 구축하여 Tensorlfow 모델의 일부 검증에 활용한 바 있다. 하지만 Main Production 시스템으로 선정하지는 않았으며, Dev 및 모델링 시점에 활용하는 용도로만 사용중이다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;가장 큰 이유는 내부에 CPU 클러스터는 수천 코어 동원이 가능하지만, GPU 클러스터는 리소스가 부족하기 때문이다.&lt;/li&gt;&lt;li&gt;Cloud 에 구성하기에도 Hadoop 및 Spark&amp;nbsp; 디펜던시, 그리고, Legacy 가 Tensorlfow 보다도 훨씬 무거워, 유연한 시스템으로 사용하기에는 다소 Over Spec 인 느낌이 커보였다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;이 시스템을 아직 Production 으로 사용하지 않는 가장 큰 이유는 Keras 가 아직 완벽하게 지원되지 않고 있다는 점이다. 우리는 많은 모델을 빠르게 실험해보고 적용해 보기 위해 Keras 를 상당한 비중으로 사용중에 있다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;논문 쓸 목적이 아니라면, 그리고, Agile 한 Deep Learning Approach 를 위해서는 Keras 는 최고의 툴이다.&amp;nbsp;&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;이 시스템에 대한 경험 그리고 세팅 방법은 아래에 블로그 했었다.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://hoondongkim.blogspot.kr/2017/09/bigdata-distributed-deep-learning.html&quot;&gt;http://hoondongkim.blogspot.kr/2017/09/bigdata-distributed-deep-learning.html&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;Tensorflow&amp;nbsp;+ Keras&amp;nbsp;+ CNTK&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&amp;nbsp;&lt;b&gt;&lt;u&gt;최종 아키텍처로 선정한 시스템이다.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;li&gt;TenosrlflowOnSpark 와 마찬가지로 CPU 와 GPU 모두 사용 가능하다.&lt;/li&gt;&lt;li&gt;Production 용 배치 프레임워크의 아키텍처로 더 적당하다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;즉, 모델 개발 및 Dev 시스템은 꼭 이 구성일 필요는 없다. 아니, Dev 시스템은 이 구성이 오히려 개발 생산성에 저해가 될 수 있다.&lt;/li&gt;&lt;li&gt;Dev 에서는 위 1,2가 사용될 수 있다.&lt;/li&gt;&lt;li&gt;모델 개발 시점에는 Cluster 보다 Scale Up된 High End 장비 혹은 GPU VM이 더 편할 수 있다.&amp;nbsp;&lt;/li&gt;&lt;li&gt;단, 다양하게 하이퍼 파라미터를 실험 적용해보는 단계에서는 Scale Out 가능한 이 구성이 더 유리하다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;앞 4개는 Open Source Deep Learning 도구들이며 각각 장단점이 있고, 때로는 보완 도구로서 함께 사용해야 한다.(특히 Keras)&lt;/li&gt;&lt;li&gt;CNTK 는 보완 및 대체제로 경우에 따라 사용이 가능하다. CNTK 는 Keras 환경에서 기존의 Keras Source Code를 거의 수정하지 않고(우리의 Production 코드는 1~2줄 미만의 수정으로, 때로는 0줄 수정으로 가능하였다.) Backend 를 Tensorlfow 에서 CNTK 로 바꾸어 모델마다 선별적으로 선택하여 사용할 수 있다. 그리고, 다음과 같은 장점이 있다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Keras 와 함께 사용시 기존 코드를 그대로, CNTK 적용이 가능하다.&lt;/li&gt;&lt;li&gt;RNN 계열에서 Tensorflow 보다 빠르다고 알려져 있다.&lt;/li&gt;&lt;li&gt;대부분의 병렬 Deep Learning 수행에 있어, 훨씬 적은 노력으로 확장이 가능하다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Horovod 는 Uber 가 만든 Deep Learning 분산 프레임워크로 Tensorflow 와 Keras 를 지원한다.&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-qNdopWcQAnA/Wku1L2ZXfZI/AAAAAAAAGkY/TGSHQjLC5_E0pC_l3nzBrHS6yU4PGcukACLcBGAs/s1600/horovod_benchmark.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;442&quot; data-original-width=&quot;1028&quot; height=&quot;274&quot; src=&quot;https://2.bp.blogspot.com/-qNdopWcQAnA/Wku1L2ZXfZI/AAAAAAAAGkY/TGSHQjLC5_E0pC_l3nzBrHS6yU4PGcukACLcBGAs/s640/horovod_benchmark.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;Tensorflow 및 Keras 의 Low Level 코드 접근 없이 Multi GPU , Multi Host 사용이 가능하다.&lt;/li&gt;&lt;li&gt;위는 BechMark&amp;nbsp; 자료이다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Azure Batch AI&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Azure 가 제공하는 Deep Learning 프레임워크의 병렬 확장 및 Auto Scale Out 을 지원하는 PaaS 이다.&lt;/li&gt;&lt;li&gt;Tensorflow , keras, cntk , horovod(Manually) 등을 지원한다.&lt;/li&gt;&lt;li&gt;CNTK 를 Backend 로 사용 시 병렬확장성에 대한 Code 접근을 추상화 할 수 있다.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;즉, 이 때문에 tensorflow&amp;nbsp;+ keras 코드를 CNTK&amp;nbsp;+ keras 코드로 수정한 후, 이를 Azure Batch AI 에 호스팅 하는 경우 Auto Scale Out 의 장점을 극대화 할 수 있다.&lt;/li&gt;&lt;li&gt;모든 것이 Compute Parallel 이 되지는 않을 수 있다.(Horovod 와는 다른 방식이다.) 모델에 따라 Data Parallel 만 될 수도 있다는 의미이며, 이 부분에 대해서는 추후 좀더 실험을 해볼 예정이다.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Tensorflow&amp;nbsp;+ Horovod&amp;nbsp; + Azure Batch AI 구성 시 native tensorflow 코드를 Low Level 코드 접근 없이 multi Gpu, multi host 확장이 가능할뿐 아니라 Auto Scale Out 이 가능하다.&lt;/li&gt;&lt;li&gt;비용에 대한 Limit 을 위해 Min Node 수와 Max Node 수 설정이 가능하다.&lt;/li&gt;&lt;li&gt;하나의 Job 의 확장 뿐 아니라, 복수 GPU 클러스터 Node 에 복수의 Deep Learning&amp;nbsp; Job 을 분배 및 할당하는 Cluster 로도 활용 가능하다.&lt;/li&gt;&lt;li&gt;이는 Production 용 Training 시스템 뿐 아니라, 다수의 Deep Learning Model 개발자들이 있는 경우 모델 개발이나 하이퍼파라미터의 빠른 튜닝을 위해서도 활용 가능한 시스템 이다.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;여기서부터는 위 최종 선정 시스템을 이용 실제 Production 용 Legacy Deep Learning 모델을 Azure Batch AI 위에서 병렬 구동한 모습이다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;Tensorlfow&amp;nbsp;+ Keras 로 작업된 Legacy 코드를 CNTK&amp;nbsp;+ Keras&amp;nbsp;+ Azure Batch AI 로 포팅하여&amp;nbsp;Multi Host&amp;nbsp;+ Multi GPU 로 수행한 화면.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-7kqdmfVBFYw/WkuwLUjStYI/AAAAAAAAGkA/k2u2EURonTo6N-cHfzchz_axD0k_jrFYQCLcBGAs/s1600/cntk_keras.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;787&quot; data-original-width=&quot;1261&quot; height=&quot;398&quot; src=&quot;https://1.bp.blogspot.com/-7kqdmfVBFYw/WkuwLUjStYI/AAAAAAAAGkA/k2u2EURonTo6N-cHfzchz_axD0k_jrFYQCLcBGAs/s640/cntk_keras.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;li&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;Tenosrflow&amp;nbsp;+ Keras 로 작업된 Legacy 코드를 Tenosorflow&amp;nbsp;+ Keras&amp;nbsp;+ horovod&amp;nbsp;+ Azure Batch AI 로 포팅하여 Multi Host&amp;nbsp;+ Multi GPU 로 수행한 화면.&lt;/u&gt;&lt;/b&gt;&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-TbhJxiGAh7Q/WkuxM8xJs4I/AAAAAAAAGkI/LCeVt8mKUHwDRbrpsMBmd4wnjzpqsvJoACLcBGAs/s1600/horovod1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1296&quot; height=&quot;492&quot; src=&quot;https://2.bp.blogspot.com/-TbhJxiGAh7Q/WkuxM8xJs4I/AAAAAAAAGkI/LCeVt8mKUHwDRbrpsMBmd4wnjzpqsvJoACLcBGAs/s640/horovod1.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-6fcci7G4axw/WkuxRe3U5HI/AAAAAAAAGkM/8jf1iZsbqy4PGyzMBF9Lba0vPCGaMgXhwCLcBGAs/s1600/horovod2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1296&quot; height=&quot;492&quot; src=&quot;https://1.bp.blogspot.com/-6fcci7G4axw/WkuxRe3U5HI/AAAAAAAAGkM/8jf1iZsbqy4PGyzMBF9Lba0vPCGaMgXhwCLcBGAs/s640/horovod2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;li&gt;수행 이후 수행 결과를 PaaS 상에서 확인&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;수행결과&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-4tw30ikJKro/WkvNRm_g2JI/AAAAAAAAGko/jAP8aD5BfEMkgVSFcbBzOUl0xejOYUsjwCLcBGAs/s1600/complete.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1280&quot; height=&quot;492&quot; src=&quot;https://1.bp.blogspot.com/-4tw30ikJKro/WkvNRm_g2JI/AAAAAAAAGko/jAP8aD5BfEMkgVSFcbBzOUl0xejOYUsjwCLcBGAs/s640/complete.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;클러스터 모니터링&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-H9UBcn5QOkQ/WkvO1coAM3I/AAAAAAAAGk0/HcQ93tk0UaIyWEcYkP7e82d63e-wB7JVQCLcBGAs/s1600/complete2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1137&quot; height=&quot;372&quot; src=&quot;https://3.bp.blogspot.com/-H9UBcn5QOkQ/WkvO1coAM3I/AAAAAAAAGk0/HcQ93tk0UaIyWEcYkP7e82d63e-wB7JVQCLcBGAs/s640/complete2.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;div&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;[결론]&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;div&gt;위 처럼 Tensorflow&amp;nbsp;+ Keras 로 작성된 Legacy Deep Learning Training 코드는 코드 수정을 거의 하지 않고, Single GPU 에서만 돌던 코드가 Azure Batch AI 위에서 Mulit GPU 와 Multi Host 로 병렬 구동됨을 확인 하였다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;위를 위해서 CNTK&amp;nbsp;+ Keras&amp;nbsp;+ Azure Batch AI 환경에서는 Legacy 코드를 단 1줄도 수정하지 않고, Backend 설정만 Tensorflow 에서 CNTK 로 변경하여 Azure Batch AI 위에서 병렬 수행됨을 확인 하였다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;하지만, 모든 Legacy 코드가 Keras&amp;nbsp; 코드로만 구성된 것은 아니다. Keras 가 지원하지 않는 최신의 기법을 사용하는 경우. (예를 들어 2017년 여름경 까지 Keras 는 LSTM 에 Attention 을 포함하는 것을 지원하지 않았었다.) Tensorlfow 코드와 Keras 코드가 섞여 사용하는 경우가 실제로 우리의 경우도 존재 한다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;때로는 github 에 구현체가 공개된 잘짜여진 open source&amp;nbsp; 코드가 keras 가 아닌 tensorflow 로만 구성된 코드일 수도 있다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;이를 위해 우리는 Horovod 를 이용한 Multi Host&amp;nbsp;+ Multi GPU 구동도 함께 테스트 했었다.&lt;/div&gt;&lt;div&gt;Tensorflow&amp;nbsp;+ Keras&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI&amp;nbsp; 구성은 그러한 시나리오를 위한 추가 확장 환경 구성이었다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;tensorflow&amp;nbsp;+ Keras&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI 환경에서는 기존 Legacy 코드를 약 4~5 줄 정도를 추가하고, 2줄 정도를 변경하였으며, 변경을 하는데 들어간 시간은 최초 작업이었을 때에도 3~5분 미만으로 매우 쉬운 편이었다. 익숙해지면, 1~2분 안에 코드 수정이 가능한 수준이다.&lt;br /&gt;&lt;br /&gt;단, CNTK&amp;nbsp;+ Keras&amp;nbsp;+ Azure Batch AI 와는 다르게, tensorflow&amp;nbsp;+ Keras&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI 는 공식 github 에 example 도 존재하지 않으며, 다소 초기 세팅시 Debug 후 수동 설정 변경 사항이 많이 필요하였다.( 최초에 한번만 수행해주면 된다. ) 이 복잡한 초기 세팅 부분은 별도로 블로깅 예정이다 . 해당 부분을 따라한다면 어렵지 않게 Legacy 코드를 수행할 수 있을 것이다.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;이번 블로깅에서는 고찰 및 구성 결론 만 다루었다. 다음 연재에서는 실제로 구성하는 방법과 구성 과정에서 구성을 debug 하는 방법, trouble shutting 하는 방법에 대하여 다루도록 하겠다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;연재 순서는 다음과 같다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;연재 1. 이번글.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;연재 2. Deep Learning Multi Host &amp;amp; Multi GPU Architecture #2 - Keras 를 이용한 Scale Up, Horovod 를 이용한 Scale Out 성능 비교&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;연재 3. CNTK&amp;nbsp;+ Keras&amp;nbsp;+ Azure Batch AI 구성 방법&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;연재 4. Tensorlfow&amp;nbsp;+ Keras&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI. Mnist 간단 버전 구성 설정 방법&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;연재 5. Tensorlfow&amp;nbsp;+ Keras&amp;nbsp;+ Horovod&amp;nbsp;+ Azure Batch AI. Legacy Code Advanced 버전 구성 설정 방법.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;연재 6. Azure Batch AI 를 이용한 Custom Deep Learning 모델 구동 시 Trouble Shutting 을 좀더 쉽게 하는 방법.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;ol&gt;&lt;/ol&gt;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/1794233329898911671/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2018/01/deep-learning-multi-host-multi-gpu.html#comment-form' title='0개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/1794233329898911671'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/1794233329898911671'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2018/01/deep-learning-multi-host-multi-gpu.html' title='Deep Learning Multi Host &amp; Multi GPU Architecture #1 - 고찰 및 구성 with tensorflow, cntk, keras, horovod, Azure Batch AI'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2.bp.blogspot.com/-qNdopWcQAnA/Wku1L2ZXfZI/AAAAAAAAGkY/TGSHQjLC5_E0pC_l3nzBrHS6yU4PGcukACLcBGAs/s72-c/horovod_benchmark.png" height="72" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-2694692755996291254</id><published>2017-12-27T01:34:00.003+09:00</published><updated>2017-12-27T02:18:10.536+09:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI Serving"/><category scheme="http://www.blogger.com/atom/ns#" term="CNTK"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="GPU"/><category scheme="http://www.blogger.com/atom/ns#" term="Keras"/><category scheme="http://www.blogger.com/atom/ns#" term="TensorFlow"/><title type='text'>Deep Learning Inference &amp; Serving Architecture 를 위한 실험 및 고찰 1 - GPU vs CPU</title><content type='html'>최근 Production을 위한 Deep Learning Serving 레이어의 아키텍처를 구성하기 위해 몇가지 실험을 했던 내용을 정리해 보았다. 몇번에 걸쳐 연재 될 수 있을 듯 한데...&lt;br /&gt;&lt;br /&gt;우선 오늘의 주제는&amp;nbsp;&lt;b&gt;&lt;u&gt;Inference 에 있어서도 정말 GPU 가 CPU보다 유리한가? (결론이 유추 되는가???)&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;아키텍처를 구성함에 있어, 다양한 Use Case 가 있을 수는 있으나, 정답이 존재하지는 않는다고 생각한다. 다양한 비지니스로직, 데이타의 양이나 성격, 사용하는 기술들, 개발 언어, 목적하는 바, 동시접속자 수, On-premise , Cloud 여부 등에 따라 다양한 조합이 있을 수 있을 것이고, 각각이 장단점이 존재할 것이기 때문이다.&lt;br /&gt;&lt;br /&gt;여러 참고 자료로 사전 조사를 해본것은 사실이지만, 우리의 Deep Learning 모델과 우리의 데이타, 우리의 로직으로 후보 아키텍처에서 직접 실험을 하고 아키텍처를 확정하고자 아래와 같은 실험을 진행하였다.&lt;br /&gt;&lt;br /&gt;실험에 앞서, 주안점은 다음과 같다.&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;우리는 Keras , Tensorflow (일부 CNTK) 등의 Deep Learning Framework 로 만들어진 Deep Neural Network 모델을 이용, 대국민 서비스를 준비하고 있다.&lt;/li&gt;&lt;li&gt;다수의 동접이 있을 수 있고, 이벤트나 행사 여부, 광고 및 홍보 여부에 따라, 트래픽이 매우 가변적일 수 있고, 다수의 동접 및 다수의 inference 가 일어날 수 있다.&lt;/li&gt;&lt;li&gt;많은 동접의 경우에도 응답은 1초 이내를 목표로 한다.&lt;/li&gt;&lt;li&gt;Deep Learning 모델은 RNN, LSTM 류와 간단한 류의 CNN 이 주를 이룬다.(우리의 모델은 vgg, inception 류의 heavy CNN&amp;nbsp; 은 아니다.)&lt;/li&gt;&lt;li&gt;우리의 모델이 특이한 점은 주기적인 Fine Tuning 과 시시 각각의 RealTime on-line Training 이 운영중인 Model 에 시시각각 이루어진다는 점이다. 이는 Serving Layer 설계에 있어 중요 고려 사항 중 하나인데, Serving 되는 모델의 Size 를 줄이고 응답 속도를 빠르게 하기 위한 알려진 기법들을 적용하는데 방해가 되는 요소이기 때문이다.&amp;nbsp;&lt;/li&gt;&lt;li&gt;Tensorflow 의 Serving 은 써본 사람들은 알겠지만, Web Service 로 만들어 배포하기에는 몇가지 제약이 느껴진다. 이 실험은 Tensorflow Serving 전용 엔진을 통한 실험은 아니며, 보다 High Level 로 접근하여 Inference Layer 를 직접 구현했을 때의 실험이다.&lt;/li&gt;&lt;/ol&gt;일반적으로 Inference 전용 Model Optimization 을 할 때는 주로 아래의 방법들이 사용된다.&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;check point 등 training 에서만 사용되는 operation 을 없앤다.&lt;/li&gt;&lt;li&gt;batch normalization ops 도 제거한다.&lt;/li&gt;&lt;li&gt;도달하지 않는 graph 영역 제거&lt;/li&gt;&lt;li&gt;Check Numeric 제거&lt;/li&gt;&lt;li&gt;각종 variable 값 들은 constant 로 바꾸어, 크기를 줄이고 속도를 빠르게 하며, Thread Safe 하게 변경한다.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;위 기법들은 Model 이 Freezing 된 경우에 한하여서이다. realtime training 과 inference 가 동시에 일어나는 경우는&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;training&amp;nbsp; model 과 inference&amp;nbsp; model 을 분리하고, 지연 동기화 시키거나&lt;/li&gt;&lt;li&gt;둘을 하나로 가져가되 constant 화 하고 freezing 하는 것을 포기해야 한다.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;div&gt;위 둘의 중간도 가능할 수는 있다. Node 가 복수개인 경우 Training&amp;nbsp; 중인 Node 가 잠시 Serving Node 에서 빠져 있는 경우가 그 경우에 해당 할 것이다. 후에 우리는 Serving Layer 에 있어, Auto Scale Out 가능한 Docker PaaS나 Microservice Serverless PaaS 를 중요 고려 요소로 낙점하고 추가적인 실험을 하였는데, (아마 이 연재가 좀더 계속되어진다면, 다시 상세하게 다루어 보도록 하겠다. ) 이 시나리오 에서는 Training Layer 와 Serving Layer 를 각각의 장점을 극대화 시키고, Model 파일의 경우만 지연 공유시키는 또다른 시나리오가 나올 수 있다.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;우선 오늘 다룰 내용은 위에서 언급된 다양한 방법론의 첫단추로서, 우리 모델이 과연 CPU 에서 더 잘 inference 되는 지, GPU 에서 더 잘 inference 되는지 의 여부에 대하여 실험 해본 결과이다. (ps. 실험에서 사용된 모델은 실험용으로 실제 모델이 다소 단순화된 General Model 임)&lt;br /&gt;다수 동접 Inference 테스트 전 training 퍼포먼스 또한 실험한 결과를 함께 정리 하였다.&lt;br /&gt;&lt;br /&gt;[&lt;b&gt;&lt;u&gt;1] 실험에 사용된 딥러닝 모델 및 모델 크기&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;- 일반적인 Language Model 용 LSTM 모델&lt;br /&gt;- categorical_cross_entropy 사용&lt;br /&gt;- Top 1 분류 모델&lt;br /&gt;- Word Embedding Layer 차원 수 : 300차원&lt;br /&gt;- Total Parameter 수 : 80,656,435 개&lt;br /&gt;&lt;br /&gt;- [특이사항] Language Model 특성상 Total Parameter 에서 앞부분이 많은 부분 차지.&lt;br /&gt;- [특이사항] LSTM 이 번역등의 문제가 아닌 Text Classification 문제에 적용된 경우 이므로, 층이 복잡하지는 않음. 층을 복잡하게 하여도 성능 향상 없었음. 그러나, 일반적인 word2vec + cnn 보다는 3% 정도 성능 향상된 모델임.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-BEoIDHIUyPA/WkJZVBzDqQI/AAAAAAAAGZM/C0ZOxb7v230a5RJrhffvaF993eONZMmIwCLcBGAs/s1600/1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;61&quot; data-original-width=&quot;246&quot; src=&quot;https://1.bp.blogspot.com/-BEoIDHIUyPA/WkJZVBzDqQI/AAAAAAAAGZM/C0ZOxb7v230a5RJrhffvaF993eONZMmIwCLcBGAs/s1600/1.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;&lt;u&gt;[2] 실험에 사용된 hyper parameter&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;- epochs=4 , batch_size=64 , optimizer=adam&lt;br /&gt;- learning Rate=0.001, beta_1=0.9 , beta_2=0.999, epsilon=1e-8&lt;br /&gt;&lt;br /&gt;- [특이사항] CPU Training 은 GPU 와 달리 batch size 를 4096등 훨씬 큰 수치를 주어도 memory resource 고갈 에러가 발생하지 않음, drop out 이나 batch bormalize 를 적절히 쓰는 경우 batch size 를 크게 주는 경우, 정확도는 비슷한데, 더 빨리 Training 이 될 수 있음.&lt;br /&gt;- 즉, CPU vs GPU 트레이닝 퍼포먼스는 실무에서는 batch_size 를 달리 주어, CPU가 아래보다 더 빨리 응답하는 것도 가능하나, 동일 hyper parameter 값에 대한 성능 비교를 위해 아래에서는 동일 값으로 수행한 결과 이다.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;b&gt;&lt;u&gt;[3] 실험에 동원된 HW 장비 Spec&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;- cpu : 12 vcore , 112gb Memory ( On Azure Cloud VM )&lt;br /&gt;- gpu : K80 GPU * 2개 , 12 vcore , 112GB Memory ( On Azure Cloud Data Science GPU VM ) (단, 모델 inference 시에는 1개 GPU 만 사용하여 실험하였음)&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;u&gt;[4] Training Data 크기&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;- training data row 수 : 1,649,415 건&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;u&gt;[5] Training 속도 및 성능&lt;/u&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;CPU Training Performance&lt;/li&gt;&lt;ol&gt;&lt;li&gt;epoch1 : 23,790 초&lt;/li&gt;&lt;li&gt;epoch2 : 24,071 초&lt;/li&gt;&lt;li&gt;epoch3 : 24,026 초&lt;/li&gt;&lt;li&gt;epoch4 : 24,100 초&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-HIzXLfsKWdU/WkJc_gCV-UI/AAAAAAAAGZY/XCAU1V1rZlsUH4EXy5FbOMnRAHQ4m9QKQCLcBGAs/s1600/2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;148&quot; data-original-width=&quot;906&quot; src=&quot;https://1.bp.blogspot.com/-HIzXLfsKWdU/WkJc_gCV-UI/AAAAAAAAGZY/XCAU1V1rZlsUH4EXy5FbOMnRAHQ4m9QKQCLcBGAs/s1600/2.png&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;GPU Training Performance&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;epoch1 : 8,612초&lt;/li&gt;&lt;li&gt;epoch2 : 8,370초&lt;/li&gt;&lt;li&gt;epoch3 : 8,377초&lt;/li&gt;&lt;li&gt;epoch4 : 8,360초&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-zzcI69ROQZ8/WkJdw7gru9I/AAAAAAAAGZg/0RfK8gprCEkUcbzDMltueIOCzW-tujMqACLcBGAs/s1600/3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;176&quot; data-original-width=&quot;715&quot; src=&quot;https://3.bp.blogspot.com/-zzcI69ROQZ8/WkJdw7gru9I/AAAAAAAAGZg/0RfK8gprCEkUcbzDMltueIOCzW-tujMqACLcBGAs/s1600/3.png&quot; /&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;div&gt;&lt;b&gt;&lt;u&gt;[6] Single Inference 성능&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://1.bp.blogspot.com/-X4gtvXIIdlg/WkJebYMfGLI/AAAAAAAAGZs/SsdRlW15kiIFdpvCjBKW8hKKSxzcbEcrgCLcBGAs/s1600/4.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: left; display: inline !important; margin-bottom: 1em; margin-right: 1em; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;273&quot; data-original-width=&quot;665&quot; src=&quot;https://1.bp.blogspot.com/-X4gtvXIIdlg/WkJebYMfGLI/AAAAAAAAGZs/SsdRlW15kiIFdpvCjBKW8hKKSxzcbEcrgCLcBGAs/s1600/4.png&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;예상과 달리 CPU가 Wall Time 이 더 빠르다. Wall Time 은 벽걸이 시계 시간을 의미한다. 즉 실제, 걸린 시간이다. CPU 의 경우 user time 은 150에 육박하는 경우가 있는데, 그래도 wall time 은 일정하게 30에서 40 사이의 값을 보여준다. 여기에서 다음과 같은 가정을 해볼 수 있다. CPU 는 멀티코어를 써서 더 빠른가??&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;[7] Multiple Sequential Inference 성능&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;이번에는 1개 Inference 가 아닌 1000번의 inference 를 sequencial&amp;nbsp; 하게 (Not 병렬) 수행해보았다. 그리고, 위 (6)의 가정이 맞는지 cpu 및 gpu 의 usage 상황을 확인해 보았다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;사용된 코드도 아래에 참고로 넣어 보았다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-c6893U5Sx4M/WkJgVl5xIxI/AAAAAAAAGZ4/i3yHu4WVluQg7yeIlsoTeUrx5uuNonQOACLcBGAs/s1600/5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;946&quot; src=&quot;https://2.bp.blogspot.com/-c6893U5Sx4M/WkJgVl5xIxI/AAAAAAAAGZ4/i3yHu4WVluQg7yeIlsoTeUrx5uuNonQOACLcBGAs/s1600/5.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-ScLgVsha_gg/WkJg8RJL0zI/AAAAAAAAGaA/QZSOlcmG8zIsuKxVwYAcIQ7cZJpQhFbUACLcBGAs/s1600/6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;686&quot; data-original-width=&quot;935&quot; src=&quot;https://2.bp.blogspot.com/-ScLgVsha_gg/WkJg8RJL0zI/AAAAAAAAGaA/QZSOlcmG8zIsuKxVwYAcIQ7cZJpQhFbUACLcBGAs/s1600/6.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;1000번을 연속하여 serial 하게 수행해보자 위와 같은 속도가 측정되었으며, 평균을 내 보면, CPU는 20ms , GPU는 60 ms 정도가 걸렸다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;특이한점은 , 처음 예상과 비슷하게, CPU는 멀티코어를 쓰고 있고,(코어 전체를 쓰진 않았음, 위 스크린샷 시점에는 420% 정도가 동작하고 있음.), GPU 는 GPU 1번 코어만을 50~70% 정도 사용하고 있으며, CPU 는 1개 CPU만 100% 사용하고 있다는 점 이었다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;여기에서 한가지 의문이 생겼다. 그렇다면 혹시 CPU가 병목인가?&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;그리고, 그렇다면, 혹시 위 코드에서 사용된 유일한 Pre Processing 인 Tokenizer 가 영향을 주고 있는 것인가?&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;그래서 위 실험에 사용된 코드에서 Tokenizer 전처리 Pre Processing 부를 1000번의 loop 바깥으로 빼보고 성능 향상 정도를 측정해 보았다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;[8]&amp;nbsp;CPU&amp;nbsp; 에서 Tokenizer 가 주는 영향&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-A8oPdXGOU0c/WkJiga3pGDI/AAAAAAAAGaM/CCEs3AK6SHA1ZOBMjqnRKYWy9ELdI5ExgCLcBGAs/s1600/7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;317&quot; data-original-width=&quot;1190&quot; src=&quot;https://1.bp.blogspot.com/-A8oPdXGOU0c/WkJiga3pGDI/AAAAAAAAGaM/CCEs3AK6SHA1ZOBMjqnRKYWy9ELdI5ExgCLcBGAs/s1600/7.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;위 처럼 CPU 실험에서는 Pre-Processing 영역인 Tokenizer 가 주는 영향은 5% 정도에 지나지 않았다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;[9] GPU에서 Tokenizer 가 주는 영향&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-RcpfJrD09tQ/WkJlcPlJPKI/AAAAAAAAGac/3U_JJ8S5JUQgkCmXCLJLaet4ajz-9gyKwCLcBGAs/s1600/8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;268&quot; data-original-width=&quot;1118&quot; src=&quot;https://4.bp.blogspot.com/-RcpfJrD09tQ/WkJlcPlJPKI/AAAAAAAAGac/3U_JJ8S5JUQgkCmXCLJLaet4ajz-9gyKwCLcBGAs/s1600/8.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;GPU에서도 마찬가지로 Pre-Processing 부분이 주는 영향은 2% 정도에 지나지 않았다. %는 줄었고, 절대치는 비슷하다. 즉, CPU에서이든 GPU에서이든 Pre-Processing 은 CPU 를 이용하기 때문에 절대치는 비슷한것이 이치에 맞다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;위 상황에서 CPU와 GPU&amp;nbsp; 의 USAGE 도 확인해 보았다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-bw-1OS_Ys7U/WkJmyDXorVI/AAAAAAAAGao/IraNAFW4SisZNCqHNfSq-F_J4NlR2DoYACLcBGAs/s1600/9.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;770&quot; data-original-width=&quot;914&quot; src=&quot;https://4.bp.blogspot.com/-bw-1OS_Ys7U/WkJmyDXorVI/AAAAAAAAGao/IraNAFW4SisZNCqHNfSq-F_J4NlR2DoYACLcBGAs/s1600/9.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;여전히 CPU 는 1Core 만 100% Full로 일하고 있고, GPU 는 50%에서 70%를 왔다갔다 하였다. 그러므로, CPU 는 전처리 때문이 아닐까 하는 가정은 False 라고 할 수 있을 것이다. 해당 가정에 대한 의문은 해소 되었다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;[10] 이제 실제 Inference 테스트를 위해 모델을 flask microservice 형태로 배포하였다.&amp;nbsp;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-5n7tCMcH_8I/WkJnlD-ezxI/AAAAAAAAGa0/PVLcglamwR4v9YsJwVOtSaNV-6yIEfaAQCLcBGAs/s1600/10.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;184&quot; data-original-width=&quot;932&quot; src=&quot;https://2.bp.blogspot.com/-5n7tCMcH_8I/WkJnlD-ezxI/AAAAAAAAGa0/PVLcglamwR4v9YsJwVOtSaNV-6yIEfaAQCLcBGAs/s1600/10.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;위 구동은 CPU 전용 머신과 GPU 전용 머신에서 각각 해 주었다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;실제 Production 에서는 성능 극대화를 위해 하나의 머신에 port 를 달리하여 여러 Process 를 띄우고 앞에 웹프록시 등을 두는 것이 일반적이지만, 이 실험은 상대적인 비교를 위함이 목적이므로, 그런 작업을 해주지는 않았다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;[11] 이제 실제 운영 Production 환경과 유사한 환경에서 동시접속수행 Test 를 해보겠다.&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;이곳에 따로 기재하진 않았으나, 앞선 시점에서의 비교는 Jupyter Notebook 에서 이루어졌다. 모두가 아는 바와 같이 Jupyter Notebook 위에서의 구동은 단일 Python 독립 프로세스보다 훨씬 느리다. 즉, 동일한 실험이 flask 위에서 구동된 경우 Jupyter 위에서 구동된 경우보다 훨씬 빠르다. 그리고, flask 는 경량의 비동기 웹 프레임워크 이기 때문에, 병렬 수행이 위의 경우보다 훨씬 빠르게 일어난다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;위에서는 Serial 하게 한번에 하나씩만 Job 을 수행하였다. 이번 flask 에서의 수행은 다수 User 에서의 Parallel 동접 수행이다. 때문에 Serial 하게 수행했을 때보다 더 많은 병렬 Job 이 수행되었고, 그 결과 Jupyter 에서의 serial 수행보다 성능이 훨씬 더 좋게 나오고 있다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;(1) CPU 에서의 최종 결과&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;[Run User]&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;[TPS]&amp;nbsp; &amp;nbsp;[Time Per Request : millisec]&amp;nbsp; &amp;nbsp;[Transfer rate: Kbytes/sec]&amp;nbsp; &amp;nbsp;&amp;nbsp; [Complete Request]&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;[Failed Request]&lt;br /&gt;1&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; 200.35&amp;nbsp; 4.991&amp;nbsp; &amp;nbsp;30.13&amp;nbsp; &amp;nbsp;201&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;11&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;273.97&amp;nbsp; 3.650&amp;nbsp; &amp;nbsp;41.20&amp;nbsp; &amp;nbsp;274&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;21&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;245.21&amp;nbsp; 4.078&amp;nbsp; &amp;nbsp;36.88&amp;nbsp; &amp;nbsp;246&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;31&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;265.94&amp;nbsp; 3.760&amp;nbsp; &amp;nbsp;39.99&amp;nbsp; &amp;nbsp;266&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;41&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;265.97&amp;nbsp; 3.760&amp;nbsp; &amp;nbsp;40.00&amp;nbsp; &amp;nbsp;266&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;51&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;265.95&amp;nbsp; 3.760&amp;nbsp; &amp;nbsp;40.00&amp;nbsp; &amp;nbsp;266&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;61&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;258.76&amp;nbsp; 3.865&amp;nbsp; &amp;nbsp;38.92&amp;nbsp; &amp;nbsp;259&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;71&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;260.95&amp;nbsp; 3.832&amp;nbsp; &amp;nbsp;39.24&amp;nbsp; &amp;nbsp;262&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;81&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;244.99&amp;nbsp; 4.082&amp;nbsp; &amp;nbsp;36.84&amp;nbsp; &amp;nbsp;245&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;91&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;251.80&amp;nbsp; 3.971&amp;nbsp; &amp;nbsp;37.87&amp;nbsp; &amp;nbsp;253&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;[Run User]&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;[TPS]&amp;nbsp; &amp;nbsp;[Time Per Request : millisec]&amp;nbsp; &amp;nbsp;[Transfer rate: Kbytes/sec]&amp;nbsp; &amp;nbsp;&amp;nbsp; [Complete Request]&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;[Failed Request]&lt;br /&gt;300&amp;nbsp; &amp;nbsp;&amp;nbsp; 274.39&amp;nbsp; 3.644&amp;nbsp; &amp;nbsp;41.27&amp;nbsp; &amp;nbsp;275&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;310&amp;nbsp; &amp;nbsp;&amp;nbsp; 233.33&amp;nbsp; 4.286&amp;nbsp; &amp;nbsp;35.09&amp;nbsp; &amp;nbsp;234&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;320&amp;nbsp; &amp;nbsp;&amp;nbsp; 173.71&amp;nbsp; 5.757&amp;nbsp; &amp;nbsp;26.12&amp;nbsp; &amp;nbsp;174&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;330&amp;nbsp; &amp;nbsp;&amp;nbsp; 133.75&amp;nbsp; 7.476&amp;nbsp; &amp;nbsp;20.12&amp;nbsp; &amp;nbsp;134&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;340&amp;nbsp; &amp;nbsp;&amp;nbsp; 142.73&amp;nbsp; 7.006&amp;nbsp; &amp;nbsp;21.46&amp;nbsp; &amp;nbsp;143&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;350&amp;nbsp; &amp;nbsp;&amp;nbsp; 130.86&amp;nbsp; 7.642&amp;nbsp; &amp;nbsp;19.68&amp;nbsp; &amp;nbsp;131&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;360&amp;nbsp; &amp;nbsp;&amp;nbsp; 128.93&amp;nbsp; 7.756&amp;nbsp; &amp;nbsp;19.39&amp;nbsp; &amp;nbsp;129&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;370&amp;nbsp; &amp;nbsp;&amp;nbsp; 280.45&amp;nbsp; 3.566&amp;nbsp; &amp;nbsp;42.18&amp;nbsp; &amp;nbsp;281&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;380&amp;nbsp; &amp;nbsp;&amp;nbsp; 267.97&amp;nbsp; 3.732&amp;nbsp; &amp;nbsp;40.30&amp;nbsp; &amp;nbsp;268&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;390&amp;nbsp; &amp;nbsp;&amp;nbsp; 278.43&amp;nbsp; 3.592&amp;nbsp; &amp;nbsp;41.87&amp;nbsp; &amp;nbsp;279&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;br /&gt;400&amp;nbsp; &amp;nbsp;&amp;nbsp; 200.84&amp;nbsp; 4.979&amp;nbsp; &amp;nbsp;30.20&amp;nbsp; &amp;nbsp;201&amp;nbsp; &amp;nbsp;&amp;nbsp; 0&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;Apach 오픈소스 부하테스트 도구를 이용하여 동시접속 수행 성능을 측정하였다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;하나의 단일 User 가 비동기로 여러개의 병렬 수행을 요청하며, 그러한 User 또한 1에서 최고 400까지 늘려가며 측정 하였다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;CPU는 TPS 가 270 정도씩 나와 주고 있다. 그 시점 flask&amp;nbsp; 의 로그를 보면, 실제 초단위로 동일한 log 가 270여개 실제로 모두 200 응답으로 존재함을 확인 할 수 있다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-fR2wfGFFl50/WkJzWRgdRqI/AAAAAAAAGbE/PeJsCcSsAXst-PcKQr33q4UMWKRIoqbygCLcBGAs/s1600/11.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;680&quot; data-original-width=&quot;808&quot; src=&quot;https://3.bp.blogspot.com/-fR2wfGFFl50/WkJzWRgdRqI/AAAAAAAAGbE/PeJsCcSsAXst-PcKQr33q4UMWKRIoqbygCLcBGAs/s1600/11.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-jQ_4tc8V4Ao/WkJzfdDjq_I/AAAAAAAAGbI/gZphSY8bhHsS-uUFM9B1Bp5DonnejA6cQCLcBGAs/s1600/13.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;157&quot; data-original-width=&quot;640&quot; src=&quot;https://3.bp.blogspot.com/-jQ_4tc8V4Ao/WkJzfdDjq_I/AAAAAAAAGbI/gZphSY8bhHsS-uUFM9B1Bp5DonnejA6cQCLcBGAs/s1600/13.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;(2) GPU 에서의 최종 결과&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;[Run User]&amp;nbsp; &amp;nbsp; &amp;nbsp; [TPS]&amp;nbsp; &amp;nbsp;[Time Per Request : millisec]&amp;nbsp; &amp;nbsp;[Transfer rate: Kbytes/sec]&amp;nbsp; &amp;nbsp; &amp;nbsp;[Complete Request]&amp;nbsp; &amp;nbsp; &amp;nbsp; [Failed Request]&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;1&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;16.47&amp;nbsp; &amp;nbsp;60.735&amp;nbsp; 2.48&amp;nbsp; &amp;nbsp; 17&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;11&amp;nbsp; &amp;nbsp; &amp;nbsp; 16.24&amp;nbsp; &amp;nbsp;61.565&amp;nbsp; 2.44&amp;nbsp; &amp;nbsp; 17&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;21&amp;nbsp; &amp;nbsp; &amp;nbsp; 15.69&amp;nbsp; &amp;nbsp;63.740&amp;nbsp; 2.36&amp;nbsp; &amp;nbsp; 16&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;31&amp;nbsp; &amp;nbsp; &amp;nbsp; 11.60&amp;nbsp; &amp;nbsp;86.215&amp;nbsp; 1.74&amp;nbsp; &amp;nbsp; 12&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;41&amp;nbsp; &amp;nbsp; &amp;nbsp; 4.98&amp;nbsp; &amp;nbsp; 200.914&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0.75&amp;nbsp; &amp;nbsp; 5&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;51&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;61&amp;nbsp; &amp;nbsp; &amp;nbsp; 17.80&amp;nbsp; &amp;nbsp;56.186&amp;nbsp; 2.68&amp;nbsp; &amp;nbsp; 18&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;71&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;81&amp;nbsp; &amp;nbsp; &amp;nbsp; 17.47&amp;nbsp; &amp;nbsp;57.242&amp;nbsp; 2.63&amp;nbsp; &amp;nbsp; 18&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;span style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;91&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;[Run User]&amp;nbsp; &amp;nbsp; &amp;nbsp; [TPS]&amp;nbsp; &amp;nbsp;[Time Per Request : millisec]&amp;nbsp; &amp;nbsp;[Transfer rate: Kbytes/sec]&amp;nbsp; &amp;nbsp; &amp;nbsp;[Complete Request]&amp;nbsp; &amp;nbsp; &amp;nbsp; [Failed Request]&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;1&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;15.87&amp;nbsp; &amp;nbsp;63.014&amp;nbsp; 2.39&amp;nbsp; &amp;nbsp; 16&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;11&amp;nbsp; &amp;nbsp; &amp;nbsp; 17.28&amp;nbsp; &amp;nbsp;57.886&amp;nbsp; 2.60&amp;nbsp; &amp;nbsp; 18&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;21&amp;nbsp; &amp;nbsp; &amp;nbsp; 15.69&amp;nbsp; &amp;nbsp;63.744&amp;nbsp; 2.36&amp;nbsp; &amp;nbsp; 16&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;31&amp;nbsp; &amp;nbsp; &amp;nbsp; 12.46&amp;nbsp; &amp;nbsp;80.263&amp;nbsp; 1.87&amp;nbsp; &amp;nbsp; 13&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;41&amp;nbsp; &amp;nbsp; &amp;nbsp; 5.76&amp;nbsp; &amp;nbsp; 173.748&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0.87&amp;nbsp; &amp;nbsp; 6&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;51&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;61&amp;nbsp; &amp;nbsp; &amp;nbsp; 17.33&amp;nbsp; &amp;nbsp;57.706&amp;nbsp; 2.61&amp;nbsp; &amp;nbsp; 18&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;71&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;81&amp;nbsp; &amp;nbsp; &amp;nbsp; 12.42&amp;nbsp; &amp;nbsp;80.529&amp;nbsp; 1.87&amp;nbsp; &amp;nbsp; 13&amp;nbsp; &amp;nbsp; &amp;nbsp; 0&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both;&quot;&gt;91&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;0&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-sRS-x1UUpvk/WkJ0IzHvsOI/AAAAAAAAGbU/sk0ZXImctAsuacykO_n3Q3HLecSAnDi3QCLcBGAs/s1600/14.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;518&quot; data-original-width=&quot;1092&quot; src=&quot;https://2.bp.blogspot.com/-sRS-x1UUpvk/WkJ0IzHvsOI/AAAAAAAAGbU/sk0ZXImctAsuacykO_n3Q3HLecSAnDi3QCLcBGAs/s1600/14.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;GPU 는 TPS 가 16 정도밖에 되지 않는 저조한 결과를 보여 주었다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;&lt;br /&gt;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;b&gt;&lt;u&gt;[결론]&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;(ps. 동시수행 과정에서의 cpu , gpu 사용량은 serial 1000건 수행의 경우와 매우 유사했다.)&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;CPU 의 경우 특징은 느려질 지언정 400동접까지도 Fail Request 가 하나도 없었다는 점이다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;그리고 Serial 하게 1000개를 수행했을때, 개당 20 ms 이던것이 병렬로 수행했을때에는 약 5배정도 빠른 성능을 보여 주었다.&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;하지만 GPU 의 경우는 동접자가 많아지면, Fail Request 가 매우 많아 졌고, TPS 자체도 CPU 에 비하여 매우 느렸다. Serial 수행에서 3배 느렸으나, 동접 수행은 거의 10배 이상 느렸다. CPU 코어가 여러개인것도 그렇지만, 메모리 등도 한몫을 했을 것으로 보여진다. GPU 의 경우는 serial 하게 측정했을때나 parallel 하게 측정했을때 모두 개당 응답 속도가 60ms&amp;nbsp; 정도로 일정하다는 특징도 보여주었다. 즉, 동시 처리가 좀 매우 취약해 보인다. 동시 수행하는 속도 개선 효과가 거의 없었다.(CPU 와는 전혀 다른 양상이다.)&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;또한 GPU inference 의 경우 단일 CPU 가 GPU 한개와 함께 힘들게 일하고 있었던 양상 또한, 병렬 동시접속 능력을 떨어뜨리는 계기가 된듯 하다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;(이는 Tensorflow 에서의 실험 내용이다. CNTK&amp;nbsp; 로도 동일한 모델을 수행할 수 있는데, 그 비교 결과는 다음 기회에 Posting 해보도록 하겠다.)&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;일단 CPU Inference 의 경우는, 메모리 사용량, CPU 사용량, 모두 저정도의 traffic 에 대하여는 안정적인 결과를 보여주는 것을 확인 할 수 있었다. GPU 의 경우는 Production 으로 대국민 서비스를 하기에 매우 위험한 수준이었다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;Developer 입장에서, 우리에게 CPU는 사실 GPU에 비하여 상대적으로 너무나 친숙하고 풍족한 리소스 이다. 게다가, 우리는 Docker , Microservice , Spark on Hadoop Yarn, Mesos&amp;nbsp; 등 다양한 CPU&amp;nbsp;+ in-Memory 병렬 cluster 솔루션이 있다. 그리고 Public Cloud 에도 Auto Scale Out&amp;nbsp; 이 가능한 Docker PaaS 나 Serverless MicroService PaaS 가 GPU 보다는 훨씬 저렴한 가격으로 구비되어 있다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;AI Serving Layer 는 이렇게 일단 우리의 시나리오와 우리의 모델에 있어서는 GPU 를 제끼고 고려할만 한 실험 결과가 나왔다. ( 물론 VGG, Inception 등 층이 깊은 Deep Learning 모델은 이것과 다른 양상이 나올 지도 모른다. TensorRT 등 Nvidia 는 Inference 전용 솔루션도 내놓은 것으로 알고 있다.)&amp;nbsp;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;지금 담담하게 결론을 적고 있지만, 사실 실험결과가 이렇게 나왔을때 나는 흥분을 감추지 못했다. 하마터면 수억원짜리 GPU가 6~8개씩 꼳 힌 장비를 수대 구매하는 프로세스를 태울 뻔 했기도 했지만.... 무엇보다, 대용량 서비스와 다수 동접자를 위한 아키텍처에, 위 결론에서는 많은 솔루션과 오픈소스 그리고 public cloud 를 쓸 수 있는 풍부한 가능성이 생겼다는 점에서 안도의 한숨을 쉴 수 있었다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;PS. 위 실험 결과와 결론 유추 과정에서 제가 범한 논리적 오류가 있었다면 지적해주시기 바랍니다. 공유 이후 컴멘트는 곧 저에게는 배움이기도 하다는 자세로 공유를 실천 하고 있습니다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 14px;&quot;&gt;&lt;/div&gt;</content><link rel='replies' type='application/atom+xml' href='http://hoondongkim.blogspot.com/feeds/2694692755996291254/comments/default' title='댓글'/><link rel='replies' type='text/html' href='http://hoondongkim.blogspot.com/2017/12/deep-learning-inference-serving.html#comment-form' title='4개의 덧글'/><link rel='edit' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/2694692755996291254'/><link rel='self' type='application/atom+xml' href='http://www.blogger.com/feeds/8166686140676460430/posts/default/2694692755996291254'/><link rel='alternate' type='text/html' href='http://hoondongkim.blogspot.com/2017/12/deep-learning-inference-serving.html' title='Deep Learning Inference &amp; Serving Architecture 를 위한 실험 및 고찰 1 - GPU vs CPU'/><author><name>HoonDong Kim</name><uri>http://www.blogger.com/profile/00470004459268852878</uri><email>noreply@blogger.com</email><gd:image rel='http://schemas.google.com/g/2005#thumbnail' width='16' height='16' src='https://img1.blogblog.com/img/b16-rounded.gif'/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-BEoIDHIUyPA/WkJZVBzDqQI/AAAAAAAAGZM/C0ZOxb7v230a5RJrhffvaF993eONZMmIwCLcBGAs/s72-c/1.png" height="72" width="72"/><thr:total>4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8166686140676460430.post-2662333968879618796</id><published>2017-09-07T05:38:00.003+09:00</published><updated>2017-12-07T08:29:39.125+09:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Spark"/><category scheme="http://www.blogger.com/atom/ns#" term="SparkML"/><category scheme="http://www.blogger.com/atom/ns#" term="TensorFlow"/><category scheme="http://www.blogger.com/atom/ns#" term="TensorFlowOnSpark"/><title type='text'>BigData와 결합한, 분산 Deep Learning 그 의미와 접근 방법에 대하여</title><content type='html'>&lt;div style=&quot;color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;&lt;span style=&quot;background-color: white;&quot;&gt;딥러닝의 대부이신 제프리 힌튼 교수님은, 머신러닝의 수십년간 암흑기가 극복될 수 있었던 계기로 3대 난재가 풀렸기 때문이라고 언급하신 바 있다. 바로 아래와 같다.&lt;/span&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; padding: 0px;&quot;&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;알고리즘적 혁신 ( Deep Neural Net 이 가능해진, &amp;nbsp;Relu, DropOut, 발견 등등..)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;하드웨어 혁신 ( GPU 를 이용한 컴퓨팅 파워 Scale Up )&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;소프트웨어 혁신 ( BigData Cluster 를 이용한 분산 컴퓨팅 파워 Scale Out )&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;이 중 1은 우리가 너무나 많이 알고 있고, 곁에서 접하고 있고, 심지어 사랑하고 있는 분야이다.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;2는 약간 비싸지만, 회사에서 안사주면, 자비를 털어서라도 집 Desktop에 2개 정도 꼳아주고, 그 날개 돋힌 파워를 충분히 느껴 볼 수 있는 분야이다.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;하지만, 3은 좀 말이 다르다. 힌튼 교수님이 말씀 하셨지만, &amp;nbsp;Lab 에서든 기업에서든, 섣불리(아직까지는) 시도되지 못하는 경향이 있고, 관련된 자료도 많지 않다. 무엇보다도, Popular 한 오픈 Data 를 통해 논문에서 통용되는 수십만 혹은 수백만건의 데이타는 그다지 Big 하지 않기 때문에, 1+2 만 가지고도 꾸엮 꾸엮 실험하고 돌려보고, 논문을 완성해갈 수도 있는 수준일 수 있다. 때문에, 구글, MS 등 몇몇 회사등을 제외하고는&amp;nbsp;수십층 짜리 Very Very Deep 류의 모델을 밑바닥부터 적합한 Neural Networks 구조를 발견하고, 이를 초기값부터 시작하여 새롭게 학습시키는 등의 작업은 일반 소규모 Lab 등에서는 하기가 매우 힘들고, 그러다 보니, 그에 대한 학계의 연구가 보편화 되어 있지는 않는 듯 보인다.&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;그래서인지 3의 접근은 일부 공룡 기업들에 의하여 자체 구축 혹은 오픈소스화 되어 일부 만이 오픈된 상태이다.&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;기업의 Production Deep Learning 프로젝트의 경우, 우선 보유 Data 의 크기가 논문에서 사용되는 Data 의 크기보다 수십배 ~ 수백배인 경우가 많다.(아닌경우도 많지만...) 그리고, 복수의 모델을 함께 쓰거나 좀더 Fine Tuning 을 위해, Online Learning , Transfer Learning &amp;nbsp;보다는 &amp;nbsp;초기부터의 Learning 을 시도하는 경우가 훨씬 많다.(특히, 한글 Deep Learning Text NLP 등은 기 학습되어 있는 Pre training Set 도 존재하지 않는다.)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;&lt;b&gt;&lt;u&gt;오늘은 그런 부분 즉, Deep Learning 을 Big Data Scale Data 를 가지고 수십대 수백대의 GPU 클러스터를 가지고서 병렬 Training 을 하기 위한 BigData Platform&amp;nbsp;+ deep Learning Platform 구성에 대하여 이야기 해보고자 한다.&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div style=&quot;color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;&lt;span style=&quot;background-color: yellow;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=&quot;color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;[1] GPU 만으로 하는 Deep Learning Approach의 한계&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; padding: 0px;&quot;&gt;가장 Popular 한 Deep Learning &amp;nbsp;프레임워크 중 하나인 Tensorflow 는 사실, 이미 멀티 GPU 를 지원하고, 멀티 노드도 지원하며, 아직 미흡하지만, 자체 Serving Layer 도 가지고 있다. 하지만, Tensorflow 가 지원하는 수준의 분산 컴퓨팅, 분산 GPU 는 마치, 분산 데이타 컴퓨팅을 Map/Reduce 로 하는 경우와 유사하게 너무 Low Level 접근을 필요로 하는 경우가 많다. 하나의 Simple 한 Neural Network 모델이 Data Parallel 이 되거나 Compute Parallel 이 되는것 특히, 그것이 High Level 로 저절로 되는 것은 아직 그 어떤 Deep Learning 프레임워크도 완벽하게 지원하고 있지는 않는 영역이다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;Caffe나 Tensorflow , Torch, CNTK 등의 deep learning &amp;nbsp;프레임워크는 그 자체만으로 은총알은 아니다. High Level Scale Up 된 장비 한두대로 논문에서 다루는 크기의 데이타를 처리하는데에는 문제가 되지 않으나, 실무 데이타, 특히 클릭 스트림을 RNN이나 LSTM 분석하는 정도의 시나리오만 되도, 데이타의 크기나 GPU 머신의 Memory 문제로 금방 문제가 드러나기 십상이다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;다중 사용자에게 Deep Learning Serving(Service Request 대응) 을 하는 도중에, Model traing 갱신이 일어나고, 실시간 업데이트가 되면서, 무중지로 Model 배포되는 시나리오는 또 다른 고급 기술을 요구하기도 한다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;&lt;b&gt;&lt;u&gt;GPU 만으로 하는 Single Scale Up, Deep Learning Approach의 한계를 정리해 보자면 아래정도 일 것이다.&amp;nbsp;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; margin-top: 10px; padding: 0px;&quot;&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;Hyper Parameter Tunning 노가다&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;Training&amp;nbsp;속도 한계있음&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;멀티 GPU 코딩의 거시기함.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;GPU의 협소한 메모리로 인한 ResourceExhaustedError.&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;강제된 작은 Batch Size&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;RealTime Inference, 다수 동접자 Serving Layer, 모델의 무중지 Rolling Upgrade&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;BigData Scale Data 들을 접근 하기 위한 Data Pipe lining &amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: #333333; font-family: &amp;quot;arial&amp;quot; , sans-serif;&quot;&gt;&lt;span style=&quot;font-size: 14px;&quot;&gt;모델 태우기 전, Python 레벨 데이타 전처리와 후처리의 한세월...&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;[2] Open Source 진영 BigData Scale Distributed Deep Learning Approach&amp;nbsp;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;하지만, 오픈소스 진영에서는 그러한 것들을 주로 hadoop + Spark (PySpark) 를 통해 해결 시도 하고 있고, 어느정도 Production 레벨까지 활용 가능한 수준의 결과물들이 나오고 있다. 즉, 서두에서 언급했던 3(BigData 혁신)의 시도가 되고 있는 것이다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;BigDL, elaphas , caffeOnSpark &amp;nbsp;등 그런류의 몇가지 오픈소스를 실제 Production Level Deep Learning 배치 시나리오를 이용 Training 해보고 테스트 해보았는데, 아래 소개하는 TensorflowOnSpakr (made by Yahoo) 오픈소스는 그중 가장 가능성이 보이는 Approach 중 하나라 할 수 있을 것이다. (참고로, Yahoo 는 TensorflowOnSpark 을 만들기 전 CaffeOnSpark을 먼저 만들었고 테스트 하였다.)&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;아래는 Inception-v3 모델을 spark 멀티 노드위에서 worker 노드 갯수를 달리해가면 분산 Training 한 속도 비교 이다.&lt;/div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-23yOv-GAePQ/WbBX-WKhqwI/AAAAAAAAEfw/VXAPvWz4ROE7NVP_RIP7vWMLqE8idqRDwCLcBGAs/s1600/scaling.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;265&quot; data-original-width=&quot;475&quot; height=&quot;356&quot; src=&quot;https://1.bp.blogspot.com/-23yOv-GAePQ/WbBX-WKhqwI/AAAAAAAAEfw/VXAPvWz4ROE7NVP_RIP7vWMLqE8idqRDwCLcBGAs/s640/scaling.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;위처럼 1대에서 48시간을 돌려도 정확도가 0.73 % 정도인게, 8대에서는 7시간만에 도달되고 이후로, 85%가 넘는 정확도에 훨씬 빠른 시간에 도달 되는 것을 확인 할 수 있다.&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;&lt;b&gt;&lt;u style=&quot;background-color: yellow;&quot;&gt;[3] TensorflowOnSpark 설치 및 세팅 사용방법&amp;nbsp;&lt;/u&gt;&lt;/b&gt;&lt;/div&gt;&lt;div style=&quot;background-color: white; color: #333333; font-family: Arial, sans-serif; font-size: 14px; margin-top: 10px; padding: 0px;&quot;&gt;아래는 그런 Producion Level 에서 BigData 분산 Traing 과 RealTime Traing 부분에 강한 강점을 가지고 있는 시스템 구성으로 Tensorflow + Spark + Hadoop 구성을 설정 하고 세팅 한 후, 간단하게 사용하는 방법이다. ( Hadoop 과 Spark &amp;nbsp;는 Yarn Cluster 모드로 이미 설정이 기 완료 되어 있다고 가정하고, 그 이후의 세팅 방법만 언급 하였다.)&lt;/div&gt;&lt;ol&gt;&lt;li&gt;Python3 관련 설정 및 설치&lt;/li&gt;&lt;ol&gt;&lt;li&gt;우선 SSL 관련&lt;/li&gt;&lt;ol&gt;&lt;li&gt;yum install openssl openssl-devel -y&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Anaconda 설치&lt;/li&gt;&lt;ol&gt;&lt;li&gt;https://repo.continuum.io/archive/ 위치에서 Linux 버전 최신 설치 파일 Download&lt;/li&gt;&lt;li&gt;각 노드에 모두 복사&lt;/li&gt;&lt;li&gt;chmod 755 Anaconda3-4.4.0-Linux-x86_64.sh&lt;/li&gt;&lt;li&gt;sudo ./Anaconda3-4.4.0-Linux-x86_64.sh&lt;/li&gt;&lt;li&gt;설치 완료 시 화면&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-2AuhOlfOwOY/WaUdsmA7a_I/AAAAAAAAEbQ/WdzJ38nlZw0ES65eLAFedo3xLGdHAUwpgCLcBGAs/s1600/anaconda_install.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;280&quot; data-original-width=&quot;1075&quot; height=&quot;166&quot; src=&quot;https://4.bp.blogspot.com/-2AuhOlfOwOY/WaUdsmA7a_I/AAAAAAAAEbQ/WdzJ38nlZw0ES65eLAFedo3xLGdHAUwpgCLcBGAs/s640/anaconda_install.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;설치 완료 화면&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;bin 경로에 심볼릭 링크 연결 (Source 경로는 각자의 경로를 따를 것!)&lt;/li&gt;&lt;ol&gt;&lt;li&gt;sudo ln -s /home/moneymall/anaconda3/bin/python3 /bin/python3&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;기존 python 을 un link&lt;/li&gt;&lt;ol&gt;&lt;li&gt;sudo unlink /bin/python&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;새로 설치한 python3 로 다시 link&lt;/li&gt;&lt;ol&gt;&lt;li&gt;sudo ln -s /bin/python3 /bin/python&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;pip 도 설정&lt;/li&gt;&lt;ol&gt;&lt;li&gt;sudo ln -s /home/moneymall/anaconda3/bin/pip /bin/pip&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;Anaconda 설치에 관하여.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Tensorflow 등의 기본이 되는 Python Dev 환경을 설치하는 여러가지 방법이 있지만, 내가 선호하는 방법은 우선 anaconda 를 깔아주는 것이다. 공간차지 등등 부수적인 단점이 있긴 하지만, 여러가지 추가적인 기능들이나 필수 모듈들이 동시에 깔려서 편하고, 이후 anaconda 가 제공하는 여러가지 관리 도구들을 이용하여 다양한 장점을 꽤할 수 있다.&lt;/li&gt;&lt;li&gt;Root 로 설치시 실제 사용하는 계정으로 수행하는데 불편함이 많음.&lt;/li&gt;&lt;li&gt;실제 사용하는 계정으로 설치하는 것이 더 편함.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Tensorflow 및 TensroflowOnSpark 설치&lt;/li&gt;&lt;ol&gt;&lt;li&gt;pip install tensorflow&lt;/li&gt;&lt;ol&gt;&lt;li&gt;CPU 모드일때는 그냥 간단히 저렇게 해줘도 됨.&lt;/li&gt;&lt;li&gt;virtual env 등에 설정할 수도 있고, conda create 한 다음 설정할 수도 있으나, spark 와 연동을 위해서는 바깥에서 전역적으로 저렇게 설치하는 것이 좋음.&lt;/li&gt;&lt;li&gt;설치 완료 화면&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-AeXnWrYdsUU/WaUiFV8S0ZI/AAAAAAAAEbc/C2md4nph57U5mxXWhfHiB0OZchqBBQp3QCLcBGAs/s1600/tensorflow_install.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;264&quot; data-original-width=&quot;1075&quot; height=&quot;157&quot; src=&quot;https://3.bp.blogspot.com/-AeXnWrYdsUU/WaUiFV8S0ZI/AAAAAAAAEbc/C2md4nph57U5mxXWhfHiB0OZchqBBQp3QCLcBGAs/s640/tensorflow_install.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;pip install tensorflowonspark&lt;/li&gt;&lt;ol&gt;&lt;li&gt;설치 무지 빨리 끝남.&lt;/li&gt;&lt;li&gt;설치 완료 화면&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-LkJAWFqH-3g/WaUixQlSjBI/AAAAAAAAEbk/CdDUcRV6YDk1yePEUJj8Ri1I4wq5RRU-ACLcBGAs/s1600/tensorflowonspark_install.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;264&quot; data-original-width=&quot;1075&quot; height=&quot;156&quot; src=&quot;https://4.bp.blogspot.com/-LkJAWFqH-3g/WaUixQlSjBI/AAAAAAAAEbk/CdDUcRV6YDk1yePEUJj8Ri1I4wq5RRU-ACLcBGAs/s640/tensorflowonspark_install.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;참고로 위 설치 방법은 Simple 설치 방법을 공유하기 위한 목적의 설치 Guide 이므로, RDMA 를 사용하는 Advanced 설정 방법은 아님.&lt;/li&gt;&lt;ol&gt;&lt;li&gt;RDMA 사용 시 훨씬 성능을 극대화 할 수 있음.&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;li&gt;Spark및 Hadoop 에서 Tensorflow 의 Records 를 직접 읽고 쓰기위 한 Jar 라이브러리 설치&lt;/li&gt;&lt;ol&gt;&lt;li&gt;아래 주소에 Spark 용 해당 Open Source 가 있음.&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-D5nKLQlQgIQ/WaUxDH_JEaI/AAAAAAAAEb0/AYGA6LqvdBMv0jsrQO-FQLqpNTxvfiEyQCLcBGAs/s1600/TFRecordsRW.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;344&quot; data-original-width=&quot;1075&quot; height=&quot;204&quot; src=&quot;https://2.bp.blogspot.com/-D5nKLQlQgIQ/WaUxDH_JEaI/AAAAAAAAEb0/AYGA6LqvdBMv0jsrQO-FQLqpNTxvfiEyQCLcBGAs/s640/TFRecordsRW.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector&quot;&gt;https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector&lt;/a&gt;&lt;/li&gt;&lt;li&gt;git clone 후 spark 디렉토리로 이동.&lt;/li&gt;&lt;li&gt;git clone https://github.com/tensorflow/ecosystem.git&lt;/li&gt;&lt;li&gt;sbt build&lt;/li&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-WdhprPs9hNg/WaU1WK_eKPI/AAAAAAAAEcA/lj8RsjLrT4sNvgmKg9jsXBe_6s1JUO_fQCLcBGAs/s1600/TFRecordsRW_build.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;728&quot; data-original-width=&quot;1075&quot; height=&quot;432&quot; src=&quot;https://1.bp.blogspot.com/-WdhprPs9hNg/WaU1WK_eKPI/AAAAAAAAEcA/lj8RsjLrT4sNvgmKg9jsXBe_6s1JUO_fQCLcBGAs/s640/TFRecordsRW_build.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ol&gt;&lt;li&gt;build.sbt 파일이 존재하는 위치까지 이동.&lt;/li&gt;&lt;li&gt;sbt clean assembly&lt;/li&gt;&lt;li&gt;위 빌드 명령어 입력 하면 아래처럼 sbt 빌드 완료 되고, target 디렉토리 하위에 jar 파일 생성 됨.&lt;/li&gt;&lt;li&gt;Jar 파일 위치&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-XCTcpD2hdM0/WaU1uD1IxII/AAAAAAAA